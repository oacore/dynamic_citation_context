unique_id	citing_id	citation_context	citing_title	cited_title	cited_authors	section_title	intent	cite_context_paragraph	citation_class_label	cite_context_paragraph_auto	cite_context_paragraph_updated	cited_abstract_auto	cited_abstract_manual	cited_abstract	cite_context	cite_context_-1_sent	cite_context_sent_+1	cite_context_-1_sent_+1	cite_context_-2_sent	cite_context_sent_+2	cite_context_-2_sent_+1	cite_context_-1_sent_+2	cite_context_-3_sent	cite_context_sent_+3
CCT1	W06-2933	Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .	a holistic lexiconbased approach to opinion mining	practical annotation scheme for an hpsg treebank of bulgarian in	['K Simov', 'P Osenova']	experiments	Background	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']	0	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian (Simov et al., 2005;Simov and Osenova, 2003), Chinese (Chen et al., 2003), Danish (Kromann, 2003), and Swedish .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian (Simov et al., 2005;Simov and Osenova, 2003), Chinese (Chen et al., 2003), Danish (Kromann, 2003), and Swedish .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']	The paper presents an HPSG-based annotation scheme for constructing a Bulgarian treebank: BulTreeBank. It differs from other grammar-based annotation schemes in having a hybrid status with respect to the partial parsing component and the full parsing module. As the parsing complexity is handled preferably by the pre-processing step, the task of the HPSG module is maximally facilitated and simplified.		The paper presents an HPSG-based annotation scheme for constructing a Bulgarian treebank: BulTreeBank. It differs from other grammar-based annotation schemes in having a hybrid status with respect to the partial parsing component and the full parsing module. As the parsing complexity is handled preferably by the pre-processing step, the task of the HPSG module is maximally facilitated and simplified.	['Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .']	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .']	['Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']	['before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .']	['Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']	['before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']	['If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .']	['Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']
CCT2	N01-1013	This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	generalizing case frames using a thesaurus and the mdl principle	['H Li', 'N Abe']		CompareOrContrast	['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']	1	[]	[]	"A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as ""cuts"" in the thesaurus tree, thus reducing the generalization problem to that of estimating a ""tree cut model"" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods."		"A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as ""cuts"" in the thesaurus tree, thus reducing the generalization problem to that of estimating a ""tree cut model"" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods."	['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']	['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']	['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']	['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']	['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']	['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']	['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']	['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']	['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']	['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']
CCT3	N04-2004	These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .	a holistic lexiconbased approach to opinion mining	on argument structure and the lexical expression of syntactic relations	['Kenneth Hale', 'Samuel Jay Keyser']		Background	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation.']	0	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation.']	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation.']				['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .']	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .']	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.']	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.']	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .']	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation.']	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.']	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation.']	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .']	['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation.']
CCT4	W06-1639	Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	mining newsgroups using networks arising from social behavior	['R Agrawal', 'S Rajagopalan', 'R Srikant', 'Y Xu']	related work	Background	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']	0	[]	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']	"Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. We investigate the feasibility of applying link-based methods in new applications domains. The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. This social behavior gives rise to a network in which the vertices are individuals and the links represent ""responded-to "" relationships. An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. This behavior is in sharp contrast to the WWW link graph, where linkage is an indicator of agreement or common interest. By analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. In contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text"		"Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. We investigate the feasibility of applying link-based methods in new applications domains. The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. This social behavior gives rise to a network in which the vertices are individuals and the links represent ""responded-to "" relationships. An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. This behavior is in sharp contrast to the WWW link graph, where linkage is an indicator of agreement or common interest. By analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. In contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text"	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']
CCT5	J06-2002	While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .	a holistic lexiconbased approach to opinion mining	psychologie der objektbenennung	['Tony Hermann', 'Roland Deutsch']	experiments	Background	"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"	0	"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"	"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"				['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .']	['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .']	"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)""]"	"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)""]"	['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .']	"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.']"	"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)""]"	"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.']"	['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .']	"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.']"
CCT6	W05-0709	Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	building an arabic stemmer for information retrieval	['Aitao Chen', 'Fredic Gey']		Background	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .', 'The plural form and the singular form may also be completely different (e.g. for woman, but for women).', 'The sound plurals are formed by adding plural suffixes to singular nouns (e.g., meaning researcher): the plural suffix is for feminine nouns in grammatical cases (e.g.,']	0	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .', 'The plural form and the singular form may also be completely different (e.g. for woman, but for women).', 'The sound plurals are formed by adding plural suffixes to singular nouns (e.g., meaning researcher): the plural suffix is for feminine nouns in grammatical cases (e.g.,']	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .', 'The plural form and the singular form may also be completely different (e.g. for woman, but for women).', 'The sound plurals are formed by adding plural suffixes to singular nouns (e.g., meaning researcher): the plural suffix is for feminine nouns in grammatical cases (e.g.,']	In TREC 2002 the Berkeley group participated only in the English-Arabic cross-language retrieval (CLIR) track. One Arabic monolingual run and three English-Arabic cross-language runs were submitted. Our approach to the crosslanguage retrieval was to translate the English topics into Arabic using online English-Arabic machine translation systems. The four official runs are named as BKYMON, BKYCL1, BKYCL2, and BKYCL3. The BKYMON is the Arabic monolingual run, and the other three runs are English-to-Arabic cross-language runs. This paper reports on the construction of an Arabic stoplist and two Arabic stemmers, and the experiments on Arabic monolingual retrieval, English-to-Arabic cross-language retrieval.		In TREC 2002 the Berkeley group participated only in the English-Arabic cross-language retrieval (CLIR) track. One Arabic monolingual run and three English-Arabic cross-language runs were submitted. Our approach to the crosslanguage retrieval was to translate the English topics into Arabic using online English-Arabic machine translation systems. The four official runs are named as BKYMON, BKYCL1, BKYCL2, and BKYCL3. The BKYMON is the Arabic monolingual run, and the other three runs are English-to-Arabic cross-language runs. This paper reports on the construction of an Arabic stoplist and two Arabic stemmers, and the experiments on Arabic monolingual retrieval, English-to-Arabic cross-language retrieval.	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .']	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .']	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.']	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.']	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .']	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .']	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.']	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .']	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .']	['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .']
CCT7	D08-1066	Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .	a holistic lexiconbased approach to opinion mining	a discriminative latent variable model for statistical machine translation	['P Blunsom', 'T Cohn', 'M Osborne']	conclusion	Future	['There are various strands of future research.', 'Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', 'We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'Apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.', 'Finally, it would be interesting to study properties of the penalized Deleted Estimation used in this paper.']	3	['There are various strands of future research.', 'Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', 'We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'Apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.', 'Finally, it would be interesting to study properties of the penalized Deleted Estimation used in this paper.']	['There are various strands of future research.', 'Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', 'We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'Apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.', 'Finally, it would be interesting to study properties of the penalized Deleted Estimation used in this paper.']	Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. (c) 2008 Association for Computational Linguistics		Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. (c) 2008 Association for Computational Linguistics	['Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .']	['There are various strands of future research.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .']	['Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .']	['There are various strands of future research.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .']	['Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'There are various strands of future research.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .']	['Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .']	['Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'There are various strands of future research.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .']	['There are various strands of future research.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .']	['Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'There are various strands of future research.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .']	['Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', 'We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'Apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.', 'Finally, it would be interesting to study properties of the penalized Deleted Estimation used in this paper.']
CCT8	J13-1008	Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .	a holistic lexiconbased approach to opinion mining	atanas chanev gulsen eryigit sandra kubler svetoslav marinov and erwin marsi	['Joakim Nivre', 'Johan Hall', 'Jens Nilsson']	related work	Background	['As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']	0	['As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']	['As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']				['Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .']	['As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .']	['Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']	['As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']	['Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .']	['Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']	['Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']	['As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']	['Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .']	['Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']
CCT9	W04-1610	More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .	a holistic lexiconbased approach to opinion mining	learning to classify text using svm	['T Joachims']	related work	Background	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']	0	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']				['More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']	['More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']	['More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']	['More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']
CCT10	W01-1510	The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .	a holistic lexiconbased approach to opinion mining	grammar conversion from fbltag to hpsg	['Naoki Yoshinaga', 'Yusuke Miyao']		Background	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']	0	[]	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']		We propose an algorithm for the conversion of grammars from an arbitrary FB-LTAG grammar into a strongly equivalent HPSG-style grammar. Our algorithm converts LTAG elementary trees into HPSG feature structures by encoding the tree structures in stacks. A set of pre-determined rules manipulate the stack to emulate substitution and adjunction. We have used our algorithm to obtain HPSG-style grammars from existing LTAG grammars. We apply this algorithm to the XTAG English grammar and report some of our findings.	We propose an algorithm for the conversion of grammars from an arbitrary FB-LTAG grammar into a strongly equivalent HPSG-style grammar. Our algorithm converts LTAG elementary trees into HPSG feature structures by encoding the tree structures in stacks. A set of pre-determined rules manipulate the stack to emulate substitution and adjunction. We have used our algorithm to obtain HPSG-style grammars from existing LTAG grammars. We apply this algorithm to the XTAG English grammar and report some of our findings.	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']
CCT11	D11-1138	This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .	a holistic lexiconbased approach to opinion mining	what is the jeopardy model a quasisynchronous grammar for qa	['M Wang', 'N A Smith', 'T Mitamura']	introduction	Background	['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']	0	['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']	['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']	This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions. Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimentalresultsusing theTRECdataset are shown to significantly outperform strong state-of-the-art baselines.		This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions. Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimentalresultsusing theTRECdataset are shown to significantly outperform strong state-of-the-art baselines.	['This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .']	['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .']	['This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).']	['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .']	['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .']	['This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.']	['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).']	['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.']	['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .']	['This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.']
CCT12	J07-1005	Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .	a holistic lexiconbased approach to opinion mining	effective mapping of biomedical text to the umls metathesaurus the metamap program	['Alan R Aronson']		Uses	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).', 'An additional feature we take advantage of (when present) is explicit section markers present in some abstracts.', 'These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical Literature (1987) to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes.', 'These abstracts loosely adhere to the introduction, methods, results, and conclusions format common in scientific writing, and delineate a study using explicitly marked sections with variations of the above headings.', 'Although many core clinical journals require structured abstracts, there is a great deal of variation in the actual headings.', 'Even when present, the headings are not organized in a manner focused on patient care.', 'In addition, abstracts of much high-quality work remain unstructured.', 'For these reasons, explicit section markers are not entirely reliable indicators for the various semantic elements we seek to extract, but must be considered along with other sources of evidence.']	5	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).', 'An additional feature we take advantage of (when present) is explicit section markers present in some abstracts.', 'These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical Literature (1987) to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes.', 'These abstracts loosely adhere to the introduction, methods, results, and conclusions format common in scientific writing, and delineate a study using explicitly marked sections with variations of the above headings.', 'Although many core clinical journals require structured abstracts, there is a great deal of variation in the actual headings.', 'Even when present, the headings are not organized in a manner focused on patient care.', 'In addition, abstracts of much high-quality work remain unstructured.', 'For these reasons, explicit section markers are not entirely reliable indicators for the various semantic elements we seek to extract, but must be considered along with other sources of evidence.']	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).', 'An additional feature we take advantage of (when present) is explicit section markers present in some abstracts.', 'These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical Literature (1987) to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes.', 'These abstracts loosely adhere to the introduction, methods, results, and conclusions format common in scientific writing, and delineate a study using explicitly marked sections with variations of the above headings.', 'Although many core clinical journals require structured abstracts, there is a great deal of variation in the actual headings.', 'Even when present, the headings are not organized in a manner focused on patient care.', 'In addition, abstracts of much high-quality work remain unstructured.', 'For these reasons, explicit section markers are not entirely reliable indicators for the various semantic elements we seek to extract, but must be considered along with other sources of evidence.']	The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library		The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .']	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .']	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).']	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).']	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .']	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).', 'An additional feature we take advantage of (when present) is explicit section markers present in some abstracts.']	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).']	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).', 'An additional feature we take advantage of (when present) is explicit section markers present in some abstracts.']	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .']	['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).', 'An additional feature we take advantage of (when present) is explicit section markers present in some abstracts.', 'These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical Literature (1987) to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes.']
CCT13	D08-1007	We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .	a holistic lexiconbased approach to opinion mining	using the web to obtain frequencies for unseen bigrams	['Frank Keller', 'Mirella Lapata']	experiments	CompareOrContrast	"['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", 'pronoun is the direct object of a verb predicate, v.', ""A pronoun's antecedent must obey v's selectional preferences."", 'If we have a better model of SP, we should be able to better select pronoun antecedents.']"	1	"['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", 'pronoun is the direct object of a verb predicate, v.', ""A pronoun's antecedent must obey v's selectional preferences."", 'If we have a better model of SP, we should be able to better select pronoun antecedents.']"	"['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", 'pronoun is the direct object of a verb predicate, v.', ""A pronoun's antecedent must obey v's selectional preferences."", 'If we have a better model of SP, we should be able to better select pronoun antecedents.']"	This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.		This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.	"[""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .""]"	"['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .""]"	"[""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", 'pronoun is the direct object of a verb predicate, v.']"	"['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .""]"	"['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .""]"	"[""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", 'pronoun is the direct object of a verb predicate, v.', ""A pronoun's antecedent must obey v's selectional preferences.""]"	"['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", 'pronoun is the direct object of a verb predicate, v.']"	"['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", 'pronoun is the direct object of a verb predicate, v.', ""A pronoun's antecedent must obey v's selectional preferences.""]"	"['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .""]"	"[""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", 'pronoun is the direct object of a verb predicate, v.', ""A pronoun's antecedent must obey v's selectional preferences."", 'If we have a better model of SP, we should be able to better select pronoun antecedents.']"
CCT14	Q13-1020	Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .	a holistic lexiconbased approach to opinion mining	a bayesian model of syntaxdirected tree to string grammar induction	['Trevor Cohn', 'Phil Blunsom']	method	Motivation	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	4	[]	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.		Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.	['Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']
CCT15	Q13-1020	Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .	a holistic lexiconbased approach to opinion mining	a gibbs sampler for phrasal synchronous grammar induction	['Phil Blunsom', 'Trevor Cohn', 'Chris Dyer', 'Miles Osborne']	method	Motivation	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	4	[]	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.		We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.	['Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']	['Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']
CCT16	W03-0806	The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .	a holistic lexiconbased approach to opinion mining	tnt  a statistical partofspeech tagger	['Thorsten Brants']	experiments	Background	['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']	0	['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']	['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']		Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora.	Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora.	['The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']	['Efficiency has not been a focus for NLP research in general.', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']	['The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']	['Efficiency has not been a focus for NLP research in general.', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']	['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'Efficiency has not been a focus for NLP research in general.', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']	['The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']	['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'Efficiency has not been a focus for NLP research in general.', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']	['Efficiency has not been a focus for NLP research in general.', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']	['An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'Efficiency has not been a focus for NLP research in general.', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']	['The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']
CCT17	P11-1134	Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .	a holistic lexiconbased approach to opinion mining	extracting paraphrase patterns from bilingual parallel corpora	['Shiqi Zhao', 'Haifeng Wang', 'Ting Liu', 'Sheng Li']	introduction	Background	['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	0	['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	Paraphrase patterns are semantically equivalent patterns, which are useful in both paraphrase recognition and generation. This paper presents a pivot approach for extracting paraphrase patterns from bilingual parallel corpora, whereby the paraphrase patterns in English are extracted using the patterns in another language as pivots. We make use of log-linear models for computing the paraphrase likelihood between pattern pairs and exploit feature functions based on maximum likelihood estimation (MLE), lexical weighting (LW), and monolingual word alignment (MWA). Using the presented method, we extract more than 1 million pairs of paraphrase patterns from about 2 million pairs of bilingual parallel sentences. The precision of the extracted paraphrase patterns is above 78%. Experimental results show that the presented method significantly outperforms a well-known method called discovery of inference rules from text (DIRT). Additionally, the log-linear model with the proposed feature functions are effective. The extracted paraphrase patterns are fully analyzed. Especially, we found that the extracted paraphrase patterns can be classified into five types, which are useful in multiple natural language processing (NLP) applications.		Paraphrase patterns are semantically equivalent patterns, which are useful in both paraphrase recognition and generation. This paper presents a pivot approach for extracting paraphrase patterns from bilingual parallel corpora, whereby the paraphrase patterns in English are extracted using the patterns in another language as pivots. We make use of log-linear models for computing the paraphrase likelihood between pattern pairs and exploit feature functions based on maximum likelihood estimation (MLE), lexical weighting (LW), and monolingual word alignment (MWA). Using the presented method, we extract more than 1 million pairs of paraphrase patterns from about 2 million pairs of bilingual parallel sentences. The precision of the extracted paraphrase patterns is above 78%. Experimental results show that the presented method significantly outperforms a well-known method called discovery of inference rules from text (DIRT). Additionally, the log-linear model with the proposed feature functions are effective. The extracted paraphrase patterns are fully analyzed. Especially, we found that the extracted paraphrase patterns can be classified into five types, which are useful in multiple natural language processing (NLP) applications.	['Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	['Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	['As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	['Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	['As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	['As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']	['Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']
CCT18	J13-1008	Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .	a holistic lexiconbased approach to opinion mining	morphology and reranking for the statistical parsing of spanish	['Brooke Cowan', 'Michael Collins']	related work	Background	['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Collins et al. (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size 3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'The results for Czech are the opposite of our results for Arabic, as we will see.', 'This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic and Vidov-Hladk 1998) compared with Arabic (14.0%, see Table 3).', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We also find that the number feature helps for Arabic.', 'Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Simaan (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.']	0	[]	['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Collins et al. (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size 3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'The results for Czech are the opposite of our results for Arabic, as we will see.', 'This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic and Vidov-Hladk 1998) compared with Arabic (14.0%, see Table 3).', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We also find that the number feature helps for Arabic.', 'Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Simaan (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.']	We present two methods for incorporating detailed features in a Spanish parser, building on a baseline model that is a lexicalized PCFG. The first method exploits Spanish morphology, and achieves an F1 constituency score of 83.6%. This is an improvement over 81.2 % accuracy for the baseline, which makes little or no use of morphological information. The second model uses a reranking approach to add arbitrary global features of parse trees to the morphological model. The reranking model reaches 85.1 % F1 accuracy on the Spanish parsing task. The resulting model for Spanish parsing combines an approach that specifically targets morphological information with an approach that makes use of general structural features		We present two methods for incorporating detailed features in a Spanish parser, building on a baseline model that is a lexicalized PCFG. The first method exploits Spanish morphology, and achieves an F1 constituency score of 83.6%. This is an improvement over 81.2 % accuracy for the baseline, which makes little or no use of morphological information. The second model uses a reranking approach to add arbitrary global features of parse trees to the morphological model. The reranking model reaches 85.1 % F1 accuracy on the Spanish parsing task. The resulting model for Spanish parsing combines an approach that specifically targets morphological information with an approach that makes use of general structural features	['Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .']	['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .']	['Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .']	['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .']	['Collins et al. (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).', 'Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .']	['Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .']	['Collins et al. (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).', 'Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .']	['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .']	['This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size 3000+).', 'Collins et al. (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).', 'Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .']	['Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We also find that the number feature helps for Arabic.', 'Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Simaan (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.']
CCT19	D13-1115	To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .	a holistic lexiconbased approach to opinion mining	online learning for latent dirichlet allocation	['Matthew Hoffman', 'David M Blei', 'Francis Bach']	method	Uses	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.', 'Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.', 'Online VBI easily scales and quickly converges in all of our experiments.', 'A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source.']	5	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.', 'Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.', 'Online VBI easily scales and quickly converges in all of our experiments.', 'A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source.']	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.', 'Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.', 'Online VBI easily scales and quickly converges in all of our experiments.', 'A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source.']	We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.		We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .']	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .']	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.']	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.']	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .']	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.']	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.']	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.']	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .']	['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.', 'Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.']
CCT20	W00-1312	Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .	a holistic lexiconbased approach to opinion mining	on relevance probabilistic indexing and information retrievalquot	['M E Maron', 'K L Kuhns']	related work	CompareOrContrast	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']	1	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']		This paper reports on a novel technique for literature indexing and searching in a mechanized library system. The notion of relevance is taken as the key concept in the theory of information retrieval and a comparative concept of relevance is explicated in terms of the theory of probability. The resulting technique called Probabilistic Indexing, allows a computing machine, given a request for information, to make a statistical inference and derive a number (called the relevance number) for each document, which is a measure of the probability that the document will satisfy the given request. The result of a search is an ordered list of those documents which satisfy the request ranked according to their probable relevance. The paper goes on to show that whereas in a conventional library system the cross-referencing (see and see also) is based solely on the semantical closeness between index terms, statistical measures of closeness between index terms can be defined and computed. Thus, given an arbitrary request consisting of one (or many) index term(s), a machine can elaborate on it to increase the probability of selecting relevant documents that would not otherwise have been selected. Finally, the paper suggests an interpretation of the whole library problem as one where the request is considered as a clue on the basis of which the library system makes a concatenated statistical inference in order to provide as an output an ordered list of those documents which most probably satisfy the information needs of the user.	This paper reports on a novel technique for literature indexing and searching in a mechanized library system. The notion of relevance is taken as the key concept in the theory of information retrieval and a comparative concept of relevance is explicated in terms of the theory of probability. The resulting technique called Probabilistic Indexing, allows a computing machine, given a request for information, to make a statistical inference and derive a number (called the relevance number) for each document, which is a measure of the probability that the document will satisfy the given request. The result of a search is an ordered list of those documents which satisfy the request ranked according to their probable relevance. The paper goes on to show that whereas in a conventional library system the cross-referencing (see and see also) is based solely on the semantical closeness between index terms, statistical measures of closeness between index terms can be defined and computed. Thus, given an arbitrary request consisting of one (or many) index term(s), a machine can elaborate on it to increase the probability of selecting relevant documents that would not otherwise have been selected. Finally, the paper suggests an interpretation of the whole library problem as one where the request is considered as a clue on the basis of which the library system makes a concatenated statistical inference in order to provide as an output an ordered list of those documents which most probably satisfy the information needs of the user.	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .']	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .']	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .']	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .']	['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']
CCT21	P07-1068	As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .	a holistic lexiconbased approach to opinion mining	exploiting semantic role labeling wordnet and wikipedia for coreference resolution	['S P Ponzetto', 'M Strube']	introduction	Background	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	0	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.		In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.	['As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']
CCT22	W01-1510	There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).	a holistic lexiconbased approach to opinion mining	twostep tag parsing revisited	['Peter Poller', 'Tilman Becker']	introduction	Background	['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']	0	[]	['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']	Based on the work in (Poller, 1994) and a minor assumption about a normal form for TAGs, we present a highly simplified version of the twostep parsing approach for TAGs which allows for a much easier analysis of run-time and space complexity. It also snggests how restrictions on the grammars might result in improvements in run-time complexity. The main advantage of a two-step parsing system shows in practical applications like Verbmobil (Bub et al., 1997) where the parser must look at multiple hypotheses supplied by a speech recognizer (encoded in a word hypotheses lattice) and filter out illicit hypotheses as early as possible. The first (context-free) step of our parser filters out some illicit hypotheses fast (O(n3 )); the constructed parsing matrix is then reused for the second step, the complete (O(n6 )) TAG parse.		Based on the work in (Poller, 1994) and a minor assumption about a normal form for TAGs, we present a highly simplified version of the twostep parsing approach for TAGs which allows for a much easier analysis of run-time and space complexity. It also snggests how restrictions on the grammars might result in improvements in run-time complexity. The main advantage of a two-step parsing system shows in practical applications like Verbmobil (Bub et al., 1997) where the parser must look at multiple hypotheses supplied by a speech recognizer (encoded in a word hypotheses lattice) and filter out illicit hypotheses as early as possible. The first (context-free) step of our parser filters out some illicit hypotheses fast (O(n3 )); the constructed parsing matrix is then reused for the second step, the complete (O(n6 )) TAG parse.	['There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).']	['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).']	['There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.']	['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.']	['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'Our concern is, however, not limited to the sharing of grammars and lexicons.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).']	['There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']	['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'Our concern is, however, not limited to the sharing of grammars and lexicons.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.']	['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']	['Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'Our concern is, however, not limited to the sharing of grammars and lexicons.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).']	['There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']
CCT23	W03-0806	An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	a comparison of algorithms for maximum entropy parameter estimation	['Robert Malouf']	experiments	Background	['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']	0	['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']	['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']	Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Sur-prisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices.		Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Sur-prisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices.	['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .']	['Efficiency has not been a focus for NLP research in general.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .']	['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).']	['Efficiency has not been a focus for NLP research in general.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).']	['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'Efficiency has not been a focus for NLP research in general.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .']	['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']	['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'Efficiency has not been a focus for NLP research in general.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).']	['Efficiency has not been a focus for NLP research in general.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']	['However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'Efficiency has not been a focus for NLP research in general.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .']	['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']
CCT24	W02-1601	For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .	a holistic lexiconbased approach to opinion mining	examplebased machine translation	['S Sato']		Background	['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	0	[]	['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	Example-based machine translation (EBMT) systems, so far, rely on heuristic measures in re-trieving translation examples. Such a heuristic measure costs time to adjust, and might make its algorithm unclear. This paper presents a probabilistic model for EBMT. Under the pro-posed model, the system searches the transla-tion example combination which has the high-est probability. The proposed model clearly for-malizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental re-sults demonstrate that the proposed model has a slightly better translation quality than state-of-the-art EBMT systems.		Example-based machine translation (EBMT) systems, so far, rely on heuristic measures in re-trieving translation examples. Such a heuristic measure costs time to adjust, and might make its algorithm unclear. This paper presents a probabilistic model for EBMT. Under the pro-posed model, the system searches the transla-tion example combination which has the high-est probability. The proposed model clearly for-malizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental re-sults demonstrate that the proposed model has a slightly better translation quality than state-of-the-art EBMT systems.	['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']	['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']
CCT25	N10-1084	Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .	a holistic lexiconbased approach to opinion mining	natural language processing for information assurance and security an overview and implementations	['Mikhail J Atallah', 'Craig J McDonough', 'Victor Raskin', 'Sergei Nirenburg']	related work	Background	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n- gram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	0	[]	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n- gram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	This research paper explores a promising interface between natural language processing (NLP) and information assurance and security (IAS). More specificall~ it is devoted to possible applications to, and further dedicated development of, the accumulated considerable resources in NLP for, IAS. The expected and partially accomplished result is in harnessing the weird, illogical ways natural languages encode meaning, the very ways that defy all the usual combinatorial approaches to mathematical--and computational--complexity and make NLP so hard, to enhance information security. The paper is of a mixed theoretical and empirical nature. Of the four possible venues of applications, (i) memorizing randomly generated passwords with the help of automatically generated funny jingles, (ii) natural language watermarking, (iii) using the available machine translation (MT) systems for (additional) encryption of text messages, and (iv) downgrading, or sanitizing classified information in networks, two venues, (i) and (iv), have been at least partially implemented and the remaining two (ii) and (iii) are being implemented to the proof-of-concept level. We must make it very clear, however, that we have done very little experimentation or evaluation at this point, though we are moving quickly in that direction. The merits of the paper, if any, are in its venture to make considerable progress achieved recently in NLE especially in knowledge representation and meaning analysis, useful for IAS needs. The NLP approach adopted here, ontological semantics, has been developed by two of the coauthors; watermarking is based on the pioneering research by another coauthor and his associates; most of the implementation of the password memorization software has been done by the fourth coauthor. All the four of us have agonized whether we should report this research now or wait till we have fully implemented all or at least some of the systems we are developing. At the end of the day, we have reached a consensus that it is important, even at this early stage, to review for the information security community what NLP can do for it and to invite feedback and further efforts and ideas on what seems likely to become a new paradigm in information security. To the body of the paper, we Mikhail J. Atallah, Craig J. McDonough, Victor Raskin Center for Education and Research in Information Assurance and Security (CERIAS, www.cerias.purdue.edu) Purdue University W. Lafayette, IN 47907 mja, raskin, mcdonoug@cerias.purdue.edu Sergei Nirenburg Computing Research Laboratory, New Mexico State University Las Cruces, NM 88003 sergei@crl.nmsu.edu have added two self-contained deliberately reference-free appendices on NLP and ontological semantics, respectively, primarily for the benefit of those IAS readers, who are interested in expanding their understanding of those fields and further exploring their possible fruitful interactions with IAS.		This research paper explores a promising interface between natural language processing (NLP) and information assurance and security (IAS). More specificall~ it is devoted to possible applications to, and further dedicated development of, the accumulated considerable resources in NLP for, IAS. The expected and partially accomplished result is in harnessing the weird, illogical ways natural languages encode meaning, the very ways that defy all the usual combinatorial approaches to mathematical--and computational--complexity and make NLP so hard, to enhance information security. The paper is of a mixed theoretical and empirical nature. Of the four possible venues of applications, (i) memorizing randomly generated passwords with the help of automatically generated funny jingles, (ii) natural language watermarking, (iii) using the available machine translation (MT) systems for (additional) encryption of text messages, and (iv) downgrading, or sanitizing classified information in networks, two venues, (i) and (iv), have been at least partially implemented and the remaining two (ii) and (iii) are being implemented to the proof-of-concept level. We must make it very clear, however, that we have done very little experimentation or evaluation at this point, though we are moving quickly in that direction. The merits of the paper, if any, are in its venture to make considerable progress achieved recently in NLE especially in knowledge representation and meaning analysis, useful for IAS needs. The NLP approach adopted here, ontological semantics, has been developed by two of the coauthors; watermarking is based on the pioneering research by another coauthor and his associates; most of the implementation of the password memorization software has been done by the fourth coauthor. All the four of us have agonized whether we should report this research now or wait till we have fully implemented all or at least some of the systems we are developing. At the end of the day, we have reached a consensus that it is important, even at this early stage, to review for the information security community what NLP can do for it and to invite feedback and further efforts and ideas on what seems likely to become a new paradigm in information security. To the body of the paper, we Mikhail J. Atallah, Craig J. McDonough, Victor Raskin Center for Education and Research in Information Assurance and Security (CERIAS, www.cerias.purdue.edu) Purdue University W. Lafayette, IN 47907 mja, raskin, mcdonoug@cerias.purdue.edu Sergei Nirenburg Computing Research Laboratory, New Mexico State University Las Cruces, NM 88003 sergei@crl.nmsu.edu have added two self-contained deliberately reference-free appendices on NLP and ontological semantics, respectively, primarily for the benefit of those IAS readers, who are interested in expanding their understanding of those fields and further exploring their possible fruitful interactions with IAS.	['Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n- gram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.']	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n- gram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.']	['The first lexical substitution method was proposed by Chapman and Davida (1997).', 'The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n- gram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	['The first lexical substitution method was proposed by Chapman and Davida (1997).', 'The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n- gram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.']	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n- gram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	['The first lexical substitution method was proposed by Chapman and Davida (1997).', 'The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n- gram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']
CCT26	J10-3007	results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En  Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	europarl a parallel corpus for statistical machine translation	['Philipp Koehn']		Uses	['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We consider a parse tree on the source language as a set of dependency edges to be transferred.', 'For each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'These edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (EnBg) and from English to Spanish (EnEs).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']	5	['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We consider a parse tree on the source language as a set of dependency edges to be transferred.', 'For each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'These edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (EnBg) and from English to Spanish (EnEs).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']	['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We consider a parse tree on the source language as a set of dependency edges to be transferred.', 'For each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'These edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (EnBg) and from English to Spanish (EnEs).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']	We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.		We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.	['results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .']	['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .']	['results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']	['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']	['We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .']	['results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']	['We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']	['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']	['This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .']	['results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']
CCT27	W06-3813	We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .	a holistic lexiconbased approach to opinion mining	junior science book of	['Nancy Larrick']	experiments	Uses	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text.']	5	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text.']	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text.']	This study aimed to describe the validity of SETS-oriented science book for junior high school students in environmental pollutions materials. In this research, researcher uses a quantitative descriptive method and uses validation sheet in the form of questionnaire as data-collection techniques. The questionnaire contains the criteria of material feasibility, language, and presentation. The result of validation gave the material feasibility score 3.9, for the language is 3.6, and the presentation criteria scored 3.9. So, it can be concluded that a science book with SETS-oriented is very well to be used as a learning facility according to the assessment of the validator		This study aimed to describe the validity of SETS-oriented science book for junior high school students in environmental pollutions materials. In this research, researcher uses a quantitative descriptive method and uses validation sheet in the form of questionnaire as data-collection techniques. The questionnaire contains the criteria of material feasibility, language, and presentation. The result of validation gave the material feasibility score 3.9, for the language is 3.6, and the presentation criteria scored 3.9. So, it can be concluded that a science book with SETS-oriented is very well to be used as a learning facility according to the assessment of the validator	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .']	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .']	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.']	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.']	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .']	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.']	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.']	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.']	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .']	['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text.']
CCT28	J91-2003	This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .	a holistic lexiconbased approach to opinion mining	a theory of truth and semantic representationquot	['H Kamp']	introduction	CompareOrContrast	['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']	1	['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']	['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']				['This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .']	['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .']	['This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.']	['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.']	['For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'All logical notions that we are going to consider, such as theory or model, will be finitary.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .']	['This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']	['For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'All logical notions that we are going to consider, such as theory or model, will be finitary.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.']	['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']	['Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'All logical notions that we are going to consider, such as theory or model, will be finitary.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .']	['This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']
CCT29	D08-1034	The candidate feature templates include : Voice from #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	shallow semantic parsing of chinese	['Honglin Sun', 'Daniel Jurafsky']		Uses	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	5	[]	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	In this paper we address the question of assigning semantic roles to sentences in Chinese. We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set. In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix. Finally, we compare English and Chinese semantic-parsing performance. While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese. We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese.		In this paper we address the question of assigning semantic roles to sentences in Chinese. We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set. In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix. Finally, we compare English and Chinese semantic-parsing performance. While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese. We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese.	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	['The candidate feature templates include : Voice from #AUTHOR_TAG .']
CCT30	P97-1063	Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .	a holistic lexiconbased approach to opinion mining	the mathematics of statistical machine translation parameter estimationquot	['P F Brown', 'V J Della Pietra', 'S A Della Pietra', 'R L Mercer']	introduction	Background	"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"	0	"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"	"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"		We describe a series o,f five statistical models o,f the translation process and give algorithms,for estimating the parameters o,f these models given a set o,f pairs o,f sentences that are translations o,f one another. We define a concept o,f word-by-word alignment between such pairs o,f sentences. For any given pair of such sentences each o,f our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable o,f these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair o,f sentences. We have a great deal o,f data in French and English from the proceedings o,f the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we,feel that because our algorithms have minimal linguistic content they would work well on other pairs o,f languages. We also ,feel, again because of the minimal linguistic content o,f our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus. 	We describe a series o,f five statistical models o,f the translation process and give algorithms,for estimating the parameters o,f these models given a set o,f pairs o,f sentences that are translations o,f one another. We define a concept o,f word-by-word alignment between such pairs o,f sentences. For any given pair of such sentences each o,f our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable o,f these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair o,f sentences. We have a great deal o,f data in French and English from the proceedings o,f the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we,feel that because our algorithms have minimal linguistic content they would work well on other pairs o,f languages. We also ,feel, again because of the minimal linguistic content o,f our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus. 	['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .']	['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .']	['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.']	['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.']	['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .']	"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.']"	['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.']	"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.']"	['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .']	"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CCT31	J07-1005	The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .	a holistic lexiconbased approach to opinion mining	the wellbuilt clinical question a key to evidencebased decisions	['W Scott Richardson', 'Mark C Wilson', 'James Nishikawa', 'Robert S Hayward']	introduction	Background	['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts.', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']	0	['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts.', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']	['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts.', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']				['The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .']	['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .']	['The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']	['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']	['This domain is well-suited for exploring the posed research questions for several reasons.', 'We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .']	['The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']	['This domain is well-suited for exploring the posed research questions for several reasons.', 'We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']	['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']	['First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .']	['The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']
CCT32	D08-1034	The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	the penn chinese treebank phrase structure annotation of a large corpus	['Nianwen Xue', 'Fei Xia', 'Fu dong Chiou', 'Martha Palmer']		Uses	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure 1 is an example from the PropBank 1 .', 'We put the word-by-word translation and the translation of the whole sentence below the example.', 'It is quite a complex sentence, as there are many semantic roles in it.', 'In this sentence, all the semantic roles of the verb  (provide) are presented in the syntactic tree.', 'We can separate the semantic roles into two groups.']	5	[]	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure 1 is an example from the PropBank 1 .', 'We put the word-by-word translation and the translation of the whole sentence below the example.', 'It is quite a complex sentence, as there are many semantic roles in it.', 'In this sentence, all the semantic roles of the verb  (provide) are presented in the syntactic tree.', 'We can separate the semantic roles into two groups.']	With growing interest in Chinese Language Processing, numerous NLP tools (e.g., word segmenters, part-of-speech taggers, and parsers) for Chinese have been developed all over the world. However, since no large-scale bracketed corpora are available to the public, these tools are trained on corpora with different segmentation criteria, part-of-speech tagsets and bracketing guidelines, and therefore, comparisons are difficult. As a first step towards addressing this issue, we have been preparing a large bracketed corpus since late 1998. The first two installments of the corpus, 250 thousand words of data, fully segmented, POS-tagged and syntactically bracketed, have been released to the public via LDC (www.ldc.upenn.edu). In this paper, we discuss several Chinese linguistic issues and their implications for our treebanking efforts and how we address these issues when developing our annotation guidelines. We also describe our engineering strategies to improve speed while ensuring annotation quality.		With growing interest in Chinese Language Processing, numerous NLP tools (e.g., word segmenters, part-of-speech taggers, and parsers) for Chinese have been developed all over the world. However, since no large-scale bracketed corpora are available to the public, these tools are trained on corpora with different segmentation criteria, part-of-speech tagsets and bracketing guidelines, and therefore, comparisons are difficult. As a first step towards addressing this issue, we have been preparing a large bracketed corpus since late 1998. The first two installments of the corpus, 250 thousand words of data, fully segmented, POS-tagged and syntactically bracketed, have been released to the public via LDC (www.ldc.upenn.edu). In this paper, we discuss several Chinese linguistic issues and their implications for our treebanking efforts and how we address these issues when developing our annotation guidelines. We also describe our engineering strategies to improve speed while ensuring annotation quality.	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .']	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .']	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'It is constituted of two parts.']	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'It is constituted of two parts.']	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .']	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.']	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'It is constituted of two parts.']	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.']	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .']	['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.']
CCT33	W06-1104	Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .	a holistic lexiconbased approach to opinion mining	computing semantic relatedness across parts of speech	['Iryna Gurevych']	related work	Background	['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e.', 'other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity.']	0	['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e.', 'other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity.']	['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e.', 'other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity.']				['Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']	['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']	['Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']	['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']	['Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']	['Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']	['Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']	['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']	['Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms.', 'Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .']	['Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e.', 'other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity.']
CCT34	W00-1017	Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	constraint projection an efficient treatment of disjunctive feature descriptions	['Mikio Nakano']		Uses	"['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'Considering the meeting room reservation system, examples of domain-related sentences are ""I need to book Room 2 on Wednesday"", ""I need to book Room 2"", and ""Room 2"" and dialogue-related ones are ""yes"", ""no"", and ""Okay"".']"	5	"['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'Considering the meeting room reservation system, examples of domain-related sentences are ""I need to book Room 2 on Wednesday"", ""I need to book Room 2"", and ""Room 2"" and dialogue-related ones are ""yes"", ""no"", and ""Okay"".']"	"['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'Considering the meeting room reservation system, examples of domain-related sentences are ""I need to book Room 2 on Wednesday"", ""I need to book Room 2"", and ""Room 2"" and dialogue-related ones are ""yes"", ""no"", and ""Okay"".']"	Unification of disjunctive feature descriptions is important for efficient unification-based parsing. This paper presents constraint projection, a new method for unification of disjunctive feature structures represented by logical constraints. Constraint projection is a generalization of constraint unification, and is more efficient because constraint projection has a mechanism for abandoning information irrelevant to a goal specified by a list of variables.		Unification of disjunctive feature descriptions is important for efficient unification-based parsing. This paper presents constraint projection, a new method for unification of disjunctive feature structures represented by logical constraints. Constraint projection is a generalization of constraint unification, and is more efficient because constraint projection has a mechanism for abandoning information irrelevant to a goal specified by a list of variables.	['Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .']	['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .']	['Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.']	['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .']	['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .']	['Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.']	['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.']	['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.']	['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .']	"['Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial.""]"
CCT35	K15-1003	Our strategy is based on the approach presented by #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	bayesian inference for pcfgs via markov chain monte carlo	['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']		Uses	['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.', 'By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities.']	5	[]	['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.', 'By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities.']	This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.		This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.	['Our strategy is based on the approach presented by #AUTHOR_TAG .']	['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .']	['Our strategy is based on the approach presented by #AUTHOR_TAG .']	['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .']	['Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .']	['Our strategy is based on the approach presented by #AUTHOR_TAG .']	['Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .']	['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .']	['Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .']	['Our strategy is based on the approach presented by #AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.', 'By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities.']
CCT36	W04-1805	Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :	a holistic lexiconbased approach to opinion mining	a comparative evaluation of collocation extraction techniques	['Darren Pearce']	method	Background	['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']	0	[]	['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']	This paper describes an experiment that attempts to compare a range of existing collocation extraction techniques as well as the implementation of a new technique based on tests for lexical substitutability. After a description of the experiment details, the techniques are discussed with particular emphasis on any adaptations that are required in order to evaluate it in the way proposed. This is followed by a discussion on the relative strengths and weaknesses of the techniques with reference to the results obtained. Since there is no general agreement on the exact nature of collocation, evaluating techniques with reference to any single standard is somewhat controversial. Departing from this point, part of the concluding discussion includes initial proposals for a common framework for evaluation of collocation extraction techniques.		This paper describes an experiment that attempts to compare a range of existing collocation extraction techniques as well as the implementation of a new technique based on tests for lexical substitutability. After a description of the experiment details, the techniques are discussed with particular emphasis on any adaptations that are required in order to evaluate it in the way proposed. This is followed by a discussion on the relative strengths and weaknesses of the techniques with reference to the results obtained. Since there is no general agreement on the exact nature of collocation, evaluating techniques with reference to any single standard is somewhat controversial. Departing from this point, part of the concluding discussion includes initial proposals for a common framework for evaluation of collocation extraction techniques.	['Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']	['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']	['Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']	['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']	['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']	['Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']	['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']	['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']	['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']	['Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']
CCT37	J06-2002	A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .	a holistic lexiconbased approach to opinion mining	basic color terms	['Brent Berlin', 'Paul Kay']	experiments	Background	['Color terms are a case apart.', 'If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue).', 'This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3).', '(The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green.)', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .', '(Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.', 'Red as in red hair, e.g., differs from red as in red chair.)', 'Different attitudes towards multidimensionality are possible.', 'One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense.', 'In this case, the program should limit the use of vague descriptions to situations where there exists a referent that has a Pareto-optimal combination of Values.', 'Alternatively, one could allow referring expressions to be ambiguous.', 'It would be consistent with this attitude, for example, to map multiple dimensions into one overall dimension, perhaps by borrowing from principles applied in perceptual grouping, where different perceptual dimensions are mapped into one (e.g., Thorisson 1994).', 'The empirical basis of this line of work, however, is still somewhat weak, so the risk of referential unclarity looms large.', 'Also, this attitude would go against the spirit of GRE, where referring expressions have always been assumed to be distinguishing.']	0	[]	['Color terms are a case apart.', 'If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue).', 'This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3).', '(The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green.)', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .', '(Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.', 'Red as in red hair, e.g., differs from red as in red chair.)', 'Different attitudes towards multidimensionality are possible.', 'One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense.', 'In this case, the program should limit the use of vague descriptions to situations where there exists a referent that has a Pareto-optimal combination of Values.', 'Alternatively, one could allow referring expressions to be ambiguous.', 'It would be consistent with this attitude, for example, to map multiple dimensions into one overall dimension, perhaps by borrowing from principles applied in perceptual grouping, where different perceptual dimensions are mapped into one (e.g., Thorisson 1994).', 'The empirical basis of this line of work, however, is still somewhat weak, so the risk of referential unclarity looms large.', 'Also, this attitude would go against the spirit of GRE, where referring expressions have always been assumed to be distinguishing.']				['A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .']	['Color terms are a case apart.', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .']	['A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .']	['Color terms are a case apart.', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .']	['If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue).', 'Color terms are a case apart.', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .']	['A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .']	['If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue).', 'Color terms are a case apart.', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .']	['Color terms are a case apart.', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .']	['This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3).', 'If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue).', 'Color terms are a case apart.', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .']	['A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .', '(Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.', 'Red as in red hair, e.g., differs from red as in red chair.)', 'Different attitudes towards multidimensionality are possible.']
CCT38	P02-1001	The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and	a holistic lexiconbased approach to opinion mining	inducing features of random fields	['S Della Pietra', 'V Della Pietra', 'J Lafferty']		Uses	[' If arc probabilities (or even , , , ) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f  whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f ( * ,  * ).', 'If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13']	5	[' If arc probabilities (or even , , , ) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f  whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f ( * ,  * ).', 'If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13']	[' If arc probabilities (or even , , , ) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f  whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f ( * ,  * ).', 'If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13']	We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.		We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.	['The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and']	['* If arc probabilities (or even l, n, u, r) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f th whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and']	['The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (S * , [?] * ).']	['* If arc probabilities (or even l, n, u, r) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f th whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (S * , [?] * ).']	['* If arc probabilities (or even l, n, u, r) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f th whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and']	['The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (S * , [?] * ).', 'If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13']	['* If arc probabilities (or even l, n, u, r) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f th whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (S * , [?] * ).']	['* If arc probabilities (or even l, n, u, r) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f th whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (S * , [?] * ).', 'If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13']	['* If arc probabilities (or even l, n, u, r) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f th whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and']	['The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (S * , [?] * ).', 'If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13']
CCT39	P10-2059	Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	gesture generation by imitation  from human behavior to computer character animation	['Michael Kipp']	introduction	Uses	['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .', 'To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.', 'This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.', 'Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.', 'Finally, the two turn management categories Turn-Take and TurnElicit were also coded.']	5	['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .', 'To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.', 'This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.', 'Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.', 'Finally, the two turn management categories Turn-Take and TurnElicit were also coded.']	['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .', 'To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.', 'This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.', 'Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.', 'Finally, the two turn management categories Turn-Take and TurnElicit were also coded.']	"In an e ort to extend traditional human-computer interfaces research has introduced embodied agents to utilize the modalities of everyday human-human communication, like facial expression, gestures and body postures. However, giving computer agents a human-like body introduces new challenges. Since human users are very sensitive and critical concerning bodily behavior the agents must act naturally and individually in order to be believable. This dissertation focuses on conversational gestures. It shows how to generate conversational gestures for an animated embodied agent based on annotated text input. The central idea is to imitate the gestural behavior of a human individual. Using TV show recordings as empirical data, gestural key parameters are extracted for the generation of natural and individual gestures. The gesture generation task is solved in three stages: observation, modeling and generation. For each stage, a software module was developed. For observation, the video annotation research tool ANVIL was created. It allows the eAEcient transcription of gesture, speech and other modalities on multiple layers. ANVIL is application-independent by allowing users to de ne their own annotation schemes, it provides various import/export facilities and it is extensible via its plugin interface. Therefore, the tool is suitable for a wide variety of research elds. For this work, selected clips of the TV talk show  Das Literarische Quartett"" were transcribed and analyzed, arriving at a total of 1,056 gestures. For the modeling stage, the NOVALIS module was created to compute individual gesture pro les from these transcriptions with statistical methods. A gesture pro le models the aspects handedness, timing and function of gestures for a single human individual using estimated conditional probabilities. The pro les are based on a shared lexicon of 68 gestures, assembled from the data. Finally, for generation, the NOVA generator was devised to create gestures based on gesture pro les in an overgenerate-andlter approach. Annotated text input is processed in a graph-based representation in multiple stages where semantic data is added, the location of potential gestures is determined by heuristic rules, and gestures are added and ltered based on a gesture pro le. NOVA outputs a linear, player-independent action script in XML."		"In an e ort to extend traditional human-computer interfaces research has introduced embodied agents to utilize the modalities of everyday human-human communication, like facial expression, gestures and body postures. However, giving computer agents a human-like body introduces new challenges. Since human users are very sensitive and critical concerning bodily behavior the agents must act naturally and individually in order to be believable. This dissertation focuses on conversational gestures. It shows how to generate conversational gestures for an animated embodied agent based on annotated text input. The central idea is to imitate the gestural behavior of a human individual. Using TV show recordings as empirical data, gestural key parameters are extracted for the generation of natural and individual gestures. The gesture generation task is solved in three stages: observation, modeling and generation. For each stage, a software module was developed. For observation, the video annotation research tool ANVIL was created. It allows the eAEcient transcription of gesture, speech and other modalities on multiple layers. ANVIL is application-independent by allowing users to de ne their own annotation schemes, it provides various import/export facilities and it is extensible via its plugin interface. Therefore, the tool is suitable for a wide variety of research elds. For this work, selected clips of the TV talk show  Das Literarische Quartett"" were transcribed and analyzed, arriving at a total of 1,056 gestures. For the modeling stage, the NOVALIS module was created to compute individual gesture pro les from these transcriptions with statistical methods. A gesture pro le models the aspects handedness, timing and function of gestures for a single human individual using estimated conditional probabilities. The pro les are based on a shared lexicon of 68 gestures, assembled from the data. Finally, for generation, the NOVA generator was devised to create gestures based on gesture pro les in an overgenerate-andlter approach. Annotated text input is processed in a graph-based representation in multiple stages where semantic data is added, the location of potential gestures is determined by heuristic rules, and gestures are added and ltered based on a gesture pro le. NOVA outputs a linear, player-independent action script in XML."	['Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .']	['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .']	['Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .']	['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .']	['In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .']	['Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .']	['In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .']	['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .']	['For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .']	['Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .', 'To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.', 'This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.', 'Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.']
CCT40	W14-1704	The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	the ui system in the hoo 2012 shared task on error correction	['A Rozovskaya', 'M Sammons', 'D Roth']	experiments	Uses	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles).']	5	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles).']	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles).']	We describe the University of Illinois (UI) system that participated in the Helping Our Own (HOO) 2012 shared task, which focuses on correcting preposition and determiner errors made by non-native English speakers. The task consisted of three metrics: Detection, Recognition, and Correction, and measured performance before and after additional revisions to the test data were made. Out of 14 teams that participated, our system scored first in Detection and Recognition and second in Correction before the revisions; and first in Detection and second in the other metrics after revisions. We describe our underlying approach, which relates to our previous work in this area, and propose an improvement to the earlier method, error inflation, which results in significant gains in performance.		We describe the University of Illinois (UI) system that participated in the Helping Our Own (HOO) 2012 shared task, which focuses on correcting preposition and determiner errors made by non-native English speakers. The task consisted of three metrics: Detection, Recognition, and Correction, and measured performance before and after additional revisions to the test data were made. Out of 14 teams that participated, our system scored first in Detection and Recognition and second in Correction before the revisions; and first in Detection and second in the other metrics after revisions. We describe our underlying approach, which relates to our previous work in this area, and propose an improvement to the earlier method, error inflation, which results in significant gains in performance.	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .']	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .']	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.']	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.']	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .']	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles).']	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.']	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles).']	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .']	['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles).']
CCT41	D13-1115	The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .	a holistic lexiconbased approach to opinion mining	modeling the shape of the scene a holistic representation of the spatial envelope	['Aude Oliva', 'Antonio Torralba']	related work	Background	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	0	[]	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.		In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']
CCT42	J02-3002	#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''	a holistic lexiconbased approach to opinion mining	one term or two in	['Kenneth Church']	introduction	Background	"['Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', 'In mixed-case texts capitalized words usually denote proper names (names of organizations, locations, people, artifacts, etc.), but there are special positions in the text where capitalization is expected.', 'Such mandatory positions include the first word in a sentence, words in titles with all significant words capitalized or table entries, a capitalized word after a colon or open quote, and the first word in a list entry, among others.', 'Capitalized words in these and some other positions present a case of ambiguity: they can stand for proper names, as in White later said . . .', ', or they can be just capitalized common words, as in White elephants are . . . .', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']"	0	[]	"['Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', 'In mixed-case texts capitalized words usually denote proper names (names of organizations, locations, people, artifacts, etc.), but there are special positions in the text where capitalization is expected.', 'Such mandatory positions include the first word in a sentence, words in titles with all significant words capitalized or table entries, a capitalized word after a colon or open quote, and the first word in a list entry, among others.', 'Capitalized words in these and some other positions present a case of ambiguity: they can stand for proper names, as in White later said . . .', ', or they can be just capitalized common words, as in White elephants are . . . .', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']"	Objective SYNTAX score II (SSII) is a long-term mortality prediction model to guide the decision making of the heart-team between coronary artery bypass grafting or percutaneous coronary intervention (PCI) in patients with left main or three-vessel coronary artery disease. This study aims to investigate the long-term predictive value of SSII for all-cause mortality in patients with one- or two-vessel disease undergoing PCI. Methods A total of 628 patients (76% men, mean age: 61+-10 years) undergoing PCI due to stable angina pectoris (43%) or acute coronary syndrome (57%), included between January 2008 and June 2013, were eligible for the current study. SSII was calculated using the original SYNTAX score website (www.syntaxscore.com). Cox regression analysis was used to assess the association between continuous SSII and long-term all-cause mortality. The area under the receiver-operating characteristic curve was used to assess the performance of SSII. Results SSII ranged from 6.6 to 58.2 (median: 20.4, interquartile range: 16.1-26.8). In multivariable analysis, SSII proved to be an independent significant predictor for 4.5-year mortality (hazard ratio per point increase: 1.10; 95% confidence interval: 1.07-1.13; p<0.001). In terms of discrimination, SSII had a concordance index of 0.77. Conclusion In addition to its established value in patients with left main and three-vessel disease, SSII may also predict long-term mortality in PCI-treated patients with one- or two-vessel disease		Objective SYNTAX score II (SSII) is a long-term mortality prediction model to guide the decision making of the heart-team between coronary artery bypass grafting or percutaneous coronary intervention (PCI) in patients with left main or three-vessel coronary artery disease. This study aims to investigate the long-term predictive value of SSII for all-cause mortality in patients with one- or two-vessel disease undergoing PCI. Methods A total of 628 patients (76% men, mean age: 61+-10 years) undergoing PCI due to stable angina pectoris (43%) or acute coronary syndrome (57%), included between January 2008 and June 2013, were eligible for the current study. SSII was calculated using the original SYNTAX score website (www.syntaxscore.com). Cox regression analysis was used to assess the association between continuous SSII and long-term all-cause mortality. The area under the receiver-operating characteristic curve was used to assess the performance of SSII. Results SSII ranged from 6.6 to 58.2 (median: 20.4, interquartile range: 16.1-26.8). In multivariable analysis, SSII proved to be an independent significant predictor for 4.5-year mortality (hazard ratio per point increase: 1.10; 95% confidence interval: 1.07-1.13; p<0.001). In terms of discrimination, SSII had a concordance index of 0.77. Conclusion In addition to its established value in patients with left main and three-vessel disease, SSII may also predict long-term mortality in PCI-treated patients with one- or two-vessel disease	"[""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''""]"	"['Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''""]"	"[""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']"	"['Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']"	"['In mixed-case texts capitalized words usually denote proper names (names of organizations, locations, people, artifacts, etc.), but there are special positions in the text where capitalization is expected.', 'Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''""]"	"[""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']"	"['In mixed-case texts capitalized words usually denote proper names (names of organizations, locations, people, artifacts, etc.), but there are special positions in the text where capitalization is expected.', 'Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']"	"['Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']"	"['Such mandatory positions include the first word in a sentence, words in titles with all significant words capitalized or table entries, a capitalized word after a colon or open quote, and the first word in a list entry, among others.', 'In mixed-case texts capitalized words usually denote proper names (names of organizations, locations, people, artifacts, etc.), but there are special positions in the text where capitalization is expected.', 'Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''""]"	"[""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']"
CCT43	J04-3001	We follow the notation convention of #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	the estimation of stochastic contextfree grammars using the insideoutside algorithm computer speech and language	['Karim A Lari', 'Steve J Young']		Uses	['For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities).', 'We follow the notation convention of #AUTHOR_TAG .']	5	['For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities).', 'We follow the notation convention of Lari and Young (1990).']	['For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities).', 'We follow the notation convention of Lari and Young (1990).']		Using an entropy argument, it is shown that stochastic context-free grammars (SCFG's) can model sources with hidden branching processes more efficiently than stochastic regular grammars (or equivalently HMM's). However, the automatic estimation of SCFG's using the Inside-Outside algorithm is limited in practice by its O(n3) complexity. In this paper, a novel pre-training algorithm is described which can give significant computational savings. Also, the need for controlling the way that non-terminals are allocated to hidden processes is discussed and a solution is presented in the form of a grammar minimization procedure.	Using an entropy argument, it is shown that stochastic context-free grammars (SCFG's) can model sources with hidden branching processes more efficiently than stochastic regular grammars (or equivalently HMM's). However, the automatic estimation of SCFG's using the Inside-Outside algorithm is limited in practice by its O(n3) complexity. In this paper, a novel pre-training algorithm is described which can give significant computational savings. Also, the need for controlling the way that non-terminals are allocated to hidden processes is discussed and a solution is presented in the form of a grammar minimization procedure.	['We follow the notation convention of #AUTHOR_TAG .']	['For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'We follow the notation convention of #AUTHOR_TAG .']	['We follow the notation convention of #AUTHOR_TAG .']	['For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'We follow the notation convention of #AUTHOR_TAG .']	['The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'We follow the notation convention of #AUTHOR_TAG .']	['We follow the notation convention of #AUTHOR_TAG .']	['The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'We follow the notation convention of #AUTHOR_TAG .']	['For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'We follow the notation convention of #AUTHOR_TAG .']	['The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities).', 'The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'We follow the notation convention of #AUTHOR_TAG .']	['We follow the notation convention of #AUTHOR_TAG .']
CCT44	N10-1084	Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .	a holistic lexiconbased approach to opinion mining	words are not enough sentence level natural language watermarking	['Mercan Topkara', 'Umut Topkara', 'Mikhail J Atallah']	related work	Background	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	0	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	"Compared to other media, natural language text presents unique challenges for information hiding. These challenges require the design of a robust algorithm that can work under following constraints: (i) low embedding bandwidth, i.e., number of sentences is comparable with message length, (ii) not all transformations can be applied to a given sentence (iii) the number of alternative forms for a sentence is relatively small, a limitation governed by the grammar and vocabulary of the natural language, as well as the requirement to preserve the style and fluency of the document. The adversary can carry out all the transformations used for embedding to remove the embedded message. In addition, the adversary can also permute the sentences, select and use a subset of sentences, and insert new sentences. We give a scheme that overcomes these challenges, together with a partial implementation and its evaluation for the English language. The present application of this scheme works at the sentence level while also using a word-level watermarking technique that was recently designed and built into a fully automatic system (""Equimark""). Unlike Equimark, whose resilience relied on the introduction of ambiguities, the present paper's sentence-level technique is more tuned to situations where very little change to the text is allowable (i.e., when style is important). Secondarily, this paper shows how to use lower-level (in this case word-level) marking to improve the resilience and embedding properties of higher level (in this case sentence level) schemes. We achieve this by using the word-based methods as a separate channel from the sentence-based methods, thereby improving the results of either one alone. The sentence level watermarking technique we introduce is novel and powerful, as it relies on multiple features of each sentence and exploits the notion of orthogonality between features."		"Compared to other media, natural language text presents unique challenges for information hiding. These challenges require the design of a robust algorithm that can work under following constraints: (i) low embedding bandwidth, i.e., number of sentences is comparable with message length, (ii) not all transformations can be applied to a given sentence (iii) the number of alternative forms for a sentence is relatively small, a limitation governed by the grammar and vocabulary of the natural language, as well as the requirement to preserve the style and fluency of the document. The adversary can carry out all the transformations used for embedding to remove the embedded message. In addition, the adversary can also permute the sentences, select and use a subset of sentences, and insert new sentences. We give a scheme that overcomes these challenges, together with a partial implementation and its evaluation for the English language. The present application of this scheme works at the sentence level while also using a word-level watermarking technique that was recently designed and built into a fully automatic system (""Equimark""). Unlike Equimark, whose resilience relied on the introduction of ambiguities, the present paper's sentence-level technique is more tuned to situations where very little change to the text is allowable (i.e., when style is important). Secondarily, this paper shows how to use lower-level (in this case word-level) marking to improve the resilience and embedding properties of higher level (in this case sentence level) schemes. We achieve this by using the word-based methods as a separate channel from the sentence-based methods, thereby improving the results of either one alone. The sentence level watermarking technique we introduce is novel and powerful, as it relies on multiple features of each sentence and exploits the notion of orthogonality between features."	['Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.']	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.']	['The first lexical substitution method was proposed by Chapman and Davida (1997).', 'The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	['The first lexical substitution method was proposed by Chapman and Davida (1997).', 'The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.']	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	['The first lexical substitution method was proposed by Chapman and Davida (1997).', 'The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']
CCT45	W06-1104	#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .	a holistic lexiconbased approach to opinion mining	placing search in context the concept revisited	['Lev Finkelstein', 'Evgeniy Gabrilovich', 'Yossi Matias', 'Ehud Rivlin', 'Zach Solan', 'Gadi Wolfman']	experiments	CompareOrContrast	['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']	1	['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']	['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']	"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web."		"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web."	['#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .']	['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .']	['#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.']	['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.']	['This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .']	['#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']	['This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.']	['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']	['Resnik (1995) reported a correlation of r=.9026. 10', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .']	['#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']
CCT46	N13-1036	This is a similar conclusion to our previous work in #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	dialectal to standard arabic paraphrasing to improve arabicenglish statistical machine translation	['Wael Salloum', 'Nizar Habash']		CompareOrContrast	['In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'Phrase-based trans-  lation is also added to word-based translation.', 'Results show that selecting and translating phrases improve the three best performers of word-based selection.', 'The best performer, shown in the last raw, suggests using phrase-based selection and restricted word-based selection.', 'The restriction is to include OOV words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'Comparing the best performer to the OOV selection mode system shows that translating low frequency in-vocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']	1	['In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'Phrase-based trans-  lation is also added to word-based translation.', 'Results show that selecting and translating phrases improve the three best performers of word-based selection.', 'The best performer, shown in the last raw, suggests using phrase-based selection and restricted word-based selection.', 'The restriction is to include OOV words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'Comparing the best performer to the OOV selection mode system shows that translating low frequency in-vocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation.', 'This is a similar conclusion to our previous work in Salloum and Habash (2011).']	['In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'Phrase-based trans-  lation is also added to word-based translation.', 'Results show that selecting and translating phrases improve the three best performers of word-based selection.', 'The best performer, shown in the last raw, suggests using phrase-based selection and restricted word-based selection.', 'The restriction is to include OOV words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'Comparing the best performer to the OOV selection mode system shows that translating low frequency in-vocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation.', 'This is a similar conclusion to our previous work in Salloum and Habash (2011).']	This paper is about improving the quality of Arabic-English statistical machine translation (SMT) on dialectal Arabic text using morphological knowledge. We present a light-weight rule-based approach to producing Modern Standard Arabic (MSA) paraphrases of dialectal Arabic out-of-vocabulary (OOV) words and low frequency words. Our approach extends an existing MSA analyzer with a small number of morphological clitics, and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrase-based SMT system. This approach improves BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative). A manual error analysis of translated dialectal words shows that our system produces correct translations in 74% of the time for OOVs and 60% of the time for low frequency words.		This paper is about improving the quality of Arabic-English statistical machine translation (SMT) on dialectal Arabic text using morphological knowledge. We present a light-weight rule-based approach to producing Modern Standard Arabic (MSA) paraphrases of dialectal Arabic out-of-vocabulary (OOV) words and low frequency words. Our approach extends an existing MSA analyzer with a small number of morphological clitics, and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrase-based SMT system. This approach improves BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative). A manual error analysis of translated dialectal words shows that our system produces correct translations in 74% of the time for OOVs and 60% of the time for low frequency words.	['This is a similar conclusion to our previous work in #AUTHOR_TAG .']	['In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']	['This is a similar conclusion to our previous work in #AUTHOR_TAG .']	['In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']	['Phrase-based trans-  lation is also added to word-based translation.', 'In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']	['This is a similar conclusion to our previous work in #AUTHOR_TAG .']	['Phrase-based trans-  lation is also added to word-based translation.', 'In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']	['In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']	['Results show that selecting and translating phrases improve the three best performers of word-based selection.', 'Phrase-based trans-  lation is also added to word-based translation.', 'In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']	['This is a similar conclusion to our previous work in #AUTHOR_TAG .']
CCT47	J09-4010	We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .	a holistic lexiconbased approach to opinion mining	an information measure for classification	['C S Wallace', 'D M Boulton']		Uses	"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']"	5	"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']"	"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']"	1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application		1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application	['We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .']	"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .']"	['We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.']	"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .']"	"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .']"	['We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15']	"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.']"	"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15']"	"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .']"	['We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']
CCT48	W00-1017	The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .	a holistic lexiconbased approach to opinion mining	understanding unsegmented user utterances in realtime spoken dialogue systems	['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']	experiments	Uses	"['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']"	5	"['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']"	"['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']"	This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.		This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.	['The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .']	['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .']	"['The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state.""]"	"['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state.""]"	['It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .']	"['The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']"	"['It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state.""]"	"['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']"	['It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .']	"['The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']"
CCT49	A00-1004	A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .	a holistic lexiconbased approach to opinion mining	a program for aligning sentences in bilingual corpora	['William A Gale', 'Kenneth W Church']	method	Background	['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']	0	['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']	['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']	Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program.		Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program.	['A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .']	['The parallel Web pages we collected from various sites are not all of the same quality.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .']	['A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .']	['The parallel Web pages we collected from various sites are not all of the same quality.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .']	['Some are highly parallel and easy to align while others can be very noisy.', 'The parallel Web pages we collected from various sites are not all of the same quality.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .']	['A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .']	['Some are highly parallel and easy to align while others can be very noisy.', 'The parallel Web pages we collected from various sites are not all of the same quality.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .']	['The parallel Web pages we collected from various sites are not all of the same quality.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .']	['Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'Some are highly parallel and easy to align while others can be very noisy.', 'The parallel Web pages we collected from various sites are not all of the same quality.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .']	['A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.']
CCT50	J00-4002	#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .	a holistic lexiconbased approach to opinion mining	monotonic semantic interpretation	['Hiyan Alshawi', 'Richard Crouch']		Background	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']	0	[]	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']	Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.		Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']	['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']
CCT51	N13-1036	We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .	a holistic lexiconbased approach to opinion mining	moses open source toolkit for statistical machine translation	['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Christopher Callison-Burch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen', 'Christine Moran', 'Richard Zens', 'Christopher Dyer', 'Ondrej Bojar', 'Alexandra Constantin', 'Evan Herbst']		Uses	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA++ (Och and Ney, 2003).', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', 'The language model for our system is trained on the English side of the bitext augmented with English Gigaword (Graff and Cieri, 2003).', 'We use a 5-gram language model with modified Kneser-Ney smoothing.', 'Feature weights are tuned to maximize BLEU on the NIST MTEval 2006 test set using Minimum Error Rate Training (Och, 2003).', 'This is only done on the baseline systems.', 'The English data is tokenized using simple punctuation-based rules.', 'The Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Habash and Rambow, 2005;Roth et al., 2008).', 'The Arabic text is also Alif/Ya normalized.', 'MADA-produced Arabic lemmas are used for word alignment.']	5	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA++ (Och and Ney, 2003).', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', 'The language model for our system is trained on the English side of the bitext augmented with English Gigaword (Graff and Cieri, 2003).', 'We use a 5-gram language model with modified Kneser-Ney smoothing.', 'Feature weights are tuned to maximize BLEU on the NIST MTEval 2006 test set using Minimum Error Rate Training (Och, 2003).', 'This is only done on the baseline systems.', 'The English data is tokenized using simple punctuation-based rules.', 'The Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Habash and Rambow, 2005;Roth et al., 2008).', 'The Arabic text is also Alif/Ya normalized.', 'MADA-produced Arabic lemmas are used for word alignment.']	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA++ (Och and Ney, 2003).', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', 'The language model for our system is trained on the English side of the bitext augmented with English Gigaword (Graff and Cieri, 2003).', 'We use a 5-gram language model with modified Kneser-Ney smoothing.', 'Feature weights are tuned to maximize BLEU on the NIST MTEval 2006 test set using Minimum Error Rate Training (Och, 2003).', 'This is only done on the baseline systems.', 'The English data is tokenized using simple punctuation-based rules.', 'The Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Habash and Rambow, 2005;Roth et al., 2008).', 'The Arabic text is also Alif/Ya normalized.', 'MADA-produced Arabic lemmas are used for word alignment.']	We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.		We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .']	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .']	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.']	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.']	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .']	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA++ (Och and Ney, 2003).']	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.']	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA++ (Och and Ney, 2003).']	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .']	['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA++ (Och and Ney, 2003).', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.']
CCT52	J97-4003	This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .	a holistic lexiconbased approach to opinion mining	the representation of lexical semantic information cognitive science research paper csrp 280	['Ann Copestake']	related work	CompareOrContrast	['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']	1	['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']	['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']				['This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .']	['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .']	['This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).']	['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).']	['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .']	['This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']	['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).']	['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']	['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .']	['This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']
CCT53	A00-2004	This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .	a holistic lexiconbased approach to opinion mining	text segmentation based on similarity between words	['Hideki Kozima']		CompareOrContrast	"['Four similarity measures were examined.', 'The cosine coefficient (R98(s,co,)) and dot density measure (R98(m,(lot)) yield similar results.', 'Our spread activation based semantic measure (R98( .....,)) improved a.ccura(:y.', ""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	1	[]	"['Four similarity measures were examined.', 'The cosine coefficient (R98(s,co,)) and dot density measure (R98(m,(lot)) yield similar results.', 'Our spread activation based semantic measure (R98( .....,)) improved a.ccura(:y.', ""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis.		This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis.	"[""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	"['Four similarity measures were examined.', ""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	"[""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	"['Four similarity measures were examined.', ""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	"['The cosine coefficient (R98(s,co,)) and dot density measure (R98(m,(lot)) yield similar results.', 'Four similarity measures were examined.', ""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	"[""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	"['The cosine coefficient (R98(s,co,)) and dot density measure (R98(m,(lot)) yield similar results.', 'Four similarity measures were examined.', ""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	"['Four similarity measures were examined.', ""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	"['Our spread activation based semantic measure (R98( .....,)) improved a.ccura(:y.', 'The cosine coefficient (R98(s,co,)) and dot density measure (R98(m,(lot)) yield similar results.', 'Four similarity measures were examined.', ""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	"[""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"
CCT54	J02-3002	#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .	a holistic lexiconbased approach to opinion mining	hybrid text mining for finding abbreviations and their definitions	['Youngja Park', 'Roy J Byrd']		Background	"['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing.', 'This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'Therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'Candidates then are filtered through a set of known common words and proper names.', 'At the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document.']"	0	"['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing.', 'This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'Therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'Candidates then are filtered through a set of known common words and proper names.', 'At the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document.']"	"['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing.', 'This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'Therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'Candidates then are filtered through a set of known common words and proper names.', 'At the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document.']"	We present a hybrid text mining method for finding abbreviations and their definitions in free format texts. To deal with the problem, this method employs pattern-based abbreviation rules in addition to text markers and cue words. The pattern-based rules describe how abbreviations are formed from definitions. Rules can be generated automatically and/or manually and can be augmented when the system processes new documents. The proposed method has the advantages of high accuracy, high flexibility, wide coverage, and fast recognition.		We present a hybrid text mining method for finding abbreviations and their definitions in free format texts. To deal with the problem, this method employs pattern-based abbreviation rules in addition to text markers and cue words. The pattern-based rules describe how abbreviations are formed from definitions. Rules can be generated automatically and/or manually and can be augmented when the system processes new documents. The proposed method has the advantages of high accuracy, high flexibility, wide coverage, and fast recognition.	['#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .']	['Not much information has been published on abbreviation identification.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .']	['#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .']	['Not much information has been published on abbreviation identification.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .']	['One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing.', 'Not much information has been published on abbreviation identification.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .']	['#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .']	['One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing.', 'Not much information has been published on abbreviation identification.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .']	['Not much information has been published on abbreviation identification.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .']	['This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing.', 'Not much information has been published on abbreviation identification.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .']	"['#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.']"
CCT55	J03-3004	More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :	a holistic lexiconbased approach to opinion mining	the phrasal lexicon	['Joseph Becker']	introduction	Background	['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Rscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	0	['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Rscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Rscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	Theoretical linguists have in recent years concentrated their attention on the productive aspect of language, wherein utterances are formed combinatorically from units the size of words or smaller. This paper will focus on the contrary aspect of language, wherein utterances are formed by repetition, modification, and concatenation of previously-known phrases consisting of more than one word. I suspect that we speak mostly by stitching together swatches of text that we have heard before; productive processes have the secondary role of adapting the old phrases to the new situation. The advantage of this point of view is that it has the potential to account for the observed linguistic behavior of native speakers, rather than discounting their actual behavior as irrelevant to their language. In particular, this point of view allows us to concede that most utterances are produced in stereotyped social situations, where the communicative and ritualistic functions of language demand not novelty, but rather an appropriate combination of formulas, cliches, idioms, allusions, slogans, and so forth. Language must have originated in such constrained social contexts, and they are still the predominant arena for language production. Therefore an understanding of the use of phrases is basic to the understanding of language as a whole.You are currently reading a much-abridged version of a paper that will be published elsewhere later.		Theoretical linguists have in recent years concentrated their attention on the productive aspect of language, wherein utterances are formed combinatorically from units the size of words or smaller. This paper will focus on the contrary aspect of language, wherein utterances are formed by repetition, modification, and concatenation of previously-known phrases consisting of more than one word. I suspect that we speak mostly by stitching together swatches of text that we have heard before; productive processes have the secondary role of adapting the old phrases to the new situation. The advantage of this point of view is that it has the potential to account for the observed linguistic behavior of native speakers, rather than discounting their actual behavior as irrelevant to their language. In particular, this point of view allows us to concede that most utterances are produced in stereotyped social situations, where the communicative and ritualistic functions of language demand not novelty, but rather an appropriate combination of formulas, cliches, idioms, allusions, slogans, and so forth. Language must have originated in such constrained social contexts, and they are still the predominant arena for language production. Therefore an understanding of the use of phrases is basic to the understanding of language as a whole.You are currently reading a much-abridged version of a paper that will be published elsewhere later.	['More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	['More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	['There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', 'All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	['More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	['There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', 'All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	[' Kay and Roscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', 'All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']	['More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']
CCT56	J05-3003	#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .	a holistic lexiconbased approach to opinion mining	the automatic acquisition of frequencies of verb subcategorization frames from tagged corpora	['Akira Ushioda', 'David Evans', 'Ted Gibson', 'Alex Waibel']	related work	Background	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"	0	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"	We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a finear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation.		We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a finear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation.	['#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .']	['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .']	['#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .']	['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .']	['Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .']	['#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .']	['Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .']	['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .']	['Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .']	"['#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.']"
CCT57	J07-1005	This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .	a holistic lexiconbased approach to opinion mining	knowledge extraction for clinical question answering preliminary results	['Dina Demner-Fushman', 'Jimmy Lin']		Extends	['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', 'Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.']	2	['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', 'Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.']	['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', 'Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.']	The combination of recent developments in question an-swering research and the unparalleled resources devel-oped specifically for automatic semantic processing of text in the medical domain provides a unique opportu-nity to explore complex question answering in the clin-ical domain. In this paper, we attempt to operationalize major aspects of evidence-based medicine in the form of knowledge extractors that serve as the fundamental building blocks of a clinical question answering sys-tem. Our evaluations demonstrate that domain-specific knowledge can be effectively leveraged to extract PICO frame elements from MEDLINE abstracts. Clinical in-formation systems in support of physicians ' decision-making process have the potential to improve the qual-ity of patient care in real-world settings		The combination of recent developments in question an-swering research and the unparalleled resources devel-oped specifically for automatic semantic processing of text in the medical domain provides a unique opportu-nity to explore complex question answering in the clin-ical domain. In this paper, we attempt to operationalize major aspects of evidence-based medicine in the form of knowledge extractors that serve as the fundamental building blocks of a clinical question answering sys-tem. Our evaluations demonstrate that domain-specific knowledge can be effectively leveraged to extract PICO frame elements from MEDLINE abstracts. Clinical in-formation systems in support of physicians ' decision-making process have the potential to improve the qual-ity of patient care in real-world settings	['This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .']	['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .']	['This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.']	['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.']	['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .']	['This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', 'Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.']	['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.']	['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', 'Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.']	['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .']	['This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', 'Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.']
CCT58	J91-2003	It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .	a holistic lexiconbased approach to opinion mining	intended models circumscription and commonsense reasoningquot	['W Zadrozny']	introduction	CompareOrContrast	['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions.', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']	1	[]	['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions.', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']	"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations."		"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations."	['It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .']	['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .']	['It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']	['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']	['This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .']	['It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']	['This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']	['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']	['Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .']	['It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']
CCT59	J06-2002	#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .	a holistic lexiconbased approach to opinion mining	achieving incremental semantic interpretation through contextual representation	['Julie Sedivy', 'Michael Tanenhaus', 'Craig Chambers', 'Gregory Carlson']	experiments	Background	"['It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', 'Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key).', ""Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied 'intrinsically' to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations."", 'The time subjects took before looking at the target for the first time was measured, and although these latency times were somewhat greater when the referent were not intrinsically tall than when they were, the average difference was tiny at 554 versus 538 miliseconds.', 'Since latency times are thought to be sensitive to most of the problems that hearers may have in processing a text, these results suggest that, for dimensional adjectives, it is forgivable to disregard global context.']"	0	"['It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', 'Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key).', ""Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied 'intrinsically' to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations."", 'The time subjects took before looking at the target for the first time was measured, and although these latency times were somewhat greater when the referent were not intrinsically tall than when they were, the average difference was tiny at 554 versus 538 miliseconds.', 'Since latency times are thought to be sensitive to most of the problems that hearers may have in processing a text, these results suggest that, for dimensional adjectives, it is forgivable to disregard global context.']"	"['It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', 'Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key).', ""Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied 'intrinsically' to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations."", 'The time subjects took before looking at the target for the first time was measured, and although these latency times were somewhat greater when the referent were not intrinsically tall than when they were, the average difference was tiny at 554 versus 538 miliseconds.', 'Since latency times are thought to be sensitive to most of the problems that hearers may have in processing a text, these results suggest that, for dimensional adjectives, it is forgivable to disregard global context.']"	While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information.		While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information.	['#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .']	['It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .']	['#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .']	['It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .']	['Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', 'It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .']	['#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .']	['Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', 'It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .']	['It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .']	['Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', 'It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .']	"['#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key).', ""Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied 'intrinsically' to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations.""]"
CCT60	D10-1002	Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .	a holistic lexiconbased approach to opinion mining	products of random latent variable grammars	['Slav Petrov']	conclusion	CompareOrContrast	['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']	1	['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']	['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']	We show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-of-the-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.		We show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-of-the-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.	['Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .']	['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .']	['Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .']	['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .']	['We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .']	['Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .']	['We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .']	['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .']	['Two primary factors appear to be determining the efficacy of our self-training approach.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .']	['Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.']
CCT61	P10-4003	Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .	a holistic lexiconbased approach to opinion mining	targeted help for spoken dialogue systems intelligent feedback improves naive users performance	['Beth Ann Hockey', 'Oliver Lemon', 'Ellen Campana', 'Laura Hiatt', 'Gregory Aist', 'James Hieronymus', 'Alexander Gruenstein', 'John Dowding']	experiments	Extends	"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']"	2	"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']"	"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']"	We present experimental evidence that providing naive users of a spoken dialogue system with immediate help messages related to their out-of-coverage utterances improves their success in using the system. A grammar-based recognizer and a Statistical Language Model (SLM) recognizer are run simultaneously. If the grammar-based recognizer suceeds, the less accurate SLM recognizer hypothesis is not used. When the grammar-based recognizer fails and the SLM recognizer produces a recognition hypothesis, this result is used by the Targeted Help agent to give the user feedback on what was recognized, a diagnosis of what was problematic about the utterance, and a related in-coverage example. The in-coverage example is intended to encourage alignment between user inputs and the language model of the system. We report on controlled experiments on a spoken dialogue system for command and control of a simulated robotic helicopter.		We present experimental evidence that providing naive users of a spoken dialogue system with immediate help messages related to their out-of-coverage utterances improves their success in using the system. A grammar-based recognizer and a Statistical Language Model (SLM) recognizer are run simultaneously. If the grammar-based recognizer suceeds, the less accurate SLM recognizer hypothesis is not used. When the grammar-based recognizer fails and the SLM recognizer produces a recognition hypothesis, this result is used by the Targeted Help agent to give the user feedback on what was recognized, a diagnosis of what was problematic about the utterance, and a related in-coverage example. The in-coverage example is intended to encourage alignment between user inputs and the language model of the system. We report on controlled experiments on a spoken dialogue system for command and control of a simulated robotic helicopter.	['Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .']	"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .']"	['Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .']	"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .']"	"['At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .']"	['Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .']	"['At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .']"	"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .']"	"['Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .']"	"['Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']"
CCT62	J06-2002	It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	efficient contextsensitive generation of referring expressions	['Emiel Krahmer', 'Mariet Theune']	experiments	CompareOrContrast	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).', 'Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking.', 'Let us see how things would work if they were ranked more highly.']	1	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).', 'Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking.', 'Let us see how things would work if they were ranked more highly.']	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).', 'Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking.', 'Let us see how things would work if they were ranked more highly.']	3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11		3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .']	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .']	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.']	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.']	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .']	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).']	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.']	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).']	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .']	['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).', 'Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking.']
CCT63	E99-1022	` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .	a holistic lexiconbased approach to opinion mining	typed feature structures as descriptions	['Paul King']		Background	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in (GStz and Meurers, 1997b) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars.']	0	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in (GStz and Meurers, 1997b) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars.']	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in (GStz and Meurers, 1997b) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars.']	A description is an entity that can be interpreted as true or false of an object, and using feature structures as descriptions accrues several computational benefits. In this paper, I create an explicit interpretation of a typed feature structure used as a description, define the notion of a satisfiable feature structure, and create a simple and effective algorithm to decide if a feature structure is satisfiable.Comment: COLING 94 reserve paper, 5 pages, LaTeX (no .sty exotica		A description is an entity that can be interpreted as true or false of an object, and using feature structures as descriptions accrues several computational benefits. In this paper, I create an explicit interpretation of a typed feature structure used as a description, define the notion of a satisfiable feature structure, and create a simple and effective algorithm to decide if a feature structure is satisfiable.Comment: COLING 94 reserve paper, 5 pages, LaTeX (no .sty exotica	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .']	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .']	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).']	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).']	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .']	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.']	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).']	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.']	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .']	['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.']
CCT64	J09-4010	Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .	a holistic lexiconbased approach to opinion mining	decision graphsan extension of decision trees	['J J Oliver']	method	Uses	['The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']	5	[]	['The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']	In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain.		In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain.	['Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .']	['The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .']	['Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.']	['The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.']	['Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .']	['Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']	['Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.']	['The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']	['Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .']	['Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']
CCT65	J00-2004	There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .	a holistic lexiconbased approach to opinion mining	building parallel ltag for french and italian	['Marie-Helene Candito']	method	Background	['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']	0	['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']	['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']	In this paper we view Lexicalized Tree Adjoining Grammars as the compilation of a more abstract and modular layer of linguistic description: the metagrammar (MG). MG provides a hierarchical representation of lexico-syntactic descriptions and principles that capture the well-formedness of lexicalized structures, expressed using syntactic functions. This makes it possible for a tool to compile an instance of MG into an LTAG, automatically performing the relevant combinations of linguistic phenomena. We then describe the instantiation of an MG for Italian and French. The work for French was performed starting with an existing LTAG, which has been augmented as a result. The work for Italian was performed by systematic contrast with the French MG. The automatic compilation gives two parallel LTAG, compatible for multilingual NLP applications.		In this paper we view Lexicalized Tree Adjoining Grammars as the compilation of a more abstract and modular layer of linguistic description: the metagrammar (MG). MG provides a hierarchical representation of lexico-syntactic descriptions and principles that capture the well-formedness of lexicalized structures, expressed using syntactic functions. This makes it possible for a tool to compile an instance of MG into an LTAG, automatically performing the relevant combinations of linguistic phenomena. We then describe the instantiation of an MG for Italian and French. The work for French was performed starting with an existing LTAG, which has been augmented as a result. The work for Italian was performed by systematic contrast with the French MG. The automatic compilation gives two parallel LTAG, compatible for multilingual NLP applications.	['There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .']	['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .']	['There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']	['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']	['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .']	['There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']	['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']	['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']	['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .']	['There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']
CCT66	P11-1134	These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .	a holistic lexiconbased approach to opinion mining	the berkeley framenet project	['Collin F Baker', 'Charles J Fillmore', 'John B Lowe']	introduction	Background	['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']	0	[]	['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']	FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work		FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work	['These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .']	['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .']	['These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.']	['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .']	['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .']	['These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.']	['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.']	['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.']	['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .']	['These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.']
CCT67	J97-4003	However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .	a holistic lexiconbased approach to opinion mining	on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars	['Detmar Meurers']	introduction	Motivation	['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'In the latter case, we can also take care of transferring the value of z.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .', 'Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.', 'So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17']	4	[]	['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'In the latter case, we can also take care of transferring the value of z.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .', 'Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.', 'So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17']	The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.		The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.	['However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .']	['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .']	['However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .']	['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .']	['In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .']	['However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .']	['In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .']	['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .']	['In the latter case, we can also take care of transferring the value of z.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .']	['However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .', 'Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.', 'So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17']
CCT68	J00-3003	It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	automatic stochastic tagging of natural language texts	['Evangelos Dermatas', 'George Kokkinakis']		Background	['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.', 'We can compute the per-utterance posterior DA probabilities by summing:']	0	[]	['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.', 'We can compute the per-utterance posterior DA probabilities by summing:']	Five language and tagset independent stochastic taggers, handling morphological and contextual information, are presented and tested in corpora of seven European languages (Dutch, English, French, German, Greek, Italian and Spanish), using two sets of grammatical tags; a small set containing the eleven main grammatical classes and a large set of grammatical categories common to all languages. The unknown words are tagged using an experimentally proven stochastic hypothesis that links the stochastic behavior of the unknown words with that of the less probable known words. A fully automatic training and tagging program has been implemented on an IBM PC-compatible 80386-based computer. Measurements of error rate, time response, and memory requirements have shown that the taggers' performance is satisfactory, even though a small training text is available. The error rate is improved when new texts are used to update the stochastic model parameters.		Five language and tagset independent stochastic taggers, handling morphological and contextual information, are presented and tested in corpora of seven European languages (Dutch, English, French, German, Greek, Italian and Spanish), using two sets of grammatical tags; a small set containing the eleven main grammatical classes and a large set of grammatical categories common to all languages. The unknown words are tagged using an experimentally proven stochastic hypothesis that links the stochastic behavior of the unknown words with that of the less probable known words. A fully automatic training and tagging program has been implemented on an IBM PC-compatible 80386-based computer. Measurements of error rate, time response, and memory requirements have shown that the taggers' performance is satisfactory, even though a small training text is available. The error rate is improved when new texts are used to update the stochastic model parameters.	['It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .']	['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .']	['It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.']	['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.']	['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .']	['It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.', 'We can compute the per-utterance posterior DA probabilities by summing:']	['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.']	['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.', 'We can compute the per-utterance posterior DA probabilities by summing:']	['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .']	['It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.', 'We can compute the per-utterance posterior DA probabilities by summing:']
CCT69	W02-1601	A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	representation trees and stringtree correspondences	['C Boitet', 'Y Zaharin']		Uses	"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"	5	"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"	"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"		"The correspondence between a string of a language and its abstract representation, usually a (decorated) tree, is not straightforward. However, it is desirable to maintain it, for example to build structured editors for texts written in natural language. As such correspondences must be compositional, we call them ""Structured String-free Correspondences"" (SSTC).We argue that a SSTC is in fact composed of two interrelated correspondences, one between nodes and substrings, and the other between subtrees and substrings, the substrings being possibly discontinucus in both cases. We then proceed to show how to define a SSTC with a Structural Correspondence Static Grammar (SCSG), and which constraints to put on the rules of the SCSG to get a ""natural"" SSTC."	"The correspondence between a string of a language and its abstract representation, usually a (decorated) tree, is not straightforward. However, it is desirable to maintain it, for example to build structured editors for texts written in natural language. As such correspondences must be compositional, we call them ""Structured String-free Correspondences"" (SSTC).We argue that a SSTC is in fact composed of two interrelated correspondences, one between nodes and substrings, and the other between subtrees and substrings, the substrings being possibly discontinucus in both cases. We then proceed to show how to define a SSTC with a Structural Correspondence Static Grammar (SCSG), and which constraints to put on the rules of the SCSG to get a ""natural"" SSTC."	['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .']	"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .']"	['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']	"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"	"['It contains a nonprojective correspondence.', 'Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .']"	['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']	"['It contains a nonprojective correspondence.', 'Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"	"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"	"['An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'It contains a nonprojective correspondence.', 'Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .']"	['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']
CCT70	D10-1056	We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .	a holistic lexiconbased approach to opinion mining	classbased ngram models of natural language	['Peter F Brown', 'Vincent J Della Pietra', 'Peter V Desouza', 'Jennifer C Lai', 'Robert L Mercer']	conclusion	Background	['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']	0	['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']	['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']		We address the problem of predicting a word from previous words in a sample of text.In particular, we discuss n-gram models based on classes of words. We also discuss severalstatistical algorithms for assigning words to classes based on the frequency of their cooccurrencewith other words. We find that we are able to extract classes that have the flavor of eithersyntactically based groupings or semantically based groupings, depending on the nature of theunderlying statistics.	We address the problem of predicting a word from previous words in a sample of text.In particular, we discuss n-gram models based on classes of words. We also discuss severalstatistical algorithms for assigning words to classes based on the frequency of their cooccurrencewith other words. We find that we are able to extract classes that have the flavor of eithersyntactically based groupings or semantically based groupings, depending on the nature of theunderlying statistics.	['We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .']	['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .']	['We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']	['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']	['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .']	['We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']	['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']	['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']	['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .']	['We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']
CCT71	N01-1012	Other definitions of predicates may be found in ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	linking wordnet verb classes to semantic interpretation	['F Gomez']		Background	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']	0	[]	[]	An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented. The selectional restrictions for the thematic roles defining the predicates are WordNet ontological categories. Thematic roles are also linked to the syntactic relations that realize them. The paper illustrates the methodology by providing a detailed analysis of some major WordNet verb classes.		An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented. The selectional restrictions for the thematic roles defining the predicates are WordNet ontological categories. Thematic roles are also linked to the syntactic relations that realize them. The paper illustrates the methodology by providing a detailed analysis of some major WordNet verb classes.	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']
CCT72	P11-1134	For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	syntacticsemantic structures for textual entailment recognition	['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']	experiments	CompareOrContrast	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"	1	[]	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"	In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.		In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) .""]"	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) .""]"	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) .""]"	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) .""]"	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"
CCT73	N01-1003	The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .	a holistic lexiconbased approach to opinion mining	sentence planning as description using tree adjoining grammar	['Matthew Stone', 'Christine Doran']		Background	['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'The interior nodes are associated with DSyntSs by executing their clausecombing operation on their two daughter nodes.', '(A PE-RIOD node results in a DSyntS headed by a period and whose daughters are the two daughter DSyntSs.)', 'If a clause combination fails, the sp-tree is discarded (for example, if we try to create a relative clause of a structure which already contains a period).', 'As a result, the DSyntS for the entire turn is associated with the root node.', 'This DSyntS can be sent to RealPro, which returns a sentence (or several sentences, if the DSyntS contains period nodes).', 'The SPG is designed in such a way that if a DSyntS is associated with the root node, it is a valid structure which can be realized.', 'Figure 2 shows some of the realizations of alternative sentence plans generated by our SPG for utterance Sys- 3 The sp-tree is inspired by (Lavoie and Rambow, 1998).', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', '4 shows the result of applying the soft-merge operation when Args 1 and 2 are implicit confirmations of the origin and destination cities. Figure 8 illustrates the relationship between the sp-tree and the DSyntS for alternative 8.', 'The labels and arrows show the DSyntSs associated with each node in the sp-tree (in Figure 7), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'The complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.', 'In our approach, we do not need to encode such constraints.', 'Rather, we generate a random sample of possible sentence plans for each text plan, up to a pre-specified maximum number of sentence plans, by randomly selecting among the operations according to some probability distribution. 4']	0	['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'The interior nodes are associated with DSyntSs by executing their clausecombing operation on their two daughter nodes.', '(A PE-RIOD node results in a DSyntS headed by a period and whose daughters are the two daughter DSyntSs.)', 'If a clause combination fails, the sp-tree is discarded (for example, if we try to create a relative clause of a structure which already contains a period).', 'As a result, the DSyntS for the entire turn is associated with the root node.', 'This DSyntS can be sent to RealPro, which returns a sentence (or several sentences, if the DSyntS contains period nodes).', 'The SPG is designed in such a way that if a DSyntS is associated with the root node, it is a valid structure which can be realized.', 'Figure 2 shows some of the realizations of alternative sentence plans generated by our SPG for utterance Sys- 3 The sp-tree is inspired by (Lavoie and Rambow, 1998).', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', '4 shows the result of applying the soft-merge operation when Args 1 and 2 are implicit confirmations of the origin and destination cities. Figure 8 illustrates the relationship between the sp-tree and the DSyntS for alternative 8.', 'The labels and arrows show the DSyntSs associated with each node in the sp-tree (in Figure 7), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'The complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.', 'In our approach, we do not need to encode such constraints.', 'Rather, we generate a random sample of possible sentence plans for each text plan, up to a pre-specified maximum number of sentence plans, by randomly selecting among the operations according to some probability distribution. 4']	['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'The interior nodes are associated with DSyntSs by executing their clausecombing operation on their two daughter nodes.', '(A PE-RIOD node results in a DSyntS headed by a period and whose daughters are the two daughter DSyntSs.)', 'If a clause combination fails, the sp-tree is discarded (for example, if we try to create a relative clause of a structure which already contains a period).', 'As a result, the DSyntS for the entire turn is associated with the root node.', 'This DSyntS can be sent to RealPro, which returns a sentence (or several sentences, if the DSyntS contains period nodes).', 'The SPG is designed in such a way that if a DSyntS is associated with the root node, it is a valid structure which can be realized.', 'Figure 2 shows some of the realizations of alternative sentence plans generated by our SPG for utterance Sys- 3 The sp-tree is inspired by (Lavoie and Rambow, 1998).', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', '4 shows the result of applying the soft-merge operation when Args 1 and 2 are implicit confirmations of the origin and destination cities. Figure 8 illustrates the relationship between the sp-tree and the DSyntS for alternative 8.', 'The labels and arrows show the DSyntSs associated with each node in the sp-tree (in Figure 7), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'The complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.', 'In our approach, we do not need to encode such constraints.', 'Rather, we generate a random sample of possible sentence plans for each text plan, up to a pre-specified maximum number of sentence plans, by randomly selecting among the operations according to some probability distribution. 4']	We present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG). This approach captures naturally and elegantly the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential interactions between multiple descriptions in a sentence. At the same time, it exploits linguistically motivated, declarative specifications of the discourse functions of syntactic constructions to make contextually appropriate syntactic choices.		We present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG). This approach captures naturally and elegantly the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential interactions between multiple descriptions in a sentence. At the same time, it exploits linguistically motivated, declarative specifications of the discourse functions of syntactic constructions to make contextually appropriate syntactic choices.	['The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .']	['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .']	['The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .']	['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .']	['Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .']	['The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .']	['Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .']	['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .']	['The interior nodes are associated with DSyntSs by executing their clausecombing operation on their two daughter nodes.', 'Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .']	['The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', '4 shows the result of applying the soft-merge operation when Args 1 and 2 are implicit confirmations of the origin and destination cities. Figure 8 illustrates the relationship between the sp-tree and the DSyntS for alternative 8.', 'The labels and arrows show the DSyntSs associated with each node in the sp-tree (in Figure 7), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'The complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.']
CCT74	W01-0706	Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .	a holistic lexiconbased approach to opinion mining	the use of classifiers in sequential inference	['V Punyakanok', 'D Roth']	experiments	Extends	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']	2	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']	We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.		We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .']	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .']	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .']	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .']	['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']
CCT75	W06-1104	If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6	a holistic lexiconbased approach to opinion mining	automatic generation of a coarse grained wordnet	['Rada Mihalcea', 'Dan Moldovan']	experiments	Background	['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'Words with a number of senses above the threshold are removed from the list.']	0	['If differences in meaning between senses are very fine-grained, distinguishing between them is hard even for humans (Mihalcea and Moldovan, 2001). 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'Words with a number of senses above the threshold are removed from the list.']	['If differences in meaning between senses are very fine-grained, distinguishing between them is hard even for humans (Mihalcea and Moldovan, 2001). 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'Words with a number of senses above the threshold are removed from the list.']		In this paper, we propose several principles that enable the automatic transformation of WordNet into a coarser grained dictionary, without affecting its existing semantic relations. We derive a new version of WordNet leading to a reduction of 26% in the average polysemy of words, while introducing a small error rate of 2.1%, as measured on a sense tagged corpus.	In this paper, we propose several principles that enable the automatic transformation of WordNet into a coarser grained dictionary, without affecting its existing semantic relations. We derive a new version of WordNet leading to a reduction of 26% in the average polysemy of words, while introducing a small error rate of 2.1%, as measured on a sense tagged corpus.	['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6']	['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6']	['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.']	['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.']	['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6']	['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.']	['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.']	['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.']	['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6']	['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'Words with a number of senses above the threshold are removed from the list.']
CCT76	J07-1005	The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .	a holistic lexiconbased approach to opinion mining	answering questions in the genomics domain	['Fabio Rinaldi', 'James Dowdall', 'Gerold Schneider', 'Andreas Persidis']	related work	Background	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']	0	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']	In this paper we describe current efforts aimed at adapting an existing Question Answering system to a new document set, namely research papers in the genomics domain. The system has been originally developed for another restricted domain, however it has already proved its portability. Nevertheless, the process is not painless, and the specific purpose of  this paper is to describe the problems encountered.		In this paper we describe current efforts aimed at adapting an existing Question Answering system to a new document set, namely research papers in the genomics domain. The system has been originally developed for another restricted domain, however it has already proved its portability. Nevertheless, the process is not painless, and the specific purpose of  this paper is to describe the problems encountered.	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .']	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .']	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .']	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .']	['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']
CCT77	D13-1115	Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .	a holistic lexiconbased approach to opinion mining	indexing by latent semantic analysis	['Scott Deerwester', 'Susan T Dumais', 'George W Furnas', 'Thomas K Landauer', 'Richard Harshman']	related work	Background	['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']	0	['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']	['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']	"A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (""semantic structure"") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."		"A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (""semantic structure"") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."	['Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .']	['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .']	['Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .']	['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .']	['Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .']	['Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .']	['Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .']	['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .']	['Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .']	['Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.']
CCT78	W06-3309	Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .	a holistic lexiconbased approach to opinion mining	catching the drift probabilistic content models with applications to generation and summarization	['Regina Barzilay', 'Lillian Lee']	related work	CompareOrContrast	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.', 'Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'In contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.', 'Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.', 'Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here.']	1	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.', 'Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'In contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.', 'Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.', 'Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here.']	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.', 'Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'In contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.', 'Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.', 'Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here.']	We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.		We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .']	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .']	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.']	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.']	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .']	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.', 'Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.']	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.']	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.', 'Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.']	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .']	['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.', 'Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'In contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.']
CCT79	D10-1123	Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .	a holistic lexiconbased approach to opinion mining	scaling textual inference to the web	['S Schoenmackers', 'O Etzioni', 'D Weld']	conclusion	Background	['We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'We release this list for further use by the research community. 2', 'Future Work: Functionality is one of the several properties a relation can possess.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties.']	0	['We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'We release this list for further use by the research community. 2', 'Future Work: Functionality is one of the several properties a relation can possess.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties.']	['We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'We release this list for further use by the research community. 2', 'Future Work: Functionality is one of the several properties a relation can possess.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties.']	"Most Web-based Q/A systems work by finding pages that contain an explicit answer to a question. These systems are helpless if the answer has to be inferred from multiple sentences, possibly on different pages. To solve this problem, we introduce the Holmes system, which utilizes textual inference (TI) over tuples extracted from text.    Whereas previous work on TI (e.g., the literature on textual entailment) has been applied to paragraph-sized texts, Holmes utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. Given only a few minutes, Holmes doubles recall for example queries in three disparate domains (geography, business, and nutrition). Importantly, Holmes's runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus---they are ""approximately"" functional in a well-defined sense."		"Most Web-based Q/A systems work by finding pages that contain an explicit answer to a question. These systems are helpless if the answer has to be inferred from multiple sentences, possibly on different pages. To solve this problem, we introduce the Holmes system, which utilizes textual inference (TI) over tuples extracted from text.    Whereas previous work on TI (e.g., the literature on textual entailment) has been applied to paragraph-sized texts, Holmes utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. Given only a few minutes, Holmes doubles recall for example queries in three disparate domains (geography, business, and nutrition). Importantly, Holmes's runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus---they are ""approximately"" functional in a well-defined sense."	['Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .']	['We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .']	['Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.']	['We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.']	['We release this list for further use by the research community. 2', 'We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .']	['Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties.']	['We release this list for further use by the research community. 2', 'We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.']	['We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties.']	['Future Work: Functionality is one of the several properties a relation can possess.', 'We release this list for further use by the research community. 2', 'We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .']	['Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties.']
CCT80	J00-2004	 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) ,  multilingual document filtering ( e.g. , Oard 1997 ) ,  computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) ,  certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) ,  concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,	a holistic lexiconbased approach to opinion mining	should we translate the documents or the queries in crosslanguage information retrieval	['J Scott McCarley']	method	Background	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']	0	[]	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']	Previous comparisons of document and query translation suffered difficulty due to differing quality of machine translation in these two opposite directions. We avoid this difficulty by training identical statistical translation models for both translation directions using the same training data. We investigate information retrieval between English and French, incorporating both translations directions into both document translation and query translation-based information retrieval, as well as into hybrid systems. We find that hybrids of document and query translation-based systems out-perform query translation systems, even human-quality query translation systems.		Previous comparisons of document and query translation suffered difficulty due to differing quality of machine translation in these two opposite directions. We avoid this difficulty by training identical statistical translation models for both translation directions using the same training data. We investigate information retrieval between English and French, incorporating both translations directions into both document translation and query translation-based information retrieval, as well as into hybrid systems. We find that hybrids of document and query translation-based systems out-perform query translation systems, even human-quality query translation systems.	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']	['\x80 cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , \x80 multilingual document filtering ( e.g. , Oard 1997 ) , \x80 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \x80 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \x80 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']
CCT81	W04-1610	For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .	a holistic lexiconbased approach to opinion mining	using clustering to boost text classificationquot	['Y C Fang', 'S Parthasarathy', 'F Schwartz']	related work	Background	['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	0	['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	In recent years we have seen a tremendous growth in the number of text document collections available on the Internet. Automatic text categorization, the process of assigning unseen documents to user-defined categories, is an important task that can help in the organization and querying of such collections. In this article we consider the problem of classifying online papers from a specific journal in the geological sciences, over a set of expert defined categories. We evaluate two general strategies and several variants thereof. The first strategy is based on Naive Bayes, a popular text classification algorithm. The second strategy is based on Principle Direction Divisive Partitioning, an unsupervised document clustering algorithm. While the performance of both approaches is quite good, some of the new variants that we propose including one, which involves a combination of these two approaches yield even better results.		In recent years we have seen a tremendous growth in the number of text document collections available on the Internet. Automatic text categorization, the process of assigning unseen documents to user-defined categories, is an important task that can help in the organization and querying of such collections. In this article we consider the problem of classifying online papers from a specific journal in the geological sciences, over a set of expert defined categories. We evaluate two general strategies and several variants thereof. The first strategy is based on Naive Bayes, a popular text classification algorithm. The second strategy is based on Principle Direction Divisive Partitioning, an unsupervised document clustering algorithm. While the performance of both approaches is quite good, some of the new variants that we propose including one, which involves a combination of these two approaches yield even better results.	['For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	['For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	['For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']	['For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']
CCT82	W05-0709	As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .	a holistic lexiconbased approach to opinion mining	proceedings of ace evaluation and pi meeting	['NIST']	experiments	Uses	"['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.', 'Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type.', 'In this paper, we report the results in terms of precision, recall, and F-measure3.']"	5	[]	"['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.', 'Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type.', 'In this paper, we report the results in terms of precision, recall, and F-measure3.']"				"[""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .""]"	"['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .""]"	"[""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.']"	"['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .""]"	"['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .""]"	"[""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.']"	"['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.']"	"['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.']"	"['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .""]"	"[""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.', 'Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type.']"
CCT83	J02-3002	Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	a maximum entropy approach to identifying sentence boundaries	['Jeffrey C Reynar', 'Adwait Ratnaparkhi']		Background	['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']	0	['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']	['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']	We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of ., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman-alphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.Comment: 4 pages, uses aclap.sty and covingtn.st		We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of ., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman-alphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.Comment: 4 pages, uses aclap.sty and covingtn.st	['Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .']	['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .']	['Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.']	['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .']	['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .']	['Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.']	['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.']	['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.']	['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .']	['Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']
CCT84	W06-1639	For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .	a holistic lexiconbased approach to opinion mining	a preliminary investigation into sentiment analysis of informal political discourse	['T Mullen', 'R Malouf']	introduction	Background	['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .', 'Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.']	0	[]	['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .', 'Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.']	With the rise of weblogs and the increasing tendency of online publications to turn to message-board style reader feedback venues, informal political discourse is becoming an important feature of the intellectual landscape of the Internet, creating a challenging and worthwhile area for experimentation in techniques for sentiment analysis. We describe preliminary statistical tests on a new dataset of political discussion group postings which indicate that posts made in direct response to other posts in a thread have a strong tendency to represent an opposing political viewpoint to the original post. We conclude that traditional text classification methods will be inadequate to the task of sentiment analysis in this domain, and that progress is to be made by exploiting information about how posters interact with each		With the rise of weblogs and the increasing tendency of online publications to turn to message-board style reader feedback venues, informal political discourse is becoming an important feature of the intellectual landscape of the Internet, creating a challenging and worthwhile area for experimentation in techniques for sentiment analysis. We describe preliminary statistical tests on a new dataset of political discussion group postings which indicate that posts made in direct response to other posts in a thread have a strong tendency to represent an opposing political viewpoint to the original post. We conclude that traditional text classification methods will be inadequate to the task of sentiment analysis in this domain, and that progress is to be made by exploiting information about how posters interact with each	['For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .']	['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .']	['For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .', 'Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.']	['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .', 'Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.']	['A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .']	['For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .', 'Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.']	['A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .', 'Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.']	['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .', 'Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.']	['Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .']	['For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .', 'Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.']
CCT85	D13-1115	The first work to do this with topic models is #AUTHOR_TAGb ) .	a holistic lexiconbased approach to opinion mining	how many words is a picture worth automatic caption generation for news images	['Yansong Feng', 'Mirella Lapata']	related work	Background	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAGb ) .', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']	0	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']	In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.		In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.	['The first work to do this with topic models is #AUTHOR_TAGb ) .']	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAGb ) .']	['The first work to do this with topic models is #AUTHOR_TAGb ) .', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.']	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAGb ) .']	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAGb ) .']	['The first work to do this with topic models is #AUTHOR_TAGb ) .', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.']	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAGb ) .', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.']	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAGb ) .', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.']	['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAGb ) .']	['The first work to do this with topic models is #AUTHOR_TAGb ) .', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.']
CCT86	W04-0910	The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	an algebra for semantic construction in constraintbased grammars	['A Copestake', 'A Lascarides', 'D Flickinger']		CompareOrContrast	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']	1	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']		We develop a framework for formalizing semantic construction within grammars expressed in typed feature structure logics, including HPSG. The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unificationbased approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability.	We develop a framework for formalizing semantic construction within grammars expressed in typed feature structure logics, including HPSG. The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unificationbased approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability.	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .']	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .']	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .']	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .']	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.']	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .']	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .']	['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']
CCT87	J01-4001	Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) .	a holistic lexiconbased approach to opinion mining	resolving pronoun references	['Jerry Hobbs']		Background	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']"	0	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']"	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']"		Two approaches to the problem of resolving pronoun references are presented. The first is a naive algorithm that works by traversing the surface parse trees of the sentences of the text in a particular order looking for noun phrases of the correct gender and number. The algorithm clearly does not work in all cases, but the results of an examination of several hundred examples from published texts show that it performs remarkably well. In the second approach, it is shown how pronoun solution can be handled in a comprehensive system for semantic analysis of English texts. The system is described, and it is shown in a detailed treatment of several examples how semantic analysis locates the antecedents of most pronouns as a by-product. Included are the classic examples of Winograd and Charniak.	Two approaches to the problem of resolving pronoun references are presented. The first is a naive algorithm that works by traversing the surface parse trees of the sentences of the text in a particular order looking for noun phrases of the correct gender and number. The algorithm clearly does not work in all cases, but the results of an examination of several hundred examples from published texts show that it performs remarkably well. In the second approach, it is shown how pronoun solution can be handled in a comprehensive system for semantic analysis of English texts. The system is described, and it is shown in a detailed treatment of several examples how semantic analysis locates the antecedents of most pronouns as a by-product. Included are the classic examples of Winograd and Charniak.	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) .""]"	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) .""]"	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.']"	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.']"	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) .""]"	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']"	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.']"	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']"	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) .""]"	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']"
CCT88	D10-1101	For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .	a holistic lexiconbased approach to opinion mining	biographies bollywood boomboxes and blenders domain adaptation for sentiment classification	['John Blitzer', 'Mark Dredze', 'Fernando Pereira']	conclusion	Future	['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.', 'Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'Our CRF-based approach also yields promising results in the crossdomain setting.', 'The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .', 'Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.']	3	['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.', 'Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'Our CRF-based approach also yields promising results in the crossdomain setting.', 'The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .', 'Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.']	['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.', 'Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'Our CRF-based approach also yields promising results in the crossdomain setting.', 'The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .', 'Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.']	Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.		Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.	['For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .']	['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .']	['For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .']	['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .']	['We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .']	['For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .']	['We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .']	['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .']	['In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.', 'We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .']	['For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .', 'Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.']
CCT89	N01-1013	The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .	a holistic lexiconbased approach to opinion mining	accurate methods for the statistics of surprise and coincidence	['T Dunning']	experiments	CompareOrContrast	['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']	1	[]	[]	Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.		Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.	['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']	['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']	['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']	['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']	['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']	['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']	['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']	['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']	['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']	['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']
CCT90	D12-1084	Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .	a holistic lexiconbased approach to opinion mining	paraphrase fragment extraction from monolingual comparable corpora	['Rui Wang', 'Chris Callison-Burch']		Extends	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.', '5.3).', 'Finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs.']	2	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.', '5.3).', 'Finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs.']	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.', '5.3).', 'Finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs.']	We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different arti-cles about the same topics or events. The pro-cedure consists of document pair extraction, sentence pair extraction, and fragment pair ex-traction. At each stage, we evaluate the in-termediate results manually, and tune the later stages accordingly. With this minimally su-pervised approach, we achieve 62 % of accu-racy on the paraphrase fragment pairs we col-lected and 67 % extracted from the MSR cor-pus. The results look promising, given the minimal supervision of the approach, which can be further scaled up.		We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different arti-cles about the same topics or events. The pro-cedure consists of document pair extraction, sentence pair extraction, and fragment pair ex-traction. At each stage, we evaluate the in-termediate results manually, and tune the later stages accordingly. With this minimally su-pervised approach, we achieve 62 % of accu-racy on the paraphrase fragment pairs we col-lected and 67 % extracted from the MSR cor-pus. The results look promising, given the minimal supervision of the approach, which can be further scaled up.	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .']	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .']	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.']	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.']	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .']	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.', '5.3).']	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.']	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.', '5.3).']	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .']	['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.', '5.3).', 'Finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs.']
CCT91	P13-3018	There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	and orthographic similarity in visual word recognition	['E Drews', 'P Zwitserlood']	introduction	Background	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']	0	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']	The differential impact of orthographic and morphological relatedness on visual word recognition was investigated in a series of priming experiments in Dutch and German. With lexical decision and naming tasks, repetition priming and contiguous priming procedures, and masked and unmasked prime presentation, a pattern of results emerged with qualitative differences between the effects of morphological and form relatedness. With lexical decision, mere orthographic similarity between primes and targets (e.g., keller-KELLER, cellar-ladle) produced negative effects, whereas morphological relatedness (e.g., kellen-KELLE, ladles-ladle) consistently resulted in facilitation. With the naming task, positive priming effects were found for morphological as well as for mere form similarity. On the basis of these results, a model of the lexicon is proposed in which information about word form is represented separately from morphological structure and in which processing at the form level is characterized in terms of activation of, and competition between, form-related entries.		The differential impact of orthographic and morphological relatedness on visual word recognition was investigated in a series of priming experiments in Dutch and German. With lexical decision and naming tasks, repetition priming and contiguous priming procedures, and masked and unmasked prime presentation, a pattern of results emerged with qualitative differences between the effects of morphological and form relatedness. With lexical decision, mere orthographic similarity between primes and targets (e.g., keller-KELLER, cellar-ladle) produced negative effects, whereas morphological relatedness (e.g., kellen-KELLE, ladles-ladle) consistently resulted in facilitation. With the naming task, positive priming effects were found for morphological as well as for mere form similarity. On the basis of these results, a model of the lexicon is proposed in which information about word form is represented separately from morphological structure and in which processing at the form level is characterized in terms of activation of, and competition between, form-related entries.	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .']	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .']	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.']	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.']	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .']	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.']	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.']	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.']	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .']	['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).']
CCT92	P97-1063	This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .	a holistic lexiconbased approach to opinion mining	the mathematics of statistical machine translation parameter estimationquot	['P F Brown', 'V J Della Pietra', 'S A Della Pietra', 'R L Mercer']	method	CompareOrContrast	['To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']	1	[]	['To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']		We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.	We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.	['This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .']	['To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .']	['This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).']	['To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).']	['Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .']	['This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']	['Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).']	['To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']	['Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .']	['This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']
CCT93	K15-1003	One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .	a holistic lexiconbased approach to opinion mining	a generative constituentcontext model for improved grammar induction	['Dan Klein', 'Christopher D Manning']	introduction	Background	"['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB.', 'This DET-VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN.', 'From this, we might deduce that DET-VERB is a likely context for a noun phrase.', 'CCM is able to learn which POS contexts are likely, and does so via a probabilistic generative model, providing a statistical, data-driven take on substitutability.', 'However, since there is nothing intrinsic about the POS pair DET-VERB that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data.', 'Baldridge (2008) observed that unlike opaque, atomic POS labels, the rich structures of Combinatory Categorial Grammar (CCG) (Steedman, 2000;Steedman and Baldridge, 2011) categories reflect universal grammatical properties.', 'CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents.', 'For example, a category might encode that ""this constituent can combine with a noun phrase to the right (an object) and then a noun phrase to the left (a subject) to produce a sentence"" instead of simply VERB.', 'CCG has proven useful as a framework for grammar induction due to its ability to incorporate linguistic knowledge to guide parser learning by, for example, specifying rules in lexical-expansion algorithms (Bisk and Hockenmaier, 2012;2013) or encoding that information as priors within a Bayesian framework (Garrette et al., 2015).']"	0	"['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB.', 'This DET-VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN.', 'From this, we might deduce that DET-VERB is a likely context for a noun phrase.', 'CCM is able to learn which POS contexts are likely, and does so via a probabilistic generative model, providing a statistical, data-driven take on substitutability.', 'However, since there is nothing intrinsic about the POS pair DET-VERB that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data.', 'Baldridge (2008) observed that unlike opaque, atomic POS labels, the rich structures of Combinatory Categorial Grammar (CCG) (Steedman, 2000;Steedman and Baldridge, 2011) categories reflect universal grammatical properties.', 'CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents.', 'For example, a category might encode that ""this constituent can combine with a noun phrase to the right (an object) and then a noun phrase to the left (a subject) to produce a sentence"" instead of simply VERB.', 'CCG has proven useful as a framework for grammar induction due to its ability to incorporate linguistic knowledge to guide parser learning by, for example, specifying rules in lexical-expansion algorithms (Bisk and Hockenmaier, 2012;2013) or encoding that information as priors within a Bayesian framework (Garrette et al., 2015).']"	"['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB.', 'This DET-VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN.', 'From this, we might deduce that DET-VERB is a likely context for a noun phrase.', 'CCM is able to learn which POS contexts are likely, and does so via a probabilistic generative model, providing a statistical, data-driven take on substitutability.', 'However, since there is nothing intrinsic about the POS pair DET-VERB that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data.', 'Baldridge (2008) observed that unlike opaque, atomic POS labels, the rich structures of Combinatory Categorial Grammar (CCG) (Steedman, 2000;Steedman and Baldridge, 2011) categories reflect universal grammatical properties.', 'CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents.', 'For example, a category might encode that ""this constituent can combine with a noun phrase to the right (an object) and then a noun phrase to the left (a subject) to produce a sentence"" instead of simply VERB.', 'CCG has proven useful as a framework for grammar induction due to its ability to incorporate linguistic knowledge to guide parser learning by, for example, specifying rules in lexical-expansion algorithms (Bisk and Hockenmaier, 2012;2013) or encoding that information as priors within a Bayesian framework (Garrette et al., 2015).']"		We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published un-supervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on non-trivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task.	We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published un-supervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on non-trivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task.	['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .']	['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .']	['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.']	['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.']	['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .']	['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB.']	['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.']	['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB.']	['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .']	['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB.', 'This DET-VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN.']
CCT94	W06-2933	Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .	a holistic lexiconbased approach to opinion mining	stylebook for the japanese treebank in verbmobil verbmobilreport 240 seminar fur sprachwissenschaft	['Y Kawata', 'J Bartels']	experiments	CompareOrContrast	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian (Simov et al., 2005;Simov and Osenova, 2003), Chinese (Chen et al., 2003), Danish (Kromann, 2003), and Swedish .', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']	1	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian (Simov et al., 2005;Simov and Osenova, 2003), Chinese (Chen et al., 2003), Danish (Kromann, 2003), and Swedish .', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian (Simov et al., 2005;Simov and Osenova, 2003), Chinese (Chen et al., 2003), Danish (Kromann, 2003), and Swedish .', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']				['Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']	['Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']	['before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']	['Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']	['before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']	['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']	['If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']	['Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']
CCT95	J13-1008	"7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) ."	a holistic lexiconbased approach to opinion mining	introduction to arabic natural language processing	['Nizar Habash']	experiments	Background	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	0	[]	['7 We ignore the rare `` false idafa  construction ( #AUTHOR_TAG , p. 102 ) .']	"Abstract This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues. The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters. Table of Contents: What is ""Arabic""? / Arabic Script / Arabic Phonology and Orthography / Arabic Mo..."		"Abstract This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues. The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters. Table of Contents: What is ""Arabic""? / Arabic Script / Arabic Phonology and Orthography / Arabic Mo..."	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"
CCT96	W06-1104	dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .	a holistic lexiconbased approach to opinion mining	verb semantics and lexical selection	['Zhibiao Wu', 'Martha Palmer']		Background	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	0	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994;Leacock and Chodorow, 1998), information-based (Resnik, 1995;Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005).', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994;Leacock and Chodorow, 1998), information-based (Resnik, 1995;Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005).', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.Comment: 6 pages, Figures and bib files are in part		This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.Comment: 6 pages, Figures and bib files are in part	['dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']
CCT97	D12-1037	Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .	a holistic lexiconbased approach to opinion mining	tuning as ranking	['Mark Hopkins', 'Jonathan May']	introduction	Background	['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']	0	['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood (Och and Ney, 2002;Blunsom et al., 2008), error rate (Och, 2003;Zhao and Chen, 2009;Pauls et al., 2009;Galley and Quirk, 2011), margin (Watanabe et al., 2007;Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one.', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']	['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood (Och and Ney, 2002;Blunsom et al., 2008), error rate (Och, 2003;Zhao and Chen, 2009;Pauls et al., 2009;Galley and Quirk, 2011), margin (Watanabe et al., 2007;Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one.', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']	We offer a simple, effective, and scalable method for statistical machine translation pa-rameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to imple-ment. It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours. We establish PRO's scalability and effective-ness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.		We offer a simple, effective, and scalable method for statistical machine translation pa-rameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to imple-ment. It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours. We establish PRO's scalability and effective-ness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.	['Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .']	['where f and e (e ) are source and target sentences, respectively.', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .']	['Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .']	['where f and e (e ) are source and target sentences, respectively.', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .']	['h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'where f and e (e ) are source and target sentences, respectively.', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .']	['Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .']	['h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'where f and e (e ) are source and target sentences, respectively.', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .']	['where f and e (e ) are source and target sentences, respectively.', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .']	['h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'where f and e (e ) are source and target sentences, respectively.', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .']	['Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.']
CCT98	K15-1003	We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .	a holistic lexiconbased approach to opinion mining	a ccg parsing with a supertagfactored model	['Mike Lewis', 'Mark Steedman']		Uses	['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X  X X of Clark and Curran (2007).']	5	['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X  X X of Clark and Curran (2007).']	['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X  X X of Clark and Curran (2007).']		We introduce a new CCG parsing model which is factored on lexical category assignments. Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation. The parser is extremely simple, with a tiny feature set, no POS tagger, and no statistical model of the derivation or dependencies. Formulating the model in this way allows a highly effective heuristic for A_ parsing, which makes parsing extremely fast. Compared to the standard C&C CCG parser, our model is more accurate out-of-domain, is four times faster, has higher coverage, and is greatly simplified. We also show that using our parser improves the performance of a state-ofthe-art question answering system.	We introduce a new CCG parsing model which is factored on lexical category assignments. Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation. The parser is extremely simple, with a tiny feature set, no POS tagger, and no statistical model of the derivation or dependencies. Formulating the model in this way allows a highly effective heuristic for A_ parsing, which makes parsing extremely fast. Compared to the standard C&C CCG parser, our model is more accurate out-of-domain, is four times faster, has higher coverage, and is greatly simplified. We also show that using our parser improves the performance of a state-ofthe-art question answering system.	['We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .']	['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .']	['We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X - X X of Clark and Curran (2007).']	['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X - X X of Clark and Curran (2007).']	['The direction of the slash operator gives the behavior of the function.', 'Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .']	['We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X - X X of Clark and Curran (2007).']	['The direction of the slash operator gives the behavior of the function.', 'Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X - X X of Clark and Curran (2007).']	['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X - X X of Clark and Curran (2007).']	['A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'The direction of the slash operator gives the behavior of the function.', 'Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .']	['We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X - X X of Clark and Curran (2007).']
CCT99	P07-1068	( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .	a holistic lexiconbased approach to opinion mining	automatic retrieval and clustering of similar words	['D Lin']	introduction	Motivation	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1, while others appear to be correlated with these ACE SCs. 2 (6) INDUCED CLASS: Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992)).', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"	4	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1, while others appear to be correlated with these ACE SCs. 2 (6) INDUCED CLASS: Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992)).', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1, while others appear to be correlated with these ACE SCs. 2 (6) INDUCED CLASS: Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992)).', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"	Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.		Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.	['( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .']	['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .']	"['( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP.""]"	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP.""]"	['Again, this represents our attempt to coarsely model subcategorization.', '(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .']	"['( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"	"['Again, this represents our attempt to coarsely model subcategorization.', '(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP.""]"	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"	"[""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'Again, this represents our attempt to coarsely model subcategorization.', '(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .']"	"['( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"
CCT100	J90-3003	Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .	a holistic lexiconbased approach to opinion mining	performance structures a psycholinguistic and linguistic appraisal	['J P Gee', 'F Grosjean']	introduction	CompareOrContrast	['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']	1	[]	['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']		Two lines of researchone in psycholinguistics and one in linguisticsare combined to deal with a long-standing problem in both fields: why the performance structures of sentences (structures based on experimental data, such as pausing and parsing values) are not fully accountable for by linguistic theories of phrase structure. Two psycholinguistic algorithms that have been used to predict these structures are described and their limitations are examined. A third algorithm, based on the prosodic structures of sentences is then proposed and shown to be a far better predictor of performance structures. It is argued that the experimental data reflect aspects of the linguistic cognitive capacity, and that, in turn, linguistic theory can offer an illuminating account of the data. The prosodic model is shown to have a wider domain of application than temporal organization per se, accounting for parsing judgments as well as pausing performance, and reflecting aspects of syntactic and semantic structure as well as purely prosodic structure. Finally, the algorithm is discussed in light of language processing.	Two lines of researchone in psycholinguistics and one in linguisticsare combined to deal with a long-standing problem in both fields: why the performance structures of sentences (structures based on experimental data, such as pausing and parsing values) are not fully accountable for by linguistic theories of phrase structure. Two psycholinguistic algorithms that have been used to predict these structures are described and their limitations are examined. A third algorithm, based on the prosodic structures of sentences is then proposed and shown to be a far better predictor of performance structures. It is argued that the experimental data reflect aspects of the linguistic cognitive capacity, and that, in turn, linguistic theory can offer an illuminating account of the data. The prosodic model is shown to have a wider domain of application than temporal organization per se, accounting for parsing judgments as well as pausing performance, and reflecting aspects of syntactic and semantic structure as well as purely prosodic structure. Finally, the algorithm is discussed in light of language processing.	['Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']	['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']	['Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']	['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']	['For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']	['Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']	['For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']	['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']	['Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']	['Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']
CCT101	D15-1148	Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .	a holistic lexiconbased approach to opinion mining	assessing the discourse factors that influence the quality of machine translation	['Junyi Jessy Li', 'Marine Carpuat', 'Ani Nenkova']	related work	CompareOrContrast	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', 'This prior work was carried over a dataset containing a single reference translation for each Chinese sentence.', 'In the work presented in this paper, we strengthen our findings by examining multiple reference translations for each Chinese sentence.', 'We define heavy sentences based on agreement of translator choices and reader preferences.']	1	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', 'This prior work was carried over a dataset containing a single reference translation for each Chinese sentence.', 'In the work presented in this paper, we strengthen our findings by examining multiple reference translations for each Chinese sentence.', 'We define heavy sentences based on agreement of translator choices and reader preferences.']	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', 'This prior work was carried over a dataset containing a single reference translation for each Chinese sentence.', 'In the work presented in this paper, we strengthen our findings by examining multiple reference translations for each Chinese sentence.', 'We define heavy sentences based on agreement of translator choices and reader preferences.']	We present a study of aspects of discourse structure -- specifically discourse devices used to organize information in a sen-tence -- that significantly impact the qual-ity of machine translation. Our analysis is based on manual evaluations of trans-lations of news from Chinese and Ara-bic to English. We find that there is a particularly strong mismatch in the no-tion of what constitutes a sentence in Chi-nese and English, which occurs often and is associated with significant degradation in translation quality. Also related to lower translation quality is the need to em-ploy multiple explicit discourse connec-tives (because, but, etc.), as well as the presence of ambiguous discourse connec-tives in the English translation. Further-more, the mismatches between discourse expressions across languages significantly impact translation quality.		We present a study of aspects of discourse structure -- specifically discourse devices used to organize information in a sen-tence -- that significantly impact the qual-ity of machine translation. Our analysis is based on manual evaluations of trans-lations of news from Chinese and Ara-bic to English. We find that there is a particularly strong mismatch in the no-tion of what constitutes a sentence in Chi-nese and English, which occurs often and is associated with significant degradation in translation quality. Also related to lower translation quality is the need to em-ploy multiple explicit discourse connec-tives (because, but, etc.), as well as the presence of ambiguous discourse connec-tives in the English translation. Further-more, the mismatches between discourse expressions across languages significantly impact translation quality.	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .']	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .']	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.']	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.']	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .']	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', 'This prior work was carried over a dataset containing a single reference translation for each Chinese sentence.']	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.']	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', 'This prior work was carried over a dataset containing a single reference translation for each Chinese sentence.']	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .']	['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', 'This prior work was carried over a dataset containing a single reference translation for each Chinese sentence.', 'In the work presented in this paper, we strengthen our findings by examining multiple reference translations for each Chinese sentence.']
CCT102	J13-1008	"For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering)."	a holistic lexiconbased approach to opinion mining	an efficient algorithm for projective dependency parsing	['Joakim Nivre']	related work	Uses	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.', 'All experiments were done using the Nivre ""eager"" algorithm.', '11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on.', 'Kbler, McDonald, and Nivre (2009) describe a ""typical"" MaltParser model configuration of attributes and features.', '13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3-1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration.', 'To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre ""eager"" algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']"	5	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1.3 (Nivre 2003(Nivre , 2008Kbler, McDonald, and Nivre 2009), a transition-based parser with an input buffer and a stack, which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.', 'All experiments were done using the Nivre ""eager"" algorithm.', '11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on.', 'Kbler, McDonald, and Nivre (2009) describe a ""typical"" MaltParser model configuration of attributes and features.', '13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3-1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration.', 'To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre ""eager"" algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']"	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1.3 (Nivre 2003(Nivre , 2008Kbler, McDonald, and Nivre 2009), a transition-based parser with an input buffer and a stack, which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.', 'All experiments were done using the Nivre ""eager"" algorithm.', '11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on.', 'Kbler, McDonald, and Nivre (2009) describe a ""typical"" MaltParser model configuration of attributes and features.', '13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3-1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration.', 'To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre ""eager"" algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']"	This paper presents a deterministic parsing algorithm for projective dependency grammar. The running time of the algorithm is linear in the length of the input string, and the dependency graph produced is guaranteed to be projective and acyclic. The algorithm has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85% with a very simple grammar.		This paper presents a deterministic parsing algorithm for projective dependency grammar. The running time of the algorithm is linear in the length of the input string, and the dependency graph produced is guaranteed to be projective and acyclic. The algorithm has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85% with a very simple grammar.	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).']"	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).']"	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").']"	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").']"	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).']"	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.']"	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").']"	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.']"	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).']"	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; Kbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.', 'All experiments were done using the Nivre ""eager"" algorithm.']"
CCT103	J01-4001	Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .	a holistic lexiconbased approach to opinion mining	robust reference resolution with limited knowledge high precision genrespecific approach for english and polish	['Ruslan Mitkov', 'Malgorzata Stys']		Background	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']	0	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']	Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labourand time-consuming task. This paper presents a robust, knowledgepoor approach to resolving pronouns in technical manuals in both English and Polish. This approach is a modification of the practical approach reported in [Mitkov 97] and operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and tested for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Preliminary evaluation reports precision of over 90%.		Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labourand time-consuming task. This paper presents a robust, knowledgepoor approach to resolving pronouns in technical manuals in both English and Polish. This approach is a modification of the practical approach reported in [Mitkov 97] and operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and tested for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Preliminary evaluation reports precision of over 90%.	['Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .']	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .']	['Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.']	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.']	['The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .']	['Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']	['The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.']	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']	['The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .']	['Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']
CCT104	D08-1034	We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	automatic semantic role labeling for chinese verbs	['Nianwen Xue', 'Martha Palmer']	experiments	CompareOrContrast	['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'to chtb_931.fid,', 'or the first 250K words of the Chinese TreeBank 5.1.', 'For the experiments, the data of PropBank is divided into three parts.', '648 files (from chtb_081 to chtb_899.fid)', 'are used as the training set.', 'The development set includes 40 files, from chtb_041.fid to chtb_080.fid.', 'The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	1	['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'to chtb_931.fid,', 'or the first 250K words of the Chinese TreeBank 5.1.', 'For the experiments, the data of PropBank is divided into three parts.', '648 files (from chtb_081 to chtb_899.fid)', 'are used as the training set.', 'The development set includes 40 files, from chtb_041.fid to chtb_080.fid.', 'The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'to chtb_931.fid,', 'or the first 250K words of the Chinese TreeBank 5.1.', 'For the experiments, the data of PropBank is divided into three parts.', '648 files (from chtb_081 to chtb_899.fid)', 'are used as the training set.', 'The development set includes 40 files, from chtb_041.fid to chtb_080.fid.', 'The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling."		"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling."	['We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	['We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	['PropBank 1.0 includes the annotations for files chtb_001.fid', 'We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	['We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	['PropBank 1.0 includes the annotations for files chtb_001.fid', 'We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	['to chtb_931.fid,', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']	['We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']
CCT105	P11-1134	They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .	a holistic lexiconbased approach to opinion mining	extending the meteor machine translation evaluation metric to the phrase level	['Michael Denkowski', 'Alon Lavie']		Motivation	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']	4	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']	This paper presents METEOR-NEXT, an ex-tended version of the METEOR metric de-signed to have high correlation with post-editing measures of machine translation qual-ity. We describe changes made to the met-ric's sentence aligner and scoring scheme as well as a method for tuning the metric's pa-rameters to optimize correlation with human-targeted Translation Edit Rate (HTER). We then show that METEOR-NEXT improves cor-relation with HTER over baseline metrics, in-cluding earlier versions of METEOR, and ap-proaches the correlation level of a state-of-the-art metric, TER-plus (TERp).		This paper presents METEOR-NEXT, an ex-tended version of the METEOR metric de-signed to have high correlation with post-editing measures of machine translation qual-ity. We describe changes made to the met-ric's sentence aligner and scoring scheme as well as a method for tuning the metric's pa-rameters to optimize correlation with human-targeted Translation Edit Rate (HTER). We then show that METEOR-NEXT improves cor-relation with HTER over baseline metrics, in-cluding earlier versions of METEOR, and ap-proaches the correlation level of a state-of-the-art metric, TER-plus (TERp).	['They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .']	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .']	['They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).']	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .']	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .']	['They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.']	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).']	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.']	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .']	['They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']
CCT106	W06-2807	Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	the wiki way  quick collaboration on the web	['Ward Cunningham', 'Bo Leuf']	introduction	Background	['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.', 'Consequently, except in the case of sandboxes, every change in the document cannot be erased.', 'This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself.']	0	['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.', 'Consequently, except in the case of sandboxes, every change in the document cannot be erased.', 'This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself.']	['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.', 'Consequently, except in the case of sandboxes, every change in the document cannot be erased.', 'This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself.']	"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001"		"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001"	['Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .']	['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .']	['Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.']	['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .']	['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .']	['Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.']	['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.']	['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.']	['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .']	['Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.', 'Consequently, except in the case of sandboxes, every change in the document cannot be erased.']
CCT107	D09-1143	Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .	a holistic lexiconbased approach to opinion mining	effective use of wordnet semantics via kernelbased learning	['Roberto Basili', 'Marco Cammisa', 'Alessandro Moschitti']	conclusion	Future	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	3	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	Research on document similarity has shown that complex representations are not more accurate than the simple bag-of-words. Term clustering, e.g. using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective. In particular, when to extend the similarity function external prior knowledge is used, e.g. WordNet, the retrieval system decreases its performance. The critical issues here are methods and conditions to integrate such knowledge.    In this paper we propose kernel functions to add prior knowledge to learning algorithms for document classification. Such kernels use a term similarity measure based on the WordNet hierarchy. The kernel trick is used to implement such space in a balanced and statistically coherent way. Cross-validation results show the benefit of the approach for the Support Vector Machines when few training data is available.		Research on document similarity has shown that complex representations are not more accurate than the simple bag-of-words. Term clustering, e.g. using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective. In particular, when to extend the similarity function external prior knowledge is used, e.g. WordNet, the retrieval system decreases its performance. The critical issues here are methods and conditions to integrate such knowledge.    In this paper we propose kernel functions to add prior knowledge to learning algorithms for document classification. Such kernels use a term similarity measure based on the WordNet hierarchy. The kernel trick is used to implement such space in a balanced and statistically coherent way. Cross-validation results show the benefit of the approach for the Support Vector Machines when few training data is available.	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']	['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']
CCT108	D13-1115	Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .	a holistic lexiconbased approach to opinion mining	perceptual inference through global lexical similarity	['Brendan T Johns', 'Michael N Jones']	introduction	Background	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	0	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009;Steyvers, 2010;Feng and Lapata, 2010b;Bruni et al., 2011;Silberer and Lapata, 2012;Johns and Jones, 2012;Bruni et al., 2012a;Bruni et al., 2012b;Silberer et al., 2013).', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009;Steyvers, 2010;Feng and Lapata, 2010b;Bruni et al., 2011;Silberer and Lapata, 2012;Johns and Jones, 2012;Bruni et al., 2012a;Bruni et al., 2012b;Silberer et al., 2013).', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	The literature contains a disconnect between accounts of how humans learn lexical semantic representations for words. Theories generally propose that lexical semantics are learned either through perceptual experience or through exposure to regularities in language. We propose here a model to integrate these two information sources. Specifically, the model uses the global structure of memory to exploit the redundancy between language and perception in order to generate inferred perceptual representations for words with which the model has no perceptual experience. We test the model on a variety of different datasets from grounded cognition experiments and demonstrate that this diverse set of results can be explained as perceptual simulation (cf. Barsalou, Simmons, Barbey, & Wilson, 2003) within a global memory model.Copyright (c) 2012 Cognitive Science Society, Inc.		The literature contains a disconnect between accounts of how humans learn lexical semantic representations for words. Theories generally propose that lexical semantics are learned either through perceptual experience or through exposure to regularities in language. We propose here a model to integrate these two information sources. Specifically, the model uses the global structure of memory to exploit the redundancy between language and perception in order to generate inferred perceptual representations for words with which the model has no perceptual experience. We test the model on a variety of different datasets from grounded cognition experiments and demonstrate that this diverse set of results can be explained as perceptual simulation (cf. Barsalou, Simmons, Barbey, & Wilson, 2003) within a global memory model.Copyright (c) 2012 Cognitive Science Society, Inc.	['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']	['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CCT109	W10-3814	Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .	a holistic lexiconbased approach to opinion mining	probabilistic inference for machine translation	['Phil Blunsom', 'Miles Osborne']	conclusion	Future	['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	3	[]	['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	Recent advances in statistical machine translation (SMT) have used dynamic programming  (DP) based beam search methods for approximate inference within probabilistic  translation models. Despite their success, these methods compromise the probabilistic  interpretation of the underlying model thus limiting the application of probabilistically  defined decision rules during training and decoding.  As an alternative, in this thesis, we propose a novel Monte Carlo sampling approach  for theoretically sound approximate probabilistic inference within these models. The  distribution we are interested in is the conditional distribution of a log-linear translation  model; however, often, there is no tractable way of computing the normalisation term  of the model. Instead, a Gibbs sampling approach for phrase-based machine translation  models is developed which obviates the need of computing this term yet produces  samples from the required distribution.  We establish that the sampler effectively explores the distribution defined by a  phrase-based models by showing that it converges in a reasonable amount of time to  the desired distribution, irrespective of initialisation. Empirical evidence is provided to  confirm that the sampler can provide accurate estimates of expectations of functions of  interest. The mix of high probability and low probability derivations obtained through  sampling is shown to provide a more accurate estimate of expectations than merely  using the n-most highly probable derivations.  Subsequently, we show that the sampler provides a tractable solution for finding the  maximum probability translation in the model. We also present a unified approach to  approximating two additional intractable problems: minimum risk training and minimum  Bayes risk decoding. Key to our approach is the use of the sampler which  allows us to explore the entire probability distribution and maintain a strict probabilistic  formulation through the translation pipeline. For these tasks, sampling allies  the simplicity of n-best list approaches with the extended view of the distribution that  lattice-based approaches benefit from, while avoiding the biases associated with beam  search. Our approach is theoretically well-motivated and can give better and more  stable results than current state of the art methods		Recent advances in statistical machine translation (SMT) have used dynamic programming  (DP) based beam search methods for approximate inference within probabilistic  translation models. Despite their success, these methods compromise the probabilistic  interpretation of the underlying model thus limiting the application of probabilistically  defined decision rules during training and decoding.  As an alternative, in this thesis, we propose a novel Monte Carlo sampling approach  for theoretically sound approximate probabilistic inference within these models. The  distribution we are interested in is the conditional distribution of a log-linear translation  model; however, often, there is no tractable way of computing the normalisation term  of the model. Instead, a Gibbs sampling approach for phrase-based machine translation  models is developed which obviates the need of computing this term yet produces  samples from the required distribution.  We establish that the sampler effectively explores the distribution defined by a  phrase-based models by showing that it converges in a reasonable amount of time to  the desired distribution, irrespective of initialisation. Empirical evidence is provided to  confirm that the sampler can provide accurate estimates of expectations of functions of  interest. The mix of high probability and low probability derivations obtained through  sampling is shown to provide a more accurate estimate of expectations than merely  using the n-most highly probable derivations.  Subsequently, we show that the sampler provides a tractable solution for finding the  maximum probability translation in the model. We also present a unified approach to  approximating two additional intractable problems: minimum risk training and minimum  Bayes risk decoding. Key to our approach is the use of the sampler which  allows us to explore the entire probability distribution and maintain a strict probabilistic  formulation through the translation pipeline. For these tasks, sampling allies  the simplicity of n-best list approaches with the extended view of the distribution that  lattice-based approaches benefit from, while avoiding the biases associated with beam  search. Our approach is theoretically well-motivated and can give better and more  stable results than current state of the art methods	['Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	['Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	['This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	['Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	['This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	['This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']	['Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']
CCT110	J92-1004	We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .	a holistic lexiconbased approach to opinion mining	integration of speech recognition and natural language processing in the mit voyager systemquot	['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']		Uses	"['When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', 'A simple word-pair grammar constrained the search space.', 'If the parse failed, then the sentence was rejected.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) ."", 'Both the A* and the Viterbi search are left-to-right search algorithms.', 'However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.', 'That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold.']"	5	"['When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', 'A simple word-pair grammar constrained the search space.', 'If the parse failed, then the sentence was rejected.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) ."", 'Both the A* and the Viterbi search are left-to-right search algorithms.', 'However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.', 'That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold.']"	"['When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', 'A simple word-pair grammar constrained the search space.', 'If the parse failed, then the sentence was rejected.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) ."", 'Both the A* and the Viterbi search are left-to-right search algorithms.', 'However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.', 'That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold.']"	The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>		The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>	"[""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"	"['When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"	"[""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"	"['When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"	"['A simple word-pair grammar constrained the search space.', 'When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"	"[""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"	"['A simple word-pair grammar constrained the search space.', 'When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"	"['When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"	"['If the parse failed, then the sentence was rejected.', 'A simple word-pair grammar constrained the search space.', 'When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"	"[""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) ."", 'Both the A* and the Viterbi search are left-to-right search algorithms.', 'However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.', 'That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold.']"
CCT111	W06-1705	#AUTHOR_TAG	a holistic lexiconbased approach to opinion mining	blueprint for a high performance nlp infrastructure	['J R Curran']	related work	Background	['#AUTHOR_TAG']	0	[]	[]	Natural Language Processing (NLP) system de-velopers face a number of new challenges. In-terest is increasing for real-world systems that use NLP tools and techniques. The quantity of text now available for training and processing is increasing dramatically. Also, the range of languages and tasks being researched contin-ues to grow rapidly. Thus it is an ideal time to consider the development of new experimen-tal frameworks. We describe the requirements, initial design and exploratory implementation of a high performance NLP infrastructure.		Natural Language Processing (NLP) system de-velopers face a number of new challenges. In-terest is increasing for real-world systems that use NLP tools and techniques. The quantity of text now available for training and processing is increasing dramatically. Also, the range of languages and tasks being researched contin-ues to grow rapidly. Thus it is an ideal time to consider the development of new experimen-tal frameworks. We describe the requirements, initial design and exploratory implementation of a high performance NLP infrastructure.	['#AUTHOR_TAG']	['#AUTHOR_TAG']	['#AUTHOR_TAG']	['#AUTHOR_TAG']	['#AUTHOR_TAG']	['#AUTHOR_TAG']	['#AUTHOR_TAG']	['#AUTHOR_TAG']	['#AUTHOR_TAG']	['#AUTHOR_TAG']
CCT112	J00-2014	"OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question ."	a holistic lexiconbased approach to opinion mining	efficient generation in primitive optimality theory	['Jason Eisner']	introduction	Background	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"	0	[]	['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of `` simple constraints  ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']	"This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT. OTP specifies the class of autosegmental representations, the universal generator Gen, and the two simple families of permissible constraints. In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994). Unfortunately these methods take time exponential on the size of the grammar. Indeed the generation problem is shown NP-complete in this sense. However, techniques are discussed for making Ellison's approach fast in the typical case, including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size. One avenue for future improvements is a new finite-state notion, ""factored automata,"" where regular languages are represented compactly via formal intersections ki=1Ai of FSAs."		"This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT. OTP specifies the class of autosegmental representations, the universal generator Gen, and the two simple families of permissible constraints. In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994). Unfortunately these methods take time exponential on the size of the grammar. Indeed the generation problem is shown NP-complete in this sense. However, techniques are discussed for making Ellison's approach fast in the typical case, including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size. One avenue for future improvements is a new finite-state notion, ""factored automata,"" where regular languages are represented compactly via formal intersections ki=1Ai of FSAs."	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"
CCT113	J97-4003	Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .	a holistic lexiconbased approach to opinion mining	headdriven phrase structure grammar	['Carl Pollard', 'Ivan Sag']	introduction	Background	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2  The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']	0	[]	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2  The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']				['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .']	['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .']	['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .']	['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']
CCT114	D13-1115	This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .	a holistic lexiconbased approach to opinion mining	integrating experiential and distributional data to learn semantic representations	['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']	experiments	CompareOrContrast	['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	1	['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.		The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.	['This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']
CCT115	W06-1104	#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .	a holistic lexiconbased approach to opinion mining	using information content to evaluate semantic similarity	['Philip Resnik']	experiments	CompareOrContrast	['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']	1	[]	['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']	Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.		Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.	['#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']	['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']	['#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']	['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']	['This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']	['#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']	['This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']	['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']	['Resnik (1995) reported a correlation of r=.9026. 10', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']	['#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']
CCT116	P13-3018	Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .	a holistic lexiconbased approach to opinion mining	lexical representation of derivational relation	['D Bradley']	related work	Background	['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']	0	['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by (Bertram et al., 2000;Bradley, 1980;Burani et al., 1987;Burani et al., 1984;Schreuder et al., 1997;Taft 1975;Taft, 2004) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']	['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by (Bertram et al., 2000;Bradley, 1980;Burani et al., 1987;Burani et al., 1984;Schreuder et al., 1997;Taft 1975;Taft, 2004) where it has been claimed that words having low surface frequency tends to decompose.', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']				['Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .']	['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .']	['Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.']	['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.']	['(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .']	['Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']	['(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.']	['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']	['(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .']	['Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']
CCT117	J10-3007	But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .	a holistic lexiconbased approach to opinion mining	inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora	['David Yarowsky', 'Grace Ngai']	introduction	Motivation	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"	4	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001;Hwa et al. 2005;Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']"	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001;Hwa et al. 2005;Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']"	This paper investigates the potential for projecting linguistic annotations including part-of-speech tags and base noun phrase bracketings from one language to another via automatically word-aligned parallel corpora. First, experiments assess the accuracy of unmodified direct transfer of tags and brackets from the source language English to the target languages French and Chinese, both for noisy machine-aligned sentences and for clean hand-aligned sentences. Performance is then substantially boosted over both of these baselines by using training techniques optimized for very noisy data, yielding 94-96% core French part-of-speech tag accuracy and 90% French bracketing F-measure for stand-alone monolingual tools trained without the need for any human-annotated data in the given language.		This paper investigates the potential for projecting linguistic annotations including part-of-speech tags and base noun phrase bracketings from one language to another via automatically word-aligned parallel corpora. First, experiments assess the accuracy of unmodified direct transfer of tags and brackets from the source language English to the target languages French and Chinese, both for noisy machine-aligned sentences and for clean hand-aligned sentences. Performance is then substantially boosted over both of these baselines by using training techniques optimized for very noisy data, yielding 94-96% core French part-of-speech tag accuracy and 90% French bracketing F-measure for stand-alone monolingual tools trained without the need for any human-annotated data in the given language.	['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"	['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"	"['Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"	['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']	"['Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"	"['Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"	['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']
CCT118	W06-1639	Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).	a holistic lexiconbased approach to opinion mining	seeing stars when there arent many stars graphbased semisupervised learning for sentiment categorization	['A B Goldberg', 'J Zhu']	related work	Background	['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	0	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005;Pang and Lee, 2005;Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005;Pang and Lee, 2005;Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	"We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., ""4 stars""), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training."		"We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., ""4 stars""), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training."	['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']	['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']
CCT119	J92-1004	However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .	a holistic lexiconbased approach to opinion mining	development and preliminary evaluation of the mit atis systemquot	['S Seneff', 'J Glass', 'D Goddeau', 'D Goodine', 'L Hirschman', 'H Leung', 'M Phillips', 'J Polifroni', 'V Zue']		Uses	"['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).', 'For example, a [subject] is a noun-phrase node with the label ""topic.""', 'During the top-down cycle, it creates a blank frame and inserts it into a ""topic"" slot in the frame that was handed to it.', 'It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.', 'It then passes along to the right sibling the same frame that was handed to it from above, with the completed topic slot filled with the information delivered by the children.']"	5	"['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).', 'For example, a [subject] is a noun-phrase node with the label ""topic.""', 'During the top-down cycle, it creates a blank frame and inserts it into a ""topic"" slot in the frame that was handed to it.', 'It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.', 'It then passes along to the right sibling the same frame that was handed to it from above, with the completed topic slot filled with the information delivered by the children.']"	"['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).', 'For example, a [subject] is a noun-phrase node with the label ""topic.""', 'During the top-down cycle, it creates a blank frame and inserts it into a ""topic"" slot in the frame that was handed to it.', 'It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.', 'It then passes along to the right sibling the same frame that was handed to it from above, with the completed topic slot filled with the information delivered by the children.']"		This paper represents a status report on the MIT ATIS system. The most significant new achievement is that we now have a speech-input mode. It is based on the MIT SUMMIT system using context independent phone models, and includes a word-pair grammar with perplexity 92 (on the June-90 test set). In addition, we have completely redesigned the back-end component, in order to emphasize portability and extensibility. The parser now produces an intermediate semantic frame representation, which serves as the focal point for all back-end operations, such as history management, text generation, and SQL query generation. Most of those aspects of the system that are tied to a particular domain are now entered through a set of tables associated with a small artificial language for decoding them. We have also improved the display of the database table, making it considerably easier for a subject to comprehend the information given. We report here on the results of the official DARPA February-91 evaluation, as well as on results of an evaluation on data collected at MIT, for both speech input and text input.	This paper represents a status report on the MIT ATIS system. The most significant new achievement is that we now have a speech-input mode. It is based on the MIT SUMMIT system using context independent phone models, and includes a word-pair grammar with perplexity 92 (on the June-90 test set). In addition, we have completely redesigned the back-end component, in order to emphasize portability and extensibility. The parser now produces an intermediate semantic frame representation, which serves as the focal point for all back-end operations, such as history management, text generation, and SQL query generation. Most of those aspects of the system that are tied to a particular domain are now entered through a set of tables associated with a small artificial language for decoding them. We have also improved the display of the database table, making it considerably easier for a subject to comprehend the information given. We report here on the results of the official DARPA February-91 evaluation, as well as on results of an evaluation on data collected at MIT, for both speech input and text input.	['However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .']	['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .']	['However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).']	['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .']	['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .']	['However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.']	['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).']	['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.']	['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .']	['However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).']
CCT120	W06-2807	Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .	a holistic lexiconbased approach to opinion mining	genre under construction the diary on the internet	['Laurie McNeill']	introduction	Background	"['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example, a 'web page' is more similar to an infinite canvas than a written page (McCloud, 2001)."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']"	0	"['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example, a 'web page' is more similar to an infinite canvas than a written page (McCloud, 2001)."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']"	"['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example, a 'web page' is more similar to an infinite canvas than a written page (McCloud, 2001)."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']"	"The emergence of the diary as a digital form has generated the kinds of introduction and explanation that typically accumulate around emerging genres, even though online diarists in many ways strive to reproduce the stereotypical print diary. However, as diarists and readers explore the nature of blogs, both in diary entries and comments pages, a tension is apparent between users ' accounts or explanations of the genre and their actual practices, and this tension provides a rich site for studying the evolution of the diary genre. Readers ' and writers ' comments illustrate the blogging community's ideas about genre as a concept and how these ideas transfer to the ""new "" world of online media. In this paper, I look at the diary's transition from page to screen, and consider how readers and writers build on and diverge from print culture practices in establishing expectations and ""rules "" for Weblogs. Examining how diarists and their communities establish and police the digital diary, and how generic knowledge is circulated and codified, helps understand the particular social actions the diary can perform only on the Internet. In contemporary Western culture the diary is among the most familiar genres, an enduring everyday form that has evolved and adapted across time yet has remained recognizable in each instance. As a genre that straddles and confuses ideals of the literary and non-literary"		"The emergence of the diary as a digital form has generated the kinds of introduction and explanation that typically accumulate around emerging genres, even though online diarists in many ways strive to reproduce the stereotypical print diary. However, as diarists and readers explore the nature of blogs, both in diary entries and comments pages, a tension is apparent between users ' accounts or explanations of the genre and their actual practices, and this tension provides a rich site for studying the evolution of the diary genre. Readers ' and writers ' comments illustrate the blogging community's ideas about genre as a concept and how these ideas transfer to the ""new "" world of online media. In this paper, I look at the diary's transition from page to screen, and consider how readers and writers build on and diverge from print culture practices in establishing expectations and ""rules "" for Weblogs. Examining how diarists and their communities establish and police the digital diary, and how generic knowledge is circulated and codified, helps understand the particular social actions the diary can perform only on the Internet. In contemporary Western culture the diary is among the most familiar genres, an enduring everyday form that has evolved and adapted across time yet has remained recognizable in each instance. As a genre that straddles and confuses ideals of the literary and non-literary"	['Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .']	['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .']	['Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .']	['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .']	"[""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", '1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .']"	['Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .']	"[""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", '1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .']"	['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .']	"['When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", '1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .']"	['Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?']
CCT121	N01-1006	The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .	a holistic lexiconbased approach to opinion mining	independence and commitment assumptions for rapid training and execution of rulebased pos taggers	['M Hepple']		Background	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	0	[]	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.		This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']
CCT122	W00-1017	To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	europa a generic framework for developing spoken dialogue systems	['Munehiko Sasajima', 'Yakehide Yano', 'Yasuyuld Kono']	introduction	Background	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.', 'However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'Another is GALAXY-II (Seneffet al., 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a dialogue system to communicate with each other.', 'It consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'Although it requires more specifications than finitestate-model-based toolkits, it places less limitations on system functions.']	0	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.', 'However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'Another is GALAXY-II (Seneffet al., 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a dialogue system to communicate with each other.', 'It consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'Although it requires more specifications than finitestate-model-based toolkits, it places less limitations on system functions.']	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.', 'However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'Another is GALAXY-II (Seneffet al., 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a dialogue system to communicate with each other.', 'It consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'Although it requires more specifications than finitestate-model-based toolkits, it places less limitations on system functions.']	Voice interfaces are not popular since they are neither useful nor user-friendly for non-specialist users. In this paper, EUROPA, a new framework for developing spoken dialogue systems, is introduced. In developing EUROPA, the authors focused on three points : (1) acceptance of spoken language, (2) portability in terms of domain and task, and (3) practical performance of the applied system. The framework is applied to prototyping a car navigation system called MINOS. MINOS is built on a portable PC, can process over 700 words of recognition vocabulary, and is able to respond to a user's question within a few seconds.		Voice interfaces are not popular since they are neither useful nor user-friendly for non-specialist users. In this paper, EUROPA, a new framework for developing spoken dialogue systems, is introduced. In developing EUROPA, the authors focused on three points : (1) acceptance of spoken language, (2) portability in terms of domain and task, and (3) practical performance of the applied system. The framework is applied to prototyping a car navigation system called MINOS. MINOS is built on a portable PC, can process over 700 words of recognition vocabulary, and is able to respond to a user's question within a few seconds.	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .']	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .']	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.']	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.']	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .']	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.']	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.']	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.']	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .']	['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.', 'However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.']
CCT123	W01-0706	Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .	a holistic lexiconbased approach to opinion mining	cascaded grammatical relation assignment	['S Buchholz', 'J Veenstra', 'W Daelemans']	introduction	Background	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']"	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']"	In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.		In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.	['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']	"['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']"	['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']
CCT124	W06-1639	Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .	a holistic lexiconbased approach to opinion mining	directionbased text interpretation as an information access refinement	['M Hearst']	introduction	Background	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"	0	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"		A Text-Based Intelligent System should provide more in-depth information about the contents of its corpus than does a standard information retrieval system, while at the same time avoiding the complexity and resource-consuming behavior of detailed text understanders. Instead of focusing on discovering documents that pertain to some topic of interest to the user, an approach is introduced based on the criterion of directionality (e.g., Is the agent in favor of, neutral, or opposed to the event?). A method is described for coercing sentence meanings into a metaphoric model such that the only semantic interpretation needed in order to determine the directionality of a sentence is done with respect to the model. This interpretation method is designed to be an integrated component of a hybrid information access system.	A Text-Based Intelligent System should provide more in-depth information about the contents of its corpus than does a standard information retrieval system, while at the same time avoiding the complexity and resource-consuming behavior of detailed text understanders. Instead of focusing on discovering documents that pertain to some topic of interest to the user, an approach is introduced based on the criterion of directionality (e.g., Is the agent in favor of, neutral, or opposed to the event?). A method is described for coercing sentence meanings into a metaphoric model such that the only semantic interpretation needed in order to determine the directionality of a sentence is done with respect to the model. This interpretation method is designed to be an integrated component of a hybrid information access system.	['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .']	['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .']	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"	['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .']	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"	['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .']	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"
CCT125	W06-1104	dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .	a holistic lexiconbased approach to opinion mining	using information content to evaluate semantic similarity	['Philip Resnik']		Background	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	0	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994;Leacock and Chodorow, 1998), information-based (Resnik, 1995;Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005).', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994;Leacock and Chodorow, 1998), information-based (Resnik, 1995;Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005).', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.		Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.	['dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']
CCT126	W05-0709	Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	a mentionsynchronous coreference resolution algorithm based on the bell tree	['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']	introduction	CompareOrContrast	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.']	1	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.']	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.']	This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.		This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .']	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .']	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).']	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).']	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .']	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.']	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).']	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.']	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .']	['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.']
CCT127	D08-1007	The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	cooccurrence retrieval a flexible framework for lexical distributional similarity	['Julie Weeds', 'David Weir']	method	CompareOrContrast	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'Our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric.']	1	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'Our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric.']	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'Our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric.']	Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events. In other applications, distributional similarity is taken to be an approximation to semantic similarity. However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted. In this work, a flexible, parameterized framework for calculating distributional similarity is proposed. Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework. In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity. First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship? Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity? Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type? Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity.		Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events. In other applications, distributional similarity is taken to be an approximation to semantic similarity. However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted. In this work, a flexible, parameterized framework for calculating distributional similarity is proposed. Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework. In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity. First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship? Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity? Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type? Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity.	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .']	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .']	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.']	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.']	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .']	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'Our approach, on the other hand, discriminatively sets millions of individual similarity values.']	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.']	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'Our approach, on the other hand, discriminatively sets millions of individual similarity values.']	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .']	['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'Our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric.']
CCT128	J91-2003	#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .	a holistic lexiconbased approach to opinion mining	the flow of thought and the flow of languagequot	['W L Chafe']	introduction	CompareOrContrast	['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']	1	['Although there are other discussions of the paragraph as a central element of discourse (e.g.', 'Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al. 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']	['Although there are other discussions of the paragraph as a central element of discourse (e.g.', 'Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al. 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure.', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']		A TV tuner has a housing and a rotating channel selector shaft mounted on the housing. There are a plurality of individual support members positioned by and mounted on parallel flatted areas of the shaft, which flatted areas vary in size to assure correct placement of the support members. Individual coil strips, each carrying a tuning screw, are fixed to the shaft assembly. A simplified clutching arrangement for controlling adjustment of the screws utilizes a coil spring to provide the initial force for moving a pinion gear into operating engagement with the tuning screws. The tuning screws are held by resilient arms which bias the head of the screws toward the shaft to thus firmly maintain the screws in any adjusted position. Each of the coil strips have contacts extending outwardly from one side and positioned to be in electrical and mechanical contact with cantilever spring members fastened to a printed circuit board which in turn is held by the housing.	A TV tuner has a housing and a rotating channel selector shaft mounted on the housing. There are a plurality of individual support members positioned by and mounted on parallel flatted areas of the shaft, which flatted areas vary in size to assure correct placement of the support members. Individual coil strips, each carrying a tuning screw, are fixed to the shaft assembly. A simplified clutching arrangement for controlling adjustment of the screws utilizes a coil spring to provide the initial force for moving a pinion gear into operating engagement with the tuning screws. The tuning screws are held by resilient arms which bias the head of the screws toward the shaft to thus firmly maintain the screws in any adjusted position. Each of the coil strips have contacts extending outwardly from one side and positioned to be in electrical and mechanical contact with cantilever spring members fastened to a printed circuit board which in turn is held by the housing.	['#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .']	['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .']	['#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.']	['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.']	['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .']	['#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']	['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.']	['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']	['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .']	['#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']
CCT129	W01-0706	Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .	a holistic lexiconbased approach to opinion mining	a linear observed time statistical parser based on maximum entropy models	['A Ratnaparkhi']	introduction	Background	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.		This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.	['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']	['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']	"['A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'Shallow parsing is studied as an alternative to full-sentence parsing.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']
CCT130	P07-1068	We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .	a holistic lexiconbased approach to opinion mining	unsupervised word sense disambiguation rivaling supervised methods	['D Yarowsky']	introduction	Motivation	['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']	4	['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']	['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']	This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%		This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%	['We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .']	['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .']	['We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']	['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']	['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .']	['We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']	['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']	['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']	['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .']	['We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']
CCT131	P02-1001	A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .	a holistic lexiconbased approach to opinion mining	speech recognition by composition of weighted finite automata	['Fernando C N Pereira', 'Michael Riley']	introduction	Background	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (); reading another b (); transducing b to p rather than q (); starting to transduce p to rather than x (). 4 To prove (1)(3), express f as an FST and apply the well-known Kleene-Schtzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'A full proof is straightforward, as are proofs of (3)( 2), ( 2)(1).']	0	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (); reading another b (); transducing b to p rather than q (); starting to transduce p to rather than x (). 4 To prove (1)(3), express f as an FST and apply the well-known Kleene-Schtzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'A full proof is straightforward, as are proofs of (3)( 2), ( 2)(1).']	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (); reading another b (); transducing b to p rather than q (); starting to transduce p to rather than x (). 4 To prove (1)(3), express f as an FST and apply the well-known Kleene-Schtzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'A full proof is straightforward, as are proofs of (3)( 2), ( 2)(1).']	We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.		We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .']	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .']	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (l); reading another b (n); transducing b to p rather than q (u); starting to transduce p to rather than x (r). 4 To prove (1)=(3), express f as an FST and apply the well-known Kleene-Schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.']	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (l); reading another b (n); transducing b to p rather than q (u); starting to transduce p to rather than x (r). 4 To prove (1)=(3), express f as an FST and apply the well-known Kleene-Schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.']	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .']	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (l); reading another b (n); transducing b to p rather than q (u); starting to transduce p to rather than x (r). 4 To prove (1)=(3), express f as an FST and apply the well-known Kleene-Schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'A full proof is straightforward, as are proofs of (3)=( 2), ( 2)=(1).']	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (l); reading another b (n); transducing b to p rather than q (u); starting to transduce p to rather than x (r). 4 To prove (1)=(3), express f as an FST and apply the well-known Kleene-Schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.']	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (l); reading another b (n); transducing b to p rather than q (u); starting to transduce p to rather than x (r). 4 To prove (1)=(3), express f as an FST and apply the well-known Kleene-Schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'A full proof is straightforward, as are proofs of (3)=( 2), ( 2)=(1).']	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .']	['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (l); reading another b (n); transducing b to p rather than q (u); starting to transduce p to rather than x (r). 4 To prove (1)=(3), express f as an FST and apply the well-known Kleene-Schutzenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'A full proof is straightforward, as are proofs of (3)=( 2), ( 2)=(1).']
CCT132	D08-1004	We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .	a holistic lexiconbased approach to opinion mining	a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts	['B Pang', 'L Lee']	method	Uses	['where f () extracts a feature vector from a classified document,  are the corresponding weights of those features, and Z  (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count  4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.', 'Thus   R 17744 , and positive weights in  favor class label y = +1 and equally discourage y = 1, while negative weights do the opposite.', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2.']	5	['where f () extracts a feature vector from a classified document,  are the corresponding weights of those features, and Z  (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count  4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.', 'Thus   R 17744 , and positive weights in  favor class label y = +1 and equally discourage y = 1, while negative weights do the opposite.', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2.']	['where f () extracts a feature vector from a classified document,  are the corresponding weights of those features, and Z  (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count  4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.', 'Thus   R 17744 , and positive weights in  favor class label y = +1 and equally discourage y = 1, while negative weights do the opposite.', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2.']	"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints."		"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints."	['We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .']	['where f (*) extracts a feature vector from a classified document, th are the corresponding weights of those features, and Z th (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .']	['We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count >= 4 in the full 2000-document corpus.']	['where f (*) extracts a feature vector from a classified document, th are the corresponding weights of those features, and Z th (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .']	['where f (*) extracts a feature vector from a classified document, th are the corresponding weights of those features, and Z th (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .']	['We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count >= 4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.']	['where f (*) extracts a feature vector from a classified document, th are the corresponding weights of those features, and Z th (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count >= 4 in the full 2000-document corpus.']	['where f (*) extracts a feature vector from a classified document, th are the corresponding weights of those features, and Z th (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count >= 4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.']	['where f (*) extracts a feature vector from a classified document, th are the corresponding weights of those features, and Z th (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .']	['We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count >= 4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.', 'Thus th [?] R 17744 , and positive weights in th favor class label y = +1 and equally discourage y = -1, while negative weights do the opposite.']
CCT133	W06-1639	Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .	a holistic lexiconbased approach to opinion mining	learning from labeled and unlabeled data using graph mincuts	['A Blum', 'S Chawla']	method	Uses	['The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']	5	['The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']	['The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']	Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data.		Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data.	['Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .']	['The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .']	['Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']	['The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']	['As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .']	['Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']	['As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']	['The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']	['As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .']	['Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']
CCT134	J13-1008	As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .	a holistic lexiconbased approach to opinion mining	better arabic parsing baselines evaluations and analysis	['Spence Green', 'Christopher D Manning']	related work	Background	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']	0	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']	In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree-banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2--5% F1.		In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree-banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2--5% F1.	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .']	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .']	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.']	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.']	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .']	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.']	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.']	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.']	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .']	['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']
CCT135	J02-3002	For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .	a holistic lexiconbased approach to opinion mining	adaptive multilingual sentence boundary disambiguation	['David D Palmer', 'Marti A Hearst']	conclusion	CompareOrContrast	"['A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time."", 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']"	1	"['A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time."", 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']"	"['A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time."", 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']"	The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.		The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.	['For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .']	['A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .']	"['For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time.""]"	"['A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time.""]"	['The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .']	"['For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time."", 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']"	"['The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time.""]"	"['A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time."", 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']"	['Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .']	"['For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time."", 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']"
CCT136	P11-1134	One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	a syntaxbased statistical translation model	['Kenji Yamada', 'Kevin Knight']	conclusion	Future	['Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .', 'Another interesting direction is to investigate the potential of paraphrase patterns (i.e.', 'patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).', 'On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.', 'As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.', '1343']	3	['Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .', 'Another interesting direction is to investigate the potential of paraphrase patterns (i.e.', 'patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).', 'On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.', 'As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.', '1343']	['Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .', 'Another interesting direction is to investigate the potential of paraphrase patterns (i.e.', 'patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).', 'On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.', 'As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.', '1343']		We present a syntax-based statistical translation model. Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node. These operations capture linguistic differences such as word order and case marking. Model parameters are estimated in polynomial time using an EM algorithm. The model produces word alignments that are better than those produced by IBM Model 5.	We present a syntax-based statistical translation model. Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node. These operations capture linguistic differences such as word order and case marking. Model parameters are estimated in polynomial time using an EM algorithm. The model produces word alignments that are better than those produced by IBM Model 5.	['One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']	['Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']	['One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']	['Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']	['On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']	['One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']	['On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']	['Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']	['On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']	['One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .', 'Another interesting direction is to investigate the potential of paraphrase patterns (i.e.', 'patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).', 'On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.']
CCT137	N10-1084	Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .	a holistic lexiconbased approach to opinion mining	a method of linguistic steganography based on coladdressallyverified synonym	['Igor A Bolshakov']	related work	Background	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	0	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']		A method is proposed of the automatic concealment of digital information in rather long orthographically and semantically correct texts. The method does not change the meaning of the source text; it only replaces some words by their synonyms. Groups of absolute synonyms are used in a context independent manner, while the groups of relative synonyms are previously tested for semantic compatibility with the collocations containing the word to be replaced. A specific replacement is determined by the hidden information. The collocations are syntactically connected and semantically compatible pairs of content words; they are massively gathered beforehand, with a wide diversity in their stability and idiomacity. Thus the necessary linguistic resources are a specific synonymy dictionary and a very large database of collocations. The steganographic algorithm is informally outlined. An example of hiding binary information in a Russian text fragment is manually traced, with a rough evaluation of the steganographic bandwidth.	A method is proposed of the automatic concealment of digital information in rather long orthographically and semantically correct texts. The method does not change the meaning of the source text; it only replaces some words by their synonyms. Groups of absolute synonyms are used in a context independent manner, while the groups of relative synonyms are previously tested for semantic compatibility with the collocations containing the word to be replaced. A specific replacement is determined by the hidden information. The collocations are syntactically connected and semantically compatible pairs of content words; they are massively gathered beforehand, with a wide diversity in their stability and idiomacity. Thus the necessary linguistic resources are a specific synonymy dictionary and a very large database of collocations. The steganographic algorithm is informally outlined. An example of hiding binary information in a Russian text fragment is manually traced, with a rough evaluation of the steganographic bandwidth.	['Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.']	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.']	['The first lexical substitution method was proposed by Chapman and Davida (1997).', 'The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	['The first lexical substitution method was proposed by Chapman and Davida (1997).', 'The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.']	['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']	['The first lexical substitution method was proposed by Chapman and Davida (1997).', 'The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']	['Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']
CCT138	J86-1002	A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .	a holistic lexiconbased approach to opinion mining	organization and operation of a connected speech understanding system at lexical syntactic and semantic levels	['J Haton', 'J Pierrel']		CompareOrContrast	['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']	1	['A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980.', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']	['A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980.', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']	This paper describes a connected speech understanding system being implemented in Nancy, thanks to the work done in automatic speech recognition since 1968. This system is made up of four parts : an acoustic recognizer which gives a string of phoneme-like segments from a spoken sentence, a syntactic parser which controls the recognition process, a word recognizer working on words predicted by the parser and a dialog procedure which takes in account semantic constraints in order to avoid some of the errors and ambiguities. Some original features of the system are pointed out : modularily (e.g. the language used is considered as a parameter), possibility of processing slightly syntactically incorrect sentences, ... The application both in data management and in oral control of a telephone center has given very promising results. Work is in progress for generalizing our model : extension of the vocabulary and of the grammar, multi-speaker operation, etc.		This paper describes a connected speech understanding system being implemented in Nancy, thanks to the work done in automatic speech recognition since 1968. This system is made up of four parts : an acoustic recognizer which gives a string of phoneme-like segments from a spoken sentence, a syntactic parser which controls the recognition process, a word recognizer working on words predicted by the parser and a dialog procedure which takes in account semantic constraints in order to avoid some of the errors and ambiguities. Some original features of the system are pointed out : modularily (e.g. the language used is considered as a parameter), possibility of processing slightly syntactically incorrect sentences, ... The application both in data management and in oral control of a telephone center has given very promising results. Work is in progress for generalizing our model : extension of the vocabulary and of the grammar, multi-speaker operation, etc.	['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .']	['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .']	['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.']	['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.']	['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .']	['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.']	['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.']	['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.']	['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .']	['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']
CCT139	E03-1002	The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .	a holistic lexiconbased approach to opinion mining	discriminative reranking for natural language parsing	['Michael Collins']	experiments	CompareOrContrast	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']	1	['The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2000;Collins, 2000;Bod, 2001).', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2000;Collins, 2000;Bod, 2001).', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']	This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation		This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.']
CCT140	W03-0806	The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .	a holistic lexiconbased approach to opinion mining	investigating gis and smoothing for maximum entropy taggers	['James R Curran', 'Stephen Clark']		Background	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']	0	[]	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']	This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.		This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']	['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']
CCT141	P97-1063	This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	building probabilistic models for natural language	['S Chen']		Background	['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']	0	['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies, clever as they might be Wu ~z Xia, 1994;Chen, 1996).', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']	['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies, clever as they might be Wu ~z Xia, 1994;Chen, 1996).', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']	Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation. In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.Engineering and Applied Science		Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation. In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.Engineering and Applied Science	['This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .']	['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .']	['This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']	['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']	['Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .']	['This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']	['Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']	['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']	['Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .']	['This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']
CCT142	W01-0706	Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .	a holistic lexiconbased approach to opinion mining	building a large annotated corpus of english the penn treebank	['M P Marcus', 'B Santorini', 'M Marcinkiewicz']	experiments	Uses	"['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']"	5	[]	"['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']"	Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.		Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.	['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .']	['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .']	['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.']	['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.']	['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .']	"['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']"	['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.']	"['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']"	['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .']	"['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']"
CCT143	P11-1134	We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .	a holistic lexiconbased approach to opinion mining	jlsi a tool for latent semantic indexing software available at httptccitcitresearchtextectoolsresourcesjlsihtml	['Claudio Giuliano']	experiments	Uses	['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).', 'In this way we obtained 13760 word pairs.']	5	['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).', 'In this way we obtained 13760 word pairs.']	['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).', 'In this way we obtained 13760 word pairs.']				['We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .']	['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .']	['We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).']	['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).']	['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .']	['We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).', 'In this way we obtained 13760 word pairs.']	['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).']	['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).', 'In this way we obtained 13760 word pairs.']	['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .']	['We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).', 'In this way we obtained 13760 word pairs.']
CCT144	D12-1027	For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .	a holistic lexiconbased approach to opinion mining	improved statistical machine translation for resourcepoor languages using related resourcerich languages	['Preslav Nakov', 'Hwee Tou Ng']	related work	CompareOrContrast	['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.', 'Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches.']	1	['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.', 'Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches.']	['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.', 'Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches.']	We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. The evaluation for Indonesian-English (using Malay) and Spanish-English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data.		We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. The evaluation for Indonesian-English (using Malay) and Spanish-English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data.	['For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .']	['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .']	['For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.']	['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.']	['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .']	['For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.', 'Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches.']	['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.']	['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.', 'Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches.']	['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .']	['For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.', 'Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches.']
CCT145	W06-1104	Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .	a holistic lexiconbased approach to opinion mining	automatic sense disambiguation using machine readable dictionaries how to tell a pine cone from an ice cream cone	['Michael Lesk']		Background	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	0	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994;Leacock and Chodorow, 1998), information-based (Resnik, 1995;Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005).', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994;Leacock and Chodorow, 1998), information-based (Resnik, 1995;Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005).', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	"The meaning of an English word can vary widely depending on which sense is intended. Does a fireman feed fires or put them out? It depends on whether or not he is on a steam locomotive. I am trying to decide automatically which sense of a word is intended (in written English) by using machine readable dictionaries, and looking for words in the sense definitions that overlap words in the definition of nearby words. The problem of deciding which sense of a word was intended by the writer is an important problem in information retrieval systems. At present most retrieval systems rely on manual indexing; if this is to be replaced with automatic text processing, it would be very desirable to recognize the correct sense of each word as often as possible. Previous work has generally either suggested (a) detailed frames describing the particular word senses,t*' or (b) global statistics about the word occurrences.3 The first has not yet been made available in any real application, and the second may give the wrong answer in specific local instances. This procedure uses available dictionaries, so that it will process any text; and uses solely the immediate context. To consider the example in the title, look at the definition of pine in the Oxford Advanced Learner's Dictionary of Current English: there are, of course, two major senses. ""kind of evergreen tree with needle-shaped leaves.. ."" and ""waste away through sorrow or illness..."" And cone has three separate definitions: ""solid body which narrows to a' point . . . . *' ""something of this shape w-hether solid or hollow...,"" and ""fruit of certain evergreen trees..."" Note that both evergreen and tree are common to two of the sense definitions: thus a program could guess that if the two words pine cone appear together, the likely senses are those of the tree and its fruit"		"The meaning of an English word can vary widely depending on which sense is intended. Does a fireman feed fires or put them out? It depends on whether or not he is on a steam locomotive. I am trying to decide automatically which sense of a word is intended (in written English) by using machine readable dictionaries, and looking for words in the sense definitions that overlap words in the definition of nearby words. The problem of deciding which sense of a word was intended by the writer is an important problem in information retrieval systems. At present most retrieval systems rely on manual indexing; if this is to be replaced with automatic text processing, it would be very desirable to recognize the correct sense of each word as often as possible. Previous work has generally either suggested (a) detailed frames describing the particular word senses,t*' or (b) global statistics about the word occurrences.3 The first has not yet been made available in any real application, and the second may give the wrong answer in specific local instances. This procedure uses available dictionaries, so that it will process any text; and uses solely the immediate context. To consider the example in the title, look at the definition of pine in the Oxford Advanced Learner's Dictionary of Current English: there are, of course, two major senses. ""kind of evergreen tree with needle-shaped leaves.. ."" and ""waste away through sorrow or illness..."" And cone has three separate definitions: ""solid body which narrows to a' point . . . . *' ""something of this shape w-hether solid or hollow...,"" and ""fruit of certain evergreen trees..."" Note that both evergreen and tree are common to two of the sense definitions: thus a program could guess that if the two words pine cone appear together, the likely senses are those of the tree and its fruit"	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']
CCT146	D13-1115	Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .	a holistic lexiconbased approach to opinion mining	distributional semantics in technicolor	['Elia Bruni', 'Gemma Boleda', 'Marco Baroni', 'NamKhanh Tran']	introduction	Background	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	0	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009;Steyvers, 2010;Feng and Lapata, 2010b;Bruni et al., 2011;Silberer and Lapata, 2012;Johns and Jones, 2012;Bruni et al., 2012a;Bruni et al., 2012b;Silberer et al., 2013).', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009;Steyvers, 2010;Feng and Lapata, 2010b;Bruni et al., 2011;Silberer and Lapata, 2012;Johns and Jones, 2012;Bruni et al., 2012a;Bruni et al., 2012b;Silberer et al., 2013).', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.		Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.	['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']	['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CCT147	W06-1104	#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .	a holistic lexiconbased approach to opinion mining	using the structure of a conceptual network in computing semantic relatedness	['Iryna Gurevych']	related work	Background	"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"	0	"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"	"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"	Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences.		Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences.	['#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']	['A comprehensive evaluation of SR measures requires a higher number of word pairs.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']	['#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']	['A comprehensive evaluation of SR measures requires a higher number of word pairs.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']	['However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'A comprehensive evaluation of SR measures requires a higher number of word pairs.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']	['#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']	['However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'A comprehensive evaluation of SR measures requires a higher number of word pairs.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']	['A comprehensive evaluation of SR measures requires a higher number of word pairs.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']	['Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'A comprehensive evaluation of SR measures requires a higher number of word pairs.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .']	['#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.']
CCT148	P00-1012	One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .	a holistic lexiconbased approach to opinion mining	generation that exploits corpusbased statistical knowledge	['Irene Langkilde', 'Kevin Knight']	method	Uses	"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.', 'It also should be straightforwardly applicable to the more specific problem we are addressing here.', 'To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.', 'This has the advantage of reducing the problem of adjective ordering to the problem of estimating n-gram probabilities, something which is relatively well understood.']"	5	"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.', 'It also should be straightforwardly applicable to the more specific problem we are addressing here.', 'To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.', 'This has the advantage of reducing the problem of adjective ordering to the problem of estimating n-gram probabilities, something which is relatively well understood.']"	"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.', 'It also should be straightforwardly applicable to the more specific problem we are addressing here.', 'To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.', 'This has the advantage of reducing the problem of adjective ordering to the problem of estimating n-gram probabilities, something which is relatively well understood.']"	We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities.		We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities.	"[""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .""]"	"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .""]"	"[""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.']"	"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .""]"	"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .""]"	"[""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.', 'It also should be straightforwardly applicable to the more specific problem we are addressing here.']"	"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.']"	"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.', 'It also should be straightforwardly applicable to the more specific problem we are addressing here.']"	"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .""]"	"[""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.', 'It also should be straightforwardly applicable to the more specific problem we are addressing here.', 'To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.']"
CCT149	W05-0709	where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	a maximum entropy approach to natural language processing	['A Berger', 'S Della Pietra', 'V Della Pietra']		Uses	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	5	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.		The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']	['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']
CCT150	J09-4010	13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .	a holistic lexiconbased approach to opinion mining	automatic evaluation of summaries using ngram cooccurrence statistics	['C Y Lin', 'E H Hovy']	method	Uses	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']	5	[]	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']		Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.	Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']	['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']
CCT151	W06-3309	Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	cuhtk conversational telephone speech transcription system	['Gunnar Evermann', 'H Y Chan', 'Mark J F Gales', 'Thomas Hain', 'Xunying Liu', 'David Mrva', 'Lan Wang', 'Phil Woodland']	conclusion	Background	['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'This technique provides two important advantages.', 'First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.', 'This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training.', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']	0	['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'This technique provides two important advantages.', 'First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.', 'This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training.', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']	['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'This technique provides two important advantages.', 'First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.', 'This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training.', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']				['Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']	['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']	['Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']	['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']	['This technique provides two important advantages.', 'An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']	['Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']	['This technique provides two important advantages.', 'An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']	['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']	['First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.', 'This technique provides two important advantages.', 'An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']	['Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']
CCT152	J13-1008	In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	an efficient algorithm for easyfirst nondirectional dependency parsing	['Yoav Goldberg', 'Michael Elhadad']	experiments	Uses	"['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).', 'Unlike MaltParser, however, it does not attempt to attach arcs ""eagerly"" as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments).', 'Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, ""easy"" attachment, to low, as estimated by the classifier).', 'Labeling the relation arcs is done in a second pass, with a separate training step, after all attachments have been decided (the code for which was added after the publication of Goldberg and Elhadad (2010), which only included an unlabeled attachment version).']"	5	"['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).', 'Unlike MaltParser, however, it does not attempt to attach arcs ""eagerly"" as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments).', 'Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, ""easy"" attachment, to low, as estimated by the classifier).', 'Labeling the relation arcs is done in a second pass, with a separate training step, after all attachments have been decided (the code for which was added after the publication of Goldberg and Elhadad (2010), which only included an unlabeled attachment version).']"	"['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).', 'Unlike MaltParser, however, it does not attempt to attach arcs ""eagerly"" as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments).', 'Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, ""easy"" attachment, to low, as estimated by the classifier).', 'Labeling the relation arcs is done in a second pass, with a separate training step, after all attachments have been decided (the code for which was added after the publication of Goldberg and Elhadad (2010), which only included an unlabeled attachment version).']"		We present a novel deterministic dependency pars- ing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models.	We present a novel deterministic dependency pars- ing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models.	['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .']	['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .']	['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.']	['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.']	['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .']	['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).']	['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.']	['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).']	['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .']	"['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).', 'Unlike MaltParser, however, it does not attempt to attach arcs ""eagerly"" as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments).']"
CCT153	N04-2004	The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .	a holistic lexiconbased approach to opinion mining	english verb classes and alternations a preliminary investigation	['Beth Levin']	introduction	CompareOrContrast	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', 'In addition, lexical rules could be formulated to derive certain alternations from more basic forms.']	1	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', 'In addition, lexical rules could be formulated to derive certain alternations from more basic forms.']	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', 'In addition, lexical rules could be formulated to derive certain alternations from more basic forms.']	"In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, ""English Verb Classes and Alternations"" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University."		"In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, ""English Verb Classes and Alternations"" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University."	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .']	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .']	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.']	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.']	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .']	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', 'In addition, lexical rules could be formulated to derive certain alternations from more basic forms.']	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.']	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', 'In addition, lexical rules could be formulated to derive certain alternations from more basic forms.']	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .']	['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', 'In addition, lexical rules could be formulated to derive certain alternations from more basic forms.']
CCT154	J91-2003	Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .	a holistic lexiconbased approach to opinion mining	coherence and coreferencequot	['J R Hobbs']	introduction	Background	"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	0	"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"				"[""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	"[""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	"[""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	"[""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"
CCT155	W00-1312	Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	finding terminology translations from nonparallel corporaquot	['P Fung', 'K Mckeown']		Background	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']	0	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']	We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance.		We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance.	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .']	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .']	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .']	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .']	['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']
CCT156	W04-1805	ASARES is presented in detail in ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	learning semantic lexicons from a partofspeech and semantically tagged corpus using inductive logic programming	['Vincent Claveau', 'Pascale Sebillot', 'Cecile Fabre', 'Pierrette Bouillon']	method	Uses	['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']	5	[]	['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']	This paper describes an inductive logic programming learning method designed to acquire from a corpus specific Noun-Verb (N-V) pairs---relevant in information retrieval applications to perform index expansion---in order to build up semantic lexicons based on Pustejovsky's generative lexicon (GL) principles (Pustejovsky, 1995). In one of the components of this lexical model, called the qualia structure, words are described in terms of semantic roles. For example, the telic role indicates the purpose or function of an item (cut for knife), the agentive role its creation mode (build for house), etc. The qualia structure of a noun is mainly made up of verbal associations, encoding relational information. The learning method enables us to automatically extract, from a morpho-syntactically and semantically tagged corpus, N-V pairs whose elements are linked by one of the semantic relations defined in the qualia structure in GL. It also infers rules explaining what in the surrounding context distinguishes such pairs from others also found in sentences of the corpus but which are not relevant. Stress is put here on the learning efficiency that is required to be able to deal with all the available contextual information, and to produce linguistically meaningful rules.		This paper describes an inductive logic programming learning method designed to acquire from a corpus specific Noun-Verb (N-V) pairs---relevant in information retrieval applications to perform index expansion---in order to build up semantic lexicons based on Pustejovsky's generative lexicon (GL) principles (Pustejovsky, 1995). In one of the components of this lexical model, called the qualia structure, words are described in terms of semantic roles. For example, the telic role indicates the purpose or function of an item (cut for knife), the agentive role its creation mode (build for house), etc. The qualia structure of a noun is mainly made up of verbal associations, encoding relational information. The learning method enables us to automatically extract, from a morpho-syntactically and semantically tagged corpus, N-V pairs whose elements are linked by one of the semantic relations defined in the qualia structure in GL. It also infers rules explaining what in the surrounding context distinguishes such pairs from others also found in sentences of the corpus but which are not relevant. Stress is put here on the learning efficiency that is required to be able to deal with all the available contextual information, and to produce linguistically meaningful rules.	['ASARES is presented in detail in ( #AUTHOR_TAG ) .']	['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .']	['ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']	['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']	['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .']	['ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']	['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']	['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']	['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .']	['ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']
CCT157	J91-2003	"Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) ."	a holistic lexiconbased approach to opinion mining	cohesion in english	['M A K Halliday', 'R Hasan']	introduction	Background	"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"	0	"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also Quirk et al. 1972, p. 672).']"	"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also Quirk et al. 1972, p. 672).']"	Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good		Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good	"['Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"	"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"	"['Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"	"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"	"['""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"	"['Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"	"['""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"	"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"	"['However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"	"['Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"
CCT158	W04-1805	A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .	a holistic lexiconbased approach to opinion mining	structures mathematiques du langage	['Zellig Harris']	related work	Background	['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']	0	[]	['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']				['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .']	['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .']	['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.']	['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.']	['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .']	['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']	['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.']	['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']	['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .']	['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']
CCT159	J13-1008	Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	parsing indian languages with maltparser	['Joakim Nivre']	related work	CompareOrContrast	"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	1	"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data.		This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data.	['Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']	['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']	['Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']	['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']	['We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']	['Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']	['We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']	['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']	['We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']	['Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.']
CCT160	J97-4003	Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .	a holistic lexiconbased approach to opinion mining	of csli lecture notes center for the study of language and information	['Carl Pollard', 'Ivan Sag']	introduction	CompareOrContrast	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2  The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']	1	[]	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2  The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']				['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .']	['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .']	['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']	['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .']	['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 deg The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']
CCT161	J09-4010	Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).	a holistic lexiconbased approach to opinion mining	detection of questionanswer pairs in email conversations	['L Shrestha', 'K R McKeown']		CompareOrContrast	['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']	1	['Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) Jijkoun and de Rijke 2005;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']	['Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) Jijkoun and de Rijke 2005;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']		While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent. In this paper, we present our work on the detection of question-answer pairs in an email conversation for the task of email summarization. We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing.	While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent. In this paper, we present our work on the detection of question-answer pairs in an email conversation for the task of email summarization. We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing.	['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).']	['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).']	['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.']	['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.']	['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).']	['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.']	['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.']	['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.']	['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).']	['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']
CCT162	D13-1115	The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .	a holistic lexiconbased approach to opinion mining	describing objects by their attributes	['Ali Farhadi', 'Ian Endres', 'Derek Hoiem', 'David Forsyth']	related work	Background	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	0	[]	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	"We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (""spotty dog"", not just ""dog""); to say something about unfamiliar objects (""hairy and four-legged"", not just ""unknown""); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (""spotty"") or discriminative (""dogs have it but sheep do not""). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attributebased framework. 1"		"We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (""spotty dog"", not just ""dog""); to say something about unfamiliar objects (""hairy and four-legged"", not just ""unknown""); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (""spotty"") or discriminative (""dogs have it but sheep do not""). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attributebased framework. 1"	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']	['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm  uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']
CCT163	Q13-1020	In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .	a holistic lexiconbased approach to opinion mining	a gibbs sampler for phrasal synchronous grammar induction	['Phil Blunsom', 'Trevor Cohn', 'Chris Dyer', 'Miles Osborne']		CompareOrContrast	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']	1	[]	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']	We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.		We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .']	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .']	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.']	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.']	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .']	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.']	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .']	['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']
CCT164	W01-1510	Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	a lexicalized tree adjoining grammar for english	['The XTAG Research Group']	introduction	Background	['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with ).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled  onto an internal node of another tree with the same symbol  (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeill and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']	0	['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with ).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled  onto an internal node of another tree with the same symbol  (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeill and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']	['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with ).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled  onto an internal node of another tree with the same symbol  (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeill and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']	This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389		This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389	['Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .']	['Figure 4: Adjunction tions called substitution and adjunction.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .']	['Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.']	['Figure 4: Adjunction tions called substitution and adjunction.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.']	['Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'Figure 4: Adjunction tions called substitution and adjunction.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .']	['Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']	['Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'Figure 4: Adjunction tions called substitution and adjunction.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.']	['Figure 4: Adjunction tions called substitution and adjunction.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']	['An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Y=).', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'Figure 4: Adjunction tions called substitution and adjunction.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .']	['Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']
CCT165	A00-2022	Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .	a holistic lexiconbased approach to opinion mining	a bag of useful techniques for efficient and robust parsing	['B Kiefer', 'H-U Krieger', 'J Carroll', 'R Malouf']	conclusion	CompareOrContrast	['In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']	1	[]	['In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']	This paper describes new and improved techniques which help a unification-based parser to process input efficiently and robustly. We show that combining these methods leads to a speed-up in parsing time of more than an order of magnitude. The methods are correct in the sense that none of them rule out legal rule applications		This paper describes new and improved techniques which help a unification-based parser to process input efficiently and robustly. We show that combining these methods leads to a speed-up in parsing time of more than an order of magnitude. The methods are correct in the sense that none of them rule out legal rule applications	['Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .']	['In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .']	['Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']	['In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']	['In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .']	['Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']	['In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']	['In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']	['In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .']	['Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']
CCT166	J10-3007	For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .	a holistic lexiconbased approach to opinion mining	annealing structural bias in multilingual weighted grammar induction	['Noah A Smith', 'Jason Eisner']	related work	Background	"['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations.', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant  > 1.', 'This encourages more transitions and hence shorter phrases.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p  (y) by a penalty term as: p  (y)  p  (y)e ( ey length(e)) , where length(e) is the surface length of edge e."", 'The factor  changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form ""scale the total length of edges"", which depending on the value of  will prefer to have more shorter/longer edges.', 'Such statements are not data dependent.', 'Depending on the value of , for instance if   0, even if the data is such that the model already uses too many short edges on average, this value of  will push for more short edges.', 'By contrast the statements we can make in PR are of the form ""there should be more short edges than long edges"".', 'Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes.']"	0	"['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations.', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant  > 1.', 'This encourages more transitions and hence shorter phrases.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p  (y) by a penalty term as: p  (y)  p  (y)e ( ey length(e)) , where length(e) is the surface length of edge e."", 'The factor  changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form ""scale the total length of edges"", which depending on the value of  will prefer to have more shorter/longer edges.', 'Such statements are not data dependent.', 'Depending on the value of , for instance if   0, even if the data is such that the model already uses too many short edges on average, this value of  will push for more short edges.', 'By contrast the statements we can make in PR are of the form ""there should be more short edges than long edges"".', 'Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes.']"	"['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations.', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant  > 1.', 'This encourages more transitions and hence shorter phrases.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p  (y) by a penalty term as: p  (y)  p  (y)e ( ey length(e)) , where length(e) is the surface length of edge e."", 'The factor  changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form ""scale the total length of edges"", which depending on the value of  will prefer to have more shorter/longer edges.', 'Such statements are not data dependent.', 'Depending on the value of , for instance if   0, even if the data is such that the model already uses too many short edges on average, this value of  will push for more short edges.', 'By contrast the statements we can make in PR are of the form ""there should be more short edges than long edges"".', 'Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes.']"	"We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). Next, by annealing the free parameter that controls this bias, we achieve further improvements. We then describe an alternative kind of structural bias, toward ""broken "" hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1-17 % (absolute) over CE (and 8-30% over EM), achieving to our knowledge the best results on this task to date. Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems."		"We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). Next, by annealing the free parameter that controls this bias, we achieve further improvements. We then describe an alternative kind of structural bias, toward ""broken "" hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1-17 % (absolute) over CE (and 8-30% over EM), achieving to our knowledge the best results on this task to date. Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems."	"[""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .""]"	"['The idea of introducing constraints over a model to better guide the learning process has appeared before.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .""]"	"[""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .""]"	"['The idea of introducing constraints over a model to better guide the learning process has appeared before.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .""]"	"['In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations.', 'The idea of introducing constraints over a model to better guide the learning process has appeared before.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .""]"	"[""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .""]"	"['In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations.', 'The idea of introducing constraints over a model to better guide the learning process has appeared before.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .""]"	"['The idea of introducing constraints over a model to better guide the learning process has appeared before.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .""]"	"['The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations.', 'The idea of introducing constraints over a model to better guide the learning process has appeared before.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .""]"	"[""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p th (y) by a penalty term as: p th (y) [?] p th (y)e (d e[?]y length(e)) , where length(e) is the surface length of edge e."", 'The factor d changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.']"
CCT167	A00-1016	The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .	a holistic lexiconbased approach to opinion mining	commandtalk a spokenlanguage interface for battlefield simulations	['R Moore', 'J Dowding', 'H Bratt', 'J Gawron']	experiments	Uses	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).', 'Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000).', 'Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domain~independent unification grammar and a domain-specific lexicon.', 'The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized.', 'Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager.', 'We refer to these as linguistic level representations.']	5	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).', 'Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000).', 'Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domain~independent unification grammar and a domain-specific lexicon.', 'The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized.', 'Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager.', 'We refer to these as linguistic level representations.']	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).', 'Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000).', 'Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domain~independent unification grammar and a domain-specific lexicon.', 'The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized.', 'Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager.', 'We refer to these as linguistic level representations.']		"CommandTalk is a spoken-language interface to battlefield simulations that allows the use of ordinary spoken English to create forces and control measures, assign missions to forces, modify missions during execution, and control simulation system functions. CommandTalk combines a number of separate components integrated through the use of the Open Agent Architecture, including the Nuance speech recognition system, the Gemini natural-language parsing and interpretation system, a contextual-interpretation module, a ""push-to-talk"" agent, the ModSAF battle-field simulator, and ""Start-It"" (a graphical processing-spawning agent). Commano Talk is installed at a number of Government and contractor sites, including NRaD and the Marine Corps Air Ground Combat Center. It is currently being extended to provide exercise-time control of all simulated U.S. forces in DARPA'S STOW 97 demonstration."	"CommandTalk is a spoken-language interface to battlefield simulations that allows the use of ordinary spoken English to create forces and control measures, assign missions to forces, modify missions during execution, and control simulation system functions. CommandTalk combines a number of separate components integrated through the use of the Open Agent Architecture, including the Nuance speech recognition system, the Gemini natural-language parsing and interpretation system, a contextual-interpretation module, a ""push-to-talk"" agent, the ModSAF battle-field simulator, and ""Start-It"" (a graphical processing-spawning agent). Commano Talk is installed at a number of Government and contractor sites, including NRaD and the Marine Corps Air Ground Combat Center. It is currently being extended to provide exercise-time control of all simulated U.S. forces in DARPA'S STOW 97 demonstration."	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .']	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .']	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).']	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).']	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .']	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).', 'Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000).']	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).']	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).', 'Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000).']	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .']	['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).', 'Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000).', 'Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domain~independent unification grammar and a domain-specific lexicon.']
CCT168	P11-1134	Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .	a holistic lexiconbased approach to opinion mining	syntacticsemantic structures for textual entailment recognition	['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']	experiments	CompareOrContrast	"['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.', 'This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', '63.50%).', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', 'In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.', ""Finally, it's worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%)."", 'This demonstrates that phrase tables can successfully replace MT systems in the CLTE task.']"	1	[]	"['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.', 'This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', '63.50%).', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', 'In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.', ""Finally, it's worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%)."", 'This demonstrates that phrase tables can successfully replace MT systems in the CLTE task.']"	In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.		In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.	['Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .']	['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .']	['Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .']	['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .']	['First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.', 'The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .']	['Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .']	['First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.', 'The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .']	['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .']	['This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.', 'The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .']	['Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', '63.50%).', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', 'In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.']
CCT169	W06-1104	Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	computing semantic relatedness across parts of speech	['Iryna Gurevych']	experiments	CompareOrContrast	['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'While most people may agree 10 Note that Resnik used the averaged correlation coefficient.', 'We computed the summarized correlation coefficient using a Fisher Z-value transformation.', 'that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group.', 'Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']	1	['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'While most people may agree 10 Note that Resnik used the averaged correlation coefficient.', 'We computed the summarized correlation coefficient using a Fisher Z-value transformation.', 'that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group.', 'Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']	['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'While most people may agree 10 Note that Resnik used the averaged correlation coefficient.', 'We computed the summarized correlation coefficient using a Fisher Z-value transformation.', 'that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group.', 'Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']				['Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']	['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']	['Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']	['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']	['Judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']	['Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']	['Judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']	['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']	['While most people may agree 10 Note that Resnik used the averaged correlation coefficient.', 'Judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']	['Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']
CCT170	N04-2004	There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	building verb meanings	['Malka Rappaport Hovav', 'Beth Levin']		Background	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']	0	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']				['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .']	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .']	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .']	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .']	['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']
CCT171	J00-4001	For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	robust text processing in automated information retrieval	['Tornek Strzalkowski']	method	Background	"['The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'However, the SCBD is a general-purpose tool and was not designed for providing stylistic information exclusively.', 'Thus, any NLP tool (e.g., part-of-speech taggers, parsers, etc.) can provide similar measures.', 'The appropriate analysis-level style markers have to be defined according to the methodology used by the tool in order to analyze the text.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .', 'This parser produces trees to represent the structure of the sentences that compose the text.', 'However, it is set to ""skip"" or surrender attempts to parse clauses after reaching a time-out threshold.', 'When the parser skips, it notes that in the parse tree.', 'The measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis-level style markers.']"	0	"['The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'However, the SCBD is a general-purpose tool and was not designed for providing stylistic information exclusively.', 'Thus, any NLP tool (e.g., part-of-speech taggers, parsers, etc.) can provide similar measures.', 'The appropriate analysis-level style markers have to be defined according to the methodology used by the tool in order to analyze the text.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .', 'This parser produces trees to represent the structure of the sentences that compose the text.', 'However, it is set to ""skip"" or surrender attempts to parse clauses after reaching a time-out threshold.', 'When the parser skips, it notes that in the parse tree.', 'The measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis-level style markers.']"	"['The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'However, the SCBD is a general-purpose tool and was not designed for providing stylistic information exclusively.', 'Thus, any NLP tool (e.g., part-of-speech taggers, parsers, etc.) can provide similar measures.', 'The appropriate analysis-level style markers have to be defined according to the methodology used by the tool in order to analyze the text.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .', 'This parser produces trees to represent the structure of the sentences that compose the text.', 'However, it is set to ""skip"" or surrender attempts to parse clauses after reaching a time-out threshold.', 'When the parser skips, it notes that in the parse tree.', 'The measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis-level style markers.']"	This paper outlines a prototype text retrieval system which uses relatively advanced natural language processing techniques in order to enhance the effectiveness of statistical document retrieval. The backbone of our system is a traditional retrieval engine which builds inverted index files from pre-processed docu- ments, and then searches and ranks the documents in response to user queries. Natural language processing is used to (1) preprocess the documents in order to extract contents-carrying terms, (2) discover interterm dependencies and build a conceptual hierarchy specific to the database domain, and (3) process user&apos;s natural language requests into effective search queries. The basic assumption of this design is that term-based representation of contents is in principle sufficient to build an effective if not optimal search query out of any users request. This has been confirmed by an experiment that compared effectiveness of expert-user prepared queries with those derived automatically from an initial narrative information request. In this paper we show that largescale natural language processing (hundreds of millions of words and more) is not only required for a better retrieval, but it is also doable, given appropriate resources. We report on selected preliminary restfits of experiments with 500 MByte database of Wall Street Journal articles, as well as some earlier restfits with a smaller document collection		This paper outlines a prototype text retrieval system which uses relatively advanced natural language processing techniques in order to enhance the effectiveness of statistical document retrieval. The backbone of our system is a traditional retrieval engine which builds inverted index files from pre-processed docu- ments, and then searches and ranks the documents in response to user queries. Natural language processing is used to (1) preprocess the documents in order to extract contents-carrying terms, (2) discover interterm dependencies and build a conceptual hierarchy specific to the database domain, and (3) process user&apos;s natural language requests into effective search queries. The basic assumption of this design is that term-based representation of contents is in principle sufficient to build an effective if not optimal search query out of any users request. This has been confirmed by an experiment that compared effectiveness of expert-user prepared queries with those derived automatically from an initial narrative information request. In this paper we show that largescale natural language processing (hundreds of millions of words and more) is not only required for a better retrieval, but it is also doable, given appropriate resources. We report on selected preliminary restfits of experiments with 500 MByte database of Wall Street Journal articles, as well as some earlier restfits with a smaller document collection	['For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .']	['The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .']	['For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .']	['The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .']	['However, the SCBD is a general-purpose tool and was not designed for providing stylistic information exclusively.', 'The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .']	['For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .']	['However, the SCBD is a general-purpose tool and was not designed for providing stylistic information exclusively.', 'The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .']	['The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .']	['Thus, any NLP tool (e.g., part-of-speech taggers, parsers, etc.) can provide similar measures.', 'However, the SCBD is a general-purpose tool and was not designed for providing stylistic information exclusively.', 'The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .']	"['For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .', 'This parser produces trees to represent the structure of the sentences that compose the text.', 'However, it is set to ""skip"" or surrender attempts to parse clauses after reaching a time-out threshold.', 'When the parser skips, it notes that in the parse tree.']"
CCT172	D08-1039	The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	word triggers and the em algorithm	['Christoph Tillmann', 'Hermann Ney']	method	CompareOrContrast	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	1	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	In this paper, we study the use of so-called word trigger pairs to improve an existing language model, which is typically a trigram model in combination with a cache component. A word trigger pair is defined as a long-distance word pair. We present two methods to select the most significant single word trigger pairs. The selected trigger pairs are used in a combined model where the interpolation parameters and trigger interaction parameters are trained by the EM algorithm.		In this paper, we study the use of so-called word trigger pairs to improve an existing language model, which is typically a trigram model in combination with a cache component. A word trigger pair is defined as a long-distance word pair. We present two methods to select the most significant single word trigger pairs. The selected trigger pairs are used in a combined model where the interpolation parameters and trigger interaction parameters are trained by the EM algorithm.	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']	['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']
CCT173	J92-1004	Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .	a holistic lexiconbased approach to opinion mining	integration of speech recognition and natural language processing in the mit voyager systemquot	['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']		Uses	"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]"	5	[]	"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]"	The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>		The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>	['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .']	['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .']	['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.']	['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.']	['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .']	"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991).""]"	['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.']	"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991).""]"	['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .']	"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]"
CCT174	Q13-1020	#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .	a holistic lexiconbased approach to opinion mining	syntax augmented machine translation via chart parsing	['Andreas Zollmann', 'Ashish Venugopal']	related work	CompareOrContrast	['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']	1	['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']	['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']	"We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop's baseline system. Our translation system is available open-source under the GNU General Public License."		"We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop's baseline system. Our translation system is available open-source under the GNU General Public License."	['#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .']	['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .']	['#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.']	['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.']	['Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .']	['#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']	['Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.']	['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']	['This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .']	['#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']
CCT175	J01-4001	Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .	a holistic lexiconbased approach to opinion mining	toward a computational theory of definite anaphora comprehension in english	['Candace Sidner']		Background	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']	0	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']	Abstract : This report investigates the process of focussing as a description and explanation of the comprehension of certain anaphoric expressions in English discourse. The investigation centers on the interpretation of definite anaphora, that is, on the personal pronouns, and noun phrases used with a definite article the, this, or that. Focussing is formalized as a process in which a speaker centers attention on a particular aspect of the discourse. An algorithmic description specifies what the speaker can focus on and how the speaker may change the focus of the discourse as the discourse unfolds. The algorithm allows for a simple focussing mechanism to be constructed: an element in focus, an ordered collection of alternate foci, and a stack of old foci. The data structure for the element in focus is a representation which encodes a limited set of associations between it and other elements from the discourse as well as from general knowledge. This report also establishes other constraints which are needed for the successful comprehension of anaphoric expressions. The focussing mechanism is designed to take advantage of syntactic and semantic information encoded as constraints on the choice of anaphora interpretation. These constraints are due to the work of language researchers; and the focussing mechanism provides a principled means for choosing when to apply the constraints in the comprehension process.		Abstract : This report investigates the process of focussing as a description and explanation of the comprehension of certain anaphoric expressions in English discourse. The investigation centers on the interpretation of definite anaphora, that is, on the personal pronouns, and noun phrases used with a definite article the, this, or that. Focussing is formalized as a process in which a speaker centers attention on a particular aspect of the discourse. An algorithmic description specifies what the speaker can focus on and how the speaker may change the focus of the discourse as the discourse unfolds. The algorithm allows for a simple focussing mechanism to be constructed: an element in focus, an ordered collection of alternate foci, and a stack of old foci. The data structure for the element in focus is a representation which encodes a limited set of associations between it and other elements from the discourse as well as from general knowledge. This report also establishes other constraints which are needed for the successful comprehension of anaphoric expressions. The focussing mechanism is designed to take advantage of syntactic and semantic information encoded as constraints on the choice of anaphora interpretation. These constraints are due to the work of language researchers; and the focussing mechanism provides a principled means for choosing when to apply the constraints in the comprehension process.	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .']	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .']	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.']	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.']	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .']	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.']	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .']	['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']
CCT176	W06-2807	"The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) ."	a holistic lexiconbased approach to opinion mining	the wiki way  quick collaboration on the web	['Ward Cunningham', 'Bo Leuf']		Background	"['On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'In fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'Generally, people avoid commenting, preferring to edit each document.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"	0	[]	['On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'In fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'Generally, people avoid commenting, preferring to edit each document.', 'The paradigm is `` write many , read many  ( #AUTHOR_TAG ) .']	"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001"		"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001"	"['The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"	"['On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"	"['The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"	"['On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"	"['In fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"	"['The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"	"['In fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"	"['On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"	"['Generally, people avoid commenting, preferring to edit each document.', 'In fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"	"['The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"
CCT177	P10-2059	The Praat tool was used ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	praat doing phonetics by computer retrieved	['Paul Boersma', 'David Weenink']	introduction	Uses	['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Grnnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']	5	[]	['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Grnnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']				['The Praat tool was used ( #AUTHOR_TAG ) .']	['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Grnnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']	['The Praat tool was used ( #AUTHOR_TAG ) .']	['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Grnnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']	['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Grnnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']	['The Praat tool was used ( #AUTHOR_TAG ) .']	['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Grnnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']	['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Grnnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']	['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Grnnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']	['The Praat tool was used ( #AUTHOR_TAG ) .']
CCT178	J06-2002	2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .	a holistic lexiconbased approach to opinion mining	two theories about adjectives	['Hans Kamp']	introduction	Background	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']	0	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']				['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']	['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']
CCT179	W03-0806	The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .	a holistic lexiconbased approach to opinion mining	investigating gis and smoothing for maximum entropy taggers	['James R Curran', 'Stephen Clark']	experiments	Motivation	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']	4	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']	This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.		This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.']	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.']	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.']	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.']	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.']	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']	['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).']
CCT180	W06-1639	Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	optimizing to arbitrary nlp metrics using ensemble selection	['A Munson', 'C Cardie', 'R Caruana']	method	Future	['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']	3	[]	['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']	While there have been many successful applications of machine learning methods to tasks in NLP, learning algorithms are not typically designed to optimize NLP performance metrics. This paper evaluates an ensemble selection framework designed to optimize arbitrary metrics and automate the process of algorithm selection and parameter tuning. We report the results of experiments that instantiate the framework for three NLP tasks, using six learning algorithms, a wide variety of parameterizations, and 15 performance metrics. Based on our results, we make recommendations for subsequent machine-learning-based research for natural language learning.		While there have been many successful applications of machine learning methods to tasks in NLP, learning algorithms are not typically designed to optimize NLP performance metrics. This paper evaluates an ensemble selection framework designed to optimize arbitrary metrics and automate the process of algorithm selection and parameter tuning. We report the results of experiments that instantiate the framework for three NLP tasks, using six learning algorithms, a wide variety of parameterizations, and 15 performance metrics. Based on our results, we make recommendations for subsequent machine-learning-based research for natural language learning.	['Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']	['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']	['Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']	['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']	['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']	['Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']	['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']	['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']	['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']	['Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']
CCT181	K15-1002	Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .	a holistic lexiconbased approach to opinion mining	a constrained latent variable model for coreference resolution	['K-W Chang', 'R Samdani', 'D Roth']		Uses	['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.', 'This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.', 'The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.', 'By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from Recasens et al. (2013).', 'This joint framework aims to improve performance on both mention head detection and on coreference.']	5	['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in Chang et al. (2013) and the ILP formulation from .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.', 'This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.', 'The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.', 'By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from Recasens et al. (2013).', 'This joint framework aims to improve performance on both mention head detection and on coreference.']	['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in Chang et al. (2013) and the ILP formulation from .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.', 'This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.', 'The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.', 'By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from Recasens et al. (2013).', 'This joint framework aims to improve performance on both mention head detection and on coreference.']	Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.		Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.	['Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .']	['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .']	['Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.']	['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .']	['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .']	['Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.']	['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.']	['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.']	['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .']	['Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.', 'This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.']
CCT182	W06-3309	Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .	a holistic lexiconbased approach to opinion mining	the interaction of domain knowledge and linguistic structure in natural language processing interpreting hypernymic propositions in biomedical text	['Thomas C Rindflesch', 'Marcelo Fiszman']	introduction	Background	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	Interpretation of semantic propositions in free-text documents such as MEDLINE citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. In this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. In order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the Unified Medical Language System (UMLS). After introducing the syntactic processing on which our system depends, we focus on the UMLS knowledge that supports interpretation of hypernymic propositions. We first use semantic groups from the Semantic Network to ensure that the two concepts involved are compatible; hierarchical information in the Metathesaurus then determines which concept is more general and which more specific. A preliminary evaluation of a sample based on the semantic group Chemicals and Drugs provides 83% precision. An error analysis was conducted and potential solutions to the problems encountered are presented. The research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. Additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. The approach has the potential to support a range of applications, including information retrieval and ontology engineering.		Interpretation of semantic propositions in free-text documents such as MEDLINE citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. In this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. In order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the Unified Medical Language System (UMLS). After introducing the syntactic processing on which our system depends, we focus on the UMLS knowledge that supports interpretation of hypernymic propositions. We first use semantic groups from the Semantic Network to ensure that the two concepts involved are compatible; hierarchical information in the Metathesaurus then determines which concept is more general and which more specific. A preliminary evaluation of a sample based on the semantic group Chemicals and Drugs provides 83% precision. An error analysis was conducted and potential solutions to the problems encountered are presented. The research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. Additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. The approach has the potential to support a range of applications, including information retrieval and ontology engineering.	['Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']	['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']	['Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']	['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']	"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']"	['Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']	"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']"	['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']	"['The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']"	['Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).']
CCT183	J00-2001	"The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) ."	a holistic lexiconbased approach to opinion mining	generating natural language linder pragmatic constraints lawrence erlbaum	['Eduard H Hovy']		Background	"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994; Panaget 1994; Wanner 1994). Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']"	0	[]	"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary ; they have been called `` strategic  and `` tactical  components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , `` planning  and `` realization  ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply `` what to say  versus `` how to say it  ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994; Panaget 1994; Wanner 1994). Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']"				"['The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .']"	"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .']"	"['The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .']"	"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .']"	"['The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .']"	"['The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .']"	"['The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .']"	"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .']"	"['The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .']"	"['The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way.""]"
CCT184	P10-4003	Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .	a holistic lexiconbased approach to opinion mining	towards modelling and using common ground in tutorial dialogue	['Mark Buckley', 'Magdalena Wolska']	introduction	Background	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']	0	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']	"In order to avoid miscommunication par-ticipants in dialogue continuously attempt to align their mutual knowledge (the ""common ground""). A setting that is per-haps most prone to misalignment is tu-toring. We propose a model of common ground in tutoring dialogues which explic-itly models the truth and falsity of do-main level contributions and show how it can be used to detect and repair stu-dents ' false conjectures and facilitate stu-dent modelling."		"In order to avoid miscommunication par-ticipants in dialogue continuously attempt to align their mutual knowledge (the ""common ground""). A setting that is per-haps most prone to misalignment is tu-toring. We propose a model of common ground in tutoring dialogues which explic-itly models the truth and falsity of do-main level contributions and show how it can be used to detect and repair stu-dents ' false conjectures and facilitate stu-dent modelling."	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.']	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.']	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.']	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.']	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.']	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']	['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.']
CCT185	P10-4003	We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .	a holistic lexiconbased approach to opinion mining	deep linguistic processing for spoken dialogue systems	['James Allen', 'Myroslava Dzikovska', 'Mehdi Manshadi', 'Mary Swift']	experiments	Uses	"['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']"	5	"['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']"	"['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']"	We describe a framework for deep linguistic processing for natural language understanding in task-oriented spoken dialogue systems. The goal is to create domaingeneral processing techniques that can be shared across all domains and dialogue tasks, combined with domain-specific optimization based on an ontology mapping from the generic LF to the application ontology. This framework has been tested in six domains that involve tasks such as interactive planning, coordination operations, tutoring, and learning.		We describe a framework for deep linguistic processing for natural language understanding in task-oriented spoken dialogue systems. The goal is to create domaingeneral processing techniques that can be shared across all domains and dialogue tasks, combined with domain-specific optimization based on an ontology mapping from the generic LF to the application ontology. This framework has been tested in six domains that involve tasks such as interactive planning, coordination operations, tutoring, and learning.	['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .']	['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .']	['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.']	['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.']	['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .']	"['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output.""]"	['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.']	"['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output.""]"	['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .']	"['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output."", 'Utterance content is represented as a set of extracted objects and relations between them.']"
CCT186	J09-4010	In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .	a holistic lexiconbased approach to opinion mining	evaluation of a largescale email response system	['Y Marom', 'I Zukerman']	method	Uses	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.', 'Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.']	5	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.', 'Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.']	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.', 'Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.']		We are working on a large-scale, corpus-based dialogue system for responding to requests in an email-based help-desk. The size of the corpus presents interesting challenges with respect to evaluation. We discuss the limitations of the automatic evaluation performed in our previous work, and present a user study to address these limitations. We show that this user study is useful for evaluating different response generation strategies, and discuss the issue of representativeness of the sample used in the study given the large corpus on which the system is based.	We are working on a large-scale, corpus-based dialogue system for responding to requests in an email-based help-desk. The size of the corpus presents interesting challenges with respect to evaluation. We discuss the limitations of the automatic evaluation performed in our previous work, and present a user study to address these limitations. We show that this user study is useful for evaluating different response generation strategies, and discuss the issue of representativeness of the sample used in the study given the large corpus on which the system is based.	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .']	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .']	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.']	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.']	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .']	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.', 'Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.']	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.']	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.', 'Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.']	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .']	['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.', 'Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.']
CCT187	W00-1017	The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .	a holistic lexiconbased approach to opinion mining	understanding unsegmented user utterances in realtime spoken dialogue systems	['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']		Uses	['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']	5	['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']	['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']	This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.		This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.	['The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .']	['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .']	['The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.']	['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.']	['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .']	['The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']	['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.']	['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']	['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .']	['The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']
CCT188	W01-1510	We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .	a holistic lexiconbased approach to opinion mining	a lexicalized tree adjoining grammar for english	['The XTAG Research Group']	experiments	Uses	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"	5	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"	This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389		This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389	['We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .']	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .']"	['We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .']	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .']"	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .']"	['We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.']	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .']"	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.']"	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .']"	['We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']
CCT189	P11-1134	After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .	a holistic lexiconbased approach to opinion mining	fluency adequacy or hter exploring different human judgments with a tunable mt metric	['Matthew Snover', 'Nitin Madnani', 'Bonnie Dorr', 'Richard Schwartz']		Background	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	0	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, mea-sure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT met-ric: TER-Plus, which extends the Transla-tion Edit Rate evaluation metric with tun-able parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest aver-age rank in terms of Pearson and Spear-man correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating sig-nificant differences between the types of human judgments.		Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, mea-sure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT met-ric: TER-Plus, which extends the Transla-tion Edit Rate evaluation metric with tun-able parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest aver-age rank in terms of Pearson and Spear-man correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating sig-nificant differences between the types of human judgments.	['After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	['After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	['They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	['After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	['They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	['One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']	['After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']
CCT190	D09-1053	In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .	a holistic lexiconbased approach to opinion mining	language model adaptation with map estimation and the perceptron algorithm	['M Bacchiani', 'B Roark', 'M Saraclar']	conclusion	Background	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	0	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	In this paper, we contrast two language model adaptation approaches: MAP estimation and the perceptron algorithm. Used in isolation, we show that MAP estimation outperforms the latter approach, for reasons which argue for combining the two approaches. When combined, the resulting system provides a 0.7 percent absolute reduction in word error rate over MAP estimation alone. In addition, we demonstrate that, in a multi-pass recognition scenario, it is better to use the perceptron algorithm on early pass word lattices, since the improved error rate improves acoustic model adaptation.		In this paper, we contrast two language model adaptation approaches: MAP estimation and the perceptron algorithm. Used in isolation, we show that MAP estimation outperforms the latter approach, for reasons which argue for combining the two approaches. When combined, the resulting system provides a 0.7 percent absolute reduction in word error rate over MAP estimation alone. In addition, we demonstrate that, in a multi-pass recognition scenario, it is better to use the perceptron algorithm on early pass word lattices, since the improved error rate improves acoustic model adaptation.	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']	['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']
CCT191	W03-0806	GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .	a holistic lexiconbased approach to opinion mining	software architecture for language engineering	['Hamish Cunningham']	experiments	Background	['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']	0	['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']	['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']	Every building, and every computer program, has an architecture: structural and organisational principles that underpin its design and construction. The garden shed  once built by one of the authors had an ad hoc architecture, extracted (somewhat painfully) from the imagination during a slow and non-deterministic process that, luckily, resulted in a structure which keeps the rain on the outside and the mower on the inside (at least for the time being). As well as being ad hoc (i.e. not informed by analysis of similar practice or relevant science or engineering) this architecture is implicit: no explicit design was made, and no records or documentation kept of the construction process.  The pyramid in the courtyard of the Louvre, by contrast, was constructed in a process involving explicit design performed by qualified engineers with a wealth of theoretical and practical knowledge of the properties of materials, the relative merits and strengths of different construction techniques, et cetera.  So it is with software: sometimes it is thrown together by  enthusiastic amateurs; sometimes it is architected, built to last, and intended to be 'not something you finish, but something you start' (to paraphrase Brand (1994). A number of researchers argued in the early and middle 1990s that the field of computational infrastructure or architecture for human language computation merited an increase in attention. The reasoning was that the increasingly large-scale and technologically significant nature of language processing science was placing increasing burdens of an engineering nature on research and development workers seeking robust and practical methods (as was the increasingly collaborative nature of research in this field, which puts a large premium on software integration and interoperation). Over the intervening period a number of significant systems and practices have been developed in what we may call Software Architecture for Language Engineering (SALE).  This special issue represented an opportunity for practitioners in this area to report their work in a coordinated setting, and to present a snapshot of the state-ofthe-art in infrastructural work, which may indicate where further development and further take-up of these systems can be of benefit		Every building, and every computer program, has an architecture: structural and organisational principles that underpin its design and construction. The garden shed  once built by one of the authors had an ad hoc architecture, extracted (somewhat painfully) from the imagination during a slow and non-deterministic process that, luckily, resulted in a structure which keeps the rain on the outside and the mower on the inside (at least for the time being). As well as being ad hoc (i.e. not informed by analysis of similar practice or relevant science or engineering) this architecture is implicit: no explicit design was made, and no records or documentation kept of the construction process.  The pyramid in the courtyard of the Louvre, by contrast, was constructed in a process involving explicit design performed by qualified engineers with a wealth of theoretical and practical knowledge of the properties of materials, the relative merits and strengths of different construction techniques, et cetera.  So it is with software: sometimes it is thrown together by  enthusiastic amateurs; sometimes it is architected, built to last, and intended to be 'not something you finish, but something you start' (to paraphrase Brand (1994). A number of researchers argued in the early and middle 1990s that the field of computational infrastructure or architecture for human language computation merited an increase in attention. The reasoning was that the increasingly large-scale and technologically significant nature of language processing science was placing increasing burdens of an engineering nature on research and development workers seeking robust and practical methods (as was the increasingly collaborative nature of research in this field, which puts a large premium on software integration and interoperation). Over the intervening period a number of significant systems and practices have been developed in what we may call Software Architecture for Language Engineering (SALE).  This special issue represented an opportunity for practitioners in this area to report their work in a coordinated setting, and to present a snapshot of the state-ofthe-art in infrastructural work, which may indicate where further development and further take-up of these systems can be of benefit	['GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .']	['There are a number of generalised NLP systems in the literature.', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .']	['GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']	['There are a number of generalised NLP systems in the literature.', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']	['Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'There are a number of generalised NLP systems in the literature.', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .']	['GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']	['Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'There are a number of generalised NLP systems in the literature.', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']	['There are a number of generalised NLP systems in the literature.', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']	['General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'There are a number of generalised NLP systems in the literature.', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .']	['GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']
CCT192	J91-2003	Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .	a holistic lexiconbased approach to opinion mining	languages with self reference i foundationsquot	['D Perlis']	introduction	Background	"['5.1.1', 'Translation to Logic.', 'The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject (e.g.', 'Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like.', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .', 'Extending and revising Jackendoff\'s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (""that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon""---ibid.).', 'However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.', 'After these remarks we can begin constructing the model of the example paragraph.', 'We assume that constants are introduced by NPs.', 'We have then (i) Constants s, m, d, i, b, 1347 satisfying: ship(s), Messina(m), disease(d), infection(i), death(b), year(1347).']"	0	"['5.1.1', 'Translation to Logic.', 'The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject (e.g.', 'Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like.', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .', 'Extending and revising Jackendoff\'s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (""that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon""---ibid.).', 'However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.', 'After these remarks we can begin constructing the model of the example paragraph.', 'We assume that constants are introduced by NPs.', 'We have then (i) Constants s, m, d, i, b, 1347 satisfying: ship(s), Messina(m), disease(d), infection(i), death(b), year(1347).']"	"['5.1.1', 'Translation to Logic.', 'The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject (e.g.', 'Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like.', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .', 'Extending and revising Jackendoff\'s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (""that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon""---ibid.).', 'However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.', 'After these remarks we can begin constructing the model of the example paragraph.', 'We assume that constants are introduced by NPs.', 'We have then (i) Constants s, m, d, i, b, 1347 satisfying: ship(s), Messina(m), disease(d), infection(i), death(b), year(1347).']"				['Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']	['5.1.1', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']	['Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']	['5.1.1', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']	['Translation to Logic.', '5.1.1', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']	['Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']	['Translation to Logic.', '5.1.1', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']	['5.1.1', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']	['The text concerns events happening in time.', 'Translation to Logic.', '5.1.1', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']	"['Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .', 'Extending and revising Jackendoff\'s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (""that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon""---ibid.).', 'However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.', 'After these remarks we can begin constructing the model of the example paragraph.']"
CCT193	P10-4003	The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .	a holistic lexiconbased approach to opinion mining	interpretation and generation in a knowledgebased tutorial system	['Myroslava O Dzikovska', 'Charles B Callaway', 'Elaine Farrow']	experiments	Uses	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']	5	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']		We discuss how deep interpretation and generation can be integrated with a know-ledge representation designed for question answering to build a tutorial dialogue sys-tem. We use a knowledge representa-tion known to perform well in answering exam-type questions and show that to sup-port tutorial dialogue it needs additional features, in particular, compositional rep-resentations for interpretation and struc-tured explanation representations.	We discuss how deep interpretation and generation can be integrated with a know-ledge representation designed for question answering to build a tutorial dialogue sys-tem. We use a knowledge representa-tion known to perform well in answering exam-type questions and show that to sup-port tutorial dialogue it needs additional features, in particular, compositional rep-resentations for interpretation and struc-tured explanation representations.	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .']	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .']	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .']	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .']	['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']
CCT194	W11-0218	A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	boosting precision and recall of dictionarybased protein name recognition	['Y Tsuruoka', 'J Tsujii']	conclusion	Future	['While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'For NLPBA, GENIA and ID we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'By contrast, for Super GREC there are several distinct classes for which we expected lexical resources to have fair coverage for SimString and Gazetteer features.', 'While an advantage over Internal was observed for Super GREC, SimString features showed no benefit over Gazetteer features.', 'The methods exhibited the expected result on only one of the six corpora, CALBC CII, where there is a clear advantage for Gazetteer over Internal and a further clear advantage for SimString over Gazetteer.', 'Disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'Although we have not been successful in Figure 6: Learning curve for Super GREC Figure 7: Learning curve for EPI proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'It may be that our notion of distance to lexical resource entries is too naive.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']	3	['While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'For NLPBA, GENIA and ID we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'By contrast, for Super GREC there are several distinct classes for which we expected lexical resources to have fair coverage for SimString and Gazetteer features.', 'While an advantage over Internal was observed for Super GREC, SimString features showed no benefit over Gazetteer features.', 'The methods exhibited the expected result on only one of the six corpora, CALBC CII, where there is a clear advantage for Gazetteer over Internal and a further clear advantage for SimString over Gazetteer.', 'Disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'Although we have not been successful in Figure 6: Learning curve for Super GREC Figure 7: Learning curve for EPI proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'It may be that our notion of distance to lexical resource entries is too naive.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']	['While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'For NLPBA, GENIA and ID we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'By contrast, for Super GREC there are several distinct classes for which we expected lexical resources to have fair coverage for SimString and Gazetteer features.', 'While an advantage over Internal was observed for Super GREC, SimString features showed no benefit over Gazetteer features.', 'The methods exhibited the expected result on only one of the six corpora, CALBC CII, where there is a clear advantage for Gazetteer over Internal and a further clear advantage for SimString over Gazetteer.', 'Disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'Although we have not been successful in Figure 6: Learning curve for Super GREC Figure 7: Learning curve for EPI proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'It may be that our notion of distance to lexical resource entries is too naive.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']	Dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches. However, dictionary based approaches have two serious problems: (1) a large number of false recognitions mainly caused by short names. (2) low recall due to spelling variation. In this paper, we tackle the former problem by using a machine learning method to filter out false positives. We also present an approximate string searching method to alleviate the latter problem. Experimental results using the GE-NIA corpus show that the filtering using a naive Bayes classifier greatly improves precision with slight loss of recall, resulting in a much better F-score.		Dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches. However, dictionary based approaches have two serious problems: (1) a large number of false recognitions mainly caused by short names. (2) low recall due to spelling variation. In this paper, we tackle the former problem by using a machine learning method to filter out false positives. We also present an approximate string searching method to alleviate the latter problem. Experimental results using the GE-NIA corpus show that the filtering using a naive Bayes classifier greatly improves precision with slight loss of recall, resulting in a much better F-score.	['A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .']	['While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .']	['A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']	['While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']	['For NLPBA, GENIA and ID we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .']	['A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']	['For NLPBA, GENIA and ID we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']	['While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']	['By contrast, for Super GREC there are several distinct classes for which we expected lexical resources to have fair coverage for SimString and Gazetteer features.', 'For NLPBA, GENIA and ID we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .']	['A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']
CCT195	J97-4003	Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )	a holistic lexiconbased approach to opinion mining	towards a semantics for lexical rules as used in hpsg	['Detmar Meurers']	introduction	Background	['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']	0	[]	['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']		We show how a mechanism that captures the functionality of lexical rules can be integrated into the logical setup for hpsg provided by King (1989, 1994) without having to enlarge the logic with a metalevel or extra notions such as defaults. A notation for lexical rules is introduced and a rewrite system is defined to map the lexical rules as specified by the linguist into ordinary constraints in the King logic. We therefore show how a denotational semantics for lexical rules as used in hpsg linguistics can be provided. Keywords: lexical rules, hpsg, logical issues of the hpsg architecture.	We show how a mechanism that captures the functionality of lexical rules can be integrated into the logical setup for hpsg provided by King (1989, 1994) without having to enlarge the logic with a metalevel or extra notions such as defaults. A notation for lexical rules is introduced and a rewrite system is defined to map the lexical rules as specified by the linguist into ordinary constraints in the King logic. We therefore show how a denotational semantics for lexical rules as used in hpsg linguistics can be provided. Keywords: lexical rules, hpsg, logical issues of the hpsg architecture.	['Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']	['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']	['Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']	['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']	['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']	['Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']	['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']	['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']	['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']	['Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']
CCT196	J03-3004	All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .	a holistic lexiconbased approach to opinion mining	a framework of a mechanical translation between japanese and english by analogy principle	['Makoto Nagao']	introduction	Background	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Rscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']	0	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Rscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Rscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']	Problems inherent in current machine translation systems have been reviewed and have been shown to be inherently inconsistent. The present paper defines a model based on a series of human language processing and in particular the use of analogical thinking. Machine translation systems developed so far have a kind of inherent contradiction in themselves. The more detailed a system has become by the additional improvements, the clearer the limitation and the boundary will be for the translation ability. To break through this difficulty we have to think about the mechanism of human translation, and have to build a model based on the fundamental function of language processing in the human brain. The following is an attempt to do this based on the ability of analogy finding in human beings.		Problems inherent in current machine translation systems have been reviewed and have been shown to be inherently inconsistent. The present paper defines a model based on a series of human language processing and in particular the use of analogical thinking. Machine translation systems developed so far have a kind of inherent contradiction in themselves. The more detailed a system has become by the additional improvements, the clearer the limitation and the boundary will be for the translation ability. To break through this difficulty we have to think about the mechanism of human translation, and have to build a model based on the fundamental function of language processing in the human brain. The following is an attempt to do this based on the ability of analogy finding in human beings.	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .']	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .']	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3']	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3']	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .']	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Roscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.']	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3']	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Roscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.']	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .']	['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Roscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.']
CCT197	J91-2003	The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .	a holistic lexiconbased approach to opinion mining	resolving pronoun referencesquot	['J R Hobbs']	introduction	Background	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']	0	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']		Two approaches to the problem of resolving pronoun references are presented. The first is a naive algorithm that works by traversing the surface parse trees of the sentences of the text in a particular order looking for noun phrases of the correct gender and number. The algorithm clearly does not work in all cases, but the results of an examination of several hundred examples from published texts show that it performs remarkably well. In the second approach, it is shown how pronoun solution can be handled in a comprehensive system for semantic analysis of English texts. The system is described, and it is shown in a detailed treatment of several examples how semantic analysis locates the antecedents of most pronouns as a by-product. Included are the classic examples of Winograd and Charniak.	Two approaches to the problem of resolving pronoun references are presented. The first is a naive algorithm that works by traversing the surface parse trees of the sentences of the text in a particular order looking for noun phrases of the correct gender and number. The algorithm clearly does not work in all cases, but the results of an examination of several hundred examples from published texts show that it performs remarkably well. In the second approach, it is shown how pronoun solution can be handled in a comprehensive system for semantic analysis of English texts. The system is described, and it is shown in a detailed treatment of several examples how semantic analysis locates the antecedents of most pronouns as a by-product. Included are the classic examples of Winograd and Charniak.	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']	['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']
CCT198	J97-4003	In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).	a holistic lexiconbased approach to opinion mining	featurebased inheritance networks for computational lexicons	['Hans-Ulrich Krieger', 'John Nerbonne']	related work	CompareOrContrast	['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']	1	[]	['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']	The virtues of viewing the lexicon as an inheritance network are its succinctness and its tendency to highlight significant clusters of linguistic properties. From its succinctness follow two practical advantages, namely its ease of maintenance and modification. In this paper we present a feature-based foundation for lexical inheritance. We argue that the feature-based foundation is both more economical and expressively more powerful than non-feature-based systems. It is more economical because it employs only mechanisms already assumed to be present elsewhere in the grammar (viz., in the feature system), and it is more expressive because feature systems are more expressive than other mechanisms used in expressing lexical inheritance (cf. DATR). The lexicon furthermore allows the use of default unification, based on the ideas of default unification, defined by Bouma. These claims are buttressed in sections sketching the opportunities for lexical description in feature-based lexicons in two central lexical topics, inflection and derivation. Briefly, we argue that the central notion of paradigm may be defined in feature structures, and that it may be more satisfactorily (in fact, immediately) linked to the syntactic information in this fashion. Our discussion of derivation is more programmatic; but here, too, we argue that feature structures of a suitably rich sort provide a foundation for the definition of lexical rules. We illustrate theoretical claims in application to German lexis. This work is currently under implementation in a natural language understanding effort (DISCO) at the German Artiffical Intelligence Center (Deutsches Forschungszentrum fur Kunstliche Intelligenz).		The virtues of viewing the lexicon as an inheritance network are its succinctness and its tendency to highlight significant clusters of linguistic properties. From its succinctness follow two practical advantages, namely its ease of maintenance and modification. In this paper we present a feature-based foundation for lexical inheritance. We argue that the feature-based foundation is both more economical and expressively more powerful than non-feature-based systems. It is more economical because it employs only mechanisms already assumed to be present elsewhere in the grammar (viz., in the feature system), and it is more expressive because feature systems are more expressive than other mechanisms used in expressing lexical inheritance (cf. DATR). The lexicon furthermore allows the use of default unification, based on the ideas of default unification, defined by Bouma. These claims are buttressed in sections sketching the opportunities for lexical description in feature-based lexicons in two central lexical topics, inflection and derivation. Briefly, we argue that the central notion of paradigm may be defined in feature structures, and that it may be more satisfactorily (in fact, immediately) linked to the syntactic information in this fashion. Our discussion of derivation is more programmatic; but here, too, we argue that feature structures of a suitably rich sort provide a foundation for the definition of lexical rules. We illustrate theoretical claims in application to German lexis. This work is currently under implementation in a natural language understanding effort (DISCO) at the German Artiffical Intelligence Center (Deutsches Forschungszentrum fur Kunstliche Intelligenz).	['In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).']	['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).']	['In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']	['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']	['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).']	['In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']	['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']	['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']	['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).']	['In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']
CCT199	P07-1068	These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	automatic acquisition of hyponyms from large text corpora	['M Hearst']	introduction	Motivation	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '(7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"	4	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '(7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '(7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"	We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. 1 Introduction  Currently there is much interest in the automatic acquisition of lexical syntax and semantics, with the goal of building up large lexicons for natural language processing. Projects..		We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. 1 Introduction  Currently there is much interest in the automatic acquisition of lexical syntax and semantics, with the goal of building up large lexicons for natural language processing. Projects..	['These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .']	['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .']	['These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .']	['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .']	['Again, this represents our attempt to coarsely model subcategorization.', '(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .']	['These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .']	['Again, this represents our attempt to coarsely model subcategorization.', '(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .']	['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .']	"[""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'Again, this represents our attempt to coarsely model subcategorization.', '(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .']"	['These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.']
CCT200	J91-2003	Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .	a holistic lexiconbased approach to opinion mining	episodes as chunks in narrative memoryquot	['J B Black', 'G H Bower']	introduction	Background	['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']	0	[]	['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']		Story statements cluster into episodes, so we expected the memory representations of statements to cluster into separate episode chunks in memory. Experiment 1 confirmed this chunking idea by showing that the recall of episode actions depends on the length of that episode, but not on the lengths of other episodes. Specifically, adding more actions to an episode increased recall of the original actions but didn't affect recall of other episodes. Further, subjects rated the original actions as more important than the added actions, thus suggesting that unimportant actions increased the recall of important actions in that episode. Another experiment showed that the more subordinate actions an episode contains, the more likely a statement summarizing that episode is to be recalled.	Story statements cluster into episodes, so we expected the memory representations of statements to cluster into separate episode chunks in memory. Experiment 1 confirmed this chunking idea by showing that the recall of episode actions depends on the length of that episode, but not on the lengths of other episodes. Specifically, adding more actions to an episode increased recall of the original actions but didn't affect recall of other episodes. Further, subjects rated the original actions as more important than the added actions, thus suggesting that unimportant actions increased the recall of important actions in that episode. Another experiment showed that the more subordinate actions an episode contains, the more likely a statement summarizing that episode is to be recalled.	['Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']	['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']	['Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']	['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']	['These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']	['Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']	['These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']	['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']	['Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']	['Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']
CCT201	E03-1002	The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	what is the minimal set of fragments that achieves maximal parse accuracy	['Rens Bod']	experiments	CompareOrContrast	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']	1	['The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2000;Collins, 2000;Bod, 2001).', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2000;Collins, 2000;Bod, 2001).', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']	We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank (a precision of 90.8% and a recall of 90.6%). We isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy.		We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank (a precision of 90.8% and a recall of 90.6%). We isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy.	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .']	['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.']
CCT202	D09-1056	Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .	a holistic lexiconbased approach to opinion mining	irstbp web people search using name entities	['Octavian Popescu', 'Bernardo Magnini']	related work	Background	['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']	0	['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']	['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']		In this paper we describe a person clustering system for web pages and report the results we have obtained on the test set of the Semeval 2007 Web Person Search task. Deciding which particular person a name refers to within a text document depends mainly on the capacity to extract the relevant information out of texts when it is present. We consider relevant here to stand primarily for two properties: (1) uniqueness and (2) appropriateness. In order to address both (1) and (2) our method gives primary importance to Name Entities (NEs), defined according to the ACE specifications. The common nouns not referring to entities are considered further as coreference clues only if they are found within already coreferred documents. 	In this paper we describe a person clustering system for web pages and report the results we have obtained on the test set of the Semeval 2007 Web Person Search task. Deciding which particular person a name refers to within a text document depends mainly on the capacity to extract the relevant information out of texts when it is present. We consider relevant here to stand primarily for two properties: (1) uniqueness and (2) appropriateness. In order to address both (1) and (2) our method gives primary importance to Name Entities (NEs), defined according to the ACE specifications. The common nouns not referring to entities are considered further as coreference clues only if they are found within already coreferred documents. 	['Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .']	['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .']	['Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .']	['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .']	['The most basic is a Bag of Words (BoW) representation of the document text.', 'Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .']	['Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .']	['The most basic is a Bag of Words (BoW) representation of the document text.', 'Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .']	['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .']	['Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .']	['Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.']
CCT203	P00-1007	In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .	a holistic lexiconbased approach to opinion mining	a maximumentropy partial parser for unrestricted text	['W Skut', 'T Brants']	conclusion	Future	['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']	3	[]	[]		This paper describes a partial parser that assigns syntactic structures to sequences of part-of-speech tags. The program uses the maximum entropy parameter estimation method, which allows a flexible combination of different knowledge sources: the hierarchical structure, parts of speech and phrasal categories. In effect, the parser goes beyond simple bracketing and recognises even fairly complex structures. We give accuracy figures for different applications of the parser.	This paper describes a partial parser that assigns syntactic structures to sequences of part-of-speech tags. The program uses the maximum entropy parameter estimation method, which allows a flexible combination of different knowledge sources: the hierarchical structure, parts of speech and phrasal categories. In effect, the parser goes beyond simple bracketing and recognises even fairly complex structures. We give accuracy figures for different applications of the parser.	['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']	['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']	['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']	['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']	['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']	['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']	['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']	['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']	['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']	['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']
CCT204	P07-1068	As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .	a holistic lexiconbased approach to opinion mining	using semantic relations to refine coreference decisions	['H Ji', 'D Westbrook', 'R Grishman']	introduction	Background	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	0	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses. Experiments show that this new framework can improve performance on two quite different languages - English and Chinese.		We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses. Experiments show that this new framework can improve performance on two quite different languages - English and Chinese.	['As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']	['As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']
CCT205	P08-1101	We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .	a holistic lexiconbased approach to opinion mining	chinese segmentation with a wordbased perceptron algorithm	['Yue Zhang', 'Stephen Clark']	experiments	Extends	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).', 'The features used by the baseline segmentor are shown in Table 1.', 'The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2.']	2	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).', 'The features used by the baseline segmentor are shown in Table 1.', 'The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2.']	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).', 'The features used by the baseline segmentor are shown in Table 1.', 'The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2.']		Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. Discrimina- tively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation. In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beam- search decoder. Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.	Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. Discrimina- tively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation. In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beam- search decoder. Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .']	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .']	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).']	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).']	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .']	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).', 'The features used by the baseline segmentor are shown in Table 1.']	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).']	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).', 'The features used by the baseline segmentor are shown in Table 1.']	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .']	['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).', 'The features used by the baseline segmentor are shown in Table 1.', 'The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2.']
CCT206	J07-1005	Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	the nlm indexing initiatives medical text indexer	['Alan R Aronson', 'James G Mork', 'Clifford W Gay', 'Susanne M Humphrey', 'Willie J Rogers']		Future	['The function (t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']	3	['The function (t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']	['The function (t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']		The Medical Text Indexer (MTI) is a program for producing MeSH indexing recommendations. It is the major product of NLM's Indexing Initiative and has been used in both semi-automated and fully automated indexing environments at the Library since mid 2002. We report here on an experiment conducted with MEDLINE indexers to evaluate MTI's performance and to generate ideas for its improvement as a tool for user-assisted indexing. We also discuss some filtering techniques developed to improve MTI's accuracy for use primarily in automatically producing the indexing for several abstracts collections.	The Medical Text Indexer (MTI) is a program for producing MeSH indexing recommendations. It is the major product of NLM's Indexing Initiative and has been used in both semi-automated and fully automated indexing environments at the Library since mid 2002. We report here on an experiment conducted with MEDLINE indexers to evaluate MTI's performance and to generate ideas for its improvement as a tool for user-assisted indexing. We also discuss some filtering techniques developed to improve MTI's accuracy for use primarily in automatically producing the indexing for several abstracts collections.	['Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']	['The function a(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']	['Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']	['The function a(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']	['The function a(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']	['Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']	['The function a(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']	['The function a(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']	['The function a(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']	['Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']
CCT207	W02-0309	Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	the semantic structure of neoclassical compounds	['A McCray', 'A Browne', 'D Moore']	introduction	Background	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']	0	[]	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']		The automated analysis of neo-classical compounds in the medical domain has been proposed and carried out by a number of researchers in recent years. This paper discusses the semantics of these compounds. The results of our work indicate that neo-classical compounds are semantically underdetermined by their constituent parts. Thus, automated analysis of these compounds will need to be supplemented by human review.	The automated analysis of neo-classical compounds in the medical domain has been proposed and carried out by a number of researchers in recent years. This paper discusses the semantics of these compounds. The results of our work indicate that neo-classical compounds are semantically underdetermined by their constituent parts. Thus, automated analysis of these compounds will need to be supplemented by human review.	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .']	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .']	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .']	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .']	['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']
CCT208	J12-4003	Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .	a holistic lexiconbased approach to opinion mining	beyond nombank a study of implicit arguments for nominal predicates	['Matthew Gerber', 'Joyce Chai']	conclusion	Extends	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section.']	2	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section.']	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section.']	Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task.		Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task.	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .']	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .']	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.']	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.']	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .']	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section.']	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.']	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section.']	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .']	['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section.']
CCT209	D10-1052	To model d ( FWi  1 , S  T ) , d ( FWi +1 , S  T ) , i.e. whether Li , S  T and Ri , S  T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	topological ordering of function words in hierarchical phrasebased translation	['Hendra Setiawan', 'Min Yen Kan', 'Haizhou Li', 'Philip Resnik']		Uses	['To model d ( FWi \x88\x92 1 , S \x86\x92 T ) , d ( FWi +1 , S \x86\x92 T ) , i.e. whether Li , S \x86\x92 T and Ri , S \x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .', 'Taking d(F W i1,ST ) as a case in point, this model takes the form']	5	['To model d(F W i1,ST ), d(F W i+1,ST ), i.e. whether L i,ST and R i,ST extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of Setiawan et al. (2009).', 'Taking d(F W i1,ST ) as a case in point, this model takes the form']	['To model d(F W i1,ST ), d(F W i+1,ST ), i.e. whether L i,ST and R i,ST extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of Setiawan et al. (2009).', 'Taking d(F W i1,ST ) as a case in point, this model takes the form']	Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotation-driven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance.		Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotation-driven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance.	['To model d ( FWi \x88\x92 1 , S \x86\x92 T ) , d ( FWi +1 , S \x86\x92 T ) , i.e. whether Li , S \x86\x92 T and Ri , S \x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .']	['To model d ( FWi \x88\x92 1 , S \x86\x92 T ) , d ( FWi +1 , S \x86\x92 T ) , i.e. whether Li , S \x86\x92 T and Ri , S \x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .']	['To model d ( FWi \x88\x92 1 , S \x86\x92 T ) , d ( FWi +1 , S \x86\x92 T ) , i.e. whether Li , S \x86\x92 T and Ri , S \x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .', 'Taking d(F W i-1,S-T ) as a case in point, this model takes the form']	['To model d ( FWi \x88\x92 1 , S \x86\x92 T ) , d ( FWi +1 , S \x86\x92 T ) , i.e. whether Li , S \x86\x92 T and Ri , S \x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .', 'Taking d(F W i-1,S-T ) as a case in point, this model takes the form']	['To model d ( FWi \x88\x92 1 , S \x86\x92 T ) , d ( FWi +1 , S \x86\x92 T ) , i.e. whether Li , S \x86\x92 T and Ri , S \x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .']	['To model d ( FWi \x88\x92 1 , S \x86\x92 T ) , d ( FWi +1 , S \x86\x92 T ) , i.e. whether Li , S \x86\x92 T and Ri , S \x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .', 'Taking d(F W i-1,S-T ) as a case in point, this model takes the form']	['To model d ( FWi \x88\x92 1 , S \x86\x92 T ) , d ( FWi +1 , S \x86\x92 T ) , i.e. whether Li , S \x86\x92 T and Ri , S \x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .', 'Taking d(F W i-1,S-T ) as a case in point, this model takes the form']	['To model d ( FWi \x88\x92 1 , S \x86\x92 T ) , d ( FWi +1 , S \x86\x92 T ) , i.e. whether Li , S \x86\x92 T and Ri , S \x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .', 'Taking d(F W i-1,S-T ) as a case in point, this model takes the form']	['To model d ( FWi \x88\x92 1 , S \x86\x92 T ) , d ( FWi +1 , S \x86\x92 T ) , i.e. whether Li , S \x86\x92 T and Ri , S \x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .']	['To model d ( FWi \x88\x92 1 , S \x86\x92 T ) , d ( FWi +1 , S \x86\x92 T ) , i.e. whether Li , S \x86\x92 T and Ri , S \x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .', 'Taking d(F W i-1,S-T ) as a case in point, this model takes the form']
CCT210	J91-2003	For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''	a holistic lexiconbased approach to opinion mining	lectures on contemporary syntactic theories csli lecture notes	['P Sells']	introduction	Motivation	"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.', 'We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.', ""That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end."", 'We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.']"	4	[]	"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.', 'We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.', ""That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end."", 'We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.']"		This book rpovides an introduction to three contemporary syntactic theories, Government-Binding Theory, Generalized Phrase Structure Grammar, and Lexical-Functional Grammar. In Successive chapters, Sells lucidly presents and illustrates the fundamental aspects of each theory. In an introductory chapter, he describes the basic syntactic concepts and assumptionsshared by each theory; in the postscript, Thomas Wasow provides a more general overview of the different perspectives of these three approaches.	This book rpovides an introduction to three contemporary syntactic theories, Government-Binding Theory, Generalized Phrase Structure Grammar, and Lexical-Functional Grammar. In Successive chapters, Sells lucidly presents and illustrates the fundamental aspects of each theory. In an introductory chapter, he describes the basic syntactic concepts and assumptionsshared by each theory; in the postscript, Thomas Wasow provides a more general overview of the different perspectives of these three approaches.	"[""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''""]"	"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''""]"	"[""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.']"	"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''""]"	"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''""]"	"[""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.']"	"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.']"	"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.']"	"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''""]"	"[""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.', 'We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.']"
CCT211	D08-1034	Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	automatic labeling of semantic roles	['Daniel Gildea', 'Daniel Jurafsky']	introduction	Background	['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation (Boas 2002).', 'With the efforts of many researchers (Carreras and Mrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']	0	['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation (Boas 2002).', 'With the efforts of many researchers (Carreras and Mrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']	['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation (Boas 2002).', 'With the efforts of many researchers (Carreras and Mrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']	We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.		We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.	['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .']	['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .']	['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.']	['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.']	['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .']	['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.']	['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.']	['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.']	['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .']	['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.']
CCT212	W06-2807	AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	ajax a new approach to web applications url httpwwwadaptivepathcompublicationsessays archives000385php retrieved the 22nd of december	['Jesse James Garrett']		Background	['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']	0	['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']	['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']				['AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']	['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']	['AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']	['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']	['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']	['AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']	['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']	['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']	['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']	['AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']
CCT213	J01-4001	The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	recognizing referential links an information extraction perspective	['Megumi Kameyama']		Background	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']	0	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']	We present an efficient and robust reference resolution algorithm in an end-to-end state-of-the-art information extraction system, which must work with a considerably impoverished syntactic analysis of the input sentences. Considering this disadvantage, the basic setup to collect, filter, then order by salience does remarkably well with third-person pronouns, but needs more semantic and discourse information to improve the treatments of other expression types.		We present an efficient and robust reference resolution algorithm in an end-to-end state-of-the-art information extraction system, which must work with a considerably impoverished syntactic analysis of the input sentences. Considering this disadvantage, the basic setup to collect, filter, then order by salience does remarkably well with third-person pronouns, but needs more semantic and discourse information to improve the treatments of other expression types.	['The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .']	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .']	['The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.']	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .']	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .']	['The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).']	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.']	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).']	['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .']	['The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.']
CCT214	P97-1063	The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .	a holistic lexiconbased approach to opinion mining	line em up advances in alignment technology and their impact on translation support toolsquot	['E Macklovitch', 'M-L Hannan']		CompareOrContrast	"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.', 'Therefore, we ignored English words that were linked to nothing.']"	1	"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.', 'Therefore, we ignored English words that were linked to nothing.']"	"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.', 'Therefore, we ignored English words that were linked to nothing.']"	We present a quantitative evaluation of one well-known word-alignment algorithm, as well as an analysis of frequent errors in terms of this model's underlying assumptions. Despite error rates that range from 22% to 32%, we argue that this technology can be put to good use in certain automated aids for human translators. We support our contention by pointing to several successful applications and outline ways in which text alignments below the sentence level would allow us to improve the performance of other translation support tools.		We present a quantitative evaluation of one well-known word-alignment algorithm, as well as an analysis of frequent errors in terms of this model's underlying assumptions. Despite error rates that range from 22% to 32%, we argue that this technology can be put to good use in certain automated aids for human translators. We support our contention by pointing to several successful applications and outline ways in which text alignments below the sentence level would allow us to improve the performance of other translation support tools.	"[""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]"	"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]"	"[""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]"	"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]"	"['Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', 'The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]"	"[""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]"	"['Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', 'The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]"	"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]"	"['Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', 'The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .""]"	"[""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.']"
CCT215	E03-1002	Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	a maximumentropyinspired parser	['Eugene Charniak']		CompareOrContrast	['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000).', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999).', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', 'We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.', 'These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features.']	1	['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000).', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999).', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', 'We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.', 'These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features.']	['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000).', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999).', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', 'We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.', 'These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features.']		"We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5, 9, 10, 15, 17] ""standard"" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a ""maximum-entropy-inspired"" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head."	"We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5, 9, 10, 15, 17] ""standard"" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a ""maximum-entropy-inspired"" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head."	['Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .']	['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .']	['Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .']	['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .']	['The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000).', 'In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .']	['Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .']	['The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000).', 'In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .']	['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .']	['The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999).', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000).', 'In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .']	['Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', 'We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.', 'These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.']
CCT216	J00-3001	While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :	a holistic lexiconbased approach to opinion mining	word association norms mutual information and lexicography	['Kenneth W Church', 'Patrick Hanks']	conclusion	Background	"[""Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161."", 'The extraction results for both tests as measured by F are 0.31 and 0.33, respectively.', 'This procedure allows us to extract 64/139 = 46.0% of the lowfrequency words and 66/170 -~ 38.8% of the high-frequency words using G 2, and 64/139 = 46.0%', 'and 79/170 = 46.7%,', ""respectively, using Fisher's exact test."", ""Note that this technique is optimal for the extraction of the lowest-frequency words, leading to identical performance for G 2 and Fisher's exact test for these words."", ""For the higherfrequency words, Fisher's exact test leads to a slightly better recall with the same precision scores (0.31 for both tests)."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	0	[]	"[""Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161."", 'The extraction results for both tests as measured by F are 0.31 and 0.33, respectively.', 'This procedure allows us to extract 64/139 = 46.0% of the lowfrequency words and 66/170 -~ 38.8% of the high-frequency words using G 2, and 64/139 = 46.0%', 'and 79/170 = 46.7%,', ""respectively, using Fisher's exact test."", ""Note that this technique is optimal for the extraction of the lowest-frequency words, leading to identical performance for G 2 and Fisher's exact test for these words."", ""For the higherfrequency words, Fisher's exact test leads to a slightly better recall with the same precision scores (0.31 for both tests)."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words		The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words	"[""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	"[""Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	"[""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	"[""Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	"['The extraction results for both tests as measured by F are 0.31 and 0.33, respectively.', ""Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	"[""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	"['The extraction results for both tests as measured by F are 0.31 and 0.33, respectively.', ""Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	"[""Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	"['This procedure allows us to extract 64/139 = 46.0% of the lowfrequency words and 66/170 -~ 38.8% of the high-frequency words using G 2, and 64/139 = 46.0%', 'The extraction results for both tests as measured by F are 0.31 and 0.33, respectively.', ""Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	"[""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"
CCT217	W02-0309	Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .	a holistic lexiconbased approach to opinion mining	responsa an operational fulltext retrieval system with linguistic components for large corpora	['Y Choueka']	introduction	Background	"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]"	0	"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]"	"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]"				['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']	['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']	"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure']).""]"	"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure']).""]"	['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']	"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]"	"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure']).""]"	"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]"	['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']	"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J  appinen and Niemist  o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]"
CCT218	W04-0910	For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .	a holistic lexiconbased approach to opinion mining	alternations and verb semantic classes for french analysis and class formation chapter 5	['P Saint-Dizier']		Background	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']	0	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']	In this paper, we show how alternations (called here contexts) can be defined for French, what their semantic properties are and how verb semantic classes can be constructed from syntactic criteria following (Levin 93). We then analyze the global quality of the results in terms of overlap with classes formed from the same verb-senses using WordNetlike classification criteria. Finally, we propose a method which combines these two approaches to form verb semantic classes better suited for natural language processing.		In this paper, we show how alternations (called here contexts) can be defined for French, what their semantic properties are and how verb semantic classes can be constructed from syntactic criteria following (Levin 93). We then analyze the global quality of the results in terms of overlap with classes formed from the same verb-senses using WordNetlike classification criteria. Finally, we propose a method which combines these two approaches to form verb semantic classes better suited for natural language processing.	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .']	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .']	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.']	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.']	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .']	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.']	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .']	['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']
CCT219	N04-2004	A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )	a holistic lexiconbased approach to opinion mining	verbs and times	['Zeno Vendler']		Background	"['Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	0	[]	"['Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	"T HE fact that verbs have tenses indicates that considerations involving the concept of time are relevant to their use. These considerations are not limited merely to the obvious discrimination between past, present, and future; there is another, a more subtle dependence on that concept: the use of a verb may also suggest the particular way in which that verb presupposes and involves the notion of time. In a number of recent publications some attention has been paid to these finer aspects, perhaps for the first time systematically. Distinctions have been made among verbs suggesting processes, states, dispositions, occurrences, tasks, achievements, and so on. Obviously these differences cannot be explained in terms of time alone: other factors, like the presence or absence of an object, conditions, intended states of affairs, also enter the picture. Nevertheless one feels that the time element remains crucial; at least it is important enough to warrant separate treatment. Indeed, as I intend to show, if we focus our attention primarily upon the time schemata presupposed by various verbs,"" we are able to throw light on some of the obscurities which still remain in these matters. These time schemata will appear as important constituents of the concepts that prompt us to use those terms the way we consistently do. There are a few such schemata of very wide application. Once they have been discovered in some typical examples, they may be used as models of comparison in exploring and clarifying the behavior of any verb whatever. In indicating these schemata, I do not claim that they represent all possible ways in which verbs can be used correctly with respect to time determination nor that a verb exhibiting a use fairly covered by one schema cannot have divergent uses, which"		"T HE fact that verbs have tenses indicates that considerations involving the concept of time are relevant to their use. These considerations are not limited merely to the obvious discrimination between past, present, and future; there is another, a more subtle dependence on that concept: the use of a verb may also suggest the particular way in which that verb presupposes and involves the notion of time. In a number of recent publications some attention has been paid to these finer aspects, perhaps for the first time systematically. Distinctions have been made among verbs suggesting processes, states, dispositions, occurrences, tasks, achievements, and so on. Obviously these differences cannot be explained in terms of time alone: other factors, like the presence or absence of an object, conditions, intended states of affairs, also enter the picture. Nevertheless one feels that the time element remains crucial; at least it is important enough to warrant separate treatment. Indeed, as I intend to show, if we focus our attention primarily upon the time schemata presupposed by various verbs,"" we are able to throw light on some of the obscurities which still remain in these matters. These time schemata will appear as important constituents of the concepts that prompt us to use those terms the way we consistently do. There are a few such schemata of very wide application. Once they have been discovered in some typical examples, they may be used as models of comparison in exploring and clarifying the behavior of any verb whatever. In indicating these schemata, I do not claim that they represent all possible ways in which verbs can be used correctly with respect to time determination nor that a verb exhibiting a use fairly covered by one schema cannot have divergent uses, which"	"[""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	"['Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	"[""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	"['Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	"['Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	"[""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	"['Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	"['Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	"['Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	"[""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"
CCT220	J03-3004	#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .	a holistic lexiconbased approach to opinion mining	a method for extracting translation patterns from translation examples	['Hideo Watanabe']	introduction	Background	"['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Gvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.']"	0	[]	"['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Gvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.']"				['#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .']	"['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .']"	['#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Guvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.']	"['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Guvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.']"	"['Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .']"	['#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Guvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.']	"['Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Guvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.']"	"['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Guvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.']"	"['Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .']"	['#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Guvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.']
CCT221	W04-0910	Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .	a holistic lexiconbased approach to opinion mining	an algebra for semantic construction in constraintbased grammars	['A Copestake', 'A Lascarides', 'D Flickinger']		Background	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"	0	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance, (Copestake and Flickinger, 2000;Copestake et al., 2001) describes a Head Driven Phrase Structure Grammar (HPSG) which supports the parallel construction of a phrase structure (or derived) tree and of a semantic representation and (Dalrymple, 1999) show how to equip Lexical Functional grammar (LFG) with a glue semantics.']"	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance, (Copestake and Flickinger, 2000;Copestake et al., 2001) describes a Head Driven Phrase Structure Grammar (HPSG) which supports the parallel construction of a phrase structure (or derived) tree and of a semantic representation and (Dalrymple, 1999) show how to equip Lexical Functional grammar (LFG) with a glue semantics.']"		We develop a framework for formalizing semantic construction within grammars expressed in typed feature structure logics, including HPSG. The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unificationbased approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability.	We develop a framework for formalizing semantic construction within grammars expressed in typed feature structure logics, including HPSG. The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unificationbased approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability.	['Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"	['Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"	['Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"	['Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']
CCT222	D10-1052	The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	topological ordering of function words in hierarchical phrasebased translation	['Hendra Setiawan', 'Min Yen Kan', 'Haizhou Li', 'Philip Resnik']		Extends	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.', 'This section provides a high level overview of our reordering model, which attempts to leverage this information.']	2	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.', 'This section provides a high level overview of our reordering model, which attempts to leverage this information.']	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.', 'This section provides a high level overview of our reordering model, which attempts to leverage this information.']	Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotation-driven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance.		Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotation-driven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance.	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .']	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .']	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.']	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.']	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .']	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.']	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.']	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.']	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .']	['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.', 'This section provides a high level overview of our reordering model, which attempts to leverage this information.']
CCT223	E03-1005	And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .	a holistic lexiconbased approach to opinion mining	new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron	['M Collins', 'N Duffy']	introduction	Motivation	"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial (e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	4	"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial (e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial (e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data."		"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data."	"[""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	"[""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	"['This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	"[""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	"['This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	"['However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	"[""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"
CCT224	J09-4010	In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	sentence fusion for multidocument news summarization	['R Barzilay', 'K R McKeown']	method	Background	['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'This task can be cast as extractive multi-document summarization.', 'Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	0	['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'This task can be cast as extractive multi-document summarization.', 'Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'This task can be cast as extractive multi-document summarization.', 'Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources.		A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources.	['In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	['In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	['An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	['In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	['An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	['This task can be cast as extractive multi-document summarization.', 'An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']	['In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']
CCT225	D11-1138	criteria and data used in our experiments are based on the work of #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	a lightweight evaluation framework for machine translation reordering	['D Talbot', 'H Kazawa', 'H Ichikawa', 'J Katz-Brown', 'M Seno', 'F Och']	experiments	Uses	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	5	[]	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	Reordering is a major challenge for machine translation between distant languages. Recent work has shown that evaluation metrics that explicitly account for target language word order correlate better with human judgments of translation quality. Here we present a simple framework for evaluating word order independently of lexical choice by comparing the system's reordering of a source sentence to reference reordering data generated from manually word-aligned translations. When used to evaluate a system that performs reordering as a preprocessing step our framework allows the parser and reordering rules to be evaluated extremely quickly without time-consuming end-to-end machine translation experiments. A novelty of our approach is that the translations used to generate the reordering reference data are generated in an alignment-oriented fashion. We show that how the alignments are generated can significantly effect the robustness of the evaluation. We also outline some ways in which this framework has allowed our group to analyze reordering errors for English to Japanese machine translation.		Reordering is a major challenge for machine translation between distant languages. Recent work has shown that evaluation metrics that explicitly account for target language word order correlate better with human judgments of translation quality. Here we present a simple framework for evaluating word order independently of lexical choice by comparing the system's reordering of a source sentence to reference reordering data generated from manually word-aligned translations. When used to evaluate a system that performs reordering as a preprocessing step our framework allows the parser and reordering rules to be evaluated extremely quickly without time-consuming end-to-end machine translation experiments. A novelty of our approach is that the translations used to generate the reordering reference data are generated in an alignment-oriented fashion. We show that how the alignments are generated can significantly effect the robustness of the evaluation. We also outline some ways in which this framework has allowed our group to analyze reordering errors for English to Japanese machine translation.	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']
CCT226	K15-1002	We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	ontonotes the 90 solution	['E Hovy', 'M Marcus', 'M Palmer', 'L Ramshaw', 'R Weischedel']	experiments	Uses	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).', 'Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets.']	5	[]	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).', 'Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets.']	We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.		We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .']	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .']	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).']	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).']	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .']	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).', 'Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets.']	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).']	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).', 'Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets.']	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .']	['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).', 'Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets.']
CCT227	J09-4010	 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	automatic generation of domain models for callcenters from noisy transcriptions	['S Roy', 'L V Subramaniam']	method	CompareOrContrast	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	1	[]	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	Call centers handle customer queries from various domains such as computer sales and support, mobile phones, car rental, etc. Each such domain generally has a domain model which is essential to handle customer complaints. These models contain common problem categories, typical customer issues and their solutions, greeting styles. Currently these models are manually created over time. Towards this, we propose an unsupervised technique to generate domain models automatically from call transcriptions. We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers, which still results in high word error rates (40%) and show that even from these noisy transcriptions of calls we can automatically build a domain model. The domain model is comprised of primarily a topic taxonomy where every node is characterized by topic(s), typical Questions-Answers (Q&As), typical actions and call statistics. We show how such a domain model can be used for topic identification of unseen calls. We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model.		Call centers handle customer queries from various domains such as computer sales and support, mobile phones, car rental, etc. Each such domain generally has a domain model which is essential to handle customer complaints. These models contain common problem categories, typical customer issues and their solutions, greeting styles. Currently these models are manually created over time. Towards this, we propose an unsupervised technique to generate domain models automatically from call transcriptions. We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers, which still results in high word error rates (40%) and show that even from these noisy transcriptions of calls we can automatically build a domain model. The domain model is comprised of primarily a topic taxonomy where every node is characterized by topic(s), typical Questions-Answers (Q&As), typical actions and call statistics. We show how such a domain model can be used for topic identification of unseen calls. We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model.	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	['\x80 Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']
CCT228	E03-1005	And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	efficient algorithms for parsing the dop model	['J Goodman']	introduction	Background	"['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', ""Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree."", 'But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constituents parse.', ""Moreover, Goodman's PCFG reduction may also be used to estimate the most probable parse by Viterbi n-best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this, Goodman's method can do the same job with a more compact grammar.""]"	0	"['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', ""Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree."", 'But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constituents parse.', ""Moreover, Goodman's PCFG reduction may also be used to estimate the most probable parse by Viterbi n-best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this, Goodman's method can do the same job with a more compact grammar.""]"	"['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', ""Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree."", 'But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constituents parse.', ""Moreover, Goodman's PCFG reduction may also be used to estimate the most probable parse by Viterbi n-best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this, Goodman's method can do the same job with a more compact grammar.""]"	Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.		Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.	['And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .']	['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .']	"['And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.""]"	['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .']	['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .']	"['And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.']"	"['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.""]"	"['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.']"	['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .']	"['And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', ""Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.""]"
CCT229	W06-2933	Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	japanese dependency analysis using cascaded chunking	['T Kudo', 'Y Matsumoto']	introduction	Uses	[' A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', ' History-based feature models for predicting the next parser action (Black et al., 1992).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .', ' Graph transformations for recovering nonprojective structures .']	5	[' A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', ' History-based feature models for predicting the next parser action (Black et al., 1992).', ' Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', ' Graph transformations for recovering nonprojective structures .']	[' A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', ' History-based feature models for predicting the next parser action (Black et al., 1992).', ' Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', ' Graph transformations for recovering nonprojective structures .']	In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model. Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable. We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.		In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model. Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable. We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.	['Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .']	['* A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .']	['Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .', '* Graph transformations for recovering nonprojective structures .']	['* A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .', '* Graph transformations for recovering nonprojective structures .']	['* History-based feature models for predicting the next parser action (Black et al., 1992).', '* A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .']	['Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .', '* Graph transformations for recovering nonprojective structures .']	['* History-based feature models for predicting the next parser action (Black et al., 1992).', '* A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .', '* Graph transformations for recovering nonprojective structures .']	['* A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .', '* Graph transformations for recovering nonprojective structures .']	['* History-based feature models for predicting the next parser action (Black et al., 1992).', '* A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .']	['Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .', '* Graph transformations for recovering nonprojective structures .']
CCT230	E03-1005	#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .	a holistic lexiconbased approach to opinion mining	efficient algorithms for parsing the dop model	['J Goodman']	introduction	Background	"['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', ""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999;Chappelier et al. 2002)."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998b showed that DOP1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'Cross-validation is needed to avoid this problem.', 'But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).', ""Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', ""Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."", ""Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees, reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ."", ""Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.""]"	0	"['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', ""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999;Chappelier et al. 2002)."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998b showed that DOP1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'Cross-validation is needed to avoid this problem.', 'But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).', ""Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', ""Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."", ""Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees, reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ."", ""Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.""]"	"['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', ""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999;Chappelier et al. 2002)."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998b showed that DOP1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'Cross-validation is needed to avoid this problem.', 'But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).', ""Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', ""Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."", ""Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees, reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ."", ""Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.""]"	Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.		Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.	['#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .']	['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .']	['#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .']	['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .']	['DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .']	['#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .']	['DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .']	['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .']	['Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .']	"['#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998b showed that DOP1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.']"
CCT231	N01-1009	#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .	a holistic lexiconbased approach to opinion mining	the generative lexicon	['James Pustejovsky']	introduction	Background	"['(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', 'The meaning of adjective-noun combinations like those in (1) and ( 2) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.', 'For example, an easy problem is ""a problem that is easy to solve"" or ""a problem that one can solve easily"".', 'In order to account for the meaning of these combinations Vendler (1968, 92) points out that ""in most cases not one verb, but a family of verbs is needed"".', 'Vendler further observes that the noun figuring in an adjective-noun combination is usually the subject or object of the paraphrasing verb.', 'Although fast usually triggers a verb-subject interpretation (see (1)), easy and difficult trigger verb-object interpretations (see (2a,b)).', 'An easy problem is usually a problem that is easy to solve, whereas a difficult language is a language that is difficult to learn, speak, or write.', 'Adjectives like good allow either verb-subject or verb-object interpretations: a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', 'Pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'For example, the telic (purpose) role of the qualia structure for problem has a value equivalent to solve.', 'When the adjective easy is combined with problem, it predicates over the telic role of problem and consequently the adjective-noun combination receives the interpretation a problem that is easy to solve.', 'Pustejovsky (1995) does not give an exhaustive list of the telic roles a given noun may have.', 'Furthermore, in cases where more than one interpretations are provided (see Vendler (1968)), no information is given with respect to the likelihood of these interpretations.', 'Outof context, the number of interpretations for fast scientist is virtually unlimited, yet some interpretations are more likely than others: fast scientist is more likely to be a scientist who performs experiments quickly or who publishes quickly than a scientist who draws or drinks quickly.']"	0	"['(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', 'The meaning of adjective-noun combinations like those in (1) and ( 2) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.', 'For example, an easy problem is ""a problem that is easy to solve"" or ""a problem that one can solve easily"".', 'In order to account for the meaning of these combinations Vendler (1968, 92) points out that ""in most cases not one verb, but a family of verbs is needed"".', 'Vendler further observes that the noun figuring in an adjective-noun combination is usually the subject or object of the paraphrasing verb.', 'Although fast usually triggers a verb-subject interpretation (see (1)), easy and difficult trigger verb-object interpretations (see (2a,b)).', 'An easy problem is usually a problem that is easy to solve, whereas a difficult language is a language that is difficult to learn, speak, or write.', 'Adjectives like good allow either verb-subject or verb-object interpretations: a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', 'Pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'For example, the telic (purpose) role of the qualia structure for problem has a value equivalent to solve.', 'When the adjective easy is combined with problem, it predicates over the telic role of problem and consequently the adjective-noun combination receives the interpretation a problem that is easy to solve.', 'Pustejovsky (1995) does not give an exhaustive list of the telic roles a given noun may have.', 'Furthermore, in cases where more than one interpretations are provided (see Vendler (1968)), no information is given with respect to the likelihood of these interpretations.', 'Outof context, the number of interpretations for fast scientist is virtually unlimited, yet some interpretations are more likely than others: fast scientist is more likely to be a scientist who performs experiments quickly or who publishes quickly than a scientist who draws or drinks quickly.']"	"['(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', 'The meaning of adjective-noun combinations like those in (1) and ( 2) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.', 'For example, an easy problem is ""a problem that is easy to solve"" or ""a problem that one can solve easily"".', 'In order to account for the meaning of these combinations Vendler (1968, 92) points out that ""in most cases not one verb, but a family of verbs is needed"".', 'Vendler further observes that the noun figuring in an adjective-noun combination is usually the subject or object of the paraphrasing verb.', 'Although fast usually triggers a verb-subject interpretation (see (1)), easy and difficult trigger verb-object interpretations (see (2a,b)).', 'An easy problem is usually a problem that is easy to solve, whereas a difficult language is a language that is difficult to learn, speak, or write.', 'Adjectives like good allow either verb-subject or verb-object interpretations: a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', 'Pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'For example, the telic (purpose) role of the qualia structure for problem has a value equivalent to solve.', 'When the adjective easy is combined with problem, it predicates over the telic role of problem and consequently the adjective-noun combination receives the interpretation a problem that is easy to solve.', 'Pustejovsky (1995) does not give an exhaustive list of the telic roles a given noun may have.', 'Furthermore, in cases where more than one interpretations are provided (see Vendler (1968)), no information is given with respect to the likelihood of these interpretations.', 'Outof context, the number of interpretations for fast scientist is virtually unlimited, yet some interpretations are more likely than others: fast scientist is more likely to be a scientist who performs experiments quickly or who publishes quickly than a scientist who draws or drinks quickly.']"	In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.		In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.	['#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .']	['(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .']	['#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .']	['(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .']	['The meaning of adjective-noun combinations like those in (1) and ( 2) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.', '(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .']	['#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .']	['The meaning of adjective-noun combinations like those in (1) and ( 2) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.', '(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .']	['(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .']	"['For example, an easy problem is ""a problem that is easy to solve"" or ""a problem that one can solve easily"".', 'The meaning of adjective-noun combinations like those in (1) and ( 2) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.', '(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .']"	['#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', 'Pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'For example, the telic (purpose) role of the qualia structure for problem has a value equivalent to solve.', 'When the adjective easy is combined with problem, it predicates over the telic role of problem and consequently the adjective-noun combination receives the interpretation a problem that is easy to solve.']
CCT232	J13-1008	#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .	a holistic lexiconbased approach to opinion mining	getting more from morphology in multilingual dependency parsing	['Matt Hohensee', 'Emily M Bender']	related work	Background	"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;).', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	0	"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;).', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;).', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser. Compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks. We find increases in accuracy of up to 5.3% absolute. While some of this results from the feature set capturing information unrelated to morphology, there is still significant improvement, up to 4.6% absolute, due to the agreement model.		We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser. Compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks. We find increases in accuracy of up to 5.3% absolute. While some of this results from the feature set capturing information unrelated to morphology, there is still significant improvement, up to 4.6% absolute, due to the agreement model.	['#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .']	['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .']	['#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.']	['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.']	['We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .']	"['#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	['We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.']	"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	['Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;).', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .']	"['#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"
CCT233	P02-1001	Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .	a holistic lexiconbased approach to opinion mining	expectation semirings flexible em for finitestate transducers	['Jason Eisner']	introduction	Background	"['o:e , and a:ae  share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"	0	"['o:e , and a:ae  share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"	"['o:e , and a:ae  share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"				['Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']	"['o:e --, and a:ae -- share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"	['Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']	"['o:e --, and a:ae -- share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"	"['The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'o:e --, and a:ae -- share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"	['Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']	"['The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'o:e --, and a:ae -- share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"	"['o:e --, and a:ae -- share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"	"['The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'o:e --, and a:ae -- share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"	['Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']
CCT234	J90-3003	The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .	a holistic lexiconbased approach to opinion mining	aspects of prosody	['J Bing']	introduction	Background	['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']	0	['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']	['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']				['The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .']	['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .']	['The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .']	['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .']	['Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .']	['The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .']	['Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .']	['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .']	['Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .']	['The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.']
CCT235	W06-2933	By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .	a holistic lexiconbased approach to opinion mining	the annotation process in the turkish treebank	['N B Atalay', 'K Oflazer', 'B Say']	experiments	CompareOrContrast	"['The results for Arabic (Haji et al., 2004;Smr et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"	1	"['The results for Arabic (Haji et al., 2004;Smr et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"	"['The results for Arabic (Haji et al., 2004;Smr et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"	We present a progress report of the Turkish Treebank concentrating on various aspects of its design and implementation. In addition to a review of the corpus compilation process and the design  of the annotation scheme, we describe the details of various pre-processing stages and the computer-assisted annotation process		We present a progress report of the Turkish Treebank concentrating on various aspects of its design and implementation. In addition to a review of the corpus compilation process and the design  of the annotation scheme, we describe the details of various pre-processing stages and the computer-assisted annotation process	['By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .']	['The results for Arabic (Hajic et al., 2004;Smrz et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .']	"['By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"	"['The results for Arabic (Hajic et al., 2004;Smrz et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"	['The results for Arabic (Hajic et al., 2004;Smrz et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .']	"['By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"	"['The results for Arabic (Hajic et al., 2004;Smrz et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"	"['The results for Arabic (Hajic et al., 2004;Smrz et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"	['The results for Arabic (Hajic et al., 2004;Smrz et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .']	"['By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"
CCT236	J04-3001	The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	selective sampling using the query by committee algorithm	['Yoav Freund', 'H Sebastian Seung', 'Eli Shamir', 'Naftali Tishby']		Background	['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']	0	['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']	['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']	"We analyze the ""query by committee"" algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons."		"We analyze the ""query by committee"" algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons."	['The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .']	['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .']	['The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']	['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']	['There are two types of selection algorithms: committee-based and single learner.', 'Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .']	['The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']	['There are two types of selection algorithms: committee-based and single learner.', 'Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']	['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']	['A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'There are two types of selection algorithms: committee-based and single learner.', 'Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .']	['The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']
CCT237	P11-1134	Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	moses open source toolkit for statistical machine translation	['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Burch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen']		Uses	['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']	5	['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']	['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']	We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.		We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.	['Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']	['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']	['Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']	['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']	['They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']	['Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']	['They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']	['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']	['There are several methods to build phrase tables.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']	['Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/']
CCT238	J92-1004	Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .	a holistic lexiconbased approach to opinion mining	the minds system using context and dialog to enhance speech recognitionquot	['S R Young']	introduction	Background	"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']"	0	"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']"	"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']"		Contextual knowledge has traditionally been used in multi-sentential textual understanding systems. In contrast, this paper describes a new approach toward using contextual, dialog-based knowledge for speech recognition. To demonstrate this approach, we have built MINDS, a system which uses contextual knowledge to predictively generate expectations about the conceptual content that may be expressed in a system user's next utterance. These expectations are expanded to constrain the possible words which may be matched from an incoming speech signal. To prevent system rigidity and allow for diverse user behavior, the system creates layered predictions which range from very specific to very general. Each time new information becomes available from the ongoing dialog, MINDS generates a different set of layered predictions for processing the next utterance. The predictions contain constraints derived from the contextual, dialog level knowledge sources and each prediction is translated into a grammar usable by our speech recognizer, SPHINX. Since speech recognizers use grammars to dictate legal word sequences and to constrain the recognition process, the dynamically generated grammars reduce the number of word candidates considered by the recognizer. The results demonstrate that speech recognition accuracy is greatly enhanced through the use of predictions.	Contextual knowledge has traditionally been used in multi-sentential textual understanding systems. In contrast, this paper describes a new approach toward using contextual, dialog-based knowledge for speech recognition. To demonstrate this approach, we have built MINDS, a system which uses contextual knowledge to predictively generate expectations about the conceptual content that may be expressed in a system user's next utterance. These expectations are expanded to constrain the possible words which may be matched from an incoming speech signal. To prevent system rigidity and allow for diverse user behavior, the system creates layered predictions which range from very specific to very general. Each time new information becomes available from the ongoing dialog, MINDS generates a different set of layered predictions for processing the next utterance. The predictions contain constraints derived from the contextual, dialog level knowledge sources and each prediction is translated into a grammar usable by our speech recognizer, SPHINX. Since speech recognizers use grammars to dictate legal word sequences and to constrain the recognition process, the dynamically generated grammars reduce the number of word candidates considered by the recognizer. The results demonstrate that speech recognition accuracy is greatly enhanced through the use of predictions.	['Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']	['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']	['Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']	['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']	"['In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']"	['Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']	"['In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']"	['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']	"['In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']"	['Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']
CCT239	J90-3003	Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .	a holistic lexiconbased approach to opinion mining	prosodic structure and spoken word recognition	['F Grosjean', 'J P Gee']	introduction	Uses	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.', 'This is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree.', 'Function words, e.g.', 'auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts.', 'Content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone.']	5	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.', 'This is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree.', 'Function words, e.g.', 'auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts.', 'Content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone.']	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.', 'This is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree.', 'Function words, e.g.', 'auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts.', 'Content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone.']		The aim of this paper is to call attention to the role played by prosodic structure in continuous word recognition. First we argue that the written language notion of the word has had too much impact on models of spoken word recognition. Next we discuss various characteristics of prosodic structure that bear on processing issues. Then we present a view of continuous word recognition which takes into account the alternating pattern of weak and strong syllables in the speech stream. A lexical search is conducted with the stressed syllables while the weak syllables are identified through a pattern-recognition-like analysis and the use of phonotactic and morphonemic rules. We end by discussing the content word vs. function word access controversy in the light of our view.	The aim of this paper is to call attention to the role played by prosodic structure in continuous word recognition. First we argue that the written language notion of the word has had too much impact on models of spoken word recognition. Next we discuss various characteristics of prosodic structure that bear on processing issues. Then we present a view of continuous word recognition which takes into account the alternating pattern of weak and strong syllables in the speech stream. A lexical search is conducted with the stressed syllables while the weak syllables are identified through a pattern-recognition-like analysis and the use of phonotactic and morphonemic rules. We end by discussing the content word vs. function word access controversy in the light of our view.	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .']	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .']	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.']	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.']	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .']	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.']	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.']	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.']	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .']	['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.']
CCT240	J05-3003	As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .	a holistic lexiconbased approach to opinion mining	from dictionary to corpus to selforganizing dictionary learning valency associations in the face of variation and change	['Edward Briscoe']		Background	"['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']"	0	"['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']"	"['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']"	ing over specific lexically-governed particles and prepositions and specific predicate selectional preferences, but including some `derived' / `alternant' semi-productive, and therefore only semipredictable, bounded dependency constructions, such as particle or dative movement, there are at least 163 valency frames associated with verbal predicates in (current) English (Briscoe, 2000). In this paper, I will review the work that my colleagues and I have done to learn (semi-)automatically this very large number of associations between individual verbal predicates and valency frames. Access to a comprehensive and accurate valency lexicon is critical for the development of robust and accurate parsing technology capable of recovering predicate-argument relations (and thus logical forms) from free text or transcribed speech. Without this information it is possible to `chunk' input into phrases but not to distinguish arguments from adjuncts or resolve most phrasal attachment ambiguities. Furthermore, for statistical parsers it is not enough to know the associations of predicates to valency frames, it is also critical to know the relative frequency of such associations given a specific predicate. Such information is a core component of that required to `lexicalize' a probabilistic parser, and it is now well-established that lexicalization is essential for accurate disambiguation (e.g. Collins, 1997, Carroll et al, 1998). While state-of-the-art wide-coverage grammars of English, capable of recovering predicateargument structure and expressed as a unification-based phrase structure grammar, have on the order of 1000 rules, it is clear that the number of associations between valency frames and predicates needed in a lexicon for such a grammar will be much higher.		ing over specific lexically-governed particles and prepositions and specific predicate selectional preferences, but including some `derived' / `alternant' semi-productive, and therefore only semipredictable, bounded dependency constructions, such as particle or dative movement, there are at least 163 valency frames associated with verbal predicates in (current) English (Briscoe, 2000). In this paper, I will review the work that my colleagues and I have done to learn (semi-)automatically this very large number of associations between individual verbal predicates and valency frames. Access to a comprehensive and accurate valency lexicon is critical for the development of robust and accurate parsing technology capable of recovering predicate-argument relations (and thus logical forms) from free text or transcribed speech. Without this information it is possible to `chunk' input into phrases but not to distinguish arguments from adjuncts or resolve most phrasal attachment ambiguities. Furthermore, for statistical parsers it is not enough to know the associations of predicates to valency frames, it is also critical to know the relative frequency of such associations given a specific predicate. Such information is a core component of that required to `lexicalize' a probabilistic parser, and it is now well-established that lexicalization is essential for accurate disambiguation (e.g. Collins, 1997, Carroll et al, 1998). While state-of-the-art wide-coverage grammars of English, capable of recovering predicateargument structure and expressed as a unification-based phrase structure grammar, have on the order of 1000 rules, it is clear that the number of associations between valency frames and predicates needed in a lexicon for such a grammar will be much higher.	['As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .']	['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .']	['As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .']	['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .']	['As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .']	['As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .']	['As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .']	['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .']	['We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .']	['As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.']
CCT241	W06-3813	Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .	a holistic lexiconbased approach to opinion mining	semantic interpretation of nominalizations	['Richard D Hull', 'Fernando Gomez']	related work	Background	['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']	0	['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']	['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']	A computational approach to the semantic interpretation of nominalizations is described. Interpretation of nominalizations involves three tasks: deciding whether the nominalization is being used in a verbal or non-verbal sense; disambiguating the nominalized verb when a verbal sense is used; and determining the fillers of the thematic roles of the verbal concept or predicate of the nominalization. A verbal sense can be recognized by the presence of modifiers that represent the arguments of the verbal concept. It is these same modifiers which provide the semantic clues to disambiguate the nominalized verb. In the absence of explicit modifiers, heuristics are used to discriminate between verbal and nonverbal senses. A correspondence between verbs and their nominalizations is exploited so that only a small amount of additional knowledge is needed to handle the nominal form. These methods are tested in the domain of encyclopedic texts and the results are shown.		A computational approach to the semantic interpretation of nominalizations is described. Interpretation of nominalizations involves three tasks: deciding whether the nominalization is being used in a verbal or non-verbal sense; disambiguating the nominalized verb when a verbal sense is used; and determining the fillers of the thematic roles of the verbal concept or predicate of the nominalization. A verbal sense can be recognized by the presence of modifiers that represent the arguments of the verbal concept. It is these same modifiers which provide the semantic clues to disambiguate the nominalized verb. In the absence of explicit modifiers, heuristics are used to discriminate between verbal and nonverbal senses. A correspondence between verbs and their nominalizations is exploited so that only a small amount of additional knowledge is needed to handle the nominal form. These methods are tested in the domain of encyclopedic texts and the results are shown.	['Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .']	['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .']	['Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']	['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']	['In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .']	['Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']	['In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']	['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']	['In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .']	['Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']
CCT242	W06-1104	Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .	a holistic lexiconbased approach to opinion mining	semantic similarity based on corpus statistics and lexical taxonomy	['Jay J Jiang', 'David W Conrath']		Background	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	0	[]	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task.		This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task.	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .']	['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']
CCT243	P11-1134	Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .	a holistic lexiconbased approach to opinion mining	recognizing textual entailment rational evaluation and approaches	['Ido Dagan', 'Bill Dolan', 'Bernardo Magnini', 'Dan Roth']	introduction	Background	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']	0	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']	The goal of identifying textual entailment - whether one piece of text can be plausibly inferred  from another - has emerged in recent years as a generic core problem in natural language  understanding. Work in this area has been largely driven by the PASCAL Recognizing  Textual Entailment (RTE) challenges, which are a series of annual competitive meetings.  The current work exhibits strong ties to some earlier lines of research, particularly automatic  acquisition of paraphrases and lexical semantic relationships and unsupervised inference in  applications such as question answering, information extraction and summarization. It has  also opened the way to newer lines of research on more involved inference methods, on  knowledge representations needed to support this natural language understanding challenge  and on the use of learning methods in this context. RTE has fostered an active and growing  community of researchers focused on the problem of applied entailment. This special issue  of the JNLE provides an opportunity to showcase some of the most important work in this  emerging area		The goal of identifying textual entailment - whether one piece of text can be plausibly inferred  from another - has emerged in recent years as a generic core problem in natural language  understanding. Work in this area has been largely driven by the PASCAL Recognizing  Textual Entailment (RTE) challenges, which are a series of annual competitive meetings.  The current work exhibits strong ties to some earlier lines of research, particularly automatic  acquisition of paraphrases and lexical semantic relationships and unsupervised inference in  applications such as question answering, information extraction and summarization. It has  also opened the way to newer lines of research on more involved inference methods, on  knowledge representations needed to support this natural language understanding challenge  and on the use of learning methods in this context. RTE has fostered an active and growing  community of researchers focused on the problem of applied entailment. This special issue  of the JNLE provides an opportunity to showcase some of the most important work in this  emerging area	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .']	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .']	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).']	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).']	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .']	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.']	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).']	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.']	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .']	['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.']
CCT244	J09-4010	The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .	a holistic lexiconbased approach to opinion mining	in question answering two heads are better than one	['J Chu-Carroll', 'K Czuba', 'J M Prager', 'A Ittycheriah']		CompareOrContrast	"['Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis."", ""A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning."", 'Instead it uses a voting mechanism to select the answer given by the majority of methods.', ""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", 'Because the results of all the methods are comparable, no learning is required: At each stage of the ""cascade of methods,"" the method that performs best is selected.', 'In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.', 'Therefore, we need to learn from experience when to use each method.']"	1	"['Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis."", ""A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning."", 'Instead it uses a voting mechanism to select the answer given by the majority of methods.', ""The question answering system developed by Chu-Carroll et al. (2003) belongs to the merging category of approaches, where the output of an individual method can be used as input to a different method (this corresponds to Burke's cascade sub-category)."", 'Because the results of all the methods are comparable, no learning is required: At each stage of the ""cascade of methods,"" the method that performs best is selected.', 'In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.', 'Therefore, we need to learn from experience when to use each method.']"	"['Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis."", ""A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning."", 'Instead it uses a voting mechanism to select the answer given by the majority of methods.', ""The question answering system developed by Chu-Carroll et al. (2003) belongs to the merging category of approaches, where the output of an individual method can be used as input to a different method (this corresponds to Burke's cascade sub-category)."", 'Because the results of all the methods are comparable, no learning is required: At each stage of the ""cascade of methods,"" the method that performs best is selected.', 'In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.', 'Therefore, we need to learn from experience when to use each method.']"	Motivated by the success of ensemble methods  in machine learning and other areas of natural  language processing, we developed a multistrategy  and multi-source approach to question  answering which is based on combining the results  from different answering agents searching  for answers in multiple corpora. The answering  agents adopt fundamentally different strategies,  one utilizing primarily knowledge-based  mechanisms and the other adopting statistical  techniques. We present our multi-level answer  resolution algorithm that combines results from  the answering agents at the question, passage,  and/or answer levels. Experiments evaluating  the effectiveness of our answer resolution algorithm  show a 35.0% relative improvement over  our baseline system in the number of questions  correctly answered, and a 32.8% improvement  according to the average precision metric		Motivated by the success of ensemble methods  in machine learning and other areas of natural  language processing, we developed a multistrategy  and multi-source approach to question  answering which is based on combining the results  from different answering agents searching  for answers in multiple corpora. The answering  agents adopt fundamentally different strategies,  one utilizing primarily knowledge-based  mechanisms and the other adopting statistical  techniques. We present our multi-level answer  resolution algorithm that combines results from  the answering agents at the question, passage,  and/or answer levels. Experiments evaluating  the effectiveness of our answer resolution algorithm  show a 35.0% relative improvement over  our baseline system in the number of questions  correctly answered, and a 32.8% improvement  according to the average precision metric	"[""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .""]"	"['Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .""]"	"[""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .""]"	"['Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .""]"	"[""More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis."", 'Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .""]"	"[""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .""]"	"[""More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis."", 'Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .""]"	"['Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .""]"	"[""A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning."", ""More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis."", 'Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .""]"	"[""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", 'Because the results of all the methods are comparable, no learning is required: At each stage of the ""cascade of methods,"" the method that performs best is selected.', 'In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.', 'Therefore, we need to learn from experience when to use each method.']"
CCT245	W04-1610	More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .	a holistic lexiconbased approach to opinion mining	a family of additive online algorithms for category ranking	['K Crammer', 'Y Singer']	related work	Background	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	0	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stem from recent advances in online learning algorithms. The algorithms are simple to implement and are also time and memory efficient. We provide a unified analysis of the family of algorithms in the mistake bound model. We then discuss experiments with the proposed family of topic-ranking algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora, the algorithms we present achieve state-of-the-art results and outperforms topic-ranking adaptations of Rocchio's algorithm and of the Perceptron algorithm.		We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stem from recent advances in online learning algorithms. The algorithms are simple to implement and are also time and memory efficient. We provide a unified analysis of the family of algorithms in the mistake bound model. We then discuss experiments with the proposed family of topic-ranking algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora, the algorithms we present achieve state-of-the-art results and outperforms topic-ranking adaptations of Rocchio's algorithm and of the Perceptron algorithm.	['More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	['More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	['More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']	['More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']
CCT246	J00-4001	Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .	a holistic lexiconbased approach to opinion mining	using registerdiversified corpora for general language studies	['Douglas Biber']		Background	['where Cx is the covariance matrix of x.', 'Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	0	['where Cx is the covariance matrix of x.', 'Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	['where Cx is the covariance matrix of x.', 'Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	The present study summarizes corpus-based research on linguistic characteristics from several different structural levels, in English as well as other languages, showing that register variation is inherent in natural language. It further argues that, due to the importance and systematicity of the linguistic differences among registers, diversified corpora representing a broad range of register variation are required as the basis for general language studies.First, the extent of cross-register differences are illustrated from consideration of individual grammatical and lexical features; these register differences are also important for probabilistic part-of-speech taggers and syntactic parsers, because the probabilities associated with grammatically ambiguous forms are often markedly different across registers. Then, corpus-based multidimensional analyses of English are summarized, showing that linguistic features from several structural levels function together as underlying dimensions of variation, with each dimension defining a different set of linguistic relations among registers. Finally, the paper discusses how such analyses, based on register-diversified corpora, can be used to address two current issues in computational linguistics: the automatic classification of texts into register categories and cross-linguistic comparisons of register variation.		The present study summarizes corpus-based research on linguistic characteristics from several different structural levels, in English as well as other languages, showing that register variation is inherent in natural language. It further argues that, due to the importance and systematicity of the linguistic differences among registers, diversified corpora representing a broad range of register variation are required as the basis for general language studies.First, the extent of cross-register differences are illustrated from consideration of individual grammatical and lexical features; these register differences are also important for probabilistic part-of-speech taggers and syntactic parsers, because the probabilities associated with grammatically ambiguous forms are often markedly different across registers. Then, corpus-based multidimensional analyses of English are summarized, showing that linguistic features from several structural levels function together as underlying dimensions of variation, with each dimension defining a different set of linguistic relations among registers. Finally, the paper discusses how such analyses, based on register-diversified corpora, can be used to address two current issues in computational linguistics: the automatic classification of texts into register categories and cross-linguistic comparisons of register variation.	['Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	['where Cx is the covariance matrix of x.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	['Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	['where Cx is the covariance matrix of x.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	['Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'where Cx is the covariance matrix of x.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	['Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	['Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'where Cx is the covariance matrix of x.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	['where Cx is the covariance matrix of x.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	['Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'where Cx is the covariance matrix of x.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']	['Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']
CCT247	D13-1115	This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .	a holistic lexiconbased approach to opinion mining	integrating experiential and distributional data to learn semantic representations	['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']	introduction	Extends	['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', 'While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.', 'We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'Finally, we describe two ways to extend the model by incorporating three or more modalities.', 'We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'We release both our code and data to the community for future research. 1']	2	['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', 'While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.', 'We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'Finally, we describe two ways to extend the model by incorporating three or more modalities.', 'We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'We release both our code and data to the community for future research. 1']	['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', 'While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.', 'We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'Finally, we describe two ways to extend the model by incorporating three or more modalities.', 'We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'We release both our code and data to the community for future research. 1']	The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.		The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.	['This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']	['This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', 'While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.', 'We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'Finally, we describe two ways to extend the model by incorporating three or more modalities.']
CCT248	J06-2002	In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .	a holistic lexiconbased approach to opinion mining	situations and attitudes	['Jon Barwise', 'John Perry']	introduction	Background	"['IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', 'In this way, IA could never describe the same animal as the large chihuahua and the small brown dog, for example.', 'This approach does not do justice to gradable adjectives, whether they are used in the base form, the superlative, or the comparative.', 'Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it.', 'Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	0	"['IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', 'In this way, IA could never describe the same animal as the large chihuahua and the small brown dog, for example.', 'This approach does not do justice to gradable adjectives, whether they are used in the base form, the superlative, or the comparative.', 'Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it.', 'Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	"['IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', 'In this way, IA could never describe the same animal as the large chihuahua and the small brown dog, for example.', 'This approach does not do justice to gradable adjectives, whether they are used in the base form, the superlative, or the comparative.', 'Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it.', 'Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	"In this provocative book, Barwise and Perry tackle the slippery subject of ""meaning, "" a subject that has long vexed linguists, language philosophers, and logicians."		"In this provocative book, Barwise and Perry tackle the slippery subject of ""meaning, "" a subject that has long vexed linguists, language philosophers, and logicians."	"[""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	"['IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	"[""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	"['IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	"['In this way, IA could never describe the same animal as the large chihuahua and the small brown dog, for example.', 'IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	"[""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	"['In this way, IA could never describe the same animal as the large chihuahua and the small brown dog, for example.', 'IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	"['IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	"['This approach does not do justice to gradable adjectives, whether they are used in the base form, the superlative, or the comparative.', 'In this way, IA could never describe the same animal as the large chihuahua and the small brown dog, for example.', 'IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	"[""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"
CCT249	J10-3007	Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).	a holistic lexiconbased approach to opinion mining	whats in a translation rule	['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']	introduction	Background	['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 15) for statistical machine translation and the concept of word-by- word alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']	0	[]	['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 15) for statistical machine translation and the concept of word-by- word alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']	Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.		Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.	['Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).']	['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 15) for statistical machine translation and the concept of word-by- word alignment, the correspondence between words in source and target languages.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).']	['Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']	['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 15) for statistical machine translation and the concept of word-by- word alignment, the correspondence between words in source and target languages.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']	['Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 15) for statistical machine translation and the concept of word-by- word alignment, the correspondence between words in source and target languages.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).']	['Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']	['Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 15) for statistical machine translation and the concept of word-by- word alignment, the correspondence between words in source and target languages.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']	['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 15) for statistical machine translation and the concept of word-by- word alignment, the correspondence between words in source and target languages.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']	['Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 15) for statistical machine translation and the concept of word-by- word alignment, the correspondence between words in source and target languages.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).']	['Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']
CCT250	W00-1312	Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .	a holistic lexiconbased approach to opinion mining	a hidden markov model information retrieval systemquot	['D Miller', 'T Leek', 'R Schwartz']		Uses	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.', '(A glossary of symbols used appears below.)']	5	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.', '(A glossary of symbols used appears below.)']	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.', '(A glossary of symbols used appears below.)']	We present a new method for information retrieval using hidden Markov models (HMMs). We develop a general framework for incorporating multiple word generation mechanisms within the same model. We then demonstrate that an extremely simple realization of this model substantially outperforms standard tf :idf ranking on both the TREC-6 and TREC-7 ad hoc retrieval tasks. We go on to present a novel method for performing blind feedback in the HMM framework, a more complex HMM that models bigram production, and several other algorithmic re nements. Together, these methods form a state-of-the-art retrieval system that ranked among the best on the TREC-7 ad hoc retrieval task.		We present a new method for information retrieval using hidden Markov models (HMMs). We develop a general framework for incorporating multiple word generation mechanisms within the same model. We then demonstrate that an extremely simple realization of this model substantially outperforms standard tf :idf ranking on both the TREC-6 and TREC-7 ad hoc retrieval tasks. We go on to present a novel method for performing blind feedback in the HMM framework, a more complex HMM that models bigram production, and several other algorithmic re nements. Together, these methods form a state-of-the-art retrieval system that ranked among the best on the TREC-7 ad hoc retrieval task.	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .']	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .']	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).']	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).']	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .']	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.']	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).']	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.']	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .']	['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.', '(A glossary of symbols used appears below.)']
CCT251	J05-3003	In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .	a holistic lexiconbased approach to opinion mining	lexicalfunctional syntax	['Joan Bresnan']	introduction	Background	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	0	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']				['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']	['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']
CCT252	J91-2003	We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .	a holistic lexiconbased approach to opinion mining	intended models circumscription and commonsense reasoningquot	['W Zadrozny']	introduction	Extends	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']	2	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']	"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations."		"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations."	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .']	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .']	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .']	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .']	['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']
CCT253	D08-1007	Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n  ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .	a holistic lexiconbased approach to opinion mining	using the web to obtain frequencies for unseen bigrams	['Frank Keller', 'Mirella Lapata']	method	Motivation	"['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', 'For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', 'In this representation, features for one predicate will be completely independent from those for every other predicate.', ""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n \x80 ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by Pantel et al. (2007).']"	4	"['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', 'For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', 'In this representation, features for one predicate will be completely independent from those for every other predicate.', ""Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a For a fixed verb, MI is proportional to Keller and Lapata (2003)'s conditional probability scores for pseudodisambiguation of (v, n, n  ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f (v, n)."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by Pantel et al. (2007).']"	"['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', 'For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', 'In this representation, features for one predicate will be completely independent from those for every other predicate.', ""Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a For a fixed verb, MI is proportional to Keller and Lapata (2003)'s conditional probability scores for pseudodisambiguation of (v, n, n  ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f (v, n)."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by Pantel et al. (2007).']"	This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.		This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.	"[""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n \x80 ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .""]"	"['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', ""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n \x80 ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .""]"	"[""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n \x80 ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.']"	"['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', ""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n \x80 ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.']"	"['For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', '. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', ""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n \x80 ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .""]"	"[""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n \x80 ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by Pantel et al. (2007).']"	"['For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', '. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', ""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n \x80 ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.']"	"['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', ""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n \x80 ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by Pantel et al. (2007).']"	"['In this representation, features for one predicate will be completely independent from those for every other predicate.', 'For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', '. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', ""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n \x80 ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .""]"	"[""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n \x80 ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by Pantel et al. (2007).']"
