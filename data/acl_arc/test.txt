unique_id	citing_id	citing_title	cited_title	cited_authors	section_title	cited_abstract_auto	citation_context	cite_context_paragraph	citation_class_label
CCT1	A00-1004	Automatic construction of parallel English-Chinese corpus for cross-language information retrieval	a program for aligning sentences in bilingual corpora	"['William A Gale', 'Kenneth W Church']"	method	"Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program."	"A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) ."	"['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']"	0
CCT2	A00-1004	Automatic construction of parallel English-Chinese corpus for cross-language information retrieval	aligning a parallel englishchinese corpus statistically with lexical criteria	['Dekai Wu']	introduction	We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale & Church's (1991) length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues.	"Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( #AUTHOR_TAG ) ."	"['However, a major obstacle to this approach is the lack of parallel corpora for model training.', 'Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( #AUTHOR_TAG ) .', 'In this paper, we will describe a method which automatically searches for parallel texts on the Web.', ""We will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in CLIR.""]"	0
CCT3	A00-1004	Automatic construction of parallel English-Chinese corpus for cross-language information retrieval	aligning sentences in bilingual corpora using lexical information	['S F Chen']	method	"In this paper, we describe a fast algorithm for aligning sentences with their translations in a bilingual corpus. Existing efficient algorithms ignore word identities and only consider sentence length (Brown et al., 1991b; Gale and Church, 1991). Our algorithm constructs a simple statistical word-to-word translation model on the fly during alignment. We find the alignment that maximizes the probability of generating the corpus with this translation model. We have achieved an error rate of approximately 0.4% on Canadian Hansard data, which is a significant improvement over previous results. The algorithm is language independent."	"A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) ."	"['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']"	0
CCT4	A00-1004	Automatic construction of parallel English-Chinese corpus for cross-language information retrieval	aligning a parallel englishchinese corpus statistically with lexical criteria	['Dekai Wu']	method	We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale & Church's (1991) length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues.	"For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( #AUTHOR_TAG ) ."	"['Beside HTML markups, other criteria may also be incorporated.', 'For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( #AUTHOR_TAG ) .', 'We hope to implement such correspondences in our future research.']"	3
CCT5	A00-1004	Automatic construction of parallel English-Chinese corpus for cross-language information retrieval	aligning sentences in parallel corpora	"['P F Brown', 'J C Lai', 'R L Mercer']"	method	"In this paper we describe a statistical tech-nique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our da.ta, the only information about the sentences that we use for calculating alignments i the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment com-putation is fast and therefore practical for appli-cation to very large collections of text. We have used this technique to align several million sen-tences in the English-French Hans~trd corpora nd have achieved an accuracy in excess of 99 % in a random selected set of 1000 sentence pairs that we checked by hand. We show that even without he benefit of anchor points the correlation between the lengths of aligned sentences i strong enough that we should expect o achieve an accuracy of between 96 % and 97%. Thus, the technique may be applicable to a wider variety of texts than we have yet tried"	"A number of alignment techniques have been proposed , varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) ."	"['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']"	0
CCT6	A00-1008	Plan-based dialogue management in a physics tutor	spelling correction using context	"['M A Elmi', 'M W Evens']"	introduction	"In computing, spell checking is the process of detecting and sometimes providing spelling suggestions for incorrectly spelled words in a text. Basically, a spell checker is a computer program that uses a dictionary of words to perform spell checking. The bigger the dictionary is, the higher is the error detection rate. The fact that spell checkers are based on regular dictionaries, they suffer from data sparseness problem as they cannot capture large vocabulary of words including proper names, domain-specific terms, technical jargons, special acronyms, and terminologies. As a result, they exhibit low error detection rate and often fail to catch major errors in the text. This paper proposes a new context-sensitive spelling correction method for detecting and correcting non-word and real-word errors in digital text documents. The approach hinges around data statistics from Google Web 1T 5-gram data set which consists of a big volume of n-gram word sequences, extracted from the World Wide Web. Fundamentally, the proposed method comprises an error detector that detects misspellings, a candidate spellings generator based on a character 2-gram model that generates correction suggestions, and an error corrector that performs contextual error correction. Experiments conducted on a set of text documents from different domains and containing misspellings, showed an outstanding spelling error correction rate and a drastic reduction of both non-word and real-word errors. In a further study, the proposed algorithm is to be parallelized so as to lower the computational cost of the error detection and correction processes.Comment: LACSC - Lebanese Association for Computational Sciences -   http://www.lacsc.or"	Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG .	"['The first system we have implemented with APE is a prototype Atlas-Andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.', 'Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar.', ""Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG .""]"	5
CCT7	A00-1014	MIMIC	using dialogue representations for concepttospeech generation	"['Christine H Nakatani', 'Jennifer Chu-Carroll']"		"We present an implemented concept-to-speech (CTS) system that offers original proposals for certain couplings of dialogue computation with prosodic computation. Specifically, the semantic interpretation, task modeling and dialogue strategy modules in a working spoken dialogue system are used to generate prosodic features to better convey the meaning of system replies. The new CTS system embodies and extends theoretical work on intonational meaning in a more general, robust and rigorous way than earlier approaches, by reflecting compositional aspects of both dialogue and intonation interepretation in an original computational framework for prosodic generation."	2 See ( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation .	"[""2 See ( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation .""]"	0
CCT8	A00-1014	MIMIC	the commandtalk spoken dialogue system	"['Amanda Stent', 'John Dowding', 'Jean Mark Gawron', 'Elizabeth Owen Bratt', 'Robert Moore']"		"CommandTalk (Moore et al., 1997) is a spokenlanguage interface to the ModSAF battlefield simulator that allows simulation operators to generate and execute military exercises by creating forces and control measures, assigning missions to forces, and controlling the display (Ceranowicz, 1994). CommandTalk consists of independent, cooperating agents interacting through SRI's Open Agent Architecture (OAA) (Martin et al., 1998). This architecture allows components to be developed independently, and then flexibly and dynamically combined to support distributed computation. Most of the agents that compose CommandTalk have been described elsewhere !for more detail, see (Moore et al., 1997)). This paper describes extensions to CommandTalk to support spoken dialogue. While we make no theoretical claims about the nature and structure of dialogue, we are influenced by the theoretical work of (Grosz and Sidner, 1986) and will use terminology from that tradition when appropriate. We also follow (Chu-Carroll and Brown, 1997) in distinguishing task initiative and dialogue initiative. Section 2 demonstrates the dialogue capabilities of CommandTalk by way of an extended example. Section 3 describes how language in CommandTalk is modeled for understanding and generation. Section 4 describes the architecture of the dialogue manager in detail. Section 5 compares CommandTalk with other spo-"	"The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ; #AUTHOR_TAG ) ) ."	"['The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ; #AUTHOR_TAG ) ) .', 'To instantiate an attribute, MIMIC adopts the lnfoSeek dialogue act to solicit the missing information.', 'In contrast, when MIMIC has both initiatives, it plays a more active role by presenting the user with additional information comprising valid instantiations of the attribute (GiveOptions).', 'Given an invalid query, MIMIC notifies the user of the failed query and provides an openended prompt when it only has dialogue initiative.', ""When MIMIC has both initiatives, however, in addition to No-tifyFailure, it suggests an alternative close to the user's original query and provides a limited prompt."", 'Finally, when MIMIC has neither initiative, it simply adopts No-tifyFailure, allowing the user to determine the next discourse goal.']"	1
CCT9	A00-1014	MIMIC	mixed initiative in dialogue an investigation into discourse segmentation	"['Marilyn Walker', 'Steve Whittaker']"		"Conversation between two people is usually of mixed-initiative, with control over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles.Comment: 8 pages, late"	"Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) ."	"['Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur-5An alternative strategy to step ( 4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1.']"	0
CCT10	A00-1014	MIMIC	cues and control in expertclient dialogues	"['Steve Whittaker', 'Phil Stenton']"			"Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) ."	"['Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur-5An alternative strategy to step ( 4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1.']"	0
CCT11	A00-1014	MIMIC	evaluating automatic dialogue strategy adaptation for a spoken dialogue system	"['Jennifer Chu-Carroll', 'Jill S Nickerson']"	experiments	"In this paper, we describe an empirical evaluation of an adaptive mixed initiative spoken dialogue system. We conducted two sets of experiments to evaluate the mixed initiative and automatic adaptation aspects of the system, and analyzed the resulting dialogues along three dimensions: performance factors, discourse features, and initiative distribution. Our results show that 1) both the mixed initiative and automatic adaptation aspects led to better system performance in terms of user satisfaction and dialogue efficiency, and 2) the system's adaptation behavior better matched user expectations, more efficiently resolved dialogue anomalies, and resulted in higher overall dialogue quality."	A companion paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ) .	"[""We conducted two experiments to evaluate MIMIC's automatic adaptation capabilities."", 'We compared MIMIC with two control systems: MIMIC-SI, a system-initiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ) .', 'Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information.', 'User satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'Furthermore, a number of performance features, largely based on the PARADISE dialogue evaluation scheme (Walker et al., 1997), were automatically logged, derived, or manually annotated.', 'In addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response.']"	2
CCT12	A00-1014	MIMIC	evaluating response strategies in a webbased spoken dialogue agent	"['Diane J Litman', 'Shimei Pan', 'Marilyn A Walker']"			"5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ) , which we leave for future work ."	"['5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ) , which we leave for future work .']"	3
CCT13	A00-1014	MIMIC	paradise a framework for evaluating spoken dialogue agents	"['Marilyn A Walker', 'Diane J Litman', 'Candance A Kamm', 'Alicia Abella']"	experiments	"This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken dialogue agents. The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity.Comment: 10 pages, uses aclap, psfig, lingmacros, time"	"Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated ."	"[""We conducted two experiments to evaluate MIMIC's automatic adaptation capabilities."", 'We compared MIMIC with two control systems: MIMIC-SI, a system-initiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail (Chu-Carroll and Nickerson, 2000).', 'Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information.', 'User satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated .', 'In addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response.']"	5
CCT14	A00-1015	Javox	natural language access to software applications	"['Paul Schmidt', 'Sibylle Rieder', 'Axel Theofilidis', 'Marius Groenendijk', 'Peter Phelan', 'Henrik Schulz', 'Thierry Declerck', 'Andrew Brenenkamp']"		"This paper reports on the ESPRIT project MELISSA (Methods and Tools for Natural-Language Interfacing with Standard Software Applications)1. MELISSA aims at developing the technology and tools enabling end users to interface with computer applications, using natural-language (NL), and to obtain a precompetitive product validated in selected enduser applications. This paper gives an overview of the approach to solving (NL) interfacing problem and outlines some of the methods and software components developed in the project."	The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project ( #AUTHOR_TAG ) .	"['Previous systems to assist in the development of spoken-langnage systems (SLSs) have focused on building stand-alone, customized applications, such as (Sutton et al., 1996) and (Pargellis et al., 1999).', 'The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project ( #AUTHOR_TAG ) .', 'It is intended to both speed the development of SLSs and to localize the speech-specific code within the application.', 'JAVOX allows developers to add speech interfaces to applications at the end of the development process; SLSs no longer need to be built from the ground up.']"	1
CCT15	A00-1019	Unit completion for a computer-aided translation typing system	a corpusbased approach to automatic compound extraction	"['Keh-Yih Su', 'Ming-Wen Wu', 'Jing-Shin Chang']"	method		"It is also possible to focus on non-compositional compounds , a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ; Lin , 99 ) ."	"['One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Ganssier, 1995;Kupiec, 1993;hua Chen and Chen, 94;Fung, 1995;Evans and Zhai, 1996).', 'It is also possible to focus on non-compositional compounds , a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ; Lin , 99 ) .', 'Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995;Furuse and Iida, 96).', 'In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags.', ""An excerpt of the association probabilities of a unit model trained considering only the NP-sequences is given in table 3. Applying this filter (referred to as JrNp in the following) to the 39,093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35,939 of them (92%)."", '4: Completion results of several translation models, spared: theoretical proportion of characters saved; ok: number of target units accepted by the user; good: number of target units that matched the expected whether they were proposed or not; nu: number of sentences for which no target unit was found by the translation model; u: number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed.']"	0
CCT16	A00-1019	Unit completion for a computer-aided translation typing system	retrieving collocations by cooccurrences and word order constraints	"['Sayori Shimohata', 'Toshiyuki Sugio', 'Junji Nagata']"	method		"This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ; #AUTHOR_TAG ; Russell , 1998 ) ."	"['Finding relevant units in a text has been explored in many areas of natural language processing.', 'Our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus.', 'For sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus.', 'This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ; #AUTHOR_TAG ; Russell , 1998 ) .']"	0
CCT17	A00-1019	Unit completion for a computer-aided translation typing system	an algorithm for finding noun phrase correspondences in bilingual corpora	['Julian Kupiec']	method	"The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus. The taggers provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages. Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers. The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated. Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings."	"One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ; Fung , 1995 ; Evans and Zhai , 1996 ) ."	"['One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ; Fung , 1995 ; Evans and Zhai , 1996 ) .', 'It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997;Lin, 99).', 'Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995;Furuse and Iida, 96).', 'In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags.', ""An excerpt of the association probabilities of a unit model trained considering only the NP-sequences is given in table 3. Applying this filter (referred to as JrNp in the following) to the 39,093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35,939 of them (92%)."", '4: Completion results of several translation models, spared: theoretical proportion of characters saved; ok: number of target units accepted by the user; good: number of target units that matched the expected whether they were proposed or not; nu: number of sentences for which no target unit was found by the translation model; u: number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed.']"	5
CCT18	A00-1016	A compact architecture for dialogue management based on scripts and meta-outputs	commandtalk a spokenlanguage interface for battlefield simulations	"['R Moore', 'J Dowding', 'H Bratt', 'J Gawron']"	experiments		"The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) ."	"['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).', 'Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000).', 'Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domain~independent unification grammar and a domain-specific lexicon.', 'The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized.', 'Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager.', 'We refer to these as linguistic level representations.']"	5
CCT19	A00-1016	A compact architecture for dialogue management based on scripts and meta-outputs	commandtalk a spokenlanguage interface for battlefield simulations	"['R Moore', 'J Dowding', 'H Bratt', 'J Gawron']"	introduction		"CornmandTalk ( #AUTHOR_TAG ) , Circuit Fix-It Shop ( Smith , 1997 ) and TRAINS-96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents ."	"['The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system.', 'As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem.', 'More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'A number of other systems have addressed part of the task.', 'CornmandTalk ( #AUTHOR_TAG ) , Circuit Fix-It Shop ( Smith , 1997 ) and TRAINS-96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .', ""Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous."", 'Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995;Pyre et al., 1995).', 'In most of this and other related work the treatment is some variant of the following.', 'If there is a speech interface, the input speech signal is converted into text.', ""Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command; this formula is then fed into a command interpreter, which executes the command.""]"	0
CCT20	A00-1018	An automatic reviser	tagging english text with a probabilistic model	['B Merialdo']		"In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined:using text that has been tagged by hand and computing relative frequency counts,using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.Experminents show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available."	We do this with a first-order HMM part-ofspeech tagger ( Merialdo #AUTHOR_TAG ) .	"['1.', 'An aligner.', 'After identification of word and sentence boundaries the text is processed into a bi-text by an alignment program.', 'This alignment is done on the basis of both length (Gale and Church [7]) and a notion of cognateness (Simard [161).', '2. Transducers.', 'In order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'This is done with transducers (Kaplan and Kay, [10]).', '3. Part-of-speech tagger.', 'Misleading similarities in graphical form can sometime induce translation mistakes (deceptive cognates).', '~ These forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'We do this with a first-order HMM part-ofspeech tagger ( Merialdo #AUTHOR_TAG ) .', 'I In the rest of the paper, we will use deceptive cognate very Iosely often to refer to normative usage of word in general.']"	5
CCT21	A00-1018	An automatic reviser	a program for aligning sentences in bilingual corpora	"['W Gale', 'K Church']"		"Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program."	This alignment is done on the basis of both length ( Gale and Church #AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] ) .	"['1.', 'An aligner.', 'After identification of word and sentence boundaries the text is processed into a bi-text by an alignment program.', 'This alignment is done on the basis of both length ( Gale and Church #AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] ) .', '2. Transducers.', 'In order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'This is done with transducers (Kaplan and Kay, [10]).', '3. Part-of-speech tagger.', 'Misleading similarities in graphical form can sometime induce translation mistakes (deceptive cognates).', '~ These forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'We do this with a first-order HMM part-ofspeech tagger (Merialdo [13]).', 'I In the rest of the paper, we will use deceptive cognate very Iosely often to refer to normative usage of word in general.']"	5
CCT22	A00-1021	Ranking suspected answers to natural language questions using predictive annotation	disambiguation of proper names in text	"['Nina Wacholder', 'Yael Ravin', 'Misook Choi']"	experiments	"Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the many-to-many mapping between names and their referents. We analyze the types of ambiguity --- structural and semantic --- that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T. J. Watson Research Center."	"â¢ Before indexing the text , we process it with Textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms ."	"['â\x80¢ Before indexing the text , we process it with Textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms .', 'We added a new module (Resporator) which annotates text segments with QA-Tokens using pattern matching.', 'Thus the text ""for 5 centuries"" matches the DURATIONS pattern ""for :CARDINAL _timeperiod"", where :CAR-DINAL is the label for cardinal numbers, and _timeperiod marks a time expression.']"	5
CCT23	A00-1022	Message classification in the call center	an information extraction core system for real world german text processing	"['Giinter Neumann', 'Rolf Backofen', 'Judith Baur', 'Markus Becker', 'Christian Braun']"	experiments	"This paper describes SMES, an information extraction core system for real world German text processing. The basic design criterion of the system is of providing a set of basic powerful, robust, and efficient natural language components and generic linguistic knowledge sources which can easily be customized for processing different tasks in a flexible manner.Comment: 9 pages; in Proc. of 5th ANLP, 199"	100000 word stems of German ( #AUTHOR_TAG ) .	"['MorphAna: Morphological Analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'We are using a lexicon of approx.', '100000 word stems of German ( #AUTHOR_TAG ) .']"	5
CCT24	A00-1022	Message classification in the call center	an information extraction core system for real world german text processing	"['Giinter Neumann', 'Rolf Backofen', 'Judith Baur', 'Markus Becker', 'Christian Braun']"		"This paper describes SMES, an information extraction core system for real world German text processing. The basic design criterion of the system is of providing a set of basic powerful, robust, and efficient natural language components and generic linguistic knowledge sources which can easily be customized for processing different tasks in a flexible manner.Comment: 9 pages; in Proc. of 5th ANLP, 199"	"Linguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing ( #AUTHOR_TAG ) ."	"['Linguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing ( #AUTHOR_TAG ) .', 'The fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient STP components and 4Almost all tools we examined build a single multicategorizer except for SVM-Light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines includes a text tokenizer, a lexical processor and a chunk parser.', 'The chunk parser itself is subdivided into three components.', 'In the first step, phrasal fragments like general nominal expressions and verb groups are recognized.', 'Next, the dependency-based structure of the fragments of each sentence is computed using a set of specific sentence patterns.', 'Third, the grammatical functions are determined for each dependency-based structure on the basis of a large subcategorization lexicon.', 'The present application benefits from the high modularity of the usage of the components.', 'Thus, it is possible to run only a subset of the components and to tailor their output.', 'The experiments described in Section 4 make use of this feature.']"	5
CCT25	A00-1025	Examining the role of statistical and linguistic knowledge sources in a general-knowledge question-answering system	errordriven pruning of treebank grammars for base noun phrase identification	"['C Cardie', 'D Pierce']"		"Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a ""treebank"" corpus; then the grammar is improved by selecting rules with high ""benefit"" scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal."	"Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in #AUTHOR_TAG ."	"['The NP-based QA System.', 'Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in #AUTHOR_TAG .', 'Empire identifies base NPs --non-recursive noun phrases --using a very simple algorithm that matches part-of-speech tag sequences based on a learned noun phrase grammar.', 'The approach is able to achieve 94% precision and recall for base NPs derived from the Penn Treebank Wall Street Journal (Marcus et al., 1993).', 'In the experiments below, the NP filter follows the application of the document retrieval and text summarization components.', 'Pronoun answer hypotheses are discarded, and the NPs are assembled into 50-byte chunks.']"	5
CCT26	A00-1025	Examining the role of statistical and linguistic knowledge sources in a general-knowledge question-answering system	the tipster summac text summarization evaluation	"['I Mani', 'T Firmin', 'D House', 'G Klein', 'B Sundheim', 'L Hirschman']"		"All too frequently, a software cost estimate is required in the early stages of the life-cycle when requirements and design specifications are immature. To produce a cost estimate under these conditions requires extensive use of expert judgment and addressing significant estimatio"	"Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( Salton et al. , 1994 ) :"	"['We next hypothesize that query-dependent text summarization algorithms will improve the performance of the QA system by focusing the system on the most relevant portions of the retrieved documents.', 'The goal for query-dependent summarization algorithms is to provide a short summary of a document with respect to a specific query.', 'Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( Salton et al. , 1994 ) :']"	1
CCT27	A00-1031	TnT	a maximum entropy model for partofspeech tagging	['Adwait Ratnaparkhi']	introduction		The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( #AUTHOR_TAG ) .	"['The aim of this paper is to give a detailed account of the techniques used in TnT.', 'Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993).', 'The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( #AUTHOR_TAG ) .', 'For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999).']"	1
CCT28	A00-1031	TnT	a maximum entropy model for partofspeech tagging	['Adwait Ratnaparkhi']	conclusion		"According to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here ."	"['We have shown that a tagger based on Markov models yields state-of-the-art results, despite contrary claims found in the literature.', 'For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers.', 'In our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor.', 'The rather large amount of freedom was not handled in detail in previous publications: handling of start-and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.', 'Note that the decisions we made yield good results for both the German and the English Corpus.', 'They do so for several other corpora as well.', 'The architecture remains applicable to a large variety of languages.', 'According to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .', 'It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both.']"	1
CCT29	A00-2004	TV program segmentation using text-visual analysis	text segmentation based on similarity between words	['Hideki Kozima']		"This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis."	"This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation ."	"['Four similarity measures were examined.', 'The cosine coefficient (R98(s,co,)) and dot density measure (R98(m,(lot)) yield similar results.', 'Our spread activation based semantic measure (R98( .....,)) improved a.ccura(:y.', ""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"	1
CCT30	A00-2004	TV program segmentation using text-visual analysis	text segmentation based on similarity between words	['Hideki Kozima']		"This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis."	"R98 ( , , , , â ) uses a variant of Kozima 's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity ."	"[""R98 ( , , , , â\x80\x9e ) uses a variant of Kozima 's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity ."", 'Word similarity is a function of word co- occurrence statistics in the given document.', 'Words that belong to the same sentence are considered to be related.', 'Given the co-occurrence frequencies f(wi, wj), the transition probability matrix t is computed by equation 10.', 'Equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm(y) converts a matrix y into a transition matrix, x = 5 was used in the experiment.']"	2
CCT31	A00-2022	Ambiguity Packing in Constraint-based Parsing Practical Results	a bag of useful techniques for efficient and robust parsing	"['B Kiefer', 'H-U Krieger', 'J Carroll', 'R Malouf']"	conclusion	This paper describes new and improved techniques which help a unification-based parser to process input efficiently and robustly. We show that combining these methods leads to a speed-up in parsing time of more than an order of magnitude. The methods are correct in the sense that none of them rule out legal rule applications	"Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification ."	"['In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']"	1
CCT32	A00-2036	Splittability of Bilexical Context-Free Grammars is Undecidable	a linear observed time statistical parser based on maximum entropy models	['A Ratnaparkhi']	conclusion	"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall."	"We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ) ."	"['In this paper we have provided an original mathematical argument in favour of this thesis.', 'Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ) .', 'We leave this for future work.']"	3
CCT33	A00-2036	Splittability of Bilexical Context-Free Grammars is Undecidable	exploiting syntactic structure for language modeling	"['C Chelba', 'F Jelinek']"	conclusion	"It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called ""syntactic distances"", where information between these two separate objectives shares the same intermediate representation. Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.Comment: ACL2"	"We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ) ."	"['In this paper we have provided an original mathematical argument in favour of this thesis.', 'Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ) .', 'We leave this for future work.']"	3
CCT34	D08-1001	Revealing the structure of medical dictations with conditional random fields	a critique and improvement of an evaluation metric for text segmentation	"['Lev Pevzner', 'Marti Hearst']"	experiments	"The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms. However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution. We propose a simple modification to the Pk metric that remedies these problems. This new metriccalled Window Diffmoves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text."	"Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by #AUTHOR_TAG ."	"['Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by #AUTHOR_TAG .', 'WindowDiff returns 0 in case of a perfect segmentation; 1 is the worst possible score.', 'However, it only takes into account segment boundaries and disregards segment types.', 'In section 5.2, we mentioned that loopy BP is not guaranteed to converge in a finite number of iterations.', 'Since we optimize pseudolikelihood for parameter estimation, we are not affected by this limitation in the training phase.', 'However, we use loopy BP with a TRP schedule during testing, so we must expect to encounter non-convergence for some examples.', 'Theoretical results on this topic are discussed by Heskes (2004).', 'We give here an empirical observation of convergence behaviour of loopy BP in our setting; the maximum number of iterations of the TRP schedule was restricted to 1,000.', 'Table 4 shows the percentage of examples converging within this limit and the average number of iterations required by the converging examples, broken down by the different corpora.', 'From these results, we conclude that there is a connection between the quality of the annotation and the convergence behaviour of loopy BP.', ""In practice, even though loopy BP didn't converge for some examples, the solutions after 1,000 iterations where satisfactory.""]"	5
CCT35	D08-1002	"It's a contradiction---no, it's not"	natural logic for textual inference	"['B MacCartney', 'C D Manning']"	experiments	"This paper presents the first use of a computational model of natural logic---a system of logical inference which operates over natural language---for textual inference. Most current approaches to the PASCAL RTE textual inference task achieve robustness by sacrificing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity. At the other extreme, systems which rely on first-order logic and theorem proving are precise, but excessively brittle. This work aims at a middle way. Our system finds a low-cost edit sequence which transforms the premise into the hypothesis; learns to classify entailment relations across atomic edits; and composes atomic entailments into a top-level entailment judgment. We provide the first reported results for any system on the FraCaS test suite. We also evaluate on RTE3 data, and show that hybridizing an existing RTE system with our natural logic system yields significant performance gains."	"Although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare ."	"['Meronyms: For some relations, there is no contradiction when y 1 and y 2 share a meronym, i.e. ""part of"" relation.', 'For example, in the set born in(Mozart,•) there is no contradiction between the y values ""Salzburg"" and ""Austria"", but ""Salzburg"" conflicts with ""Vienna"".', 'Although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .', 'We therefore simply assigned contradictions between meronyms a probability close to zero.', 'We used the Tipster Gazetteer 4 and WordNet to identify meronyms, both of which have high precision but low coverage.']"	4
CCT36	D08-1004	Modeling annotators	headdriven statistical models for natural language parsing	['Michael Collins']		"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."	We parse each sentence with the Collins parser ( #AUTHOR_TAG ) .	"['To train our model, we use L-BFGS to locally maximize the log of the objective function (1): 15 These are the function words with count ≥ 40 in a random sample of 100 documents, and which were associated with the O-I tag transition at more than twice the average rate.', 'We do not use any other lexical φ-features that reference x, for fear that they would enable the learner to explain the rationales without changing θ as desired (see the end of section 5.3). 14', 'We parse each sentence with the Collins parser ( #AUTHOR_TAG ) .', 'Then the document has one big parse tree, whose root is DOC, with each sentence being a child of DOC.']"	5
CCT37	D08-1004	Modeling annotators	a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts	"['B Pang', 'L Lee']"	experiments	"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints."	"It is based on the dataset of #AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) ."	"['In Zaidan et al. (2007), we introduced the �Movie Review Polarity Dataset Enriched with Annotator Rationales.�8', 'It is based on the dataset of #AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .', 'All our experiments use F9 as their final blind test set.']"	2
CCT38	D08-1004	Modeling annotators	a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts	"['B Pang', 'L Lee']"		"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints."	We collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator .	"['A human annotator can provide hints to a machine learner by highlighting contextual ""rationales"" for each of his or her annotations (Zaidan et al., 2007).', 'How can one exploit this side information to better learn the desired parameters θ?', 'We present a generative model of how a given annotator, knowing the true θ, stochastically chooses rationales.', 'Thus, observing the rationales helps us infer the true θ.', 'We collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator .', 'Our new generative approach exploits the rationales more effectively than our previous ""masking SVM"" approach.', 'It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks.']"	5
CCT39	D08-1004	Modeling annotators	a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts	"['B Pang', 'L Lee']"	method	"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints."	"We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) ."	"['where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.', 'Thus θ ∈ R 17744 , and positive weights in θ favor class label y = +1 and equally discourage y = −1, while negative weights do the opposite.', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2.']"	5
CCT40	D08-1007	Discriminative learning of selectional preference from unlabeled text	automatic retrieval and clustering of similar words	['Dekang Lin']	experiments	Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.	"We gather similar words using #AUTHOR_TAGa ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) 's approach to obtaining web-counts ."	"['We first evaluate D S P on disambiguating positives from pseudo-negatives, comparing to recently-proposed systems that also require no manually- compiled resources like WordNet.', 'We convert Da- gan et al. (1999)�s similarity-smoothed probability to MI by replacing the empirical Pr n|v in Equa- tion (2) with the smoothed PrSIM from Equation (1).', 'We also test an MI model inspired by Erk (2007): MISIM n;v =log X Simn_;n Prv;n_ n__SIMS n', ""We gather similar words using #AUTHOR_TAGa ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) 's approach to obtaining web-counts .""]"	5
CCT41	D08-1007	Discriminative learning of selectional preference from unlabeled text	offline strategies for online question answering answering questions before they are asked	"['Michael Fleischman', 'Eduard Hovy', 'Abdessamad Echihabi']"	method		"We also made use of the person-name/instance pairs automatically extracted by #AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist."	"['We also made use of the person-name/instance pairs automatically extracted by #AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist.', 'We have features for all concepts and therefore learn their association with each verb.']"	5
CCT42	D08-1007	Discriminative learning of selectional preference from unlabeled text	using the web to obtain frequencies for unseen bigrams	"['Frank Keller', 'Mirella Lapata']"	experiments	"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task."	"We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."	"['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", 'pronoun is the direct object of a verb predicate, v.', ""A pronoun's antecedent must obey v's selectional preferences."", 'If we have a better model of SP, we should be able to better select pronoun antecedents.']"	1
CCT43	D08-1007	Discriminative learning of selectional preference from unlabeled text	automatic retrieval and clustering of similar words	['Dekang Lin']	experiments	Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.	"#AUTHOR_TAGa ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat ."	"['It is interesting to inspect the feature weights returned by our system.', 'In particular, the weights on the verb co-occurrence features (Section 3.3.1)', 'provide a high-quality, argument-specific similarityranking of other verb contexts.', 'The DSP parameters for eat, for example, place high weight on features like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).', ""#AUTHOR_TAGa ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat ."", 'Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSP cooc on the verb join, 3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin, 1998a Other features are also weighted intuitively.', 'Note that case is a strong indicator for some arguments, for example the weight on being lower-case is high for become (0.972) and eat (0.505), but highly negative for accuse (-0.675) and embroil (-0.573) which often take names of people and organizations.']"	0
CCT44	D08-1007	Discriminative learning of selectional preference from unlabeled text	using the web to obtain frequencies for unseen bigrams	"['Frank Keller', 'Mirella Lapata']"	experiments	"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task."	"Also , the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web ."	"['5 Available from the LDC as LDC2006T13.', 'This collection was generated from approximately 1 trillion tokens of online text.', 'Unfortunately, tokens appearing less than 200 times have been mapped to the UNK symbol, and only N-grams appearing more than 40 times are included.', 'Unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'The similarity-smoothed examples will be undefined if SIMS(w) is empty.', 'Also , the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web .', 'As a reasonable default for these cases, we assign them a negative decision.']"	5
CCT45	D08-1007	Discriminative learning of selectional preference from unlabeled text	inducing a semantically annotated lexicon via embased clustering	"['Mats Rooth', 'Stefan Riezler', 'Detlef Prescher', 'Glenn Carroll', 'Franz Beil']"	method	"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries."	"Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) ."	"['Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', 'We refer to this as Pairwise Disambiguation.', 'Unlike this task, we classify each predicate-argument pair independently as plausible/implausible.', 'We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise. 1']"	1
CCT46	D08-1007	Discriminative learning of selectional preference from unlabeled text	cooccurrence retrieval a flexible framework for lexical distributional similarity	"['Julie Weeds', 'David Weir']"	method	"Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events. In other applications, distributional similarity is taken to be an approximation to semantic similarity. However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted. In this work, a flexible, parameterized framework for calculating distributional similarity is proposed. Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework. In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity. First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship? Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity? Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type? Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity."	The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .	"['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'Our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric.']"	1
CCT47	D08-1007	Discriminative learning of selectional preference from unlabeled text	isp learning inferential selectional preferences	"['Patrick Pantel', 'Rahul Bhagat', 'Bonaventura Coppola', 'Timothy Chklovski', 'Eduard Hovy']"	method	"Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness."	MI was also recently used for inference-rule SPs by #AUTHOR_TAG .	"['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', 'For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', 'In this representation, features for one predicate will be completely independent from those for every other predicate.', ""Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a For a fixed verb, MI is proportional to Keller and Lapata (2003)'s conditional probability scores for pseudodisambiguation of (v, n, n ′ ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f (v, n)."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by #AUTHOR_TAG .']"	0
CCT48	D08-1007	Discriminative learning of selectional preference from unlabeled text	using the web to obtain frequencies for unseen bigrams	"['Frank Keller', 'Mirella Lapata']"	method	"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task."	"Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) ."	"['Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', 'We refer to this as Pairwise Disambiguation.', 'Unlike this task, we classify each predicate-argument pair independently as plausible/implausible.', 'We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise. 1']"	1
CCT49	D08-1007	Discriminative learning of selectional preference from unlabeled text	inducing a semantically annotated lexicon via embased clustering	"['Mats Rooth', 'Stefan Riezler', 'Detlef Prescher', 'Glenn Carroll', 'Franz Beil']"	related work	"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries."	"Usually , the classes are from WordNet ( Miller et al. , 1990 ) , although they can also be inferred from clustering ( #AUTHOR_TAG ) ."	"['Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996).', 'For example, we might have a class Mexican Food and learn that the entire class is suitable for eating.', 'Usually , the classes are from WordNet ( Miller et al. , 1990 ) , although they can also be inferred from clustering ( #AUTHOR_TAG ) .', 'Brockmann and Lapata (2003) compare a number of WordNet-based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class-based approaches do not always outperform simple frequency-based models.']"	0
CCT50	D08-1007	Discriminative learning of selectional preference from unlabeled text	inducing a semantically annotated lexicon via embased clustering	"['Mats Rooth', 'Stefan Riezler', 'Detlef Prescher', 'Glenn Carroll', 'Franz Beil']"	experiments	"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries."	"Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) ."	"['Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)).', 'We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature.', 'When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns.']"	1
CCT51	D08-1007	Discriminative learning of selectional preference from unlabeled text	isp learning inferential selectional preferences	"['Patrick Pantel', 'Rahul Bhagat', 'Bonaventura Coppola', 'Timothy Chklovski', 'Eduard Hovy']"	related work	"Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness."	"Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ) ."	"['Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ) .', 'Inferences such as ""[X wins Y] ⇒ [X plays Y]"" are only valid for certain argu-ments X and Y.', 'We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments.']"	0
CCT52	D08-1007	Discriminative learning of selectional preference from unlabeled text	automatic retrieval and clustering of similar words	['Dekang Lin']	related work	Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.	Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and #AUTHOR_TAGa ) 's information-theoretic metric work best .	"['where Sim(v ′ , v) returns a real-valued similarity between two verbs v ′ and v (normalized over all pair similarities in the sum).', 'In contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross-product of similar pairs.', 'One key issue is how to define the set of similar words, SIMS(w).', ""Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and #AUTHOR_TAGa ) 's information-theoretic metric work best ."", 'Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet.']"	0
CCT53	D08-1007	Discriminative learning of selectional preference from unlabeled text	automatic retrieval and clustering of similar words	['Dekang Lin']	experiments	Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.	"We parsed the 3 GB AQUAINT corpus ( Voorhees , 2002 ) using Minipar ( #AUTHOR_TAGb ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data ."	"['We parsed the 3 GB AQUAINT corpus ( Voorhees , 2002 ) using Minipar ( #AUTHOR_TAGb ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data .', 'Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved.', 'Passive subjects (the car was bought) were converted to objects (bought car).', 'We set the MI-threshold, τ , to be 0, and the negative-to-positive ratio, K, to be 2.']"	5
CCT54	D08-1007	Discriminative learning of selectional preference from unlabeled text	using the web to obtain frequencies for unseen bigrams	"['Frank Keller', 'Mirella Lapata']"	experiments	"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task."	"Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) ."	"['Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)).', 'We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature.', 'When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns.']"	1
CCT55	D08-1007	Discriminative learning of selectional preference from unlabeled text	word association norms mutual information and lexicography	"['Kenneth Ward Church', 'Patrick Hanks']"	method	"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words"	We measure this association using pointwise Mutual Information ( MI ) ( #AUTHOR_TAG ) .	"['To learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'To create the positives, we automatically parse a large corpus, and then extract the predicate-argument pairs that have a statistical association in this data.', 'We measure this association using pointwise Mutual Information ( MI ) ( #AUTHOR_TAG ) .', 'The MI between a verb predicate, v, and its object argument, n, is:']"	5
CCT56	D08-1007	Discriminative learning of selectional preference from unlabeled text	using the web to obtain frequencies for unseen bigrams	"['Frank Keller', 'Mirella Lapata']"	method	"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task."	"Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."	"['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', 'For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', 'In this representation, features for one predicate will be completely independent from those for every other predicate.', ""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â\x80² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by Pantel et al. (2007).']"	4
CCT57	D08-1009	Scaling textual inference to the web	natural logic for textual inference	"['B MacCartney', 'C D Manning']"	introduction	"This paper presents the first use of a computational model of natural logic---a system of logical inference which operates over natural language---for textual inference. Most current approaches to the PASCAL RTE textual inference task achieve robustness by sacrificing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity. At the other extreme, systems which rely on first-order logic and theorem proving are precise, but excessively brittle. This work aims at a middle way. Our system finds a low-cost edit sequence which transforms the premise into the hypothesis; learns to classify entailment relations across atomic edits; and composes atomic entailments into a top-level entailment judgment. We provide the first reported results for any system on the FraCaS test suite. We also evaluate on RTE3 data, and show that hybridizing an existing RTE system with our natural logic system yields significant performance gains."	"HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG ) ."	"['HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG ) .']"	1
CCT58	D08-1009	Scaling textual inference to the web	natural logic for textual inference	"['B MacCartney', 'C D Manning']"	related work	"This paper presents the first use of a computational model of natural logic---a system of logical inference which operates over natural language---for textual inference. Most current approaches to the PASCAL RTE textual inference task achieve robustness by sacrificing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity. At the other extreme, systems which rely on first-order logic and theorem proving are precise, but excessively brittle. This work aims at a middle way. Our system finds a low-cost edit sequence which transforms the premise into the hypothesis; learns to classify entailment relations across atomic edits; and composes atomic entailments into a top-level entailment judgment. We provide the first reported results for any system on the FraCaS test suite. We also evaluate on RTE3 data, and show that hybridizing an existing RTE system with our natural logic system yields significant performance gains."	"While many approaches have addressed this problem , our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms ."	"['Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005).', ""While many approaches have addressed this problem , our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms ."", 'For instance, (Braz et al., 2005) represents T , H , and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer linear program derived from that representation.']"	1
CCT59	D08-1036	A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers	a fully bayesian approach to unsupervised partofspeech tagging	"['Sharon Goldwater', 'Tom Griffiths']"			The studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used .	"['The studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used .', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.', 'We ran all our estimators in both conditions here (thanks to Noah Smith for supplying us with his tag set).']"	1
CCT60	D08-1036	A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers	a fully bayesian approach to unsupervised partofspeech tagging	"['Sharon Goldwater', 'Tom Griffiths']"	conclusion		"On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in #AUTHOR_TAG ."	"['As might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in #AUTHOR_TAG .', 'This is perhaps not too surprising, as the Bayesian prior plays a comparatively stronger role with a smaller training corpus (which makes the likelihood term smaller) and the approximation used by Variational Bayes is likely to be less accurate on smaller data sets.']"	1
CCT61	D08-1039	Triplet lexicon models for statistical machine translation	word triggers and the em algorithm	"['Christoph Tillmann', 'Hermann Ney']"	method	"In this paper, we study the use of so-called word trigger pairs to improve an existing language model, which is typically a trigram model in combination with a cache component. A word trigger pair is defined as a long-distance word pair. We present two methods to select the most significant single word trigger pairs. The selected trigger pairs are used in a combined model where the interpolation parameters and trigger interaction parameters are trained by the EM algorithm."	"The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) ."	"['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']"	1
CCT62	D08-1042	A dependency-based word subsequence kernel	kernel methods for relation extraction	"['D Zelenko', 'C Aone', 'A Richardella']"	related work	"We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results."	"Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ) ."	"['A few kernels based on dependency trees have also been proposed.', 'Zelenko et al. (2003) proposed a tree kernel over shallow parse tree representations of sentences.', 'This tree kernel was slightly generalized by Culotta and Sorensen (2004) to compute similarity between two dependency trees.', 'In addition to the words, this kernel also incorporates word classes into the kernel.', 'The kernel is based on counting matching subsequences of children of matching nodes.', 'But as was also noted in (Bunescu and Mooney, 2005a), this kernel is opaque i.e. it is not obvious what the implicit features are and the authors do not describe it either.', 'In contrast, our dependency-based word subsequence kernel, which also computes similarity between two dependency trees, is very transparent with the implicit features being simply the dependency paths.', 'Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ) .', 'Bunescu and Mooney (2005a) give a shortest path dependency kernel for relation extraction.', 'Their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'This kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.', 'Their kernel also uses word classes in addition to the words themselves.']"	3
CCT63	D08-1066	Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective	a discriminative latent variable model for statistical machine translation	"['P Blunsom', 'T Cohn', 'M Osborne']"	conclusion	"Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. (c) 2008 Association for Computational Linguistics"	"Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance ."	"['There are various strands of future research.', 'Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', 'We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'Apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.', 'Finally, it would be interesting to study properties of the penalized Deleted Estimation used in this paper.']"	3
CCT64	D08-1066	Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective	a family of additive online algorithms for category ranking”	"['K Crammer', 'Y Singer']"	related work	"We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stem from recent advances in online learning algorithms. The algorithms are simple to implement and are also time and memory efficient. We provide a unified analysis of the family of algorithms in the mistake bound model. We then discuss experiments with the proposed family of topic-ranking algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora, the algorithms we present achieve state-of-the-art results and outperforms topic-ranking adaptations of Rocchio's algorithm and of the Perceptron algorithm."	"More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) ."	"['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']"	0
CCT65	D08-1066	Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective	an iterativelytrained segmentationfree phrase translation model for statistical machine translation	['Quirk']	conclusion	"Attempts to estimate phrase translation probablities for statistical machine translation using iteratively-trained models have repeatedly failed to produce translations as good as those obtained by estimating phrase translation probablities from surface statistics of bilingual word alignments as described by Koehn, et al. (2003). We propose a new iteratively-trained phrase translation model that produces translations of quality equal to or better than those produced by Koehn, et al.'s model. Moreover, with the new model, translation quality degrades much more slowly as pruning is tightend to reduce translation time."	Based on this advise ( Moore and #AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure .	"['The most similar efforts to ours, mainly (DeNero et al., 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'Based on this advise ( Moore and #AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure .', 'In this work we also start out from a generative model with latent segmentation variables.', 'However, we find out that concentrating the learning effort on smoothing is crucial for good performance.', 'For this, we devise ITG-based priors over segmentations and employ a penalized version of Deleted Estimation working with EM at its core.', 'The fact that our results (at least) match the heuristic estimates on a reasonably sized data set (947k parallel sentence pairs) is rather encouraging.']"	1
CCT66	D08-1113	Latent-variable modeling of string transductions with finite-state methods	learning structured models for phone recognition	"['Slav Petrov', 'Adam Pauls', 'Dan Klein']"	conclusion	"We present a maximally streamlined approach to learning HMM-based acoustic models for automatic speech recognition. In our approach, an initial monophone HMM is iteratively refined using a split-merge EM procedure which makes no assumptions about subphone structure or context-dependent structure, and which uses only a single Gaussian per HMM state. Despite the much simplified training process, our acoustic model achieves state-of-the-art results on phone classification (where it outperforms almost all other methods) and competitive performance on phone recognition (where it outperforms standard CD triphone / subphone / GMM approaches). We also present an analysis of what is and is not learned by our system."	Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .	"['In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side, which is inexpensive (Jiampojamarn et al., 2007).', 'Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .', 'Indeed, our local log-linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'Finally, rather than define a fixed set of feature templates as in Fig. 2, we would like to refine empirically useful features during training, resulting in language-specific backoff patterns and adaptively sized n-gram windows.', 'Many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods.']"	0
CCT67	D08-1113	Latent-variable modeling of string transductions with finite-state methods	applying manytomany alignments and hidden markov models to lettertophoneme conversion	"['Sittichai Jiampojamarn', 'Grzegorz Kondrak', 'Tarek Sherif']"	conclusion		"We would like to use features that look at wide context on the input side , which is inexpensive ( #AUTHOR_TAG ) ."	"['In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side , which is inexpensive ( #AUTHOR_TAG ) .', 'Latent variables we wish to consider are an increased number of word classes; more flexible regions-see Petrov et al. (2007) on learning a state transition diagram for acoustic regions in phone recognition-and phonological features and syllable boundaries.', 'Indeed, our local log-linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'Finally, rather than define a fixed set of feature templates as in Fig. 2, we would like to refine empirically useful features during training, resulting in language-specific backoff patterns and adaptively sized n-gram windows.', 'Many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods.']"	3
CCT68	D09-1003	Semi-supervised semantic role labeling using the latent words language model	automatic retrieval and clustering of similar words	['Dekang Lin']	conclusion	Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.	#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples .	"['We have presented the Latent Words Language Model and showed how it learns, from unlabeled texts, latent words that capture the meaning of a certain word, depending on the context.', 'We then experimented with different methods to incorporate the latent words for Semantic Role Labeling, and tested different methods on the PropBank dataset.', 'Our best performing method showed a significant improvement over the supervised model and over methods previously proposed in the literature.', 'On the full training set the best method performed 2.33% better than the fully supervised model, which is a 10.91% error reduction.', 'Using only 5% of the training data the best semi-supervised model still achieved 60.29%, compared to 40.49% by the supervised model, which is an error reduction of 33.27%.', 'These results demonstrate that the latent words learned by the LWLM help for this complex information extraction task.', 'Furthermore we have shown that the latent words are simple to incorporate in an existing classifier by adding additional features.', 'We would like to perform experiments on employing this model in other information extraction tasks, such as Word Sense Disambiguation or Named Entity Recognition.', 'The current model uses the context in a very straightforward way, i.e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates.', '#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples .']"	0
CCT69	D09-1053	Model adaptation via model interpolation and boosting for web search ranking	language model adaptation with map estimation and the perceptron algorithm	"['M Bacchiani', 'B Roark', 'M Saraclar']"	conclusion	"In this paper, we contrast two language model adaptation approaches: MAP estimation and the perceptron algorithm. Used in isolation, we show that MAP estimation outperforms the latter approach, for reasons which argue for combining the two approaches. When combined, the resulting system provides a 0.7 percent absolute reduction in word error rate over MAP estimation alone. In addition, we demonstrate that, in a multi-pass recognition scenario, it is better to use the perceptron algorithm on early pass word lattices, since the improved error rate improves acoustic model adaptation."	"In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications ."	"['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']"	0
CCT70	D09-1144	Automatic acquisition of theargument-predicaterelations from a frame-annotated corpus	a probabilistic account of logical metonymy	"['Mirella Lapata', 'Alex Lascarides']"	conclusion	"In this article we investigate logical metonymy, that is, constructions in which the argument of a word in syntax appears to be different from that argument in logical form (e.g., enjoy the book means enjoy reading the book, and easy problem means a problem that is easy to solve). The systematic variation in the interpretation of such constructions suggests a rich and complex theory of composition on the syntax/semantics interface. Linguistic accounts of logical metonymy typically fail to describe exhaustively all the possible interpretations, or they don&apos;t rank those interpretations in terms of their likelihood. In view of this, we acquire the meanings of metonymic verbs and adjectives from a large corpus and propose a probabilistic model that provides a ranking on the set of possible interpretations. We identify the interpretations automatically by exploiting the consistent correspondences between surface syntactic cues and meaning. We evaluate our results against paraphrase judgments elicited experimentally from humans and show that the model&apos;s ranking of meanings correlates reliably with human intuitions"	"As already mentioned in the literature , see for example ( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation ."	"['As already mentioned in the literature , see for example ( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .', 'Many applications of semantic relations in NLP are connected to paraphrasing or query expansion, see for example (Voorhees, 1994).', ""Suppose that a search engine or a question answering system receives the query schnelle Bombe 'quick bomb'."", 'Probably, in this case the user is interested in finding information about bombs that explode quickly rather then about bombs in general.', ""Knowledge about predicates associated with the noun Bombe 'bomb' could be used for predicting a set of probable implicit predicates."", 'For generation of the semantically and syntactically correct paraphrases it is sometimes not enough to guess the most probable argument-predicate pairs.', 'Information about types of an argument-predicate relation could be helpful, i.e. which semantic and syntactic position does the argument fill in the argument structure of the predicate.', ""For example, compare eine Bombe explodiert schnell 'a bomb explodes quickly' for schnelle Bombe with ein Buch schnell lesen/schreiben 'to read/write a book quickly' for schnelles Buch 'quick book'."", 'In the first case the argument Bombe fills the subject position, while in the second case Buch fills the object position.', 'Since FrameNet contains information about syntactic realization patterns for frame elements, representation of argument-predicate relations in terms of frames directly supports generation of semantically and syntactically correct paraphrases.']"	0
CCT71	D10-1002	Self-training PCFG grammars with latent annotations across languages	kbest combination of syntactic parsers	"['Hui Zhang', 'Min Zhang', 'Chew Lim Tan', 'Haizhou Li']"	conclusion		Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ) .	"['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative PCFG.', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ) .', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']"	1
CCT72	D10-1002	Self-training PCFG grammars with latent annotations across languages	coarsetofine nbest parsing and maxent discriminative reranking	"['Eugene Charniak', 'Mark Johnson']"	conclusion		"Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ) , despite being a single generative PCFG ."	"['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']"	1
CCT73	D10-1002	Self-training PCFG grammars with latent annotations across languages	products of random latent variable grammars	['Slav Petrov']	conclusion	"We show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-of-the-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German."	"Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG ."	"['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']"	1
CCT74	D10-1038	Topic Segmentation and Labeling in Asynchronous Conversations	you talking to me a corpus and algorithm for conversation disentanglement	"['Micha Elsner', 'Eugene Charniak']"	conclusion	"When multiple conversations occur simultaneously, a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately. We refer to this task as disentanglement. We present a corpus of Internet Relay Chat (IRC) dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability. This is, to our knowledge, the first such corpus for internet chat. We propose a graph-theoretic model for disentanglement, using discourse-based features which have not been previously applied to this task. The model's predicted disentanglements are highly correlated with manual annotations."	In multi-party discussion people usually mention each other 's name for the purpose of disentanglement ( #AUTHOR_TAG ) .	"[""Another possibly critical feature is the 'mention of names'."", ""In multi-party discussion people usually mention each other 's name for the purpose of disentanglement ( #AUTHOR_TAG ) ."", ""In our corpus we found 175 instances where a participant mentions other participant's name."", ""In addition to these, 'Subject of the email', 'topic-shift cue words' can also be beneficial for a model."", 'As a next step for this research, we will investigate how to exploit these features in our methods.', 'We are also interested in the near future to transfer our approach to other similar domains by hierarchical Bayesian multi-task learning and other domain adaptation methods.', 'We plan to work on both synchronous (e.g., chats, meetings) and asynchronous (e.g., blogs) domains.']"	0
CCT75	D10-1056	Two Decades of Unsupervised POS induction: How far have we come?	classbased ngram models of natural language	"['Peter F Brown', 'Vincent J Della Pietra', 'Peter V Desouza', 'Jennifer C Lai', 'Robert L Mercer']"	conclusion		"We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora ."	"['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']"	0
CCT76	D10-1123	Discovery of Semantic Relations between Web Services	scaling textual inference to the web	"['S Schoenmackers', 'O Etzioni', 'D Weld']"	conclusion	"Most Web-based Q/A systems work by finding pages that contain an explicit answer to a question. These systems are helpless if the answer has to be inferred from multiple sentences, possibly on different pages. To solve this problem, we introduce the Holmes system, which utilizes textual inference (TI) over tuples extracted from text.    Whereas previous work on TI (e.g., the literature on textual entailment) has been applied to paragraph-sized texts, Holmes utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. Given only a few minutes, Holmes doubles recall for example queries in three disparate domains (geography, business, and nutrition). Importantly, Holmes's runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus---they are ""approximately"" functional in a well-defined sense."	"Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. ."	"['We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'We release this list for further use by the research community. 2', 'Future Work: Functionality is one of the several properties a relation can possess.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties.']"	0
CCT77	D12-1027	Source Language Adaptation Approaches for Resource-Poor Machine Translation	improved statistical machine translation for resourcepoor languages using related resourcerich languages	"['Preslav Nakov', 'Hwee Tou Ng']"	method	"We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. The evaluation for Indonesian-English (using Malay) and Spanish-English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data."	"Finally , we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) ."	"['Sophisticated phrase table combination.', 'Finally , we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) .', 'The first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the Indonesian-English bi-text.', 'The second table is built from the simple concatenation.', 'The two tables are then merged as follows: all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'Each phrase pair retains its original scores, which are further augmented with 1-3 additional feature scores indicating its origin: the first/second/third feature is 1 if the pair came from the first/second/both table(s), and 0 otherwise.', 'We experiment using all three, the first two, or the first feature only; we also try setting the features to 0.5 instead of 0. This makes the following six combinations (0, 00, 000, .5, .5.5, .5.5.5); on testing, we use the one that achieves the highest BLEU score on the development set.']"	5
CCT78	D12-1027	Source Language Adaptation Approaches for Resource-Poor Machine Translation	improved statistical machine translation for resourcepoor languages using related resourcerich languages	"['Preslav Nakov', 'Hwee Tou Ng']"	related work	"We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. The evaluation for Indonesian-English (using Malay) and Spanish-English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data."	"For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English ."	"['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.', 'Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches.']"	1
CCT79	D12-1084	Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder	paraphrase fragment extraction from monolingual comparable corpora	"['Rui Wang', 'Chris Callison-Burch']"	experiments	"We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different arti-cles about the same topics or events. The pro-cedure consists of document pair extraction, sentence pair extraction, and fragment pair ex-traction. At each stage, we evaluate the in-termediate results manually, and tune the later stages accordingly. With this minimally su-pervised approach, we achieve 62 % of accu-racy on the paraphrase fragment pairs we col-lected and 67 % extracted from the MSR cor-pus. The results look promising, given the minimal supervision of the approach, which can be further scaled up."	"We found the same number using our previous approach ( #AUTHOR_TAG ) , which is roughly equivalent to our core module ."	"['We mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""However, we do include a measure we call productivity to indicate the algorithm's completeness."", 'It is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'Tab. 2 shows the evaluation results.', 'We reach our best precision by using the VP-fragment heuristics, which is still more productive than the LCS method.', 'The grammatical filter gives us a higher precision compared to the purely alignment-based approaches.', 'Enhancing the system with coreference resolution raises the score even further.', 'We cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'However, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work: One state-of-theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0.67.', 'We found the same number using our previous approach ( #AUTHOR_TAG ) , which is roughly equivalent to our core module .', 'Our approach outperforms both by 17% with similar estimated productivity.']"	2
CCT80	D12-1084	Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder	learning script knowledge with web experiments	"['Michaela Regneri', 'Alexander Koller', 'Manfred Pinkal']"	related work	"We describe a novel approach to unsupervised learning of the events that make up a script, along with constraints on their temporal ordering. We collect natural-language descriptions of script-specific event sequences from volunteers over the Internet. Then we compute a graph representation of the script's temporal structure using a multiple sequence alignment algorithm. The evaluation of our system shows that we outperform two informed baselines."	We take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ) .	"['We take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ) .', 'In this earlier work, we focused on event structures and their possible realizations in natural language.', 'The corpus used in those experiments were short crowd-sourced descriptions of everyday tasks written in bullet point style.', 'We aligned them with a hand-crafted similarity measure that was specifically designed for this text type.', 'In this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task.', 'The current approach uses a domainindependent similarity measure instead of a specific hand-crafted similarity score and is thus applicable to standard texts.']"	2
CCT81	D12-1084	Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder	paraphrase fragment extraction from monolingual comparable corpora	"['Rui Wang', 'Chris Callison-Burch']"	related work	"We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different arti-cles about the same topics or events. The pro-cedure consists of document pair extraction, sentence pair extraction, and fragment pair ex-traction. At each stage, we evaluate the in-termediate results manually, and tune the later stages accordingly. With this minimally su-pervised approach, we achieve 62 % of accu-racy on the paraphrase fragment pairs we col-lected and 67 % extracted from the MSR cor-pus. The results look promising, given the minimal supervision of the approach, which can be further scaled up."	Our own work ( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .	"['From an applicational point of view, sentential paraphrases are difficult to use in other NLP tasks.', 'At the phrasal level, interchangeable patterns (Shinyama et al., 2002;Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted.', 'In both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e.g., named entities (NE) or content words.', 'They are quite successful in NE-centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like Recognizing Textual Entailment (Dinu and Wang, 2009).', 'The research on general paraphrase fragment extraction at the sub-sentential level is mainly based on phrase pair extraction techniques from the MT literature.', 'Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability.', 'Quirk et al. (2007) extract fragments using a generative model of noisy translations.', 'Our own work ( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .', 'Our current approach also uses word-word alignment, however, we use syntactic dependency trees to compute grammatical fragments.', 'Our use of dependency trees is inspired by the constituent-tree-based experiments of Callison-Burch (2008).']"	2
CCT82	D12-1084	Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder	paraphrase fragment extraction from monolingual comparable corpora	"['Rui Wang', 'Chris Callison-Burch']"		"We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different arti-cles about the same topics or events. The pro-cedure consists of document pair extraction, sentence pair extraction, and fragment pair ex-traction. At each stage, we evaluate the in-termediate results manually, and tune the later stages accordingly. With this minimally su-pervised approach, we achieve 62 % of accu-racy on the paraphrase fragment pairs we col-lected and 67 % extracted from the MSR cor-pus. The results look promising, given the minimal supervision of the approach, which can be further scaled up."	"Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase ."	"['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.', '5.3).', 'Finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs.']"	2
CCT83	D14-1076	Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees	document summarization via guided sentence compression	"['Chen Li', 'Fei Liu', 'Fuliang Weng', 'Yang Liu']"	experiments	"Joint compression and summarization has been used recently to generate high quality summaries. However, such word-based joint optimization is computationally expensive. In this paper we adopt the 'sentence compression + sentence selection ' pipeline approach for compressive summarization, but propose to perform summary guided compression, rather than generic sentence-based compression. To create an annotated corpus, the human anno-tators were asked to compress sentences while explicitly given the important summary words in the sentences. Using this corpus, we train a supervised sentence compression model us-ing a set of word-, syntax-, and document-level features. During summarization, we use multiple compressed sentences in the inte-ger linear programming framework to select salient summary sentences. Our results on the TAC 2008 and 2011 summarization data sets show that by incorporating the guided sen-tence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art."	"Similar to ( #AUTHOR_TAGa ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences ."	"['Similar to ( #AUTHOR_TAGa ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences .']"	1
CCT84	D15-1148	Detecting Content-Heavy Sentences: A Cross-Language Case Study	assessing the discourse factors that influence the quality of machine translation	"['Junyi Jessy Li', 'Marine Carpuat', 'Ani Nenkova']"	related work	"We present a study of aspects of discourse structure -- specifically discourse devices used to organize information in a sen-tence -- that significantly impact the qual-ity of machine translation. Our analysis is based on manual evaluations of trans-lations of news from Chinese and Ara-bic to English. We find that there is a particularly strong mismatch in the no-tion of what constitutes a sentence in Chi-nese and English, which occurs often and is associated with significant degradation in translation quality. Also related to lower translation quality is the need to em-ploy multiple explicit discourse connec-tives (because, but, etc.), as well as the presence of ambiguous discourse connec-tives in the English translation. Further-more, the mismatches between discourse expressions across languages significantly impact translation quality."	Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .	"['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', 'This prior work was carried over a dataset containing a single reference translation for each Chinese sentence.', 'In the work presented in this paper, we strengthen our findings by examining multiple reference translations for each Chinese sentence.', 'We define heavy sentences based on agreement of translator choices and reader preferences.']"	1
CCT85	E03-1004	Czech-English Dependency-based Machine Translation	automatic procedures in tectogrammatical tagging	['Alena Bohmova']	experiments	"This paper describes a specific part of the Prague Dependency Treebank annotation, the step from the surface dependency structure towards the underlying representation of the sentence. The first section explains the theoretical basis of the project. In Section 2 all the procedure of conversion to the tectogrammatical structure is summarized and Section 3 presents in detail the present stage of the automated part of the conversion procedure."	These automatic transformations are based on linguistic rules ( #AUTHOR_TAG ) .	"['During the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one.', 'These automatic transformations are based on linguistic rules ( #AUTHOR_TAG ) .', 'Subsequently, tectogrammatical functors are assigned by the C4.5 classifier (2abokrtsk9 et al., 2002).']"	5
CCT86	E03-1004	Czech-English Dependency-based Machine Translation	a maximumentropyinspired parser	['Eugene Charniak']	experiments		"We carried out two parallel experiments with two parsers available for Czech , parser I ( Hajie et al. , 1998 ) and parser II ( #AUTHOR_TAG ) ."	"['The analytical parsing of Czech runs in two steps: the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors.', 'We carried out two parallel experiments with two parsers available for Czech , parser I ( Hajie et al. , 1998 ) and parser II ( #AUTHOR_TAG ) .', 'In the second step, we used a module for automatic analytical functor assignment (2abokrtskyT et al., 2002).']"	5
CCT87	E03-1004	Czech-English Dependency-based Machine Translation	improved statistical alignment models	"['F J Och', 'H Ney']"		"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignmen"	"To make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see #AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary."	"['To make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see #AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.', 'As a result, the entry/translation pairs seen in the parallel corpus of WSJ become more probable.', 'For entry/translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'The translation is ""GIZA++ se- lected"" if its probability is higher than a threshold, which is in our case set to 0.10.']"	5
CCT88	E03-1004	Czech-English Dependency-based Machine Translation	bleu a method for automatic evaluation of machine translation	"['Kishore Papineni', 'Salim Roukos', 'Todd Ward', 'WeiJing Zhu']"	introduction	"Human evaluations of machine translation are extensive but expensive. Human eval-uations can take months to finish and in-volve human labor that can not be reused. We propose a method of automatic ma-chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu-ation, and that has little marginal cost per run. We present this method as an auto-mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."	For the evaluation of the results we use the BLEU score ( #AUTHOR_TAG ) .	"['First, we summarize resources available for the experiments (Section 2).', 'Section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of Czech input.', 'In Section 4 we describe the process of the filtering of dictionaries used in the transfer procedure (for its characterization, see Section 5).', 'The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec-tion 7.', 'For the evaluation of the results we use the BLEU score ( #AUTHOR_TAG ) .', 'Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'We also compare the results with the output generated by the statistical translation system GIZA++/ISI ReWrite Decoder (Al-Onaizan et al., 1999;Och and Ney, 2000;Germann et al., 2001), trained on the same parallel corpus.']"	5
CCT89	E03-1004	Czech-English Dependency-based Machine Translation	improved statistical alignment models	"['F J Och', 'H Ney']"	introduction	"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignmen"	"We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus ."	"['First, we summarize resources available for the experiments (Section 2).', 'Section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of Czech input.', 'In Section 4 we describe the process of the filtering of dictionaries used in the transfer procedure (for its characterization, see Section 5).', 'The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec-tion 7.', 'For the evaluation of the results we use the BLEU score (Papineni et al., 2001).', 'Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus .']"	1
CCT90	E03-1004	Czech-English Dependency-based Machine Translation	bleu a method for automatic evaluation of machine translation	"['Kishore Papineni', 'Salim Roukos', 'Todd Ward', 'WeiJing Zhu']"	experiments	"Human evaluations of machine translation are extensive but expensive. Human eval-uations can take months to finish and in-volve human labor that can not be reused. We propose a method of automatic ma-chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu-ation, and that has little marginal cost per run. We present this method as an auto-mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."	"We evaluated our translations with IBM 's BLEU evaluation metric ( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) ."	"[""We evaluated our translations with IBM 's BLEU evaluation metric ( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) ."", 'We used four reference retranslations of 490 sentences selected from the WSJ sections 22, 23, and 24, which were themselves used as the fifth reference.', 'The evaluation method used is to hold out each reference in turn and evaluate it against the remaining four, averaging the five BLEU scores.']"	5
CCT91	E03-1007	Using POS information for statistical machine translation into morphologically rich languages	the mathematics of statistical machine translation parameter estimation	"['P F Brown', 'S A Della Pietra', 'V J Della Pietra', 'R L Mercer']"	experiments	"We describe a series o,f ive statistical models o,f the translation process and give algorithms,for estimating the parameters o,f these models given a set o,f pairs o,f sentences that are translations o,f one another. We define a concept o,f word-by-word alignment between such pairs o,f sentences. For any given pair of such sentences each o,f our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable o,f these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair o,f sentences. We have a great deal o,f data in French and English from the proceedings o,f the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we,feel that because our algorithms have minimal inguistic content hey would work well on other pairs o,f languages. We also,feel, again because of the minimal inguistic content o,f our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus. 1"	We performed translation experiments with an implementation of the IBM-4 translation model ( #AUTHOR_TAG ) .	"['We compared the two statistical lexica obtained from the baseline system and from the maximum entropy training on the transformed corpus.', 'For the baseline lexicon, we observed an average of 5.82 Catalan translation candidates per English word and 6.16 Spanish translation candidates.', 'These numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy: there, we have an average of 4.20 for Catalan and 4.46 for Spanish.', 'Especially for (nominative) English pronouns (which have many verbs as translation candidates in the baseline lexicon), the number of translation candidates was substantially scaled down by a factor around 4. This shows that our method was successful in producing a more focused lexicon probability distribution.', 'We performed translation experiments with an implementation of the IBM-4 translation model ( #AUTHOR_TAG ) .', 'A description of the system can be found in (Tillmann and Ney, 2002).', 'Table 5 presents an assessment of translation quality for both the language pairs English-Catalan and English-Spanish.', 'We see that there is a significant decrease in error rate for the translation into Catalan.', 'This change is consistent across both error rates, the WER and 100-BLEU.', 'For translations from English into Spanish, the improvement is less substantial.', 'A reason for this might be that the Spanish vocabulary contains more entries and the ratio between fullforms and baseforms is higher: 1.57 for Spanish versus 1.53 for Catalan4 .', 'This makes it more difficult for the system to choose the correct inflection when generating a Spanish sentence.', 'We assume that the extension of our approach to other word classes than verbs will yield a quality gain for translations into Spanish.']"	5
CCT92	E03-1007	Using POS information for statistical machine translation into morphologically rich languages	a maximum entropy approach to natural language processing	"['A L Berger', 'S A Della Pietra', 'V J Della Pietra']"		"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."	The maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .	"['The maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .', 'This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'The distribution is required to satisfy constraints, which represent facts known from the data.', 'These constraints are expressed on the basis of feature functions hu,(s,t),']"	5
CCT93	E03-1007	Using POS information for statistical machine translation into morphologically rich languages	improved alignment models for statistical machine translation	"['F J Och', 'C Tillmann', 'H Ney']"		"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words. We present results using the Verbmobil task (German-English, 6000word vocabulary) which is a limited-domain spoken-language task. The experimental tests were performed on both the text transcription and the speech recognizer output. 1 S t a t i s t i c a l M a c h i n e T r a n s l a t i o n The goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string f / = fl...fj...fJ, which is to be translated into a target string e{ = el...ei...ex. Among all possible target strings, we will choose the string with the highest probability: = argmax {Pr(ezIlflJ)}"	"For descriptions of SMT systems see for example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) ."	"['The input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .']"	0
CCT94	E03-1007	Using POS information for statistical machine translation into morphologically rich languages	a maximum entropy approach to natural language processing	"['A L Berger', 'S A Della Pietra', 'V J Della Pietra']"		"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."	"For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 ) ."	"['where A = {Am } is the set of model parameters with one weight A, for each feature function hm .', 'For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 ) .']"	0
CCT95	E03-1007	Using POS information for statistical machine translation into morphologically rich languages	fast decoding and optimal decoding for machine translation	"['U Germann', 'M Jahr', 'K Knight', 'D Marcu', 'K Yamada']"		"A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder's job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem."	"For descriptions of SMT systems see for example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) ."	"['The input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .']"	0
CCT96	E09-1100	Character-level dependencies in Chinese	exploiting unlabeled text with different unsupervised segmentation criteria for chinese word segmentation	"['Hai Zhao', 'Chunyu Kit']"	method	This paper presents a novel approach to improve Chinese word seg- mentation (CWS) that attempts to utilize unlabeled data such as training and test data without annotation for further enhancement of the state-of-the-art perfor- mance of supervised learning. The lexical information plays the role of infor- mation transformation from unlabeled text to supervised learning model. Four types of unsupervised segmentation criteria are used for word candidate extrac- tion and the corresponding word likelihood computation. The information output by unsupervised segmentation criteria as features therefore is integrated into su- pervised learning model to strengthen the learning for the matching subsequence. The effectiveness of the proposed method is verified in data sets from the latest in- ternational CWS evaluation. Our experimental results show that character-based conditional random fields framework can effectively make use of such informa- tion from unlabeled data for performance enhancement on top of the best existing results.	"Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAGc ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 ."	"['Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAGc ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .', 'Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus.']"	1
CCT97	J00-1003	Practical Experiments with Regular Approximation of Context-Free Languages	dynamic compilation of weighted contextfree grammars	"['Mehryar Mohri', 'Fernando C N Pereira']"		"Weighted context-free grammars are a convenient formalism for representing grammatical constructions and their likelihoods in a variety of language-processing applications. In particular, speech understanding applications require appropriate grammars both to constrain speech recognition and to help extract the meaning of utterances. In many of those applications, the actual languages described are regular, but context-free representations are much more concise and easier to create. We describe an efficient algorithm for compiling into weighted finite automata an interesting class of weighted context-free grammars that represent regular languages. The resulting automata can then be combined with other speech recognition components. Our method allows the recognizer to dynamically activate or deactivate grammar rules and substitute a new regular language for some terminal symbols, depending on previously recognized inputs, all without recompilation. We also report experimental results showing the practicality of the approach."	1 The representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .	"['1 The representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .', 'However, in this paper we use our representation as an intermediate result in approximating an unrestricted context-free grammar, with the final objective of obtaining a single minimal deterministic automaton.', ""For this purpose, Mohri and Pereira's representation offers little advantage.""]"	1
CCT98	J00-1003	Practical Experiments with Regular Approximation of Context-Free Languages	finitestate approximation of constraintbased grammars using leftcorner grammar transforms	['Mark Johnson']			"This idea was proposed by Krauwer and des Tombe ( 1981 ) , Langendoen and Langsam ( 1987 ) , and Pulman ( 1986 ) , and was rediscovered by Black ( 1989 ) and recently by #AUTHOR_TAG ."	"['By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results.', 'This idea was proposed by Krauwer and des Tombe ( 1981 ) , Langendoen and Langsam ( 1987 ) , and Pulman ( 1986 ) , and was rediscovered by Black ( 1989 ) and recently by #AUTHOR_TAG .', 'Since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method.']"	0
CCT99	J00-1003	Practical Experiments with Regular Approximation of Context-Free Languages	approximating contextfree grammars with a finitestate calculus	['Edmund Grimley-Evans']			"We rephrase the method of #AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above ."	"['We rephrase the method of #AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .', 'Then an additional mechanism is introduced that ensures for each rule A --~ X1 • .. Xm separately that the list of visits to the states qo,.. • • qm satisfies some reasonable criteria: a visit to qi, with 0 < i < m, should be followed by one to qi+l or q0.', 'The latter option amounts to a nested incarnation of the rule.', 'There is a complementary condition for what should precede a visit to qi, with 0 < i < m.']"	5
CCT100	J00-1003	Practical Experiments with Regular Approximation of Context-Free Languages	precise ngram probabilities from stochastic contextfree grammars	"['Andreas Stolcke', 'Jonathan Segal']"		"We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others). The method operates via the computation of substring expectations, which in turn is accomplished by solving systems of linear equations derived from the grammar. The procedure is fully implemented and has proved viable and useful in practice."	"This method can be generalized , inspired by #AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars ."	"['This method can be generalized , inspired by #AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars .', 'By ignoring the probabilities, each N = 1, 2, 3 .... gives rise to a superset approximation that can be described as follows: The set of strings derivable from a nonterminal A is approximated by the set of strings al ... an such that • for each substring v = ai+l ... ai+N (0 < i < n --N) we have A --+* wvy, for some w and y,']"	0
CCT101	J00-1003	Practical Experiments with Regular Approximation of Context-Free Languages	contextfree parsing through regular approximation	['Mark-Jan Nederhof']		"We show that context-free parsing can be realised by a 2-phase process, relying on an approximated context-free grammar. In the first phase a finite transducer performs parsing according to the approximation. In the second phase, the approximated parses are refined according to the original grammar."	See #AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata .	['See #AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata .']	0
CCT102	J00-1003	Practical Experiments with Regular Approximation of Context-Free Languages	contextfree parsing through regular approximation	['Mark-Jan Nederhof']		"We show that context-free parsing can be realised by a 2-phase process, relying on an approximated context-free grammar. In the first phase a finite transducer performs parsing according to the approximation. In the second phase, the approximated parses are refined according to the original grammar."	"A very similar formulation , for another grammar transformation , is given in #AUTHOR_TAG ."	"['Figure 10 (b) is no longer possible, since no nonterminal in the transformed grammar would contain 1 in its superscript.Because of the demonstrated increase of the counter f, this transformation is guaranteed to remove self-embedding from the grammar.', 'However, it is not as selective as the transformation we saw before, in the sense that it may also block subderivations that are not of the form A --** ~Afl.', 'Consider for example the subderivation from Figure10, but replacing the lower occurrence of S by any other nonterminal C that is mutually recursive with S, A, and B. Such a subderivation S ---** b c C d a would also be blocked by choosing d = 0.', 'In general, increasing d allows more of such derivations that are not of the form A ~"" o~Afl but also allows more derivations that are of that form.The reason for considering this transformation rather than any other that eliminates self-embedding is purely pragmatic: of the many variants we have tried that yield nontrivial subset approximations, this transformation has the lowest complexity in terms of the sizes of intermediate structures and of the resulting finite automata.In the actual implementation, we have integrated the grammar transformation and the construction of the finite automaton, which avoids reanalysis of the grammar to determine the partition of mutually recursive nonterminals after transformation.', 'This integration makes use, for example, of the fact that for fixed Ni and fixed f, the set of nonterminals of the form A,f, with A c Ni, is (potentially) mutually right-recursive.A set of such nonterminals can therefore be treated as the corresponding case from Figure2, assuming the value right.The full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'A very similar formulation , for another grammar transformation , is given in #AUTHOR_TAG .']"	1
CCT103	J00-2003	A Multistrategy Approach to Improving Pronunciation by Analogy	algorithms for graphemephoneme translation for english and french applications for database searches and speech synthesis	"['Michel Divay', 'Anthony J Vitale']"	introduction	"Letter-to-sound rules, also known as grapheme-to-phoneme rules, are important computational tools and have been used for a variety of purposes including word or name lookups for database searches and speech synthesis.These rules are especially useful when integrated into database searches on names and addresses, since they can complement orthographic search algorithms that make use of permutation, deletion, and insertion by allowing for a comparison with the phonetic equivalent. In databases, phonetics can help retrieve a word or a proper name without the user needing to know the correct spelling. A phonetic index is built with the vocabulary of the application. This could be an entire dictionary, or a list of proper names. The searched word is then converted into phonetics and retrieved with its information, if the word is in the phonetic index. This phonetic lookup can be used to retrieve a misspelled word in a dictionary or a database, or in a text editor to suggest corrections.Such rules are also necessary to formalize grapheme-phoneme correspondences in speech synthesis architecture. In text-to-speech systems, these rules are typically used to create phonemes from computer text. These phonemic symbols, in turn, are used to feed lower-level phonetic modules (such as timing, intonation, vowel formant trajectories, etc.) which, in turn, feed a vocal tract model and finally output a waveform and, via a digital-analogue converter, synthesized speech. Such rules are a necessary and integral part of a text-to-speech system since a database lookup (dictionary search) is not sufficient to handle derived forms, new words, nonce forms, proper nouns, low-frequency technical jargon, and the like; such forms typically are not included in the database. And while the use of a dictionary is more important now that denser and faster memory is available to smaller systems, letter-to-sound still plays a crucial and central role in speech synthesis technology.Grapheme-to-phoneme technology is also useful in speech recognition, as a way of generating pronunciations for new words that may be available in grapheme form, or for naive users to add new words more easily. In that case, the system must generate the multiple variations of the word.While there are different problems in languages that use non-alphabetic writing systems (syllabaries, as in Japanese, or logographic systems, as in Chinese) (DeFrancis 1984), all alphabetic systems have a structured set of correspondences. These range from the trivial in languages like Spanish or Swahili, to extremely complex in languages such as English and French. This paper will outline some of the previous attempts to construct such rule sets and will describe new and successful approaches to the construction of letter-to-sound rules for English and French."	"Typical letter-to-sound rule sets are those described by Ainsworth ( 1973 ) , McIlroy ( 1973 ) , Elovitz et al. ( 1976 ) , Hurmicutt ( 1976 ) , and #AUTHOR_TAG ."	"['which states that the letter substring B with left context A and right context C receives the pronunciation (i.e., phoneme substring) D. Such rules can also be straightforwardly cast in the IF... THEN form commonly featured in high-level programming languages and employed in expert, knowledge-based systems technology.', 'They also constitute a formal model of universal computation (Post 1943).', 'Conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'Typical letter-to-sound rule sets are those described by Ainsworth ( 1973 ) , McIlroy ( 1973 ) , Elovitz et al. ( 1976 ) , Hurmicutt ( 1976 ) , and #AUTHOR_TAG .']"	0
CCT104	J00-2003	A Multistrategy Approach to Improving Pronunciation by Analogy	algorithms for graphemephoneme translation for english and french applications for database searches and speech synthesis	"['Michel Divay', 'Anthony J Vitale']"	introduction	"Letter-to-sound rules, also known as grapheme-to-phoneme rules, are important computational tools and have been used for a variety of purposes including word or name lookups for database searches and speech synthesis.These rules are especially useful when integrated into database searches on names and addresses, since they can complement orthographic search algorithms that make use of permutation, deletion, and insertion by allowing for a comparison with the phonetic equivalent. In databases, phonetics can help retrieve a word or a proper name without the user needing to know the correct spelling. A phonetic index is built with the vocabulary of the application. This could be an entire dictionary, or a list of proper names. The searched word is then converted into phonetics and retrieved with its information, if the word is in the phonetic index. This phonetic lookup can be used to retrieve a misspelled word in a dictionary or a database, or in a text editor to suggest corrections.Such rules are also necessary to formalize grapheme-phoneme correspondences in speech synthesis architecture. In text-to-speech systems, these rules are typically used to create phonemes from computer text. These phonemic symbols, in turn, are used to feed lower-level phonetic modules (such as timing, intonation, vowel formant trajectories, etc.) which, in turn, feed a vocal tract model and finally output a waveform and, via a digital-analogue converter, synthesized speech. Such rules are a necessary and integral part of a text-to-speech system since a database lookup (dictionary search) is not sufficient to handle derived forms, new words, nonce forms, proper nouns, low-frequency technical jargon, and the like; such forms typically are not included in the database. And while the use of a dictionary is more important now that denser and faster memory is available to smaller systems, letter-to-sound still plays a crucial and central role in speech synthesis technology.Grapheme-to-phoneme technology is also useful in speech recognition, as a way of generating pronunciations for new words that may be available in grapheme form, or for naive users to add new words more easily. In that case, the system must generate the multiple variations of the word.While there are different problems in languages that use non-alphabetic writing systems (syllabaries, as in Japanese, or logographic systems, as in Chinese) (DeFrancis 1984), all alphabetic systems have a structured set of correspondences. These range from the trivial in languages like Spanish or Swahili, to extremely complex in languages such as English and French. This paper will outline some of the previous attempts to construct such rule sets and will describe new and successful approaches to the construction of letter-to-sound rules for English and French."	"For instance , #AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) ."	"['It is also conceivable that data-driven techniques can actually outperform traditional rules.', 'However, this possibility is not usually given much credence.', ""For instance , #AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) ."", '520).', 'Dutoit (1997) takes this further, stating ""such training-based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores"" (p.', '115, note 14).']"	0
CCT105	J00-2003	A Multistrategy Approach to Improving Pronunciation by Analogy	using an online dictionary to find rhyming words and pronunciations for unknown words	"['Roy J Byrd', 'Martin S Chodorow']"	introduction	"Humans know a great deal about relationships among words. This paper discusses relationships among word pronunciations. We describe a computer system which models human judgement of rhyme by assigning specific roles to the location of primary stress, the similarity of phonetic segments, and other factors. By using the model as an experimental tool, we expect to improve our understanding of rhyme. A related computer model will attempt to generate pronunciations for unknown words by analogy with those for known words. The analogical processes involve techniques for segmenting and matching word spellings, and for mapping spelling to sound in known words. As in the case of rhyme, the computer model will be an important tool for improving our understanding of these processes. Both models serve as the basis for functions in the WordSmith automated dictionary system."	"See also the work of #AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis ."	"['Pronunciation by analogy (PbA) is a data-driven technique for the automatic phonemization of text, originally proposed as a model of reading, e.g., by Glushko (1979) and Kay and Marcel (1981).', 'It was first proposed for TTS applications over a decade ago by Dedina andNusbaum (1986, 1991).', 'See also the work of #AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .', 'As detailed by Damper (1995) and Damper and Eastmond (1997), PbA shares many similarities with the artificial intelligence paradigms variously called case-based, memory-based, or instance-based reasoning as applied to letter-to-phoneme conversion (Stanfill and Waltz 1986;Lehnert 1987;Stanfill 1987Stanfill , 1988Golding 1991;Golding and Rosenbloom 1991;van den Bosch and Daelemans 1993).']"	0
CCT106	J00-2003	A Multistrategy Approach to Improving Pronunciation by Analogy	classifier combination for improved lexical disambiguation	"['Eric Brill', 'Jun Wu']"	experiments	"One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees.."	"Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. , #AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) ."	"['Clearly, the above characterization is very wide ranging.', 'Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. , #AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .', 'According to Abbott (1999, 290), ""While the reasons [that] combining models works so well are not rigorously understood, there is ample evidence that improvements over single models are typical....', 'A strong case can be made for combining models across algorithm families as a means of providing uncorrelated output estimates.""', 'Our purpose in this paper is to study and exploit such fusion by model (or strategy) combination as a way of achieving performance gains in PbA.']"	0
CCT107	J00-2004	Models of Translational Equivalence among Words	automatic evaluation and uniform filter cascades for inducing nbest translation lexicons	['I Dan Melamed']		"This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora."	"Many researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994; #AUTHOR_TAG 1995; Wu and Xia 1994)."	"['Many researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994; #AUTHOR_TAG 1995; Wu and Xia 1994).', 'Most of these algorithms can be summarized as follows:']"	0
CCT108	J00-2004	Models of Translational Equivalence among Words	but dictionaries are data too	"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']"	related work	"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio"	Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAGb ) .	"['Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAGb ) .', 'These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'I shall review these models using the notation in Table 1.']"	0
CCT109	J00-2004	Models of Translational Equivalence among Words	but dictionaries are data too	"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']"	method	"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio"	"Unlike the models proposed by #AUTHOR_TAGb ) , this model is symmetric , because both word bags are generated together from a joint probability distribution ."	"['The probability distribution trans (.1, ~) is a word-to-word translation model.', 'Unlike the models proposed by #AUTHOR_TAGb ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .', ""Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions."", 'A sequenceto-sequence translation model can be obtained from a word-to-word translation model by combining Equation 11 with order information as in Equation 8.']"	1
CCT110	J00-2004	Models of Translational Equivalence among Words	a perspective on word sense disambiguation methods and their evaluation	"['Philip Resnik', 'David Yarowsky']"		"In this position paper, we make several observations about the state of the art in automatic word sense disambiguation. Motivated by these observations, we offer several specific proposals to the community regarding improved evaluation criteria, common training and testing resources, and the definition of sense inventories."	"If each word 's translation is treated as a sense tag ( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !"	"['in bitext space is another kind of collocation.', ""If each word 's translation is treated as a sense tag ( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !"", 'Method B exploits this property under the hypothesis that ""one sense per collocation"" holds for translational collocations.', 'This hypothesis implies that if u and v are possible mutual translations, and a token u co-occurs with a token v in the bitext, then with very high probability the pair (u, v) was generated from the same concept and should be linked.', 'To test this hypothesis, I ran one iteration of Method A on 300,000 aligned sentence pairs from the Canadian Hansards bitext.', 'I then plotted the links (u,v) ratio ~ for several values of cooc (u, v) in Figure 2. The curves show that the ratio links (u,v) cooc (u,v) tends to be either very high or very low.', 'This bimodality is not an artifact of the competitive linking process, because in the first iteration, linking decisions are based only on the initial similarity metric.']"	5
CCT111	J00-2004	Models of Translational Equivalence among Words	automatic evaluation and uniform filter cascades for inducing nbest translation lexicons	['I Dan Melamed']		"This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora."	"In informal experiments described elsewhere ( #AUTHOR_TAG ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 ."	"['In informal experiments described elsewhere ( #AUTHOR_TAG ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows:']"	2
CCT112	J00-2004	Models of Translational Equivalence among Words	reading more into foreign languages	"['John Nerbonne', 'Lauri Karttunen', 'Elena Paskaleva', 'Gabor Proszeky', 'Tiit Roosmaa']"	method	"GLOSSER is designed to support reading and learning to read in a foreign language. There are four language pairs currently supported by GLOSSER: English Bulgarian, English-Estonian, English Hungarian and French-Dutch. The program is operational on UNIX and Windows '95 platforms, and has undergone a pilot user-study. A demonstration (in UNIX) for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis (ICALL), including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples."	"â¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â¢ multilingual document filtering ( e.g. , Oard 1997 ) , â¢ computer-assisted language learning ( e.g. , #AUTHOR_TAG ) , â¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,"	"['â\x80¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , #AUTHOR_TAG ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']"	0
CCT113	J00-2004	Models of Translational Equivalence among Words	but dictionaries are data too	"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']"		"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio"	"In this situation , #AUTHOR_TAGb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''"	"[""In this situation , #AUTHOR_TAGb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''"", 'The single most probable assignment Ama~ is the maximum a posteriori (MAP) assignment:']"	4
CCT114	J00-2004	Models of Translational Equivalence among Words	but dictionaries are data too	"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']"		"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio"	"Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX #AUTHOR_TAG ) ."	"['In Method B, the estimation of the auxiliary parameters A + and A-depends only on the overall distribution of co-occurrence counts and link frequencies.', 'All word pairs that co-occur the same number of times and are linked the same number of times are assigned the same score.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example, frequent words are translated less consistently than rare words (Catizone, Russell, and Warwick 1989).', 'To account for these differences, we can estimate separate values of A + and A-for different ranges of cooc (u, v).', 'Similarly, the auxiliary parameters can be conditioned on the linked parts of speech.', 'A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts.', 'Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX #AUTHOR_TAG ) .', 'Brown et al. 1993).', 'When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B (links (u, v)[cooc(u, v), A +) scorec (u, vlZ = class(u, v)) = log B(links(u, v)[cooc(u, v), A z)"" (37) Section 6.1.1 describes the link classes used in the experiments below.']"	5
CCT115	J00-2004	Models of Translational Equivalence among Words	but dictionaries are data too	"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']"		"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio"	"Until now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models ( #AUTHOR_TAGb ) ."	"[""Until now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models ( #AUTHOR_TAGb ) ."", 'Objective and more accurate tests can be carried out using a ""gold standard.""', 'I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English.', 'This bitext was selected to facilitate widespread use and standardization (see Melamed [1998c] for details).', 'The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool.', 'The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular translation model.', 'The annotation was replicated five times by seven different annotators.']"	1
CCT116	J00-2004	Models of Translational Equivalence among Words	accurate methods for the statistics of surprise and coincidence	['Ted Dunning']		"Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text."	"In informal experiments described elsewhere ( Melamed 1995 ) , I found that the G2 statistic suggested by #AUTHOR_TAG slightly outperforms 02 ."	"['In informal experiments described elsewhere ( Melamed 1995 ) , I found that the G2 statistic suggested by #AUTHOR_TAG slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows:']"	0
CCT117	J00-2004	Models of Translational Equivalence among Words	building parallel ltag for french and italian	['Marie-Helene Candito']	method	"In this paper we view Lexicalized Tree Adjoining Grammars as the compilation of a more abstract and modular layer of linguistic description: the metagrammar (MG). MG provides a hierarchical representation of lexico-syntactic descriptions and principles that capture the well-formedness of lexicalized structures, expressed using syntactic functions. This makes it possible for a tool to compile an instance of MG into an LTAG, automatically performing the relevant combinations of linguistic phenomena. We then describe the instantiation of an MG for Italian and French. The work for French was performed starting with an existing LTAG, which has been augmented as a result. The work for Italian was performed by systematic contrast with the French MG. The automatic compilation gives two parallel LTAG, compatible for multilingual NLP applications."	"There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) ."	"['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']"	0
CCT118	J00-2004	Models of Translational Equivalence among Words	should we translate the documents or the queries in crosslanguage information retrieval	['J Scott McCarley']	method	"Previous comparisons of document and query translation suffered difficulty due to differing quality of machine translation in these two opposite directions. We avoid this difficulty by training identical statistical translation models for both translation directions using the same training data. We investigate information retrieval between English and French, incorporating both translations directions into both document translation and query translation-based information retrieval, as well as into hybrid systems. We find that hybrids of document and query translation-based systems out-perform query translation systems, even human-quality query translation systems."	"â¢ cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , â¢ multilingual document filtering ( e.g. , Oard 1997 ) , â¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , â¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,"	"['â\x80¢ cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']"	0
CCT119	J00-2005	Pipelines and Size Constraints	the linguistic basis of text generation	['Laurence Danlos']	introduction	"This study presents an original and penetrating analysis of the complex problems surrounding the automatic generation of natural language text. Laurence Danlos provides a valuable critical review of current research in this important and increasingly active field, and goes on to describe a new theoretical model that is thoroughly grounded in linguistic principles.The model emphasizes the semantic, syntactic and lexical constraints that must be dealt with when establishing a relationship between meaning and form, and it is consideration of such linguistic constraints that determines Danlos' generation algorithm. The book concludes with a description of a generation system based on this algorithm which produces texts in several domains and also a system for the synthesis of spoken messages from semantic representations.The book is a significant addition to the literature on text generation, and will be of particular interest to all computational linguists and AI researchers who have wrestled with the problem of vocabulary selection."	Many other such cases are described in Danlos 's book ( #AUTHOR_TAG ) .	"[""Many other such cases are described in Danlos 's book ( #AUTHOR_TAG ) ."", 'The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules.', 'This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module.']"	0
CCT120	J00-2005	Pipelines and Size Constraints	describing complex charts in natural language a caption generation system	"['Vibhu Mittal', 'Johanna Moore', 'Guiseppe Carenini', 'Steven Roth']"	introduction	"Graphical presentations can be used to communicate information in relational data sets succinctly and effectively. However, novel graphical presentations that represent many attributes and relationships are often difficult to understand completely until explained. Automatically generated graphical presentations must therefore either be limited to generating simple, conventionalized graphical presentations, or risk incomprehensibility. A possible solution to this problem would be to extend automatic graphical presentation systems to generate explanatory captions in natural language, to enable users to understand the information expressed in the graphic. This paper presents a system to do so. It uses a text planner to determine the content and structure of the captions based on: (1) a representation of the structure of the graphical presentation and its mapping to the data it depicts, (2) a framework for identifying the perceptual complexity of graphical elements, and (3) the structure of the data expressed in the graphic. The output of the planner is further processed regarding issues such as ordering, aggregation, centering, generating referring expressions, and lexical choice. We discuss the architecture of our system and its strengths and limitations. Our implementation is currently limited to 2-D charts and maps, but, except for lexical information, it is completely domain independent. We illustrate our discussion with figures and generated captions about housing sales in Pittsburgh."	"This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( #AUTHOR_TAG ) ."	"['Despite these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed by Reiter (1994) and Paiva (1998).', 'This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( #AUTHOR_TAG ) .']"	0
CCT121	J00-2005	Pipelines and Size Constraints	has a consensus nl generation architecture appeared and is it psycholinguistically plausible	['Ehud Reiter']	introduction	"I survey some recent applications-oriented  NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations  these modules perform, and the way the modules interact with each other. I also  compare this &apos;consensus architecture&apos; among  applied NLG systems with psycholinguistic  knowledge about how humans speak, and argue  that at least some aspects of the consensns  architecture seem to be in agreement  with what is known about human language production, despite the fact that psycholinguistic  plausibility was not in general a goal  of the developers of the surveyed systems"	"Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ) ."	"['Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ) .', 'This may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems (Mittal et al. 1998).']"	0
CCT122	J00-2009	Book Reviews: Lexical Semantics and Knowledge Representation in Multilingual Text Generation	upper modeling organizing knowledge for natural language processing	['John A Bateman']		"Abstract : A general, reusable computational resource has been developed within the Penman text generation project for organizing domain knowledge appropriately for linguistic realization. This resource, called the upper model, provides a domain- and task-independent classification system' that supports sophisticated natural language processing while significantly simplifying the interface between domain-specific knowledge and general linguistic resources. This paper presents the results of our experiences in designing and using the upper model in a variety of applications over the past 5 years. In particular, we present our conclusions concerning the appropriate organization of an upper model, its domain- independence, and the types of interrelationships that need to be supported between upper model and grammar and semantics."	"The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model ( #AUTHOR_TAG ) ."	"['At first I found Chapters 4 through 8 slightly overwhelming, as they introduce several levels of representation, each with its own terminology and acronyms.', 'However, at a second, more-careful, reading, everything falls into place.', ""The resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as Jackendoff's (1990) and Levin's (1993)."", 'The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model ( #AUTHOR_TAG ) .', 'Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system.']"	0
CCT123	J00-2013	"Extended Finite State Models of Language András Kornai (editor) (BBN Technologies) Cambridge University Press (Studies in natural language processing), 1999, xii+278 pp and CD-ROM; hardbound, ISBN 0-521-63198-X, $59.95"	regular models of phonological rule systems	"['Ronald M Kaplan', 'Martin Kay']"		This paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology. It shows in detail how this framework applies to ordered sets of context-sensitive rewriting rules and also to grammars in Koskenniemi's two-level formalism. This analysis provides a common representation of phonological constraints that supports efficient generation and recognition by a single simple interpreter.	"Shortly after the publication of The Sound Pattern of English ( Chomsky and Halle 1968 ) , Kornai points out , `` Johnson ( 1970 ) demonstrated that the context-sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in #AUTHOR_TAG . ''"	"[""Shortly after the publication of The Sound Pattern of English ( Chomsky and Halle 1968 ) , Kornai points out , `` Johnson ( 1970 ) demonstrated that the context-sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in #AUTHOR_TAG . ''"", ""These works inspired Koskenniemi's two-level system, and the Xerox rule compiler (Dalrymple et al. 1987)."", 'Both are now dominant tools in the fields of computational phonology and morphology, as exemplified by Tateno et al. (Chapter 6), ""The Japanese lexical transducer based on stem-suffix style forms"" and Kim and Jang (Chapter 7), ""Acquiring rules for reducing morphological ambiguity from POS tagged corpus in Korean.""', 'The latter includes an algorithm for automatically inferring regular grammar rules for morphological relations directly from part-of-speech tagged corpora.']"	0
CCT124	J00-2014	Surface Opacity of Metrical Structure in Optimality Theory	efficient generation in primitive optimality theory	['Jason Eisner']	introduction	"This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT. OTP specifies the class of autosegmental representations, the universal generator Gen, and the two simple families of permissible constraints. In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994). Unfortunately these methods take time exponential on the size of the grammar. Indeed the generation problem is shown NP-complete in this sense. However, techniques are discussed for making Ellison's approach fast in the typical case, including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size. One avenue for future improvements is a new finite-state notion, ""factored automata,"" where regular languages are represented compactly via formal intersections ki=1Ai of FSAs."	"OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question ."	"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"	0
CCT125	J00-2014	Surface Opacity of Metrical Structure in Optimality Theory	comparing a linguistic and a stochastic tagger	"['Christer Samuelsson', 'Atro Voutilainen']"		"Concerning different approaches to automatic PoS tagging: EngCG-2, a constraint-based morphological tagger, is compared in a double-blind test with a state-of-the-art statistical tagger on a common disambiguation task using a common tag set. The experiments show that for the same amount of remaining ambiguity, the error rate of the statistical tagger is one order of magnitude greater than that of the rule-based one. The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed.Comment: 8 pages, LaTeX, 2 postscript figures. E-ACL'9"	"#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences ."	"['Second, weights are an annoyance when writing grammars by hand.', 'In some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences .', ""The tension between preserving the original author's text (faithfulness to the underlying form) and making it readable in various ways (well-formedness) is right up OT's alley."", 'The same applies to document layout: I have often wished I could write OT-style TeX macros~ Third, even in statistical corpus-based NLP, estimating a full Gibbs distribution is not always feasible.', 'Even if strict ranking is not quite accurate, sparse data or the complexity of parameter estimation may make it easier to learn a good OT grammar than a good arbitrary Gibbs model.', ""A well-known example is Yarowsky's (1996) work on word sense disambiguation using decision lists (a kind of OT grammar)."", 'Although decision lists are not very powerful because of their simple output space, they have the characteristic OT property that each generalization partially masks lower-ranked generalizations.']"	0
CCT126	J00-2014	Surface Opacity of Metrical Structure in Optimality Theory	efficient generation in primitive optimality theory	['Jason Eisner']		"This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT. OTP specifies the class of autosegmental representations, the universal generator Gen, and the two simple families of permissible constraints. In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994). Unfortunately these methods take time exponential on the size of the grammar. Indeed the generation problem is shown NP-complete in this sense. However, techniques are discussed for making Ellison's approach fast in the typical case, including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size. One avenue for future improvements is a new finite-state notion, ""factored automata,"" where regular languages are represented compactly via formal intersections ki=1Ai of FSAs."	But typical OT grammars offer much richer finite-state models of left context ( #AUTHOR_TAGa ) than provided by the traditional HMM finite-state topologies .	"['For example, consider the relevance to hidden Markov models (HMMs), another restricted class of Gibbs distributions used in speech recognition or part-of-speech tagging.', ""Just like OT grammars, HMM Viterbi decoders are functions that pick the optimal output from ~', based on criteria of well-formedness (transition probabilities) and faithfulness to the input (emission probabilities)."", 'But typical OT grammars offer much richer finite-state models of left context ( #AUTHOR_TAGa ) than provided by the traditional HMM finite-state topologies .', 'Now, among methods that use a Gibbs distribution to choose among linguistic forms, OT generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs.', 'When is this appropriate?', 'It seems to me that there are three possible uses.']"	0
CCT127	J00-3001	Extracting the Lowest-Frequency Words: Pitfalls and Possibilities	word association norms mutual information and lexicography	"['Kenneth W Church', 'Patrick Hanks']"	introduction	"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words"	"#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five ."	"['Our original question concerned the extent to which recall and precision are influenced by the size of the window.', 'It turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest-frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'It is common practice in information retrieval to discard the lowest-frequency words a priori as nonsignificant (Rijsbergen 1979).', ""In Smadja's collocation algorithm Xtract, the lowest-frequency words are effectively discarded as well (Smadja 1993)."", '#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five .']"	0
CCT128	J00-3001	Extracting the Lowest-Frequency Words: Pitfalls and Possibilities	word association norms mutual information and lexicography	"['Kenneth W Church', 'Patrick Hanks']"	conclusion	"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words"	"While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :"	"[""Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161."", 'The extraction results for both tests as measured by F are 0.31 and 0.33, respectively.', 'This procedure allows us to extract 64/139 = 46.0% of the lowfrequency words and 66/170 -~ 38.8% of the high-frequency words using G 2, and 64/139 = 46.0%', 'and 79/170 = 46.7%,', ""respectively, using Fisher's exact test."", ""Note that this technique is optimal for the extraction of the lowest-frequency words, leading to identical performance for G 2 and Fisher's exact test for these words."", ""For the higherfrequency words, Fisher's exact test leads to a slightly better recall with the same precision scores (0.31 for both tests)."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"	0
CCT129	J00-3001	Extracting the Lowest-Frequency Words: Pitfalls and Possibilities	word association norms mutual information and lexicography	"['Kenneth W Church', 'Patrick Hanks']"		"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words"	"Given the definition of Mutual Information ( #AUTOR_TAG 1990), P(x, y) I(x, y) = log2 p(x)p(y), we consider the distribution of a window word according to the contingency table (a) in Table 4."	"['Given the definition of Mutual Information ( #AUTOR_TAG 1990), P(x, y) I(x, y) = log2 p(x)p(y), we consider the distribution of a window word according to the contingency table (a) in Table 4.', 'P(x) is the relative frequency of the target word, P(y) is the relative frequency of the seed term, and P(x, y) is the frequency of the target word in the window.', 'In terms of the contingency table, we have: nu n++ I(x, y) = log2 n1+ s where S n++ n++ —u, we find that is the frequency of the seed.', 'Substituting nn = nl+ - n12, we find that /11+ -- F/12 I(x,y) = log 2 n++ nl+ S \' //++ //++ 1 = log 2 n++ //1+ S \' n++(nl+ -- nu) "" n++ = log2(n++) - log2(S) - log2(nl+) + log2(nl+ - n12).']"	5
CCT130	J00-3006	Language Communication	chinese numbernames tree adjoining languages and mild contextsensitivity	['Daniel Radzinsky']			"For example , #AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time ."	"['Second, the complexity of a grammar class is measured by the worst case: a grammar class has a complexity x if there exists some grammar in this class such that there exists an infinite series of long-enough sentences that parse in time x by this grammar.', 'However, what matters in engineering practice is the average case for a specific grammar.', 'Specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time-consuming behavior of the algorithm never happens for this grammar.', 'Average, since it can happen that the grammar does admit hard-toparse sentences that are not used (or at least not frequently used) in the real corpus.', 'For example , #AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time .', 'Do such arguments--no doubt important for mathematical linguistics--have any direct consequences for an engineering linguistics?', 'Even if a Chinese grammar includes a non-context-flee rule for parsing such numerals, how frequently will it be activated?', 'Does it imply impossibility of processing real Chinese texts in reasonable time?', 'Clearly, the average time for a specific grammar cannot be calculated in such a mathematically elegant way as the worst-case complexity of a grammar class; for the time being, the only practical way to compare the complexity of natural language processing formalisms is the hard one--building real-world systems and comparing their efficiency and coverage.']"	0
CCT131	J00-4001	Automatic Text Categorization in Terms of Genre and Author	robust text processing in automated information retrieval	['Tornek Strzalkowski']	method	"This paper outlines a prototype text retrieval system which uses relatively advanced natural language processing techniques in order to enhance the effectiveness of statistical document retrieval. The backbone of our system is a traditional retrieval engine which builds inverted index files from pre-processed docu- ments, and then searches and ranks the documents in response to user queries. Natural language processing is used to (1) preprocess the documents in order to extract contents-carrying terms, (2) discover interterm dependencies and build a conceptual hierarchy specific to the database domain, and (3) process user&apos;s natural language requests into effective search queries. The basic assumption of this design is that term-based representation of contents is in principle sufficient to build an effective if not optimal search query out of any users request. This has been confirmed by an experiment that compared effectiveness of expert-user prepared queries with those derived automatically from an initial narrative information request. In this paper we show that largescale natural language processing (hundreds of millions of words and more) is not only required for a better retrieval, but it is also doable, given appropriate resources. We report on selected preliminary restfits of experiments with 500 MByte database of Wall Street Journal articles, as well as some earlier restfits with a smaller document collection"	"For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) ."	"['The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'However, the SCBD is a general-purpose tool and was not designed for providing stylistic information exclusively.', 'Thus, any NLP tool (e.g., part-of-speech taggers, parsers, etc.) can provide similar measures.', 'The appropriate analysis-level style markers have to be defined according to the methodology used by the tool in order to analyze the text.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .', 'This parser produces trees to represent the structure of the sentences that compose the text.', 'However, it is set to ""skip"" or surrender attempts to parse clauses after reaching a time-out threshold.', 'When the parser skips, it notes that in the parse tree.', 'The measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis-level style markers.']"	0
CCT132	J00-4001	Automatic Text Categorization in Terms of Genre and Author	using registerdiversified corpora for general language studies	['Douglas Biber']		"The present study summarizes corpus-based research on linguistic characteristics from several different structural levels, in English as well as other languages, showing that register variation is inherent in natural language. It further argues that, due to the importance and systematicity of the linguistic differences among registers, diversified corpora representing a broad range of register variation are required as the basis for general language studies.First, the extent of cross-register differences are illustrated from consideration of individual grammatical and lexical features; these register differences are also important for probabilistic part-of-speech taggers and syntactic parsers, because the probabilities associated with grammatically ambiguous forms are often markedly different across registers. Then, corpus-based multidimensional analyses of English are summarized, showing that linguistic features from several structural levels function together as underlying dimensions of variation, with each dimension defining a different set of linguistic relations among registers. Finally, the paper discusses how such analyses, based on register-diversified corpora, can be used to address two current issues in computational linguistics: the automatic classification of texts into register categories and cross-linguistic comparisons of register variation."	Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .	"['where Cx is the covariance matrix of x.', 'Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']"	0
CCT133	J06-2003	Building and Using a Lexical Knowledge Base of Near-Synonym Differences	unsupervised word sense disambiguation rivaling supervised methods	['David Yarowsky']		"This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%"	The algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation .	"['The algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation .', 'He classified the senses of a word on the basis of other words that the given word co-occurs with.', 'Collins and Singer (1999)']"	4
CCT134	J06-3002	The Notion of Argument in Prepositional Phrase Attachment	corpus based pp attachment ambiguity resolution with a semantic dictionary	"['Jiri Stetina', 'Makoto Nagao']"	method	"This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity. We propose a new supervised learning method for PPattachment based on a semantically tagged corpus. Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags. We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods."	"The changes made were inspired by those described in #AUTHOR_TAG , page 75 ) ."	"['The automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files.', 'Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet.', 'The changes made were inspired by those described in #AUTHOR_TAG , page 75 ) .', 'To lemmatize the words we used �morpha,� a lemmatizer developed by John A. Carroll and freely available at the address: http://www.informatics.susx.ac.uk./research/nlp/carroll/morph.html.', 'Upon simple observation, it showed a better performance than the frequently used Porter Stemmer for this task.']"	4
CCT135	J11-1005	Syntactic Processing Using the Generalized Perceptron and Beam Search	joint word segmentation and pos tagging using a single perceptron	"['Yue Zhang', 'Stephen Clark']"	introduction	"For Chinese POS tagging, word segmentation is a preliminary step. To avoid error propagation and improve segmentation by utilizing POS information, segmentation and tagging can be performed simultaneously. A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard. Recent research has explored the integration of segmentation and POS tagging, by decoding under restricted versions of the full combined search space. In this paper, we propose a joint segmentation and POS tagging model that does not impose any hard constraints on the interaction between word and POS information. Fast decoding is achieved by using a novel multiple-beam search algorithm. The system uses a discriminative statistical model, trained using the generalized perceptron algorithm. The joint model gives an error reduction in segmentation accuracy of 14.6% and an error reduction in tagging accuracy of 12.2%, compared to the traditional pipeline approach."	"For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAGa ) , while being more than an order of magnitude faster ."	"['In Section 2 we describe our general framework of the generic beam-search algorithm and the generalized perceptron.', 'Then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, presented in our single coherent framework.', 'We give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'For the segmentation task, we also compare our beam-search framework with alternative decoding algorithms including an exact dynamic-programming method, showing that the beam-search method is significantly faster with comparable accuracy.', 'For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAGa ) , while being more than an order of magnitude faster .']"	1
CCT136	J12-4003	Semantic Role Labeling of Implicit Arguments for Nominal Predicates	beyond nombank a study of implicit arguments for nominal predicates	"['Matthew Gerber', 'Joyce Chai']"	conclusion	"Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task."	"Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests ."	"['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section.']"	2
CCT137	J12-4003	Semantic Role Labeling of Implicit Arguments for Nominal Predicates	beyond nombank a study of implicit arguments for nominal predicates	"['Matthew Gerber', 'Joyce Chai']"	experiments	"Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task."	"This evaluation set-up is an improvement versus the one we previously reported ( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing ."	"['In order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances.', 'Following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'This evaluation set-up is an improvement versus the one we previously reported ( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing .']"	2
CCT138	J15-3005	Discriminative Syntax-Based Word Ordering for Text Generation	ccgbank a corpus of ccg derivations and dependency structures extracted from the penn treebank	"['Julia Hockenmaier', 'Mark Steedman']"	introduction	"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word-word dependencies. The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium, and has been used to train wide-coverage statistical parsers that obtain state-of-the-art rates of dependency recovery. In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary. We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design of future treebanks."	CCGBank ( #AUTHOR_TAG ) is used to train the model .	"['We now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'CCGBank ( #AUTHOR_TAG ) is used to train the model .', 'For each training sentence, the corresponding CCGBank derivation together with all its sub-derivations are treated as gold-standard hypotheses.', 'All other hypotheses that can be constructed from the same bag of words are non-gold hypotheses.', 'From the generation perspective this assumption is too strong, because sentences can have multiple orderings (with multiple derivations) that are both gram- matical and fluent.', 'Nevertheless, it is the most feasible choice given the training data available.']"	5
CCT139	J15-3005	Discriminative Syntax-Based Word Ordering for Text Generation	syntactic processing using the generalized perceptron and beam search	"['Yue Zhang', 'Stephen Clark']"	introduction	"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model, trained by the generalized perceptron together with a generic beam-search decoder. We apply the framework to word segmentation, joint segmentation and POS-tagging, dependency parsing, and phrase-structure parsing. Both components of the framework are conceptually and computationally very simple. The beam-search decoder only requires the syntactic processing task to be broken into a sequence of decisions, such that, at each stage in the process, the decoder is able to consider the top-n candidates and generate all possibilities for the next stage. Once the decoder has been defined, it is applied to the training data, using trivial updates according to the generalized perceptron to induce a model. This simple framework performs surprisingly well, giving accuracy results competitive with the state-of-the-art on all the tasks we consider. The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives, including log-linear and large-margin training algorithms and dynamic-programming for decoding. Moreover, the framework offers the freedom to define arbitrary features which can make alternative training and decoding algorithms prohibitively slow. We discuss how the general framework is applied to each of the problems studied in this article, making comparisons with alternative learning and decoding algorithms. We also show how the comparability of candidates considered by the beam is an important factor in the performance. We argue that the conceptual and computational simplicity of the framework, together with its language-independent nature, make it a competitive choice for a range of syntactic processing tasks and one that should be considered for comparison by developers of alternative approaches."	"In our previous papers ( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010 ) ."	"['In our formulation of the word ordering problem, a hypothesis is a phrase or sentence together with its CCG derivation.', 'Hypotheses are constructed bottom-up: starting from single words, smaller phrases are combined into larger ones according to CCG rules.', 'To allow the combination of hypotheses, we use an additional structure to store a set of hypotheses that have been expanded, which we call accepted hypotheses.', 'When a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses.', 'The data structure for accepted hypotheses is similar to that used for best-first parsing (Caraballo and Charniak 1998), and we adopt the term chart for this structure.', 'However, note there are important differences to the parsing problem.', 'First, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling.', 'Second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem.', 'In our previous papers ( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010 ) .', 'However, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'We will also investigate the possibility of applying dynamic-programming-style pruning to the chart.']"	1
CCT140	K15-1001	A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation	minimum error rate training in statistical machine translation	['Franz Josef Och']	experiments	"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT."	This was done by MERT optimization ( #AUTHOR_TAG ) towards post-edits under the TER target metric .	"['this experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'We select feedback of varying grade by directly inspecting the optimal w * , thus this feedback is idealized.', 'However, the experiment also has a realistic background since we show that α-informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized TER, and that learning from weak and strong feedback leads to convergence in TER on test data.', 'For this experiment, the post-edit data from the LIG corpus were randomly split into 3 subsets: PE-train (6,881 sentences), PE-dev, and PE-test (2,000 sentences each).', 'PE-train was used for our online learning experiments.', ""PE-test was held out for testing the algorithms' progress on unseen data."", 'PE-dev was used to obtain w * to define the utility model.', 'This was done by MERT optimization ( #AUTHOR_TAG ) towards post-edits under the TER target metric .', 'Note that the goal of our experi-  ments is not to improve SMT performance over any algorithm that has access to full information to compute w * .', 'Rather, we want to show that learning from weak feedback leads to convergence in regret with respect to the optimal model, albeit at a slower rate than learning from strong feedback.', 'The feedback data in this experiment were generated by searching the n-best list for translations that are α-informative at α ∈ {0.1, 0.5, 1.0} (with possible non-zero slack).', 'This is achieved by scanning the n-best list output for every input x t and returning the firstȳ t = y t that satisfies Equation (2). 5 This setting can be thought of as an idealized scenario where a user picks translations from the n-best list that are considered improvements under the optimal w * .']"	5
CCT141	K15-1001	A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation	discriminative training methods for hidden markov models theory and experiments with perceptron algorithms	['Michael Collins']		"We describe new algorithms for train-ing tagging models, as an alternative to maximum-entropy models or condi-tional random elds (CRFs). The al-gorithms rely on Viterbi decoding of training examples, combined with sim-ple additive updates. We describe the-ory justifying the algorithms through a modication of the proof of conver-gence of the perceptron algorithm for classi cation problems. We give exper-imental results on part-of-speech tag-ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger."	"In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ) ."	"['Generalization for Online-to-Batch Conversion.', 'In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ) .', 'The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector w T,K whose expected loss on unseen data we would like to bound.', 'We assume that the algorithm is fed with a sequence of examples x 1 , . . .', ', x T , and at each epoch k = 1, . . .', ', K it makes a prediction y t,k .', 'The correct label is y * t .', 'For k = 1, . . .', ', K and t = 1, . . .', ', T , let t,k = U (x t , y * t ) − U (x t , y t,k ), and denote by ∆ t,k and ξ t,k the distance at epoch k for example t, and the slack at epoch k for example t, respectively.', 'Finally, we denote by D T,K = T t=1 ∆ 2 t,K , and by w T,K the final weight vector returned after K epochs.', 'We state a condition of convergence :']"	1
CCT142	K15-1001	A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation	moses open source toolkit for statistical machine translation	"['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Birch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen']"	experiments	"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks."	"To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase-based decoder ( #AUTHOR_TAG ) with dense features ."	"['We used the LIG corpus 3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012).', 'The corpus is a subset of the newscommentary dataset provided at WMT 4 and contains input French sentences, MT outputs, postedited outputs and English references.', 'To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase-based decoder ( #AUTHOR_TAG ) with dense features .', 'We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en', 'data (48.65M', 'sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010).', 'perceptron cycling theorem (Block and Levin, 1970;Gelfand et al., 2010) should suffice to show a similar bound.', 'Parallel data (europarl+news-comm, 1.64M sentences) were similarly pre-processed and aligned with fast align (Dyer et al., 2013).', 'In all experiments, training is started with the Moses default weights.', 'The size of the n-best list, where used, was set to 1,000.', 'Irrespective of the use of re-scaling in perceptron training, a constant learning rate of −5 was used for learning from simulated feedback, and 10 −4 for learning from surrogate translations.']"	5
CCT143	N01-1001	Instance-Based Natural Language Generation	forestbased statistical sentence generation	['Irene Langkilde']		"This paper presents a new approach to statistical sentence generation in which alternative phrases are represented as packed sets of trees, or forests, and then ranked statistically to choose the best one. This representation offers advantages in compactness and in the ability to represent syntactic information. It also facilitates more efficient statistical ranking than a previous approach to statistical generation. An efficient ranking algorithm is described, together with experimental results showing significant improvements over simple enumeration or a lattice-based approach."	"In contrast , a single statistical model allows one to maintain a single table ( #AUTHOR_TAG ) ."	"['In contrast , a single statistical model allows one to maintain a single table ( #AUTHOR_TAG ) .']"	0
CCT144	N01-1001	Instance-Based Natural Language Generation	chart generation	['Martin Kay']	experiments	"This paper presents a compilation procedure which determines internal and external indices for signs in a unification based grammar to be used in improving the computational efficiency of lexicalist chart generation. The procedure takes as input a grammar and a set of feature paths indicating the position of semantic indices in a sign, and calculates the fixed-point of a set of equations derived from the grammar. The result is a set of independent constraints stating which indices in a sign can be bound to other signs within a complete sentence. Based on these constraints, two tests are formulated which reduce the search space during generation.Comment: 8 pages, Latex; to appear in 7th International Conference on   Theoretical and Methodological Issues in Machine Translation (TMI-97"	IGEN uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates .	['IGEN uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates .']	0
CCT145	N01-1002	Corpus-based NP Modifier Generation	can nominal expressions achieve multiple goals an empirical study	['P Jordan']		"While previous work suggests that multiple goals can be addressed by a nominal expression, there is no systematic work describing what goals in addition to identification might be relevant and how speakers can use nominal expressions to achieve them. In this paper, we first hypothesize a number of communicative goals that could be addressed by nominal expressions in task-oriented dialogues. We then describe the intentional influences model for nominal expression generation that attempts to simultaneously address the identification goal and these additional goals with a single nominal expression. Our evaluation results show that the intentional influences model fits the nominal expressions in the COCONUT corpus as well as previous accounts that focus solely on the identification goal."	"Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain ( #AUTHOR_TAG ) ."	"[""Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain ( #AUTHOR_TAG ) .""]"	0
CCT146	N01-1002	Corpus-based NP Modifier Generation	capturing the interaction between aggregation and text planning in two generation systems	"['H Cheng', 'C Mellish']"	introduction	"In natural language generation, different generation tasks often interact with each other in a complex way. We think that how to resolve the complex interactions inside and between tasks is more important to the generation of a coherent text than how to model each individual factor. This paper focuses on the interaction between aggregation and text planning, and tries to explore what preferences exist among the features considered by the two tasks. The preferences are implemented in two generation systems, namely ILEX-TS and a text planner using a Genetic Algorithm. The evaluation emphasises the second implementation and shows that capturing these preferences properly can lead to coherent text."	"It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAGa ) ."	"['It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAGa ) .']"	0
CCT147	N01-1002	Corpus-based NP Modifier Generation	capturing the interaction between aggregation and text planning in two generation systems	"['H Cheng', 'C Mellish']"		"In natural language generation, different generation tasks often interact with each other in a complex way. We think that how to resolve the complex interactions inside and between tasks is more important to the generation of a coherent text than how to model each individual factor. This paper focuses on the interaction between aggregation and text planning, and tries to explore what preferences exist among the features considered by the two tasks. The preferences are implemented in two generation systems, namely ILEX-TS and a text planner using a Genetic Algorithm. The evaluation emphasises the second implementation and shows that capturing these preferences properly can lead to coherent text."	"Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in ( #AUTHOR_TAGb ) ."	"['Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in ( #AUTHOR_TAGb ) .']"	0
CCT148	N01-1002	Corpus-based NP Modifier Generation	can nominal expressions achieve multiple goals an empirical study	['P Jordan']	introduction	"While previous work suggests that multiple goals can be addressed by a nominal expression, there is no systematic work describing what goals in addition to identification might be relevant and how speakers can use nominal expressions to achieve them. In this paper, we first hypothesize a number of communicative goals that could be addressed by nominal expressions in task-oriented dialogues. We then describe the intentional influences model for nominal expression generation that attempts to simultaneously address the identification goal and these additional goals with a single nominal expression. Our evaluation results show that the intentional influences model fits the nominal expressions in the COCONUT corpus as well as previous accounts that focus solely on the identification goal."	"In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG ) ."	"[""In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG ) .""]"	0
CCT149	N01-1009	A corpus-based account of regular polysemy	the generative lexicon	['James Pustejovsky']	method	"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole."	Table 1 gives the interpretations of eight adjective-noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ) .	"['In what follows we explain the properties of the model by applying it to a small number of adjective-noun combinations taken from the lexical semantics literature.', 'Table 1 gives the interpretations of eight adjective-noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ) .', 'Table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections (v is the most likely interpretation, v 2 is the second most likely interpretation, etc.).']"	5
CCT150	N01-1009	A corpus-based account of regular polysemy	the generative lexicon	['James Pustejovsky']	introduction	"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole."	"Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see #AUTHOR_TAG and the references therein ) ."	"['Much recent work in lexical semantics has been concerned with accounting for regular polysemy, i.e., the regular and predictable sense alternations certain classes of words are subject to.', 'Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see #AUTHOR_TAG and the references therein ) .']"	0
CCT151	N01-1009	A corpus-based account of regular polysemy	the generative lexicon	['James Pustejovsky']	experiments	"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole."	"We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ) ."	"['We chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the BNC.', 'We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ) .', 'From these we randomly sampled nine adjectives (difficult, easy, fast, good, hard, right, safe, slow, wrong).', 'These adjectives had to be unambiguous with respect to their part-of-speech: each adjective was unambiguously tagged as ""adjective"" 98.6% of the time, measured as the number of different part-of-speech tags assigned to the word in the BNC.', 'We identified adjective-noun pairs using Gsearch (Corley et al., 2000), a chart parser which detects syntactic patterns in a tagged corpus by exploiting a userspecified context free grammar and a syntactic query.', 'Gsearch was run on a lemmatized version of the BNC so as to compile a comprehensive corpus count of all nouns occurring in a modifier-head relationship with each of the nine adjectives.', 'From the syntactic analysis provided by  We used the model outlined in Section 2 to derive meanings for the 90 adjective-noun combinations.', 'We employed no threshold on the frequencies f (a, v) and f (rel, v, n).', 'In order to obtain the frequency f (a, v) the adjective was mapped to its corresponding adverb.', 'In particular, good was mapped to good and well, fast to fast, easy to easily, hard to hard, right to rightly and right, safe to safely and safe, slow to slowly and slow and wrong to wrongly and wrong.', 'The adverbial function of the adjective difficult is expressed only periphrastically (i.e., in a difficult manner, with difficulty).', 'As a result, the frequency f (difficult, v) was estimated only on the basis of infinitival constructions (see ( 17)).', 'We estimated the probability P(a, n, v, rel) for each adjective-noun pair by varying both the terms v and rel.']"	5
CCT152	N01-1009	A corpus-based account of regular polysemy	the generative lexicon	['James Pustejovsky']	introduction	"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole."	#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .	"['(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', 'The meaning of adjective-noun combinations like those in (1) and ( 2) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.', 'For example, an easy problem is ""a problem that is easy to solve"" or ""a problem that one can solve easily"".', 'In order to account for the meaning of these combinations Vendler (1968, 92) points out that ""in most cases not one verb, but a family of verbs is needed"".', 'Vendler further observes that the noun figuring in an adjective-noun combination is usually the subject or object of the paraphrasing verb.', 'Although fast usually triggers a verb-subject interpretation (see (1)), easy and difficult trigger verb-object interpretations (see (2a,b)).', 'An easy problem is usually a problem that is easy to solve, whereas a difficult language is a language that is difficult to learn, speak, or write.', 'Adjectives like good allow either verb-subject or verb-object interpretations: a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', 'Pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'For example, the telic (purpose) role of the qualia structure for problem has a value equivalent to solve.', 'When the adjective easy is combined with problem, it predicates over the telic role of problem and consequently the adjective-noun combination receives the interpretation a problem that is easy to solve.', 'Pustejovsky (1995) does not give an exhaustive list of the telic roles a given noun may have.', 'Furthermore, in cases where more than one interpretations are provided (see Vendler (1968)), no information is given with respect to the likelihood of these interpretations.', 'Outof context, the number of interpretations for fast scientist is virtually unlimited, yet some interpretations are more likely than others: fast scientist is more likely to be a scientist who performs experiments quickly or who publishes quickly than a scientist who draws or drinks quickly.']"	0
CCT153	N01-1011	A Decision Tree of Bigrams is an Accurate Predictor of Word Sense	a simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation	['T Pedersen']	conclusion	"This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results."	"We have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line ."	"['We have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .']"	0
CCT154	N01-1012	An Algorithm for Aspects of Semantic Interpretation Using an Enhanced WordNet	linking wordnet verb classes to semantic interpretation	['F Gomez']		An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented. The selectional restrictions for the thematic roles defining the predicates are WordNet ontological categories. Thematic roles are also linked to the syntactic relations that realize them. The paper illustrates the methodology by providing a detailed analysis of some major WordNet verb classes.	See ( #AUTHOR_TAG ) for a discussion .	['See ( #AUTHOR_TAG ) for a discussion .']	0
CCT155	N01-1012	An Algorithm for Aspects of Semantic Interpretation Using an Enhanced WordNet	linking wordnet verb classes to semantic interpretation	['F Gomez']		An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented. The selectional restrictions for the thematic roles defining the predicates are WordNet ontological categories. Thematic roles are also linked to the syntactic relations that realize them. The paper illustrates the methodology by providing a detailed analysis of some major WordNet verb classes.	Other definitions of predicates may be found in ( #AUTHOR_TAG ) .	['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']	0
CCT156	N01-1013	Class-Based Probability Estimation Using a Semantic Hierarchy	generalizing case frames using a thesaurus and the mdl principle	"['H Li', 'N Abe']"		"A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as ""cuts"" in the thesaurus tree, thus reducing the generalization problem to that of estimating a ""tree cut model"" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods."	"This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG ."	"['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']"	1
CCT157	N01-1013	Class-Based Probability Estimation Using a Semantic Hierarchy	distributional clustering of english words	"['F Pereira', 'N Tishby', 'L Lee']"	experiments	"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."	The task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 ) .	['The task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 ) .']	1
CCT158	N01-1013	Class-Based Probability Estimation Using a Semantic Hierarchy	prepositional phrase attachment through a backedoff model	"['M Collins', 'J Brooks']"		"Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events -- ignoring events which occur less than 5 times in training data reduces performance to 81.6%."	"The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( #AUTHOR_TAG ) ."	"['The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( #AUTHOR_TAG ) .']"	4
CCT159	N01-1013	Class-Based Probability Estimation Using a Semantic Hierarchy	accurate methods for the statistics of surprise and coincidence	['T Dunning']	Motivation	"Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text."	"However , #AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP ."	"['However , #AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP .']"	4
CCT160	N01-1013	Class-Based Probability Estimation Using a Semantic Hierarchy	inducing a semantically annotated lexicon via embased clustering	"['M Rooth', 'S Riezler', 'D Prescher', 'G Carroll', 'F Beil']"	experiments	"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries."	The task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG .	['The task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG .']	1
CCT161	N01-1013	Class-Based Probability Estimation Using a Semantic Hierarchy	accurate methods for the statistics of surprise and coincidence	['T Dunning']	experiments	"Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text."	"The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP ."	"['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']"	1
CCT162	N04-2004	A computational framework for non-lexicalist semantics	derivational minimalism	['Edward Stabler']		"International audienceMinimalist grammars (MGs) constitute a mildly context-sensitive formalism when being equipped with a particular locality condition (LC), the shortest move condition. In this format MGs define the same class of derivable string languages as multiple context-free grammars (MCFGs). Adding another LC to MGs, the specifier island condition (SPIC), results in a proper subclass of derivable languages. It is rather straightforward to see this class is embedded within the class of languages derivable by some well-nested MCFG (MCFG wn ). In this paper we show that the embedding is even proper. We partially do so adapting the methods used in [13] to characterize the separation of MCFG wn -languages from MCFG-languages by means of a ""simple copying"" theorem. The separation of strict derivational minimalism from well-nested MCFGs is then characterized by means of a ""simple reverse copying"" theorem. Since for MGs, well-nestedness seems to be a rather ad hoc restriction, whereas for MCFGs, this holds regarding the SPIC, our result may suggest we are concerned here with a structural difference between MGs and MCFGs which cannot immediately be overcome in a non-stipulated manner"	"The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 ) ."	"['My theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 ) .""]"	1
CCT163	N04-2004	A computational framework for non-lexicalist semantics	on argument structure and the lexical expression of syntactic relations	"['Kenneth Hale', 'Samuel Jay Keyser']"			"These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically ."	"['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation.']"	0
CCT164	N04-2004	A computational framework for non-lexicalist semantics	the event argument and the semantics of voice	['Angelika Kratzer']			"It projects a functional head , voice ( #AUTHOR_TAG ) , whose specifier is the external argument ."	"['The light verb v DO licenses an atelic non-inchoative event, and is compatible with verbal roots expressing activity.', 'It projects a functional head , voice ( #AUTHOR_TAG ) , whose specifier is the external argument .', 'Lexical entries in the system are minimally specified, each consisting of a phonetic form, a list of relevant features, and semantics in the form of a λ expression.']"	0
CCT165	N04-2004	A computational framework for non-lexicalist semantics	a recognizer for minimalist grammars	['Henk Harkema']		"Minimalist Grammars are a rigorous formalization of the sort of grammars proposed in the linguistic framework of Chomsky's Minimalist Program. One notable property of Minimalist Grammars is that they allow constituents to move during the derivation of a sentence, thus creating discontinuous constituents. In this paper we will present a bottom-up parsing method for Minimalist Grammars, prove its correctness, and discuss its complexity."	"The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 ) ."	"['My theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 ) .""]"	1
CCT166	N04-2004	A computational framework for non-lexicalist semantics	distributed morphology and the pieces of inflection	"['Morris Halle', 'Alec Marantz']"	introduction	"The last few years have seen the emergence of several clearly articulated alternative approaches to morphology. One such approach rests on the notion that only stems of the so-called lexical categories (N, V, A) are morpheme &quot;pieces &quot; in the traditional sense--connections between (bun-dles of) meaning (features) and (bundles of) sound (features). What look like affixes on this view are merely the by-product of morphophonological rules called word formation rules (WFRs) that are sensitive to features associated with the lexical categories, called lexemes. Such an a-morphous or affixless theory, adumbrated by Beard (1966) and Aronoff (1976), has been articulated most notably by Anderson (1992) and in major ne"	"In this paper , I present a computational implementation of Distributed Morphology ( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation ."	"['In this paper , I present a computational implementation of Distributed Morphology ( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .', 'This framework leads to finer-grained semantics capable of better capturing linguistic generalizations.']"	5
CCT167	N04-2004	A computational framework for non-lexicalist semantics	adding semantic annotation to the penn treebank	"['Paul Kingsbury', 'Martha Palmer', 'Mitch Marcus']"	introduction	"This paper presents our basic approach to creating Proposition Bank, which involves adding a layer of semantic annotation to the Penn English TreeBank. Without attempting to confirm or disconfirm any particular semantic theory, our goal is to provide consistent argument labeling that will facilitate the automatic extraction of relational data. An argument such asthe window in John broke the window and in The window brokewould receive the same label in both sentences. In order to ensure reliable human annotation, we provide our annotators with explicit guidelines for labeling all of the syntactic and semantic frames of each particular verb. We give several examples of these guidelines and discuss the inter-annotator agreement figures. We also discuss our current experiments on the automatic expansion of our verb guidelines based on verb class membership. Our current rate of progress and our consistency of annotation demonstrate the feasibility of the task."	"This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( Baker et al. , 1998 ) and PropBank ( #AUTHOR_TAG ) ."	"['A common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots).', 'Verbs are viewed as simple predicates over their arguments.', ""This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( Baker et al. , 1998 ) and PropBank ( #AUTHOR_TAG ) .""]"	0
CCT168	N04-2004	A computational framework for non-lexicalist semantics	event structure and the encoding of arguments the syntax of the english and mandarin verb phrase	['Jimmy Lin']		"This work presents a theory of linguistic representation that attempts to capture the syntactic structure of verbs and their arguments. My framework is based on the assumption that the proper representation of argument structure is event structure. Furthermore, I develop the hypothesis that event structure is syntactic structure, and argue that verb meanings are compositionally derived in the syntax from verbalizing heads, functional elements that license eventive interpretations, and verbal roots, abstract concepts drawn from encyclopedic knowledge. The overall goal of the enterprise is to develop a theory that is able to transparently relate the structure and meaning of verbal arguments. By hypothesis, languages share the same inventory of primitive building blocks and are governed by the same set of constraints--all endowed by principles of Universal Grammar and subjected to parametric variations. Support for my theory is drawn from both Mandarin Chinese and English. In particular, the organization of the Mandarin verbal system provides strong evidence for the claim that activity and state are the only two primitive verb types in Chinese-- achievements and accomplishments are syntactically-derived complex categories. As a specific instance of complex event composition, I examine Mandarin resultative verb compounds and demonstrate that a broad range of variations can be perspicuously captured in my framework. I show that patterns of argument sharing in these verbal compounds can be analyzed as control, thus grounding argument structure in wellknown syntactic constraints such as the Minimum Distance Principle. Finally, I argue that cross-linguistic differences in the realization of verbal arguments can be reduced to variations in the way functional elements interact with verbal roots. Overall, my work not only contributes to our understanding of how events are syntactically represented, but also explicates interactions at the syntax-semantics interface, clarifying the relationship between surface form, syntactic structure, and logical form. A theory of argument structure grounded in independently-motivated syntactic constraints, on the one hand, and the semantic structure of events, on the other hand, is able to account for a wide range of empirical facts with few stipulations. Thesis Supervisor: Boris Katz Title: Principal Research Scientist"	"In ( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track ."	"['In ( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track .', 'The rest of this paper, however, will be concerned with the computational implementation of my theoretical framework.']"	2
CCT169	N04-2004	A computational framework for non-lexicalist semantics	immediate head parsing for language models	['Eugene Charniak']	introduction	"We present two language models based upon an  immediate-head&quot; parser | our name for a parser that conditions all events below a constituent  c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models signicantly improve upon the trigram model base-line as well as the best previous grammar based language model. For the better of our two models these improvements are 24% and 13% respectively. We also suggest that improvement of the underlying parser should signicantly improve the model&apos;s perplexity and that even in the near term there is a lot of porential for improvement in immediate-head language models.  1 Introduction  All of the most accurate statistical parsers [2,4, 6,7,10,12] are lexicalized in the sense that they condition probabilities on the lexical content of the sentences being parsed. Furthermore, all of these parsers are wh.."	"Due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences ."	"['The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .']"	0
CCT170	N04-2004	A computational framework for non-lexicalist semantics	the generative lexicon	['James Pustejovsky']		"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole."	"There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAGb ; Rappaport Hovav and Levin , 1998 ) ."	"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAGb ; Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example:']"	0
CCT171	N04-2004	A computational framework for non-lexicalist semantics	on argument structure and the lexical expression of syntactic relations	"['Kenneth Hale', 'Samuel Jay Keyser']"			"With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis ."	"['The currently implemented system is still at the ""toy parser"" stage.', 'Although the effectiveness and coverage  of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis .', 'I believe that with a suitable lexicon (either hand crafted or automatically induced), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles.']"	0
CCT172	N04-2004	A computational framework for non-lexicalist semantics	grammaticalizing aspect and affectedness	['Carol Tenny']		"Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Linguistics and Philosophy, 1987."	"#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity ."	"[""Activities know run believe walk Accomplishments Achievements paint a picture recognize make a chair find Under Vendler's classification, activities and states both depict situations that are inherently temporally unbounded (atelic); states denote static situations, whereas activities denote on-going dynamic situations."", 'Accomplishments and achievements both express a change of state, and hence are temporally bounded (telic); achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity .']"	0
CCT173	N04-2004	A computational framework for non-lexicalist semantics	english verb classes and alternations a preliminary investigation	['Beth Levin']		"In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, ""English Verb Classes and Alternations"" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University."	"With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis ."	"['The currently implemented system is still at the ""toy parser"" stage.', 'Although the effectiveness and coverage  of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis .', 'I believe that with a suitable lexicon (either hand crafted or automatically induced), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles.']"	0
CCT174	N04-2004	A computational framework for non-lexicalist semantics	the generative lexicon	['James Pustejovsky']		"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole."	"This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by #AUTHOR_TAGa ) , among many others ."	"['Considering that the only difference between flat.ADJ and flatten.V is the suffix -en, it must be the source of inchoativity and contribute the change of state reading that distinguishes the verb from the adjective.', 'Here, we have evidence that derivational affixes affect the semantic representation of lexical items, that is, fragments of event structure are directly associated with derivational morphemes.', 'We have the following situation: In this case, the complete event structure of a word can be compositionally derived from its component morphemes.', ""This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by #AUTHOR_TAGa ) , among many others ."", 'Note that such an approach is no longer lexicalist: each lexical item does not fully encode its associated syntactic and semantic structures.', 'Rather, meanings are composed from component morphemes.']"	0
CCT175	N04-2004	A computational framework for non-lexicalist semantics	a minimalist implementation of verb subcategorization	['Sourabh Niyogi']		"Traditional accounts of verb subcategorization, from the classic work of Fillmore on, require either a considerable number of syntactic rules to account for diverse sentence constructions, including crosslanguage variation, or else complex linking rules mapping the thematic roles of semantic event templates with possible syntactic forms. In this paper we exhibit a third approach: we implement, via an explicit parser and lexicon, the incorporation theory of Hale and Keyser (1993, 1998) to systematically cover most patterns in English Verb Classes and Alternations (Levin 1993), typically using only 1 or 2 lexical entries per verb to subsume a large number of syntactic constructions and also most information typically contained in semantic event templates, and, further, replacing the notion of ""thematic roles"" with precise structural configurations. The implemented parser uses the merge and move operations formalized by Stabler (1997) in the minimalist framework of Chomsky (2001). As a side benefit, we extend the minimalist recognizer of Harkema (2000) to a full parsing implementation. We summarize the current compactness and coverage of our account and provide this minimalist lexicon and parser online at http://web.mit.edu/niyogi/www/minimal.htm 1 The Problem of Verb Subcategorization Why do certain verbs undergo particular certain alternations and not others? On some accounts, e.g. Levin (1993), referred to hereafter as EVCA, alternations provide insight into verb subcategorization and hence hooks to parsing, cross-language variation, machine translation, and class based verb learning. However, fully implemented accounts of the phenomena remains an open problem, with at least three alternative models, shown in Figure 1. Accounts may be solely descriptive - for example, classifying verbs as having an intransitive, a transitive, and/or ditransitive form, as is familiar. Traditional computational accounts (see 1) map these forms into individual grammar rules, (perhaps by macro expansion-like techniques) adding as many rules as necessary to account for naturally' occurring constructions (wh-movement, passive forms, etc.) For each grammatical rule, a separate semantic decomposition is required, typically labeling component phrases with one of several ""thematic roles."" A richer account provided by lexical semantics (see 2), exemplified in Jackendoff (1983, 1990) and Rappaport Hovav and Levin (1998), is one that hypothesizes semantic templates, but requires linking rules mapping syntactic frames with semantic templates governed by a particular verb. Often these semantic templates are constructed in an ad hoc manner, and the corresponding linking rules are consquently a collection of difficult-toimplement heuristics. In this paper we implement a rather different formalism (Hale and Keyser's Incorporation theory, see 3), wherein fewer lexical entries govern syntactic and semantic behavior, with no appeal to thematic roles or complex linking rules. 0. Verb Subcategorization Phenomena * Bob put. Butter was put on the bread. * Bob put butter. What was put on the bread? Bob put butter on the bread. Where was the butter put? 1. Traditional Account VP - V0 NP PPloc V0 - put VP - was VPass VPass - V0 PPloc VP/NP - V0 NP/NP PPloc VP/NP - V0 NP PPloc/NP PPloc - Ploc NP Ploc - on | in | ... PPloc/NP - Ploc NP/NP Exhaustive modelling with a considerable number of grammatical rules. Semantics separate, otherwise unspecified. 2. Lexical Semantics Account   put V NPjPPk CAUSE ( [BOB]i , GO ( [BUTTER]j , TO ( [BREAD]k )))   Syntax handled by numerous argument-fusing ""linking rules"", typically difficult to formalize. Semantic templates mirror alternation patterns, but are ad-hocly constructed. 3. Minimalist/Incorporation Account /put/ =ploc =d vcause (l(=ploc) (l(=d) (=ploc =d))) /on/ =d +k ploc (l(=d) (l(x) ((go x) (path self =d)))) // >vcause +k =d pred (l(>vcause) (l(=d) ((cause >vcause) =d))) /-ed/ >pred ++k t (l(>pred) (tense >pred past)) Small number of lexical entries handle all syntactic phenomena. Semantics directly encoded in lexical entry. Entries structurally governed by small number of rules, specifying how N/A/P are related. Figure 1: Three Different Accounts of Verb Subcategorization 2 Incorporation Theory At the heart of our new contribution to modeling verb subcategorization is the marriage of Hale and Keyser's (1993, 1998) argument structure theory with Stabler's (1997) 'minimalist' structure building rules. In the Hale and Keyser's theory, using the terminology of X-bar syntax, a particular head (labeled X), may or may or may not take a complement (labeled Y) and may or may not project a specifier (labeled S), resulting in 4 possible structural configurations: X (c) H X Y X (c)(c) H H S X (c) H X Y a (c)(c) H H S a (c) H a X X (a) -subj, +comp (V) (b) +subj, +comp (P) (c) +subj, -comp (A) (d) -subj, -comp (N) Figure 2: Four fundamental primitives in Hale and Keyser's incorporation theory The combinatorial possibilities of incorporation with X=V, A, N, P heads, plus 'head movement', is designed to yield the space of possible syntactic argument structure configurations, presumably across all languages. Notions of agent, patient, instrument, theme, goal, etc. are not 'primitives', but are derived from positions in structural configurations. In English (but not necessarily in all languages), (a) the category V takes a complement but projects no specifier; (b) the category P takes both a complement and projects a specifier; (c) the category A takes no complement but projects a specifier; (d) the category N takes neither complement nor specifier. A particular verbal entry, being of category V, may incorporate one or more of these structures as its complement, as shown in Figure 3: * Nouns incorporated directly into a verbal entry yield structures such as (a): no subject is projected by the N. The phonetic material of the noun head incorporates (undergoes head movement) into the phonetic material of the verb head, which itself may undergo further movement. Verbs such as these are intransitive by nature, generating, e.g., /The light glow -ed/ but */Bob glow -ed the light/. This argument structure typifies purely internally caused processes."	"The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG ) ."	"['My theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG ) .""]"	1
CCT176	N04-2004	A computational framework for non-lexicalist semantics	no escape from syntax don’t try morphological analysis in the privacy of your own lexicon	['Alec Marantz']		"So Lexicalism claims that the syntax manipulates internally complex words, not unanalyzable atomic units. The leading idea of Lexicalism might be summarized as follows: Everyone agrees that there has to be a list of sound/meaning connections for the atomic building blocks of language (=the ""morphemes""). There also has to be a list of idiosyncratic properties associated with the building blocks. Perhaps the storage house of sound/meaning connections for building blocks and the storage house of idiosyncratic information associated with building blocks is the same house. Perhaps the distinction between this unified storage house and the computational system of syntax could be used to correlate and localize various other crucial distinctions: non-syntax vs. syntax, ""lexical"" phonological rules vs. phrasal and everywhere phonological rules, unpredictable composition vs. predictable composition ... Syntax is for the ruly, the lexicon for the unruly (see, e.g., DiSciullo and Williams 1987). The Lexicalist view of the computational lexicon may be pictured as in (3), where both the Lexicon and the Syntax connect sound and meaning by relating the sound and meaning of complex constituents systematically to the sounds and meanings of their constitutive parts."	"Here , I adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots ."	"['Following the non-lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so-called light verbs.', 'Here , I adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots .', 'Verbalizing heads introduce relevant eventive interpretations in the syntax, and correspond to (assumed) universal primitives of the human cognitive system.', 'On the other hand, verbal roots represent abstract (categoryless) concepts and basically correspond to open-class items drawn from encyclopedic knowledge.', 'I assume an inventory of three verbalizing heads, each corresponding to an aforementioned primitive:']"	5
CCT177	N04-2004	A computational framework for non-lexicalist semantics	the berkeley framenet project	"['Collin F Baker', 'Charles J Fillmore', 'John B Lowe']"	introduction	"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work"	"This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #AUTHOR_TAG ) and PropBank ( Kingsbury et al. , 2002 ) ."	"['A common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots).', 'Verbs are viewed as simple predicates over their arguments.', ""This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #AUTHOR_TAG ) and PropBank ( Kingsbury et al. , 2002 ) .""]"	0
CCT178	N04-2004	A computational framework for non-lexicalist semantics	word meaning and	['David Dowty']		"Reading and listening involve complex psychological processes that recruit many brain areas. The anatomy of processing English words has been studied by a variety of imaging methods. Although there is widespread agreement on the general anatomical areas involved in comprehending words, there are still disputes about the computations that go on in these areas. Examination of the time relations (circuitry) among these anatomical areas can aid in under-standing their computations. In this paper we concentrate on tasks which involve obtaining the meaning of a word in isolation or in relation to a sentence. Our current data support a finding in the literature that frontal semantic areas are active well before posterior areas. We use the subjects attention to amplify relevant brain areas involved either in semantic classification or in judging the relation of the word to a sentence in order to test the hypothesis that frontal areas are concerned with lexical semantics while posterior areas are more involved in comprehension of propositions that involve several words"	"There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) ."	"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example:']"	0
CCT179	N04-2004	A computational framework for non-lexicalist semantics	three generative lexicalized models for statistical parsing	['Michael Collins']	introduction	"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96)."	"Due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences ."	"['The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .']"	0
CCT180	N04-2004	A computational framework for non-lexicalist semantics	semantics and cognition	['Ray Jackendoff']		"A new perspective on cognition views cortical cell assemblies linking together knowledge about actions and perceptions not only as the vehicles of integrated action and perception processing but, furthermore, as a brain basis for a wide range of higher cortical functions, including attention, meaning and concepts, sequences, goals and intentions, and even communicative social interaction. This article explains mechanisms relevant to mechanistic action perception theory, points to concrete neuronal circuits in brains along with artificial neuronal network simulations, and summarizes recent brain imaging and other experimental data documenting the role of action perception circuits in cognition, language and communication"	"There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) ."	"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example:']"	0
CCT181	N04-2004	A computational framework for non-lexicalist semantics	a minimalist implementation of verb subcategorization	['Sourabh Niyogi']		"Traditional accounts of verb subcategorization, from the classic work of Fillmore on, require either a considerable number of syntactic rules to account for diverse sentence constructions, including crosslanguage variation, or else complex linking rules mapping the thematic roles of semantic event templates with possible syntactic forms. In this paper we exhibit a third approach: we implement, via an explicit parser and lexicon, the incorporation theory of Hale and Keyser (1993, 1998) to systematically cover most patterns in English Verb Classes and Alternations (Levin 1993), typically using only 1 or 2 lexical entries per verb to subsume a large number of syntactic constructions and also most information typically contained in semantic event templates, and, further, replacing the notion of ""thematic roles"" with precise structural configurations. The implemented parser uses the merge and move operations formalized by Stabler (1997) in the minimalist framework of Chomsky (2001). As a side benefit, we extend the minimalist recognizer of Harkema (2000) to a full parsing implementation. We summarize the current compactness and coverage of our account and provide this minimalist lexicon and parser online at http://web.mit.edu/niyogi/www/minimal.htm 1 The Problem of Verb Subcategorization Why do certain verbs undergo particular certain alternations and not others? On some accounts, e.g. Levin (1993), referred to hereafter as EVCA, alternations provide insight into verb subcategorization and hence hooks to parsing, cross-language variation, machine translation, and class based verb learning. However, fully implemented accounts of the phenomena remains an open problem, with at least three alternative models, shown in Figure 1. Accounts may be solely descriptive - for example, classifying verbs as having an intransitive, a transitive, and/or ditransitive form, as is familiar. Traditional computational accounts (see 1) map these forms into individual grammar rules, (perhaps by macro expansion-like techniques) adding as many rules as necessary to account for naturally' occurring constructions (wh-movement, passive forms, etc.) For each grammatical rule, a separate semantic decomposition is required, typically labeling component phrases with one of several ""thematic roles."" A richer account provided by lexical semantics (see 2), exemplified in Jackendoff (1983, 1990) and Rappaport Hovav and Levin (1998), is one that hypothesizes semantic templates, but requires linking rules mapping syntactic frames with semantic templates governed by a particular verb. Often these semantic templates are constructed in an ad hoc manner, and the corresponding linking rules are consquently a collection of difficult-toimplement heuristics. In this paper we implement a rather different formalism (Hale and Keyser's Incorporation theory, see 3), wherein fewer lexical entries govern syntactic and semantic behavior, with no appeal to thematic roles or complex linking rules. 0. Verb Subcategorization Phenomena * Bob put. Butter was put on the bread. * Bob put butter. What was put on the bread? Bob put butter on the bread. Where was the butter put? 1. Traditional Account VP - V0 NP PPloc V0 - put VP - was VPass VPass - V0 PPloc VP/NP - V0 NP/NP PPloc VP/NP - V0 NP PPloc/NP PPloc - Ploc NP Ploc - on | in | ... PPloc/NP - Ploc NP/NP Exhaustive modelling with a considerable number of grammatical rules. Semantics separate, otherwise unspecified. 2. Lexical Semantics Account   put V NPjPPk CAUSE ( [BOB]i , GO ( [BUTTER]j , TO ( [BREAD]k )))   Syntax handled by numerous argument-fusing ""linking rules"", typically difficult to formalize. Semantic templates mirror alternation patterns, but are ad-hocly constructed. 3. Minimalist/Incorporation Account /put/ =ploc =d vcause (l(=ploc) (l(=d) (=ploc =d))) /on/ =d +k ploc (l(=d) (l(x) ((go x) (path self =d)))) // >vcause +k =d pred (l(>vcause) (l(=d) ((cause >vcause) =d))) /-ed/ >pred ++k t (l(>pred) (tense >pred past)) Small number of lexical entries handle all syntactic phenomena. Semantics directly encoded in lexical entry. Entries structurally governed by small number of rules, specifying how N/A/P are related. Figure 1: Three Different Accounts of Verb Subcategorization 2 Incorporation Theory At the heart of our new contribution to modeling verb subcategorization is the marriage of Hale and Keyser's (1993, 1998) argument structure theory with Stabler's (1997) 'minimalist' structure building rules. In the Hale and Keyser's theory, using the terminology of X-bar syntax, a particular head (labeled X), may or may or may not take a complement (labeled Y) and may or may not project a specifier (labeled S), resulting in 4 possible structural configurations: X (c) H X Y X (c)(c) H H S X (c) H X Y a (c)(c) H H S a (c) H a X X (a) -subj, +comp (V) (b) +subj, +comp (P) (c) +subj, -comp (A) (d) -subj, -comp (N) Figure 2: Four fundamental primitives in Hale and Keyser's incorporation theory The combinatorial possibilities of incorporation with X=V, A, N, P heads, plus 'head movement', is designed to yield the space of possible syntactic argument structure configurations, presumably across all languages. Notions of agent, patient, instrument, theme, goal, etc. are not 'primitives', but are derived from positions in structural configurations. In English (but not necessarily in all languages), (a) the category V takes a complement but projects no specifier; (b) the category P takes both a complement and projects a specifier; (c) the category A takes no complement but projects a specifier; (d) the category N takes neither complement nor specifier. A particular verbal entry, being of category V, may incorporate one or more of these structures as its complement, as shown in Figure 3: * Nouns incorporated directly into a verbal entry yield structures such as (a): no subject is projected by the N. The phonetic material of the noun head incorporates (undergoes head movement) into the phonetic material of the verb head, which itself may undergo further movement. Verbs such as these are intransitive by nature, generating, e.g., /The light glow -ed/ but */Bob glow -ed the light/. This argument structure typifies purely internally caused processes."	#AUTHOR_TAG has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .	"['#AUTHOR_TAG has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .', 'I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework.', 'As an example, a simplified derivation of the sentence ""The tire flattened."" is shown in Figure 1.']"	2
CCT182	N04-2004	A computational framework for non-lexicalist semantics	english verb classes and alternations a preliminary investigation	['Beth Levin']	introduction	"In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, ""English Verb Classes and Alternations"" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University."	"The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy ."	"['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', 'In addition, lexical rules could be formulated to derive certain alternations from more basic forms.']"	1
CCT183	N04-2004	A computational framework for non-lexicalist semantics	building verb meanings	"['Malka Rappaport Hovav', 'Beth Levin']"			"There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) ."	"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']"	0
CCT184	N04-2004	A computational framework for non-lexicalist semantics	verbs and times	['Zeno Vendler']		"T HE fact that verbs have tenses indicates that considerations involving the concept of time are relevant to their use. These considerations are not limited merely to the obvious discrimination between past, present, and future; there is another, a more subtle dependence on that concept: the use of a verb may also suggest the particular way in which that verb presupposes and involves the notion of time. In a number of recent publications some attention has been paid to these finer aspects, perhaps for the first time systematically. Distinctions have been made among verbs suggesting processes, states, dispositions, occurrences, tasks, achievements, and so on. Obviously these differences cannot be explained in terms of time alone: other factors, like the presence or absence of an object, conditions, intended states of affairs, also enter the picture. Nevertheless one feels that the time element remains crucial; at least it is important enough to warrant separate treatment. Indeed, as I intend to show, if we focus our attention primarily upon the time schemata presupposed by various verbs,"" we are able to throw light on some of the obscurities which still remain in these matters. These time schemata will appear as important constituents of the concepts that prompt us to use those terms the way we consistently do. There are a few such schemata of very wide application. Once they have been discovered in some typical examples, they may be used as models of comparison in exploring and clarifying the behavior of any verb whatever. In indicating these schemata, I do not claim that they represent all possible ways in which verbs can be used correctly with respect to time determination nor that a verb exhibiting a use fairly covered by one schema cannot have divergent uses, which"	"A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )"	"['Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"	0
CCT185	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	natural language processing for information assurance and security an overview and implementations	"['Mikhail J Atallah', 'Craig J McDonough', 'Victor Raskin', 'Sergei Nirenburg']"	related work	"This research paper explores a promising interface between natural language processing (NLP) and information assurance and security (IAS). More specificall~ it is devoted to possible applications to, and further dedicated development of, the accumulated considerable resources in NLP for, IAS. The expected and partially accomplished result is in harnessing the weird, illogical ways natural languages encode meaning, the very ways that defy all the usual combinatorial approaches to mathematical--and computational--complexity and make NLP so hard, to enhance information security. The paper is of a mixed theoretical and empirical nature. Of the four possible venues of applications, (i) memorizing randomly generated passwords with the help of automatically generated funny jingles, (ii) natural language watermarking, (iii) using the available machine translation (MT) systems for (additional) encryption of text messages, and (iv) downgrading, or sanitizing classified information in networks, two venues, (i) and (iv), have been at least partially implemented and the remaining two (ii) and (iii) are being implemented to the proof-of-concept level. We must make it very clear, however, that we have done very little experimentation or evaluation at this point, though we are moving quickly in that direction. The merits of the paper, if any, are in its venture to make considerable progress achieved recently in NLE especially in knowledge representation and meaning analysis, useful for IAS needs. The NLP approach adopted here, ontological semantics, has been developed by two of the coauthors; watermarking is based on the pioneering research by another coauthor and his associates; most of the implementation of the password memorization software has been done by the fourth coauthor. All the four of us have agonized whether we should report this research now or wait till we have fully implemented all or at least some of the systems we are developing. At the end of the day, we have reached a consensus that it is important, even at this early stage, to review for the information security community what NLP can do for it and to invite feedback and further efforts and ideas on what seems likely to become a new paradigm in information security. To the body of the paper, we Mikhail J. Atallah, Craig J. McDonough, Victor Raskin Center for Education and Research in Information Assurance and Security (CERIAS, www.cerias.purdue.edu) Purdue University W. Lafayette, IN 47907 mja, raskin, mcdonoug@cerias.purdue.edu Sergei Nirenburg Computing Research Laboratory, New Mexico State University Las Cruces, NM 88003 sergei@crl.nmsu.edu have added two self-contained deliberately reference-free appendices on NLP and ontological semantics, respectively, primarily for the benefit of those IAS readers, who are interested in expanding their understanding of those fields and further exploring their possible fruitful interactions with IAS."	#AUTHOR_TAGb ) and Topkara et al. ( 2006a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method .	"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. (2005), Meral et al. (2007, Murphy (2001), Murphy and Vogel (2007) and Topkara et al. (2006a) all belong to the syntactic transformation category.', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', '#AUTHOR_TAGb ) and Topkara et al. ( 2006a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method .']"	0
CCT186	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	hiding the hidden a software system for concealing ciphertext as innocuous text	"['Mark Chapman', 'George I Davida']"	related work	"In this paper we present a system for protecting the privacy of cryptograms to avoid detection by censors. The system transforms ciphertext into innocuous text which can be transformed back into the original ciphertext. The expandable set of tools allows experimentation with custom dictionaries, automatic simulation of writing style, and the use of Context-Free-Grammars to control text generation. The scope of this paper is to provide an overview of the basic transformation processes and to demonstrate the quality of the generated text."	The first lexical substitution method was proposed by #AUTHOR_TAG .	"['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by #AUTHOR_TAG .', 'Later works, such as Atallah et al. (2001a), Bolshakov (2004), Taskiran et al. (2006 and Topkara et al. (2006b), further made use of part-ofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']"	0
CCT187	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	the syntax of concealment reliable methods for plain text information hiding	"['Brian Murphy', 'Carl Vogel']"	related work	"Many plain text information hiding techniques demand deep semantic processing, and so suffer in reliability. In contrast, syntactic processing is a more mature and reliable technology. Assuming a perfect parser, this paper evaluates a set of automated and reversible syntactic transforms that can hide information in plain text without changing the meaning or style of a document. A large representative collection of newspaper text is fed through a prototype system. In contrast to previous work, the output is subjected to human testing to verify that the text has not been significantly compromised by the information hiding procedure, yielding a success rate of 96% and bandwidth of 0.3 bits per sentence."	"Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , #AUTHOR_TAG and Topkara et al. ( 2006a ) all belong to the syntactic transformation category ."	"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , #AUTHOR_TAG and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']"	0
CCT188	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	natural language processing for information assurance and security an overview and implementations	"['Mikhail J Atallah', 'Craig J McDonough', 'Victor Raskin', 'Sergei Nirenburg']"	related work	"This research paper explores a promising interface between natural language processing (NLP) and information assurance and security (IAS). More specificall~ it is devoted to possible applications to, and further dedicated development of, the accumulated considerable resources in NLP for, IAS. The expected and partially accomplished result is in harnessing the weird, illogical ways natural languages encode meaning, the very ways that defy all the usual combinatorial approaches to mathematical--and computational--complexity and make NLP so hard, to enhance information security. The paper is of a mixed theoretical and empirical nature. Of the four possible venues of applications, (i) memorizing randomly generated passwords with the help of automatically generated funny jingles, (ii) natural language watermarking, (iii) using the available machine translation (MT) systems for (additional) encryption of text messages, and (iv) downgrading, or sanitizing classified information in networks, two venues, (i) and (iv), have been at least partially implemented and the remaining two (ii) and (iii) are being implemented to the proof-of-concept level. We must make it very clear, however, that we have done very little experimentation or evaluation at this point, though we are moving quickly in that direction. The merits of the paper, if any, are in its venture to make considerable progress achieved recently in NLE especially in knowledge representation and meaning analysis, useful for IAS needs. The NLP approach adopted here, ontological semantics, has been developed by two of the coauthors; watermarking is based on the pioneering research by another coauthor and his associates; most of the implementation of the password memorization software has been done by the fourth coauthor. All the four of us have agonized whether we should report this research now or wait till we have fully implemented all or at least some of the systems we are developing. At the end of the day, we have reached a consensus that it is important, even at this early stage, to review for the information security community what NLP can do for it and to invite feedback and further efforts and ideas on what seems likely to become a new paradigm in information security. To the body of the paper, we Mikhail J. Atallah, Craig J. McDonough, Victor Raskin Center for Education and Research in Information Assurance and Security (CERIAS, www.cerias.purdue.edu) Purdue University W. Lafayette, IN 47907 mja, raskin, mcdonoug@cerias.purdue.edu Sergei Nirenburg Computing Research Laboratory, New Mexico State University Las Cruces, NM 88003 sergei@crl.nmsu.edu have added two self-contained deliberately reference-free appendices on NLP and ontological semantics, respectively, primarily for the benefit of those IAS readers, who are interested in expanding their understanding of those fields and further exploring their possible fruitful interactions with IAS."	"Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method ."	"['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n- gram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']"	0
CCT189	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	words are not enough sentence level natural language watermarking	"['Mercan Topkara', 'Umut Topkara', 'Mikhail J Atallah']"	related work	"Compared to other media, natural language text presents unique challenges for information hiding. These challenges require the design of a robust algorithm that can work under following constraints: (i) low embedding bandwidth, i.e., number of sentences is comparable with message length, (ii) not all transformations can be applied to a given sentence (iii) the number of alternative forms for a sentence is relatively small, a limitation governed by the grammar and vocabulary of the natural language, as well as the requirement to preserve the style and fluency of the document. The adversary can carry out all the transformations used for embedding to remove the embedded message. In addition, the adversary can also permute the sentences, select and use a subset of sentences, and insert new sentences. We give a scheme that overcomes these challenges, together with a partial implementation and its evaluation for the English language. The present application of this scheme works at the sentence level while also using a word-level watermarking technique that was recently designed and built into a fully automatic system (""Equimark""). Unlike Equimark, whose resilience relied on the introduction of ambiguities, the present paper's sentence-level technique is more tuned to situations where very little change to the text is allowable (i.e., when style is important). Secondarily, this paper shows how to use lower-level (in this case word-level) marking to improve the resilience and embedding properties of higher level (in this case sentence level) schemes. We achieve this by using the word-based methods as a separate channel from the sentence-based methods, thereby improving the results of either one alone. The sentence level watermarking technique we introduce is novel and powerful, as it relies on multiple features of each sentence and exploits the notion of orthogonality between features."	"Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method ."	"['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']"	0
CCT190	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	syntactic information hiding in plain text masters thesis trinity college dublin	['Brian Murphy']	related work		"Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , #AUTHOR_TAG , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category ."	"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , #AUTHOR_TAG , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']"	0
CCT191	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	a method of linguistic steganography based on coladdressallyverified synonym	['Igor A Bolshakov']	related work		"Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method ."	"['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']"	0
CCT192	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	webscale ngram models for lexical disambiguation	"['Shane Bergsma', 'Dekang Lin', 'Randy Goebel']"	experiments		"The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 ."	"['The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .', 'The striking feature of the n-gram corpus is the large number of n-grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of English text on publicly accessible Web pages collected in January 2006.', 'For example, the 5-gram phrase the part that you were has a count of 103.', 'The compressed data is around 24 GB on disk.']"	0
CCT193	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	steganography in digital media principles algorithms and applications	['Jessica Fridrich']	introduction	"Steganography, the art of hiding of information in apparently innocuous objects or images, is a field with a rich heritage, and an area of rapid current development. This clear, self-contained guide shows you how to understand the building blocks of covert communication in digital media files and how to apply the techniques in practice, including those of steganalysis, the detection of steganography. Assuming only a basic knowledge in calculus and statistics, the book blends the various strands of steganography, including information theory, coding, signal estimation and detection, and statistical signal processing. Experiments on real media files demonstrate the performance of the techniques in real life, and most techniques are supplied with pseudo-code, making it easy to implement the algorithms. The book is ideal for students taking courses on steganography and information hiding, and is also a useful reference for engineers and practitioners working in media security and information assurance."	"Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ) ."	"['Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ) .', 'The covert communication is such that the very act of communication is to be kept secret from outside observers.', 'A related area is Watermarking, in which modifications are made to a cover medium in order to identify it, for example for the purposes of copyright.', 'Here the changes may be known to an observer, and the task is to make the changes in such a way that the watermark cannot easily be removed.']"	0
CCT194	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	words are not enough sentence level natural language watermarking	"['Mercan Topkara', 'Umut Topkara', 'Mikhail J Atallah']"	related work	"Compared to other media, natural language text presents unique challenges for information hiding. These challenges require the design of a robust algorithm that can work under following constraints: (i) low embedding bandwidth, i.e., number of sentences is comparable with message length, (ii) not all transformations can be applied to a given sentence (iii) the number of alternative forms for a sentence is relatively small, a limitation governed by the grammar and vocabulary of the natural language, as well as the requirement to preserve the style and fluency of the document. The adversary can carry out all the transformations used for embedding to remove the embedded message. In addition, the adversary can also permute the sentences, select and use a subset of sentences, and insert new sentences. We give a scheme that overcomes these challenges, together with a partial implementation and its evaluation for the English language. The present application of this scheme works at the sentence level while also using a word-level watermarking technique that was recently designed and built into a fully automatic system (""Equimark""). Unlike Equimark, whose resilience relied on the introduction of ambiguities, the present paper's sentence-level technique is more tuned to situations where very little change to the text is allowable (i.e., when style is important). Secondarily, this paper shows how to use lower-level (in this case word-level) marking to improve the resilience and embedding properties of higher level (in this case sentence level) schemes. We achieve this by using the word-based methods as a separate channel from the sentence-based methods, thereby improving the results of either one alone. The sentence level watermarking technique we introduce is novel and powerful, as it relies on multiple features of each sentence and exploits the notion of orthogonality between features."	"Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and #AUTHOR_TAGa ) all belong to the syntactic transformation category ."	"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and #AUTHOR_TAGa ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']"	0
CCT195	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	a method of text watermarking using presuppositions	"['M Olga Vybornova', 'Benoit Macq']"	related work	"We propose a method for watermarking texts of arbitrary length using natural-language semantic structures. For the key of our approach we use the linguistic semantic phenomenon of presuppositions. Presupposition is the implicit information considered as well-known or which readers of the text are supposed to treat as well-known; this information is a semantic component of certain linguistic expressions (lexical items and syntactical constructions called presupposition triggers). The same sentence can be used with or without presupposition, or with a different presupposition trigger, provided that all the relations between subjects, objects and other discourse referents are preserved - such transformations will not change the meaning of the sentence. We define the distinct rules for presupposition identification for each trigger and regular transformation rules for using/non-using the presupposition in a given sentence (one bit per sentence in this case). Isolated sentences can carry the proposed watermarks. However, the longer is the text, the more efficient is the watermark. The proposed approach is resilient to main types of random transformations, like passivization, topicalization, extraposition, preposing, etc. The web of resolved presupposed information in the text will hold the watermark of the text (e.g. integrity watermark, or prove of ownership), introducing ""secret ordering"" into the text structure to make it resilient to ""data loss"" attacks and ""data altering"" attacks.Anglai"	"#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence ."	"['The semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state-ofthe-art for NLP technology.', 'It requires some sophisticated tools and knowledge to model natural language semantics.', 'Atallah et al. (2002) used semantic transformations and embed information in textmeaning representation (TMR) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence .']"	0
CCT196	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	extracting paraphrases from a parallel corpus	"['Regina Barzilay', 'Kathleen R McKeown']"	experiments	"While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases. We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text. Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases."	#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .	"['Table 1 gives summary statistics of the paraphrase dictionary and its coverage on Section 00 of the Penn Treebank.', 'The length of the extracted n-gram phrases ranges from unigrams to five-grams.', 'The coverage figure gives the percentage of sentences which have at least one phrase in the dictionary.', 'The coverage is important for us because it determines the payload capacity of the embedding method described in Section 5.  Original phrase Paraphrases the end of this year later this year the end of the year year end a number of people some of my colleagues differences the European peoples party the PPE group dictionary is a mapping from phrases to sets of possible paraphrases.', 'Each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'The examples show that, while some of the paraphrases are of a high quality, some are not.', 'For example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'Moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'For example, year end is an unsuitable paraphrase for the end of this year in the sentence The chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .', 'Section 4 describes our method for determining if a paraphrase is suitable in a given context.']"	0
CCT197	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	syntactic constraints on paraphrases extracted from parallel corpora	['Chris Callison-Burch']	experiments	"ccb cs jhu edu We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method."	"The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in #AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation ."	"['The cover text used for our experiments consists of newspaper sentences from Section 00 of the Penn Treebank (Marcus et al., 1993).', 'Hence we require possible paraphrases for phrases that occur in Section 00.', 'The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in #AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation .']"	5
CCT198	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	widecoverage efficient statistical parsing with ccg and loglinear models	"['Stephen Clark', 'James R Curran']"	introduction		"We also experiment with a CCG parser (Clark and Curran, 2007), requiring that the contexts surrounding the original phrase and paraphrase are assigned the same CCG lexical categories by the parser."	"['In order to test the grammaticality and meaning preserving nature of a paraphrase, we employ a simple technique based on checking whether the contexts containing the paraphrase are in the Google ngram corpus.', 'This technique is based on the simple hypothesis that, if the paraphrase in context has been used many times before on the web, then it is an appropriate use.', 'We test our n-gram-based system against some human judgements of the grammaticality of paraphrases in context.', 'We find that using larger contexts leads to a high precision system (100% when using 5-grams), but at the cost of a reduced recall.', 'This precision-recall tradeoff reflects the inherent tradeoff between imperceptibility and payload in a Linguistic Steganography system.', 'We also experiment with a CCG parser (Clark and Curran, 2007), requiring that the contexts surrounding the original phrase and paraphrase are assigned the same CCG lexical categories by the parser.', 'This method increases the precision of the Google n-gram check with a slight loss in recall.']"	5
CCT199	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	a natural language watermarking based on chinese syntax	"['Yuling Liu', 'Xingming Sun', 'Yong Wu']"	related work	"A novel text watermarking algorithm is presented. It combines natural language watermarking and Chinese syntax based on BP neural networks. Since the watermarking signals are embedded into some Chinese syntactic structure rather than the appearance of text elements, the algorithm is totally based on the content that can prove to be very resilient. It will play an important role in protecting the security of Chinese documents over Internet."	"#AUTHOR_TAG , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category ."	"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']"	0
CCT200	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	whitesteg a new scheme in information hiding using text steganography	"['Lip Y Por', 'Ang T Fong', 'B Delina']"	introduction	"Abstract:- Sending encrypted messages frequently will draw the attention of third parties, i.e. crackers and hackers, perhaps causing attempts to break and reveal the original messages. In this digital world, steganography is introduced to hide the existence of the communication by concealing a secret message inside another unsuspicious message. The hidden message maybe plaintext, or any data that can be represented as a stream of bits. Steganography is often being used together with cryptography and offers an acceptable amount of privacy and security over the communication channel. This paper presents an overview of text steganography and a brief history of steganography along with various existing techniques of text steganography. Highlighted are some of the problems inherent in text steganography as well as issues with existing solutions. A new approach, named WhiteSteg is proposed in information hiding using inter-word spacing and inter-paragraph spacing as a hybrid method to reduce the visible detection of the embedded messages. WhiteSteg offers dynamic generated cover-text with six options of maximum capacity according to the length of the secret message. Besides, the advantage of exploiting whitespaces in information hiding is discussed. This paper also analyzes the significant drawbacks of each existing method and how WhiteSteg could be recommended as a solution"	"Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( #AUTHOR_TAG ) ."	"['Section 2 describes some of the previous transformations used in Linguistic Steganography.', 'Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( #AUTHOR_TAG ) .', 'Our proposed method is based on the automatically acquired paraphrase dictionary described in Callison-Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits.', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text.']"	1
CCT201	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	syntactic constraints on paraphrases extracted from parallel corpora	['Chris Callison-Burch']		"ccb cs jhu edu We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method."	"Our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits ."	"['Section 2 describes some of the previous transformations used in Linguistic Steganography.', 'Note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e.g. the amount of white space between words (Por et al., 2008).', 'Our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits .', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text.']"	5
CCT202	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	a comprehensive bibliography of linguistic steganography	['Richard Bergmair']	introduction	"In this paper, we will attempt to give a comprehensive bibliographic account of the work in linguistic steganography published up to date. As the field is still in its infancy there is no widely accepted publication venue. Relevant work on the subject is scattered throughout the literature on information security, information hiding, imaging and watermarking, cryptology, and natural language processing. Bibliographic references within the field are very sparse. This makes literature research on linguistic steganography a tedious task and a comprehensive bibliography a valuable aid to the researcher."	"However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media ( #AUTHOR_TAG ) ."	"['There is a large literature on image steganography and watermarking, in which images are modified to encode a hidden message or watermark.', 'Image stegosystems exploit the redundancy in an image representation together with limitations of the human visual system.', 'For example, a standard image stegosystem uses the least-significant-bit (LSB) substitution technique.', 'Since the difference between 11111111 and 11111110 in the value for red/green/blue intensity is likely to be undetectable by the human eye, the LSB can be used to hide information other than colour, without being perceptable by a human observer. 1', ' key question for any steganography system is the choice of cover medium.', 'Given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider.', 'However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media ( #AUTHOR_TAG ) .', 'The likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'Language has the property that even small local changes to a text, e.g.', 'replacing a word by a word with similar meaning, may result in text which is anomalous at the document level, or anomalous with respect to the state of the world.', 'Hence finding linguistic transformations which can be applied reliably and often is a challenging problem for Linguistic Steganography.']"	0
CCT203	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	widecoverage efficient statistical parsing with ccg and loglinear models	"['Stephen Clark', 'James R Curran']"	method		We use the #AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .	"['In order to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method.', 'We use the #AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .', 'Combinatory Categorial Grammar (CCG) is a lexicalised grammar formalism, in which CCG lexical categories -typically expressing subcategorisation information -are assigned to each word in a sentence.', 'The grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'If there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'Hence the grammar check is at the word, rather than derivation, level; however, CCG lexical categories contain a large amount of syntactic information which this method is able to exploit.']"	5
CCT204	N10-1084	Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method	syntactic tools for text watermarking	"['Hasan M Meral', 'Emre Sevinc', 'Ersin Unkar', 'Bulent Sankur', 'A Sumru Ozsoy', 'Tunga Gungor']"	related work	"This paper explores the morphosyntactic tools for text watermarking and develops a syntax-based natural language watermarking scheme. Turkish, an agglutinative language, provides a good ground for the syntax-based natural language watermarking with its relatively free word order possibilities and rich repertoire of morphosyntactic structures. The unmarked text is first transformed into a syntactic tree diagram in which the syntactic hierarchies and the functional dependencies are coded. The watermarking software then operates on the sentences in syntax tree format and executes binary changes under control of Wordnet to avoid semantic drops. The key-controlled randomization of morphosyntactic tool order and the insertion of void watermark provide a certain level of security. The embedding capacity is calculated statistically, and the imperceptibility is measured using edit hit counts."	"Liu et al. ( 2005 ) , #AUTHOR_TAG , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category ."	"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , #AUTHOR_TAG , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']"	0
CCT205	N13-1036	Unsupervised Arabic dialect segmentation for machine translation	dialectal to standard arabic paraphrasing to improve arabicenglish statistical machine translation	"['Wael Salloum', 'Nizar Habash']"	related work	"This paper is about improving the quality of Arabic-English statistical machine translation (SMT) on dialectal Arabic text using morphological knowledge. We present a light-weight rule-based approach to producing Modern Standard Arabic (MSA) paraphrases of dialectal Arabic out-of-vocabulary (OOV) words and low frequency words. Our approach extends an existing MSA analyzer with a small number of morphological clitics, and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrase-based SMT system. This approach improves BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative). A manual error analysis of translated dialectal words shows that our system produces correct translations in 74% of the time for OOVs and 60% of the time for low frequency words."	"In our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only ."	"['In our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .', 'We did not use a language model to pick the best path; instead we kept the ambiguity in the lattice and passed it to our SMT system.', 'In contrast, in this paper, we run ELISSA on untokenized Arabic, we use feature, lemma, and surface form transfer rules, and we pick the best path of the generated MSA lattice through a language model.', ""Certain aspects of our approach are similar to Riesa and Yarowsky (2006)'s, in that we use morphological analysis for DA to help DA-English MT; but unlike them, we use a rule-based approach to model DA morphology.""]"	1
CCT206	N13-1036	Unsupervised Arabic dialect segmentation for machine translation	a systematic comparison of various statistical alignment models	"['F J Och', 'H Ney']"		"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."	The parallel corpus is word-aligned using GIZA + + ( #AUTHOR_TAG ) .	"['We use the open-source Moses toolkit (Koehn et al., 2007) to build a phrase-based SMT system trained on mostly MSA data (64M words on the Arabic side) obtained from several LDC corpora including some limited DA data.', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA + + ( #AUTHOR_TAG ) .', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', 'The language model for our system is trained on the English side of the bitext augmented with English Gigaword (Graff and Cieri, 2003).', 'We use a 5-gram language model with modified Kneser-Ney smoothing.', 'Feature weights are tuned to maximize BLEU on the NIST MTEval 2006 test set using Minimum Error Rate Training (Och, 2003).', 'This is only done on the baseline systems.', 'The English data is tokenized using simple punctuation-based rules.', 'The Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Habash and Rambow, 2005;Roth et al., 2008).', 'The Arabic text is also Alif/Ya normalized.', 'MADA-produced Arabic lemmas are used for word alignment.']"	5
CCT207	N13-1036	Unsupervised Arabic dialect segmentation for machine translation	dialectal to standard arabic paraphrasing to improve arabicenglish statistical machine translation	"['Wael Salloum', 'Nizar Habash']"		"This paper is about improving the quality of Arabic-English statistical machine translation (SMT) on dialectal Arabic text using morphological knowledge. We present a light-weight rule-based approach to producing Modern Standard Arabic (MSA) paraphrases of dialectal Arabic out-of-vocabulary (OOV) words and low frequency words. Our approach extends an existing MSA analyzer with a small number of morphological clitics, and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrase-based SMT system. This approach improves BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative). A manual error analysis of translated dialectal words shows that our system produces correct translations in 74% of the time for OOVs and 60% of the time for low frequency words."	This is a similar conclusion to our previous work in #AUTHOR_TAG .	"['In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'Phrase-based trans-  lation is also added to word-based translation.', 'Results show that selecting and translating phrases improve the three best performers of word-based selection.', 'The best performer, shown in the last raw, suggests using phrase-based selection and restricted word-based selection.', 'The restriction is to include OOV words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'Comparing the best performer to the OOV selection mode system shows that translating low frequency in-vocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']"	1
CCT208	N13-1036	Unsupervised Arabic dialect segmentation for machine translation	moses open source toolkit for statistical machine translation	"['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Christopher Callison-Burch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen', 'Christine Moran', 'Richard Zens', 'Christopher Dyer', 'Ondrej Bojar', 'Alexandra Constantin', 'Evan Herbst']"		"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks."	We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .	"['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA++ (Och and Ney, 2003).', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', 'The language model for our system is trained on the English side of the bitext augmented with English Gigaword (Graff and Cieri, 2003).', 'We use a 5-gram language model with modified Kneser-Ney smoothing.', 'Feature weights are tuned to maximize BLEU on the NIST MTEval 2006 test set using Minimum Error Rate Training (Och, 2003).', 'This is only done on the baseline systems.', 'The English data is tokenized using simple punctuation-based rules.', 'The Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Habash and Rambow, 2005;Roth et al., 2008).', 'The Arabic text is also Alif/Ya normalized.', 'MADA-produced Arabic lemmas are used for word alignment.']"	5
CCT209	P00-1001	Processes that shape conversation and their implications for computational linguistics	deterministic parsing of syntactic nonfluencies	['D Hindle']	introduction		"Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; Shriberg , Bear , & Dowding , 1992 ) ."	"[""The implicit assumptions of psychological and computational theories that ignore disfluencies must be either that people aren't disfluent, or that disfluencies make processing more difficult, and so theories of fluent speech processing should be developed before the research agenda turns to disfluent speech processing."", 'The first assumption is clearly false; disfluency rates in spontaneous speech are estimated by Fox Tree (1995) and by Bortfeld, Leon, Bloom, Schober, and Brennan (2000) to be about 6 disfluencies per 100 words, not including silent pauses.', 'The rate is lower for speech to machines (Oviatt, 1995;Shriberg, 1996), due in part to utterance length; that is, disfluency rates are higher in longer utterances, where planning is more difficult, and utterances addressed to machines tend to be shorter than those addressed to people, often because dialogue interfaces are designed to take on more initiative.', 'The average speaker may believe, quite rightly, that machines are imperfect speech processors, and plan their utterances to machines more carefully.', 'The good news is that speakers can adapt to machines; the bad news is that they do so by recruiting limited cognitive resources that could otherwise be focused on the task itself.', 'As for the second assumption, if the goal is to eventually process unrestricted, natural human speech, then committing to an early and exclusive focus on processing fluent utterances is risky.', 'In humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing (see, e.g., Tanenhaus et al. 1995).', 'This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well-formed utterance.', 'Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; Shriberg , Bear , & Dowding , 1992 ) .']"	0
CCT210	P06-1012	Estimating class priors in domain adaptation for word sense disambiguation	an empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation	"['Yoong Keok Lee', 'Hwee Tou Ng']"	experiments	"In this paper, we evaluate a variety  of knowledge sources and supervised  learning algorithms for word sense  disambiguation on SENSEVAL-2 and  SENSEVAL-1 data. Our knowledge  sources include the part-of-speech of  neighboring words, single words in the  surrounding context, local collocations,  and syntactic relations. The learning algorithms  evaluated include Support Vector  Machines (SVM), Naive Bayes, AdaBoost,  and decision tree algorithms. We  present empirical results showing the relative  contribution of the component knowledge  sources and the different learning  algorithms. In particular, using all of  these knowledge sources and SVM (i.e.,  a single learning algorithm) achieves accuracy  higher than the best official scores  on both SENSEVAL-2 and SENSEVAL-1  test data"	"Similar to our previous work ( Chan and Ng , 2005b ) , we used the supervised WSD approach described in ( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier ."	"['Similar to our previous work ( Chan and Ng , 2005b ) , we used the supervised WSD approach described in ( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier .', 'Knowledge sources used include partsof-speech, surrounding words, and local collocations.', 'This approach achieves state-of-the-art accuracy.', 'All accuracies reported in our experiments are micro-averages over all test examples.']"	5
CCT211	P08-1101	"Regularized Structured Perceptron: A Case Study on Chinese Word Segmentation, POS Tagging and Parsing"	chinese partofspeech tagging oneatatime or allatonce wordbased or characterbased	"['Hwee Tou Ng', 'Jin Kiat Low']"	experiments		We chose to follow #AUTHOR_TAG and split the sentences evenly to facilitate further comparison .	"['The overall tagging accuracy of our joint model was comparable to but less than the joint model of Shi and Wang (2007).', 'Despite the higher accuracy improvement from the baseline, the joint system did not give higher overall accuracy.', 'One likely reason is that Shi and Wang (2007) included knowledge about special characters and semantic knowledge from web corpora (which may explain the higher baseline accuracy), while our system is completely data-driven.', 'However, the comparison is indirect because our partitions of the CTB corpus are different.', 'Shi and Wang (2007) also chunked the sentences before doing 10-fold cross validation, but used an uneven split.', 'We chose to follow #AUTHOR_TAG and split the sentences evenly to facilitate further comparison .']"	5
CCT212	P08-1101	"Regularized Structured Perceptron: A Case Study on Chinese Word Segmentation, POS Tagging and Parsing"	chinese segmentation with a wordbased perceptron algorithm	"['Yue Zhang', 'Stephen Clark']"	experiments		"We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) ."	"['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).', 'The features used by the baseline segmentor are shown in Table 1.', 'The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2.']"	2
CCT213	P10-1143	Unsupervised Event Coreference Resolution with Rich Linguistic Features	unsupervised coreference resolution in a nonparametric bayesian model	"['Aria Haghighi', 'Dan Klein']"	method	"We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results."	Our HDP extension is also inspired from the Bayesian model proposed by #AUTHOR_TAG .	"['We present an extension of the hierarchical Dirichlet process (HDP) model which is able to represent each observable object (i.e., event mention) by a finite number of feature types L.', 'Our HDP extension is also inspired from the Bayesian model proposed by #AUTHOR_TAG .', 'However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008).']"	4
CCT214	P10-2002	A Joint Rule Selection Model for Hierarchical Phrase-based Translation *	soft syntactic constraints for hierarchical phrasedbased translation	"['Yuval Marton', 'Philip Resnik']"	method	"In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then nding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English."	"These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 ."	"['ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .']"	4
CCT215	P10-2002	A Joint Rule Selection Model for Hierarchical Phrase-based Translation *	rich sourceside context for statistical machine translation	"['Kevin Gimpel', 'Noah A Smith']"	method	"We explore the augmentation of statistical machine translation models with features of the context of each phrase to be translated. This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al., 2007). The context features we consider use surrounding words and part-of-speech tags, local syntactic structure, and other properties of the source language sentence to help predict each phrase's translation. Our approach requires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chinese-to-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach."	"These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 ."	"['ME approach has the merit of easily combining different features to predict the probability of each class.', 'We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .']"	4
CCT216	P10-2005	Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages	confidence measure for word alignment	['Fei Huang']	introduction	"In this paper we present a confidence mea-sure for word alignment based on the posterior probability of alignment links. We introduce sentence alignment confi-dence measure and alignment link con-fidence measure. Based on these mea-sures, we improve the alignment qual-ity by selecting high confidence sentence alignments and alignment links from mul-tiple word alignments of the same sen-tence pair. Additionally, we remove low confidence alignment links from the word alignment of a bilingual training corpus, which increases the alignment F-score, improves Chinese-English and Arabic-English translation quality and sig-nificantly reduces the phrase translation table size."	"More recently , an alignment selection approach was proposed in ( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold ."	"['More recently , an alignment selection approach was proposed in ( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .', 'The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model).', 'In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'There is no need for a pre-determined threshold as used in (Huang, 2009).', 'Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners.', 'Our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low-resource languages.']"	1
CCT217	P10-2019	Chinese semantic role labeling with shallow parsing	accurate unlexicalized parsing	"['Dan Klein', 'Christopher D Manning']"		"We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize."	"Inspired by ( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent ."	"['We introduce two types of chunks.', 'The first is simply the phrase type, such as NP, PP, of current chunk.', 'The column CHUNK 1 illustrates this kind of chunk type definition.', 'The second is more complicated.', ""Inspired by ( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent ."", 'For example, an NP immediately dominated by a S, will be substituted by NPˆS.', 'This strategy severely increases the number of chunk types and make it hard to train chunking models.', 'To shrink this number, we linguistically use a cluster of CTB phrasal types, which was introduced in .', 'The column CHUNK 2 illustrates this definition.', 'E.g., NPˆS implicitly represents Subject while NPˆVP represents Object.']"	4
CCT218	P10-2026	Learning Semantic Representations for Nonterminals in Hierarchical Phrase-Based Translation	comparison of extended lexicon models in search and rescoring for smt	"['Saˇsa Hasan', 'Hermann Ney']"	related work	We show how the integration of an extended lexicon model into the decoder can improve translation performance. The model is based on lexical triggers that capture long-distance dependencies on the sentence level. The results are compared to variants of the model that are applied in reranking of n-best lists. We present how a combined application of these models in search and rescoring gives promising results. Experiments are reported on the GALE Chinese-English task with improvements of up to +0.9% BLEU and -1.5% TER absolute on a competitive baseline.	The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( #AUTHOR_TAG ) .	"['The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( #AUTHOR_TAG ) .', 'Hasan and Ney ( 2009) introduced a second word to trigger the target word without considering any linguistic information.', 'Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'However, as the size of the corpus increases, the maximum entropy model will become larger.', 'Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection.', 'Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information.']"	4
CCT219	W06-1639	Get out the vote	mining newsgroups using networks arising from social behavior	"['R Agrawal', 'S Rajagopalan', 'R Srikant', 'Y Xu']"	related work	"Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. We investigate the feasibility of applying link-based methods in new applications domains. The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. This social behavior gives rise to a network in which the vertices are individuals and the links represent ""responded-to "" relationships. An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. This behavior is in sharp contrast to the WWW link graph, where linkage is an indicator of agreement or common interest. By analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. In contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text"	"Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) ."	"['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']"	0
CCT220	W06-1639	Get out the vote	extracting policy positions from political texts using words as data american political science review	"['M Laver', 'K Benoit', 'J Garry']"	related work		"There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) ."	"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) .', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']"	0
CCT221	W06-1639	Get out the vote	maxmargin markov networks	"['B Taskar', 'C Guestrin', 'D Koller']"	related work	"In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."	"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) ."	"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']"	0
CCT222	W06-1639	Get out the vote	cultural orientation classifying subjective documents by cociation sic analysis	['M Efron']	related work	"This paper introduces a simple method for estimating cultural orientation, the affiliations of hypertext documents in a polarized field of discourse. Using a probabilistic model based on cocitation information, two experiments are reported. The first experiment tests the modeli? 1/2 s ability to discriminate between left- and right-wing documents about politics. In this context the model is tested on two sets of data, 695 partisan web documents, and 162 political weblogs. Accuracy above 90% is obtained from the cocitation model, outperforming lexically based classifiers at statistically significant levels. In the second experiment, the proposed method is used to classify the home pages of musical artists with respect to their mainstream or ""alternative"" appeal. For musical artists the model is tested on a set of 515 artist home pages, achieving 88% accuracy."	"There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) ."	"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) .', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']"	0
CCT223	W06-1639	Get out the vote	transductive learning via spectral graph partitioning	['T Joachims']	related work	"We present a new method for transductive learning, which can be seen as a transductive version of the k nearest-neighbor classifier. Unlike for many other transductive learning methods, the training problem has a meaningful relaxation that can be solved globally optimally using spectral methods. We propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently. A key advantage of the algorithm is that it does not require additional heuristics to avoid unbalanced splits. Furthermore, we show a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case."	"Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and #AUTHOR_TAG ."	"['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and #AUTHOR_TAG .', 'Zhu (2005) maintains a survey of this area.']"	0
CCT224	W06-1639	Get out the vote	iterative classification in relational data	"['J Neville', 'D Jensen']"	related work	"Relational data offer a unique opportunity for improving the classification accuracy of statistical models. If two objects are related, inferring something about one object can aid inferences about the other. We present an iterative classification procedure that exploits this characteristic of relational data. This approach uses simple Bayesian classifiers in an iterative fashion, dynamically updating the attributes of some objects as inferences are made about related objects. Inferences made with high confidence in initial iterations are fed back into the data and are used to strengthen subsequent inferences about related objects. We evaluate the performance of iterative classification on a corporate dataset, using a binary classification task. Experiments indicate that iterative classification significantly increases accuracy when compared to a single-pass approach. 1"	"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) ."	"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']"	0
CCT225	W06-1639	Get out the vote	detection of agreement vs disagreement in meetings training with unlabeled data	"['D Hillard', 'M Ostendorf', 'E Shriberg']"	related work	"To support summarization of automatically transcribed meetings, we introduce a classifier to recognize agreement or disagreement utterances, utilizing both word-based and prosodic cues. We show that hand-labeling efforts can be minimized by using unsupervised training on a large unlabeled data set combined with supervised training on a small amount of data. For ASR transcripts with over 45% WER, the system recovers nearly 80% of agree/disagree utterances with a confusion rate of only 3%."	"More sophisticated approaches have been proposed ( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ) ."	"['More sophisticated approaches have been proposed ( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ) .', 'Also relevant is work on the gen- eral problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002).']"	0
CCT226	W06-1639	Get out the vote	the theory and practice of discourse parsing and summarization	['D Marcu']	related work	"From the Publisher:  Until now, most discourse researchers have assumed that full semantic understanding is necessary to derive the discourse structure of texts. This book documents the first serious attempt to construct automatically and use nonsemantic computational structures for text summarization. Daniel Marcu develops a semantics-free theoretical framework that is both general enough to be applicable to naturally occurring texts and concise enough to facilitate an algorithmic approach to discourse analysis. He presents and evaluates two discourse parsing methods: one uses manually written rules that reflect common patterns of usage of cue phrases such as ""however"" and ""in addition to""; the other uses rules that are learned automatically from a corpus of discourse structures. By means of a psycholinguistic experiment, Marcu demonstrates how a discourse-based summarizer identifies the most important parts of texts at levels of performance that are close to those of humans.  Marcu also discusses how the automatic derivation of discourse structures may be used to improve the performance of current natural language generation, machine translation, summarization, question answering, and information retrieval systems."	"Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 ) ."	"['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 ) .']"	0
CCT227	W06-1639	Get out the vote	multidimensional text analysis for erulemaking	"['N Kwon', 'S Shulman', 'E Hovy']"	related work	"To support rule-writers, we are developing techniques to automatically analyze large number of public comments on proposed regulations. A document is analyzed in various ways including argument structure, topics, and opinions. The individual results are integrated into a unified output. The experiments reported here were performed on comments submitted to the Environmental Protection Agency in response to their proposed rule for mercury regulation."	"Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) ."	"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']"	0
CCT228	W06-1639	Get out the vote	mining newsgroups using networks arising from social behavior	"['R Agrawal', 'S Rajagopalan', 'R Srikant', 'Y Xu']"	introduction	"Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. We investigate the feasibility of applying link-based methods in new applications domains. The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. This social behavior gives rise to a network in which the vertices are individuals and the links represent ""responded-to "" relationships. An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. This behavior is in sharp contrast to the WWW link graph, where linkage is an indicator of agreement or common interest. By analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. In contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text"	"For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cfXXX #AUTHOR_TAG."	"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document in- dependently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Many interesting opinion-oriented docu- ments, however, can be linked through certain re- lationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cfXXX #AUTHOR_TAG.', 'Agreement evidence can be a powerful aid in our classification task: for ex- ample, we can easily categorize a complicated (or overly terse) document if we find within it indica- tions of agreement with a clearly positive text.']"	0
CCT229	W06-1639	Get out the vote	a preliminary investigation into sentiment analysis of informal political discourse	"['T Mullen', 'R Malouf']"	related work	"With the rise of weblogs and the increasing tendency of online publications to turn to message-board style reader feedback venues, informal political discourse is becoming an important feature of the intellectual landscape of the Internet, creating a challenging and worthwhile area for experimentation in techniques for sentiment analysis. We describe preliminary statistical tests on a new dataset of political discussion group postings which indicate that posts made in direct response to other posts in a thread have a strong tendency to represent an opposing political viewpoint to the original post. We conclude that traditional text classification methods will be inadequate to the task of sentiment analysis in this domain, and that progress is to be made by exploiting information about how posters interact with each"	"There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) ."	"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) .', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']"	0
CCT230	W06-1639	Get out the vote	thumbs up sentiment classification using machine learning techniques	"['B Pang', 'L Lee', 'S Vaithyanathan']"	method	"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging."	"Fol- lowing standard practice in sentiment analysis ( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors."	"['In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5', 'Fol- lowing standard practice in sentiment analysis ( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.', 'The ind value for each speech segment s was based on the signed distanceds fromthevectorrepresentingstothe trained SVM decision plane: \x0e � ds \x0e23�4s� d s �23�4s� ds ��23�4s def ds = \x0e+ 23�4s 2 ind s;Y where 3�4s is the standard deviation of d s over all speech segments s in the debate in question, and def ind s;N = \x0e�ind s;Y .']"	5
CCT231	W06-1639	Get out the vote	automated classification of congressional legislation	"['S Purpura', 'D Hillard']"	related work	"For social science researchers, content analysis and classification of United States Congressional legislative activities have been time consuming and costly. The Library of Congress THOMAS system provides detailed information about bills and laws, but its classification system, the Legislative Indexing Vocabulary (LIV), is geared toward information retrieval instead of the pattern or historical trend recognition that social scientists value. The same event (a bill) may be coded with many subjects at the same time, with little indication of its primary emphasis. In addition, because the LIV system has not been applied to other activities, it cannot be used to compare (for example) legislative issue attention to executive, media, or public issue attention.This paper presents the Congressional Bills Project's (www.congressionalbills.org) automated classification system. This system applies a topic spotting classification algorithm to the task of coding legislative activities into one of 226 subtopic areas. The algorithm uses a traditional bag-of-words document representation, an extensive set of human coded examples, and an exhaustive topic coding system developed for use by the Congressional Bills Project and the Policy Agendas Project (www.policyagendas.org). Experimental results demonstrate that the automated system is about as effective as human assessors, but with significant time and cost savings. The paper concludes by discussing challenges to moving the system into operational use."	"Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG ) ."	"['Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG ) .']"	0
CCT232	W06-1639	Get out the vote	conditional models of identity uncertainty with application to noun coreference	"['A McCallum', 'B Wellner']"	related work	"Coreference analysis, also known as record linkage or identity uncer-tainty, is a difficult and important problem in natural language process-ing, databases, citation matching and many other tasks. This paper intro-duces several discriminative, conditional-probability models for coref-erence analysis, all examples of undirected graphical models. Unlike many historical approaches to coreference, the models presented here are relational--they do not assume that pairwise coreference decisions should be made independently from each other. Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies--paralleling the advantages of con-ditional random fields over hidden Markov models. We present positive results on noun phrase coreference in two standard text data sets."	"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) ."	"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']"	0
CCT233	W06-1639	Get out the vote	thumbs up or thumbs down semantic orientation applied to unsupervised classification of reviews	['P Turney']	introduction	"This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., ""subtle nuances"") and a negative semantic orientation when it has bad associations (e.g., ""very cavalier""). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word ""excellent"" minus the mutual information between the given phrase and the word ""poor"". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews."	"In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 ) ."	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 ) .""]"	0
CCT234	W06-1639	Get out the vote	sentiment analysis a new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified	"['A Agarwal', 'P Bhattacharyya']"	related work	"Sentiment Analysis aims at determining the overall polarity of a document, for instance, identifying whether a movie review appreciates or criticizes a movie. We present a machine learning based approach to this problem similar to text categorization. The technique is made more effective by incorporating linguistic knowledge gathered through Wordnet1 synonymy graphs. A method to improve the accuracy of classification over a set of test documents is finally given."	"Previous sentiment-analysis work in different domains has considered inter-document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links (Agrawal et al., 2003)."	"['Previous sentiment-analysis work in different domains has considered inter-document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links (Agrawal et al., 2003).']"	0
CCT235	W06-1639	Get out the vote	conditional random fields probabilistic models for segmenting and labeling sequence data	"['J Lafferty', 'A McCallum', 'F Pereira']"	related work	"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."	"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) ."	"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']"	0
CCT236	W06-1639	Get out the vote	evaluation of machine learning methods for natural language processing tasks	"['W Daelemans', 'V Hoste']"	method	"We show that the methodology currently in use for comparing symbolic supervised learning methods applied to human language technology tasks is unreliable. We show that the interaction between algorithm parameter settings and feature selection within a single algorithm often accounts for a higher variation in results than differences between different algorithms or information sources. We illustrate this with experiments on a number of linguistic datasets. The consequences of this phenomenon are far-reaching, and we discuss possible solutions to this methodological problem."	"Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 ) ."	"['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 ) .']"	3
CCT237	W06-1639	Get out the vote	analyzing research papers using citation sentences	"['W Lehnert', 'C Cardie', 'E Riloff']"	related work		"Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( #AUTHOR_TAG ) , and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 ) ."	"['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( #AUTHOR_TAG ) , and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 ) .']"	0
CCT238	W06-1639	Get out the vote	seeing stars exploiting class relationships for sentiment categorization with respect to rating scales	"['B Pang', 'L Lee']"	introduction	"We address the rating-inference problem, wherein rather than simply decide whether a review is ""thumbs up"" or ""thumbs down"", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five ""stars""). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, ""three stars"" is intuitively closer to ""four stars"" than to ""one star"".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem."	"A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) ."	"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual 4 evidence of a high likelihood of agreement be- 4 Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information.', 'For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data.', ""Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.""]"	0
CCT239	W06-1639	Get out the vote	sentiment analysis a new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified	"['A Agarwal', 'P Bhattacharyya']"	introduction	"Sentiment Analysis aims at determining the overall polarity of a document, for instance, identifying whether a movie review appreciates or criticizes a movie. We present a machine learning based approach to this problem similar to text categorization. The technique is made more effective by incorporating linguistic knowledge gathered through Wordnet1 synonymy graphs. A method to improve the accuracy of classification over a set of test documents is finally given."	"A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) ."	"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual 4 evidence of a high likelihood of agreement be- 4 Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information.', 'For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data.', ""Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.""]"	0
CCT240	W06-1639	Get out the vote	seeing stars when there aren’t many stars graphbased semisupervised learning for sentiment categorization	"['A B Goldberg', 'J Zhu']"	introduction	"We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., ""4 stars""), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training."	"A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) ."	"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual 4 evidence of a high likelihood of agreement be- 4 Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information.', 'For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data.', ""Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.""]"	0
CCT241	W06-1639	Get out the vote	learning associative markov networks	"['B Taskar', 'V Chatalbashev', 'D Koller']"	related work	"Markov networks are extensively used to model complex sequential, spatial, and relational interactions in fields as diverse as image processing, natural language analysis, and bioinformatics. However, inference and learning in general Markov networks is intractable. In this paper, we focus on learning a large subclass of such models (called associative Markov networks) that are tractable or closely approximable. This subclass contains networks of discrete variables with K labels each and clique potentials that favor the same labels for all variables in the clique. Such networks capture the ""guilt by association "" pattern of reasoning present in many domains, in which connected (""associated"") variables tend to have the same label. Our approach exploits a linear programming relaxation for the task of finding the best joint assignment in such networks, which provides an approximate quadratic program (QP) for the problem of learning a marginmaximizing Markov network. We show that for associative Markov network over binary-valued variables, this approximate QP is guaranteed to return an optimal parameterization for Markov networks of arbitrary topology. For the nonbinary case, optimality is not guaranteed, but the relaxation produces good solutions in practice. Experimental results with hypertext and newswire classification show significant advantages over standard approaches. 1"	"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) ."	"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']"	0
CCT242	W06-1639	Get out the vote	identifying agreement and disagreement in conversational speech use of bayesian networks to model pragmatic dependencies	"['M Galley', 'K McKeown', 'J Hirschberg', 'E Shriberg']"	related work	"We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work."	"More sophisticated approaches have been proposed ( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ) ."	"['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed ( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ) .', 'Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002).']"	0
CCT243	W06-1639	Get out the vote	a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts	"['B Pang', 'L Lee']"	method	"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints."	"As has been previously observed and exploited in the NLP literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs ."	"['As has been previously observed and exploited in the NLP literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']"	1
CCT244	W06-1639	Get out the vote	discriminative probabilistic models for relational data	"['B Taskar', 'P Abbeel', 'D Koller']"	related work	"In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies."	"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) ."	"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']"	0
CCT245	W06-1639	Get out the vote	electronic rulemaking new frontiers in public participation prepared for the annual meeting of the american political science association	"['S Shulman', 'D Schlosberg']"	introduction		"In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship '' ( #AUTHOR_TAG ) ."	"[""In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship '' ( #AUTHOR_TAG ) ."", 'Additionally, much media attention has been focused recently on the potential impact that Internet sites may have on politics 2 , or at least on political journalism 3 .', 'Regardless of whether one views such claims as clear-sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process.']"	0
CCT246	W06-1639	Get out the vote	yahoo for amazon extracting market sentiment from stock message boards	"['S Das', 'M Chen']"	introduction		"In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 ) ."	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 ) .']"	0
CCT247	W06-1639	Get out the vote	semisupervised learning literature survey computer sciences	['J Zhu']	related work		#AUTHOR_TAG maintains a survey of this area .	"['Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor andLafferty (2002), andJoachims (2003).', '#AUTHOR_TAG maintains a survey of this area .']"	0
CCT248	W06-1639	Get out the vote	seeing stars exploiting class relationships for sentiment categorization with respect to rating scales	"['B Pang', 'L Lee']"	related work	"We address the rating-inference problem, wherein rather than simply decide whether a review is ""thumbs up"" or ""thumbs down"", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five ""stars""). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, ""three stars"" is intuitively closer to ""four stars"" than to ""one star"".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem."	"Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003)."	"['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']"	0
CCT249	W06-1639	Get out the vote	on the computation of point of view	['W Sack']	introduction	"Previous work in AI story understanding has largely been used to build tools which can summarize stories and categorize them according to the events they describe (e.g., the technologies developed for the Message Understanding Conferences). These sorts of technologies are built around the assumptions that (1) events reported as facts in news stories should be ""understood"" as facts; (2) the style of a story, i.e., the way in which a story is told, is not of interest; and, (3) the source of a story should not influence its analysis. These assumptions are obviously unrealistic. Everyone knows that one should not believe everything in the news. But, by making these simplifying assumptions most existing story understanding systems function as gullible ""readers."" The focus of my current research is to build a less gullible story understander by encoding in it a means to recognize point of view. The techniques that I am developing will be useful, not only for information retrieval tasks which demand a search for credible stories, but also in future entertainment technologies which will be capable of finding and then assembling together into a unified presentation a set of texts or video clips to tell a story from an ensemble of points of view."	"Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , #AUTHOR_TAG , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) ."	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , #AUTHOR_TAG , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"	0
CCT250	W06-1639	Get out the vote	sentiment analysis a new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified	"['A Agarwal', 'P Bhattacharyya']"	method	"Sentiment Analysis aims at determining the overall polarity of a document, for instance, identifying whether a movie review appreciates or criticizes a movie. We present a machine learning based approach to this problem similar to text categorization. The technique is made more effective by incorporating linguistic knowledge gathered through Wordnet1 synonymy graphs. A method to improve the accuracy of classification over a set of test documents is finally given."	"As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs ."	"['As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']"	1
CCT251	W06-1639	Get out the vote	learning from labeled and unlabeled data using graph mincuts	"['A Blum', 'S Chawla']"	method	"Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data."	"Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label ."	"['The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']"	5
CCT252	W06-1639	Get out the vote	nearduplicate detection for erulemaking	"['H Yang', 'J Callan']"	related work		"Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 ) ."	"['Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 ) .']"	0
CCT253	W06-1639	Get out the vote	tracking point of view in narrative	['J M Wiebe']	introduction	"Third-person fictional narrative text is composed not only of passages that objectively narrate events, but also of passages that present characters' thoughts, perceptions, and inner states. Such passages take a character's psychological point of view. A language understander must determine the current psychological point of view in order to distinguish the beliefs of the characters from the facts of the story, to correctly attribute beliefs and other attitudes to their sources, and to understand the discourse relations among sentences. Tracking the psychological point of view is not a trivial problem, because many sentences are not explicitly marked for point of view, and whether the point of view of a sentence is objective or that of a character (and if the latter, which character it is) often depends on the context in which the sentence appears. Tracking the psychological point of view is the problem addressed in this work. The approach is to seek, by extensive examinations of naturally occurring narrative, regularities in the ways that authors manipulate point of view, and to develop an algorithm that tracks point of view on the basis of the regularities found. This paper presents this algorithm, gives demonstrations of an implemented system, and describes the results of some preliminary empirical studies, which lend support to the algorithm."	"Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and #AUTHOR_TAG ; see Esuli ( 2006 ) for an active bibliography ) ."	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and #AUTHOR_TAG ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"	0
CCT254	W06-1639	Get out the vote	learning from labeled and unlabeled data using graph mincuts	"['A Blum', 'S Chawla']"	related work	"Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data."	"Notable early papers on graph-based semisupervised learning include #AUTHOR_TAG , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) ."	"['Notable early papers on graph-based semisupervised learning include #AUTHOR_TAG , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area.']"	0
CCT255	W06-1639	Get out the vote	on the collective classification of email “speech acts”	"['V Carvalho', 'W W Cohen']"	related work	"We consider classification of email messages as to whether or not they contain certain ""email acts"", such as a request or a commitment. We show that exploiting the sequential correlation among email messages in the same thread can improve email-act classification. More specifically, we describe a new text-classification algorithm based on a dependency-network based collective classification method, in which the local classifiers are maximum entropy models based on words and certain relational features. We show that statistically significant improvements over a bag-of-words baseline classifier can be obtained for some, but not all, email-act classes. Performance improvements obtained by collective classification appears to be consistent across many email acts suggested by prior speech-act theory."	"Relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations ."	"['We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work.', 'Relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations .']"	0
CCT256	W06-1639	Get out the vote	the american congress	"['S S Smith', 'J M Roberts', 'R J Vander Wielen']"	introduction		"People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ) ."	"[""Evaluative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ) .', 'Moreover, political opinions are exsional bills and related data was launched in January 1995, when Mosaic was not quite two years old and Altavista did not yet exist.']"	0
CCT257	W06-1639	Get out the vote	collective content selection for concepttotext generation	"['R Barzilay', 'M Lapata']"	method	"A content selection component determines which information should be conveyed in the output of a natural language generation system. We present an efficient method for automatically learning content selection rules from a corpus and its related database. Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items. Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods."	"As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs ."	"['As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']"	1
CCT258	W06-1639	Get out the vote	thumbs up sentiment classification using machine learning techniques	"['B Pang', 'L Lee', 'S Vaithyanathan']"	introduction	"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging."	"In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 ) ."	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 ) .""]"	0
CCT259	W06-1639	Get out the vote	correlation clustering	"['N Bansal', 'A Blum', 'S Chawla']"	related work	"In this paper, we introduce and study the Robust-Correlation-Clustering problem: given a graph G = (V,E) where every edge is either labeled + or - (denoting similar or dissimilar pairs of vertices), and a parameter m, the goal is to delete a set D of m vertices, and partition the remaining vertices V  D into clusters to minimize the cost of the clustering, which is the sum of the number of + edges with end-points in different clusters and the number of - edges with end-points in the same cluster. This generalizes the classical Correlation-Clustering problem which is the special case when m = 0. Correlation clustering is useful when we have (only) qualitative information about the similarity or dissimilarity of pairs of points, and Robust-Correlation-Clustering equips this model with the capability to handle noise in datasets. In this work, we present a constant-factor bi-criteria algorithm for Robust-Correlation-Clustering on complete graphs (where our solution is O(1)-approximate w.r.t the cost while however discarding O(1) m points as outliers), and also complement this by showing that no finite approximation is possible if we do not violate the outlier budget. Our algorithm is very simple in that it first does a simple LP-based pre-processing to delete O(m) vertices, and subsequently runs a particular Correlation-Clustering algorithm ACNAlg [Ailon et al., 2005] on the residual instance. We then consider general graphs, and show (O(log n), O(log^2 n)) bi-criteria algorithms while also showing a hardness of alpha_MC on both the cost and the outlier violation, where alpha_MC is the lower bound for the Minimum-Multicut problem"	"Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , #AUTHOR_TAG , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) ."	"['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , #AUTHOR_TAG , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area.']"	0
CCT260	W06-1639	Get out the vote	mining the peanut gallery opinion extraction and semantic classification of product reviews	"['K Dave', 'S Lawrence', 'D M Pennock']"	introduction	"The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful."	"In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG ) ."	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG ) .""]"	0
CCT261	W06-1639	Get out the vote	using natural language processing to improve erulemaking	"['C Cardie', 'C Farina', 'T Bruce', 'E Wagner']"	related work	"This paper describes in brief Cornell's interdisciplinary eRulemaking project that was recently funded (December, 2005) by the National Science Foundation."	"Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) ."	"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']"	0
CCT262	W06-1639	Get out the vote	a computational theory of perspective and reference in narrative	"['J M Wiebe', 'W J Rapaport']"	introduction	"William J. end Shapiro, Stuart C. (1984), &quot;Quasi-lndexical Reference in Propositional Semantic Networks, &quot; Proceedings of the loth International Conference on Computational Linguistics ( COLING-84 ; Stanford Univ.) (Morristown, NJ: Assoc. for Computational Linguistics): 65-70.  Rapaport, William J. (1986), &quot;Logical Foundations for Belief Representation,&quot; Cognitiv e Science 10: 371-422.  Reiser, Brian J. (1981), &quot;Character Tracking and the Understanding of Narrative,&quot; Proceedings of the 7th International  Joint Conference on Artificial Intelligence (IJCAI-81; Van. couver) (Los Altos, CA: Morgen Kanhmmn): 209-211.  Roach, Eleanor and Lloyd, B.B. (1978). Cognition and Categorization (Hillsdale, NJ: Lawrence Erlbaum Associ- ates).  Shapiro, Smart C. (1979). &quot;The SNePS Sementic Network Processing System,&quot; in N.V. Findlet (ed.), Associative Network. v (New York: Academic): 179-203.  Shapiro, Stuart C. end Rapaport, William J. (1987). &quot;SNePS Considered as a Fully Intensional Propositional"	"Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #AUTHOR_TAG , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) ."	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #AUTHOR_TAG , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"	0
CCT263	W06-1639	Get out the vote	learning probabilistic models of relational structure	"['L Getoor', 'N Friedman', 'D Koller', 'B Taskar']"	related work	"Most real-world data is stored in relational form. In contrast, most statistical learning methods work with ""flat"" data representations, forcing us to convert our data into a form that loses much of the relational structure. The recently introduced framework of probabilistic relational models (PRMs) allows us to represent probabilistic models over multiple entities that utilize the relations between them. In this paper, we propose the use of probabilistic models not only for the attributes in a relational model, but for the relational structure itself. We propose two mechanisms for modeling structural uncertainty: reference uncertainty and existence uncertainty. We describe the appropriate conditions for using each model and present learning algorithms for each. We present experimental results showing that the learned models can be used to predict relational structure and, moreover, the observed relational structure can be used to provide better predictions for the attributes in the model."	"Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) ."	"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']"	0
CCT264	W06-1639	Get out the vote	diffusion kernels on graphs and other discrete input spaces	"['R I Kondor', 'J D Lafferty']"	related work	"The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space."	"Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , #AUTHOR_TAG , and Joachims ( 2003 ) ."	"['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , #AUTHOR_TAG , and Joachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area.']"	0
CCT265	W06-1639	Get out the vote	summarizing scientific articles experiments with relevance and rhetorical status	"['S Teufel', 'M Moens']"	related work	"In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work. We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles. We present several experiments measuring our judges' agreement on these annotations. We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories. The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field."	"Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG ) ."	"['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG ) .']"	0
CCT266	W06-1639	Get out the vote	coupling niche browsers and affect analysis for an opinion mining application	"['G Grefenstette', 'Y Qu', 'J G Shanahan', 'D A Evans']"	related work	"Newspapers generally attempt to present the news objectively. But textual affect analysis shows that many words carry positive or negative emotional charge. In this article, we show that coupling niche browsing technology and affect analysis technology allows us to create a new application that measures the slant in opinion given to public figures in the popular press."	"An exception is #AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site ."	"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is #AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site .']"	0
CCT267	W06-1639	Get out the vote	optimizing to arbitrary nlp metrics using ensemble selection	"['A Munson', 'C Cardie', 'R Caruana']"	method	"While there have been many successful applications of machine learning methods to tasks in NLP, learning algorithms are not typically designed to optimize NLP performance metrics. This paper evaluates an ensemble selection framework designed to optimize arbitrary metrics and automate the process of algorithm selection and parameter tuning. We report the results of experiments that instantiate the framework for three NLP tasks, using six learning algorithms, a wide variety of parameterizations, and 15 performance metrics. Based on our results, we make recommendations for subsequent machine-learning-based research for natural language learning."	"Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) ."	"['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']"	3
CCT268	W06-1639	Get out the vote	language processing technologies for electronic rulemaking a project highlight	"['S Shulman', 'J Callan', 'E Hovy', 'S Zavestoski']"	related work	"In this project, we are developing new text processing tools that help people perform advanced analysis of large collections of text commentary. This problem is increasingly faced by the United States federal government's regulation writers who formulate the rules and regulations that define the details of laws enacted by Congress. Our research focuses on text clustering, text searching using information retrieval, near-duplicate detection, opinion identification, stakeholder characterization, and extractive summarization, as well as the impact of such tools on the process of rulemaking itself. Versions of a Rule-Writer's Workbench will be built by Computer Science researchers at ISI and CMU, deployed annually for experimental use by our government partners, and evaluated by social science researchers from the Library and Information Science and Sociology departments at the Universities of Pittsburgh and San Francisco respectively. This three-year project started in October 2004 and is funded under the National Science Foundation's Digital Government program."	"Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) ."	"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']"	0
CCT269	W06-1639	Get out the vote	a preliminary investigation into sentiment analysis of informal political discourse	"['T Mullen', 'R Malouf']"	introduction	"With the rise of weblogs and the increasing tendency of online publications to turn to message-board style reader feedback venues, informal political discourse is becoming an important feature of the intellectual landscape of the Internet, creating a challenging and worthwhile area for experimentation in techniques for sentiment analysis. We describe preliminary statistical tests on a new dataset of political discussion group postings which indicate that posts made in direct response to other posts in a thread have a strong tendency to represent an opposing political viewpoint to the original post. We conclude that traditional text classification methods will be inadequate to the task of sentiment analysis in this domain, and that progress is to be made by exploiting information about how posters interact with each"	"For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) ."	"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .', 'Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.']"	0
CCT270	W06-1639	Get out the vote	seeing stars when there aren’t many stars graphbased semisupervised learning for sentiment categorization	"['A B Goldberg', 'J Zhu']"	related work	"We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., ""4 stars""), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training."	"Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003)."	"['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']"	0
CCT271	W06-1639	Get out the vote	directionbased text interpretation as an information access refinement	['M Hearst']	introduction		"Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) ."	"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']"	0
CCT272	W10-1758	From “disciplined subjectivity” to “taming wild thoughts”: Bion's elaboration of the analysing instrument	online largemargin training of syntactic and structural translation features	"['David Chiang', 'Yuval Marton', 'Philis Resnik']"	conclusion	"Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize. Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT. We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost. We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrase-based model: first, we simultaneously train a large number of Marton and Resnik's soft syntactic constraints, and, second, we introduce a novel structural distortion model. In both cases we obtain significant improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 Bleu on a subset of the NIST 2006 Arabic-English evaluation data."	Our plan is to implement a windowed or moving-average version of BLEU as in ( #AUTHOR_TAG ) .	"['We have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'When the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'For example, when using BLEU, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single-sentence objective functions.', 'Our plan is to implement a windowed or moving-average version of BLEU as in ( #AUTHOR_TAG ) .']"	3
CCT273	W10-2910	The effect of syntactic representation on semantic role labeling	effective use of wordnet semantics via kernelbased learning	"['Roberto Basili', 'Marco Cammisa', 'Alessandro Moschitti']"	conclusion	"Research on document similarity has shown that complex representations are not more accurate than the simple bag-of-words. Term clustering, e.g. using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective. In particular, when to extend the similarity function external prior knowledge is used, e.g. WordNet, the retrieval system decreases its performance. The critical issues here are methods and conditions to integrate such knowledge.    In this paper we propose kernel functions to add prior knowledge to learning algorithms for document classification. Such kernels use a term similarity measure based on the WordNet hierarchy. The kernel trick is used to implement such space in a balanced and statistically coherent way. Cross-validation results show the benefit of the approach for the Support Vector Machines when few training data is available."	"The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) ."	"['The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .', '(Basili et al., 2005;Bloehdorn et al., 2006).', 'However, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a;Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy.', '(ii) The latter can be further boosted by studying complex structural kernels, e.g.', '(Moschitti, 2008;Nguyen et al., 2009;Dinarelli et al., 2009).', '(iii) More specific predicate argument structures such those proposed in FrameNet, e.g.', '(Baker et al., 1998;Giuglea and Moschitti, 2004;Giuglea and Moschitti, 2006;Johansson and Nugues, 2008b) may be useful to characterize the opinion holder and the sentence semantic context.', 'Finally, while the strategy based on reranking resulted in a significant performance boost, it remains to be seen whether a higher accuracy can be achieved by developing a more sophisticated inference algorithm based on dynamic programming.', 'However, while the development of such an algorithm is an interesting problem, it will not necessarily result in a more usable system -when using a reranker, it is easy to trade accuracy for efficiency.']"	0
CCT274	W10-4005	Identifying word translations in non-parallel texts	utilizing citations of foreign words in corpusbased dictionary generation	"['Reinhard Rapp', 'Michael Zock']"	experiments	"Previous work concerned with the identification of word translations from text collections has been either based on parallel or on comparable corpora of the respective languages. In the case of comparable corpora basic dictionaries have been necessary to form a bridge between the languages under consideration. We present here a novel approach to identify word translations from a single monolingual corpus without necessarily requiring dictionaries, although, as will be shown, a dictionary can still be useful for improving the results. Our approach is based on the observation that for various reasons monolingual corpora typically contain many foreign words (for example citations). Relying on standard newsticker texts, we will show that their cooccurrence-based associations can be successfully used to identify word translations."	This contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .	"['The right column in Table 1 shows the scores if (using the product-of-ranks algorithm) four source languages are taken into account in parallel.', 'As can be seen, with an average score of 51.8 the improvement over the English only variant (50.6) is minimal.', 'This contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .', 'So this casts some doubt on these.', 'However, as English was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'This is not the case here, where we try to improve on a score of around 50 for English.', 'Remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'As this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'Therefore, perhaps we should take it as a success that the product-of-ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non-English languages was probably mostly detrimental.']"	1
CCT275	W10-4005	Identifying word translations in non-parallel texts	utilizing citations of foreign words in corpusbased dictionary generation	"['Reinhard Rapp', 'Michael Zock']"	introduction	"Previous work concerned with the identification of word translations from text collections has been either based on parallel or on comparable corpora of the respective languages. In the case of comparable corpora basic dictionaries have been necessary to form a bridge between the languages under consideration. We present here a novel approach to identify word translations from a single monolingual corpus without necessarily requiring dictionaries, although, as will be shown, a dictionary can still be useful for improving the results. Our approach is based on the observation that for various reasons monolingual corpora typically contain many foreign words (for example citations). Relying on standard newsticker texts, we will show that their cooccurrence-based associations can be successfully used to identify word translations."	"As suggested in #AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks ."	"['So far, we always computed translations to single source words.', 'However, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm.', 'As suggested in #AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .', 'So for each candidate we obtain a product of ranks.', 'We then assume that the candidate with the smallest product will be the best translation. 3', 'et us illustrate this by an example: If the given words are the variants of the word nervous in English, French, German, and Spanish, i.e. nervous, nerveux, nervös, and nervioso, and if we want to find out their translation into Italian, we would look at the association vectors of each word in our Italian target vocabulary.', 'The association strengths in these vectors need to be inversely sorted, and in each of them we will look up the positions of our four given words.', 'Then for each vector we compute the product of the four ranks, and finally sort the Italian vocabulary according to these products.', 'We would then expect that the correct Italian translation, namely nervoso, ends up in the first position, i.e. has the smallest value for its product of ranks.']"	4
CCT276	W10-4005	Identifying word translations in non-parallel texts	utilizing citations of foreign words in corpusbased dictionary generation	"['Reinhard Rapp', 'Michael Zock']"	conclusion	"Previous work concerned with the identification of word translations from text collections has been either based on parallel or on comparable corpora of the respective languages. In the case of comparable corpora basic dictionaries have been necessary to form a bridge between the languages under consideration. We present here a novel approach to identify word translations from a single monolingual corpus without necessarily requiring dictionaries, although, as will be shown, a dictionary can still be useful for improving the results. Our approach is based on the observation that for various reasons monolingual corpora typically contain many foreign words (for example citations). Relying on standard newsticker texts, we will show that their cooccurrence-based associations can be successfully used to identify word translations."	"Whereas #AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora ."	"['Whereas #AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .', 'We were able to shed some light on criteria influencing performance, such as the selection of text type and the direction of a language pair.', 'For example, it is more promising to look at occurrences of English words in a German corpus rather than the other way around.', 'Because of the special status of English it is also advisable to use it as a pivot wherever possible.']"	1
CCT277	W10-4215	Concept Type Prediction and Responsive Adaptation in a Dialogue System	extracting paraphrases from a parallel corpus	"['R Barzilay', 'K McKeown']"	conclusion	"While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases. We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text. Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases."	For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG .	"['The current work has focussed on high-level mapping rules which can be used both for generation from databases and knowledge representations and also for generation from text.', 'In future work, we will focus on mapping text (in monologue form) to dialogue.', 'For this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form.', 'For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG .', 'An important component of our future effort will be to evaluate whether automatically generating dialogues from naturally-occurring monologues, following the approach described here, results in dialogues that are fluent and coherent and preserve the information from the input monologue.']"	3
CCT278	W11-0218	Generalising semantic category disambiguation with large lexical resources for fun and profit	boosting precision and recall of dictionarybased protein name recognition	"['Y Tsuruoka', 'J Tsujii']"	conclusion	"Dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches. However, dictionary based approaches have two serious problems: (1) a large number of false recognitions mainly caused by short names. (2) low recall due to spelling variation. In this paper, we tackle the former problem by using a machine learning method to filter out false positives. We also present an approximate string searching method to alleviate the latter problem. Experimental results using the GE-NIA corpus show that the filtering using a naive Bayes classifier greatly improves precision with slight loss of recall, resulting in a much better F-score."	A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .	"['While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'For NLPBA, GENIA and ID we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'By contrast, for Super GREC there are several distinct classes for which we expected lexical resources to have fair coverage for SimString and Gazetteer features.', 'While an advantage over Internal was observed for Super GREC, SimString features showed no benefit over Gazetteer features.', 'The methods exhibited the expected result on only one of the six corpora, CALBC CII, where there is a clear advantage for Gazetteer over Internal and a further clear advantage for SimString over Gazetteer.', 'Disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'Although we have not been successful in Figure 6: Learning curve for Super GREC Figure 7: Learning curve for EPI proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'It may be that our notion of distance to lexical resource entries is too naive.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']"	3
CCT279	W14-1704	The Illinois-Columbia System in the CoNLL-2014 Shared Task	the ui system in the hoo 2012 shared task on error correction	"['A Rozovskaya', 'M Sammons', 'D Roth']"	experiments	"We describe the University of Illinois (UI) system that participated in the Helping Our Own (HOO) 2012 shared task, which focuses on correcting preposition and determiner errors made by non-native English speakers. The task consisted of three metrics: Detection, Recognition, and Correction, and measured performance before and after additional revisions to the test data were made. Out of 14 teams that participated, our system scored first in Detection and Recognition and second in Correction before the revisions; and first in Detection and second in the other metrics after revisions. We describe our underlying approach, which relates to our previous work in this area, and propose an improvement to the earlier method, error inflation, which results in significant gains in performance."	The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .	"['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles).']"	5
CCT280	W14-1704	The Illinois-Columbia System in the CoNLL-2014 Shared Task	algorithm selection and model adaptation for esl correction tasks	"['A Rozovskaya', 'D Roth']"	experiments	"We consider the problem of correcting errors made by English as a Second Language (ESL) writers and address two issues that are essential to making progress in ESL error correction - algorithm selection and model adaptation to the first language of the ESL learner.    A variety of learning algorithms have been applied to correct ESL mistakes, but often comparisons were made between incomparable data sets. We conduct an extensive, fair comparison of four popular learning methods for the task, reversing conclusions from earlier evaluations. Our results hold for different training sets, genres, and feature sets.    A second key issue in ESL error correction is the adaptation of a model to the first language of the writer. Errors made by non-native speakers exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the non-native writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods."	The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( #AUTHOR_TAG ) .	"['The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( #AUTHOR_TAG ) .', 'Thus, the classifiers trained on the learner data make use of a discriminative model.', 'Because the Google corpus does not contain complete sentences but only n-gram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in Rozovskaya and Roth (2011)).']"	4
CCT281	W14-2106	Analyzing Argumentative Discourse Units in Online Interactions	identifying agreement and disagreement in conversational speech use of bayesian networks to model pragmatic dependencies	"['Michel Galley', 'Kathleen McKeown', 'Julia Hirschberg', 'Elizabeth Shriberg']"	related work	"We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work."	"Another line of research that is correlated with ours is recognition of agreement/disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums ."	"['Another line of research that is correlated with ours is recognition of agreement/disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', 'For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy.']"	1
CCT282	W14-2106	Analyzing Argumentative Discourse Units in Online Interactions	topic independent identification of agreement and disagreement in social media dialogue	"['Amita Misra', 'Marilyn A Walker']"		"Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available. This has impacted the dialogue research community's ability to develop better theories, as well as good off the shelf tools for dialogue processing. Happily, an increasing amount of information and opinion exchange occur in natural dialogue in online forums, where people share their opinions about a vast range of topics. In particular we are interested in rejection in dialogue, also called disagreement and denial, where the size of available dialogue corpora, for the first time, offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue. In this paper, we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic independent way. Our results show that our theoretically motivated features achieve 66% accuracy, an improvement over a unigram baseline of an absolute 6%.Comment: @inproceedings{Misra2013TopicII, title={Topic Independent   Identification of Agreement and Disagreement in Social Media Dialogue},   author={Amita Misra and Marilyn A. Walker}, booktitle={SIGDIAL Conference},   year={2013}"	"In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG ) ."	"['The first row of the Table 6 represents the baseline results.', 'Though the precision is high for agreement category, the recall is quite low and that results in a poor overall F1 measure.', ""This shows that even though markers like 'agree' or 'disagree'  For the next set of experiments we used a supervised machine learning approach for the two-way classification (Agree/Disagree)."", 'We use Support Vector Machines (SVM) as our machine-learning algorithm for classification as implemented in Weka (Hall et al., 2009) and ran 10-fold cross validation.', 'As a SVM baseline, we first use all unigrams in Callout and Target as features (Table 6, Row 2).', 'We notice that the recall improves significantly when compared with the rule-based method.', 'To further improve the classification accuracy, we use Mutual Information (MI) to select the words in the Callouts and Targets that are likely to be associated with the categories Agree and Disagree, respectively.', 'Specifically, we sort each word based on its MI value and then select the first 180 words in each of the two categories to represent our new vocabulary set of 360 words.', 'The feature vector includes only words present in the MI list.', 'Compared to the all unigrams baseline, the MI-based unigrams improve the F1 by 4% (Agree) and 2% (Disagree) (Table 6).', 'The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification.', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG ) .']"	4
CCT283	W14-2106	Analyzing Argumentative Discourse Units in Online Interactions	topic independent identification of agreement and disagreement in social media dialogue	"['Amita Misra', 'Marilyn A Walker']"	related work	"Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available. This has impacted the dialogue research community's ability to develop better theories, as well as good off the shelf tools for dialogue processing. Happily, an increasing amount of information and opinion exchange occur in natural dialogue in online forums, where people share their opinions about a vast range of topics. In particular we are interested in rejection in dialogue, also called disagreement and denial, where the size of available dialogue corpora, for the first time, offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue. In this paper, we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic independent way. Our results show that our theoretically motivated features achieve 66% accuracy, an improvement over a unigram baseline of an absolute 6%.Comment: @inproceedings{Misra2013TopicII, title={Topic Independent   Identification of Agreement and Disagreement in Social Media Dialogue},   author={Amita Misra and Marilyn A. Walker}, booktitle={SIGDIAL Conference},   year={2013}"	"Another line of research that is correlated with ours is recognition of agreement/disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums ."	"['Another line of research that is correlated with ours is recognition of agreement/disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', 'For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy.']"	1
CCT284	W14-2106	Analyzing Argumentative Discourse Units in Online Interactions	identifying agreement and disagreement in conversational speech use of bayesian networks to model pragmatic dependencies	"['Michel Galley', 'Kathleen McKeown', 'Julia Hirschberg', 'Elizabeth Shriberg']"		"We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work."	"In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 ) ."	"['The first row of the Table 6 represents the baseline results.', 'Though the precision is high for agreement category, the recall is quite low and that results in a poor overall F1 measure.', ""This shows that even though markers like 'agree' or 'disagree'  For the next set of experiments we used a supervised machine learning approach for the two-way classification (Agree/Disagree)."", 'We use Support Vector Machines (SVM) as our machine-learning algorithm for classification as implemented in Weka (Hall et al., 2009) and ran 10-fold cross validation.', 'As a SVM baseline, we first use all unigrams in Callout and Target as features (Table 6, Row 2).', 'We notice that the recall improves significantly when compared with the rule-based method.', 'To further improve the classification accuracy, we use Mutual Information (MI) to select the words in the Callouts and Targets that are likely to be associated with the categories Agree and Disagree, respectively.', 'Specifically, we sort each word based on its MI value and then select the first 180 words in each of the two categories to represent our new vocabulary set of 360 words.', 'The feature vector includes only words present in the MI list.', 'Compared to the all unigrams baseline, the MI-based unigrams improve the F1 by 4% (Agree) and 2% (Disagree) (Table 6).', 'The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification.', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 ) .']"	4