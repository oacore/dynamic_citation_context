unique_id	citing_id	citing_title	cited_title	cited_authors	section_title	cited_abstract	citation_context	cite_context_paragraph	citation_class_label
CC1	A00-1009	A framework for MT and multilingual NLG systems based on uniform lexico-structural processing	a fast and portable realizer for text generation systems	"['B Lavoie', 'Rambow']"		"{PG,1 A mixer block used in rotary drums to improve the efficiency of mixing, drying, cooling, heating, or calcining of solid materials such as gravel, stone, fluxes and the like to produce a more uniform product with minimal production of fines and dust is described. The mixer block is especially useful when used as part of the refractory lining in a rotary kiln to calcine fluxstone such as limestone, dolomite, dolomitic limestone, magnesite and the like."	The framework was originally developed for the realization of deep-syntactic structures in NLG ( #AUTHOR_TAG ) .	"['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al., 1992), and JOYCE (Rambow and Korelsky, 1992).', 'The framework was originally developed for the realization of deep-syntactic structures in NLG ( #AUTHOR_TAG ) .']"	0
CC2	A00-1009	A framework for MT and multilingual NLG systems based on uniform lexico-structural processing	a fast and portable realizer for text generation systems	"['B Lavoie', 'Rambow']"		"{PG,1 A mixer block used in rotary drums to improve the efficiency of mixing, drying, cooling, heating, or calcining of solid materials such as gravel, stone, fluxes and the like to produce a more uniform product with minimal production of fines and dust is described. The mixer block is especially useful when used as part of the refractory lining in a rotary kiln to calcine fluxstone such as limestone, dolomite, dolomitic limestone, magnesite and the like."	"Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #AUTHOR_TAG ) ."	"['The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991 In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997).', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #AUTHOR_TAG ) .']"	2
CC3	A00-1009	A framework for MT and multilingual NLG systems based on uniform lexico-structural processing	machine translation divergences a formal description and proposed solution	['B J Don']		"There are many cases in which the natural translation of one language into another results in a very different form than that of the original. The existence of translation divergences (i.e., crosslinguistic distinctions) makes the straightforward transfer from source structures into target structures impractical. Many existing translation systems have mechanisms for handling divergent structures but do not provide a general procedure that takes advantage of takes advantage of the systematic relation between lexical-semantic structure and syntactic structure. This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved. This formalization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system."	"More details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 ) ."	"['--o II a failli pleuvoir.', 'More details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 ) .']"	0
CC4	A00-1009	A framework for MT and multilingual NLG systems based on uniform lexico-structural processing	applied text generation	['T Korelsky']	introduction	"While we were able to exploit existing research for many of the design issues, it turned out that we needed to develop our own approach to text planning (Rambow 1990). This paper will present the system and a t tempt to show how these design objectives led to particular design decisions. The structure of the paper is as follows. In Section 2, we will present the underlying application and give examples of the output of the System. In Section 3, we will discuss the overall s tructure of Joyce. We then discuss the three main components in turn: the text planner in Section 4, the sentence planner in Section 5 and the realizer in Section 6. We will discuss the text planner in some detail since it represents a new approach to the problem. Section 7 traces the generation of a short text. In Section 8, we address the problem of portability, and wind up by discussing some shortcomings of Joyce in the conclusion."	"Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and #AUTHOR_TAG ) , and LFS ( Iordanskaja et al. , 1992 ) ."	"['In this paper we present a linguistically motivated framework for uniform lexicostructural processing.', 'It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).', 'Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and #AUTHOR_TAG ) , and LFS ( Iordanskaja et al. , 1992 ) .', 'Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT.']"	2
CC5	A00-1009	A framework for MT and multilingual NLG systems based on uniform lexico-structural processing	applied text generation	['T Korelsky']		"While we were able to exploit existing research for many of the design issues, it turned out that we needed to develop our own approach to text planning (Rambow 1990). This paper will present the system and a t tempt to show how these design objectives led to particular design decisions. The structure of the paper is as follows. In Section 2, we will present the underlying application and give examples of the output of the System. In Section 3, we will discuss the overall s tructure of Joyce. We then discuss the three main components in turn: the text planner in Section 4, the sentence planner in Section 5 and the realizer in Section 6. We will discuss the text planner in some detail since it represents a new approach to the problem. Section 7 traces the generation of a short text. In Section 8, we address the problem of portability, and wind up by discussing some shortcomings of Joyce in the conclusion."	"The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and #AUTHOR_TAG ) ."	"['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and #AUTHOR_TAG ) .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG .']"	2
CC6	A00-1012	Experiments on sentence boundary detection	a simple rulebased part of speech tagger	['E Brill']		"Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rulebased tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below. 1. I N T R O D U C T I O N There has been a dramat ic increase in the application of probabilistic models to natural language processing over the last few years. The appeal of stochastic techniques over traditional rule-based techniques comes from the ease with which the necessary statistics can be automatically acquired and the fact tha t very little handcrafted knowledge need be built into the system. In contrast, the rules in rule-based systems are usually difficult to construct and are typically not very robust. One area in which the statistical approach has done particularly well is automat ic par t of speech tagging, assigning each word in an input sentence its proper part of speech [1, 2, 3, 4, 6, 9, 11, 12]. Stochastic taggers have *A version of this paper appears in Proceedings of the Third Conference on Applied Computational Linguistics (ACL), Trento, Italy, 1992. Used by permission of the Association for Computational Linguistics; copies of the publication from which this material is derived can can be obtained from Dr. Donald E. Walker (ACL), Bellcore, MRE 2A379, 445 South Street, Box 1910, Morristown, NJ 07960-1910, USA. The author would like to thank Mitch Marcus and Rich Pito for valuable input. This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO grant No. DAAL 03-89-G0031 PRI. 112 obtained a high degree of accuracy without performing any syntactic analysis on the input. These stochastic par t of speech taggers make use of a Markov model which captures lexical and contextual information. The parameters of the model can be est imated from tagged [1, 3, 4, 6, 12] or untagged [2, 9, 11] text. Once the parameters of the model are est imated, a sentence can then be automatical ly tagged by assigning it the tag sequence which is assigned the highest probabil i ty by the model. Performance is often enhanced with the aid of various higher level preand postprocessing procedures or by manually tuning the model. A number of rule-based taggers have been built [10, 7, 8]. [10] and [7] both have error rates substantial ly higher than s tate of the art stochastic taggers. [8] disambiguates words within a deterministic parser. We wanted to determine whether a simple rule-based tagger without any knowledge of syntax can perform as well as a stochastic tagger, or if par t of speech tagging really is a domain to which stochastic techniques are bet ter suited. In this paper we describe a rule-based tagger which performs as well as taggers based upon probabilistic models. The rule-based tagger overcomes the limitations common in rule-based approaches to language processing: it is robust, and the rules are automatical ly acquired. In addition, the tagger has many advantages over stochastic taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules as opposed to the large tables of statistics needed for stochastic taggers, ease of finding and implementing improvements to the tagger, and bet ter portabil i ty from one tag set or corpus genre to another. 2. T H E T A G G E R The tagger works by automatical ly recognizing and remedying its weaknesses, thereby incrementally improving its performance. The tagger initially tags by assigning each word its most likely tag, est imated by examining a large tagged corpus, without regard to context. In both sentences below, run would be tagged as a verb: Report Documentation Page Form Approved OMB No. 0704-0188 Public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden, to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington VA 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to a penalty for failing to comply with a collection of information if it does not display a currently valid OMB control number."	Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence .	"['Figure 1: Example text shown in standard and ASR format ation which is not available in ASR output is sentence boundary information.', 'However, knowledge of sentence boundaries is required by many NLP technologies.', ""Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence .""]"	0
CC7	A00-1020	Multilingual coreference resolution	knowledgelean coreference resolution and its relation to textual cohesion and coherence	"['Sanda M Harabagiu', 'Steve J Maiorano']"	introduction	"In this paper we present a new empirical method for coreference resolution, implemented in the COCKTAIL system. The resuits of COCKTAIL are used for lightweight abduction of cohesion and coherence structures. We show that referential cohesion can be integrated with lexical cohesion to produce pragmatic knowledge. Upon this knowledge coherence abduction takes place. I M o t i v a t i o n Coreference evaluation was introduced as a new domain-independent task at the 6th Message Understandi~ Conference (MUC-6) in 1995. The task focused on a subset of coreference, namely the ide~tiQ/ coreference, established between nouns, pronouns and noun phrases (including proper names) that refer to the same entity. In d ~ ; , ~ the coreference task (d. (Hirschnum and Chinchor, 1997)) special care was taken to use the coreference output not only for supporting Information Extraction(IE), the central task of the MUCs, but also to create means for re .arch on corefea~mce and discourse p h e n o m ~ independent of IE. Annotated corpora were made available, using SGML tagging with~, the text stream. The annotated texts served as tralz~g examples for a variety. of corderence resolution methods, that had to focus not only on precision and recall, but also on robustness. Two general classes of approaches were distinguished. The first class is characterized by adaptations of previously known reference algon'thms (e.g. (Lappin and Leass, 1994), (Brennan et al., 1987)) the scarce syntactic and semantic knowledge available m an w. system (e.g. (Kameyama, 1997)). The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations (e.g. (Aone and Bennett, 1994) (Kehler, 1997)). * In the past two MUC competitions, the high scoring systems achieved a recall in the high 50's to low 60's and a precision in the low 70's (d. (Hirschman et al., 1998)). A study z of the contribution of each form of coreference to the overall performance shows that generally, proper name anaphora resolution have the highest precision (69%), followed by pronominal reference (62%). T h e worse .precision is obtained by the resolution of d~_ n!te nominals anaphors (46%). However, these results need to be contrasted with the distribution of coreferential links on the tagged corpora. The majority of coreference links (38.42%) connect names Of people, organizations or locations. In addition, 19.68% of the tagged c o ~ c e links are accounted by appositives. Only 16.35% of the tagged coreferences are pronominal. Nominal anaphors account for 25.55% of the coreference links, and their resolution is generally poorly represented in IE systems. Due to the distribution of coreference links in newswire texts, a coreference module that is merely capable of handling recognition of appositives with high precision and incorporates rules of name alias identification can achieve a baseline coreference precision up to 58.1%, without sophisticated syntactic or discourse information. Precision increase is obtained by extending lfigh-performance pronoun resolution methods (e.g. (Lappin and Leass, 1994)) to nominal corderence as well. Such enhancements rely on semantic and discourse knowledge. In this paper we describe COCKTAIL, a highperformance coreference resolution system that operatas on a mixture of heuristics that combine semantic and discourse information. The resulting tThe study, reported in (Kameyama, 1997), was performed on the coreference module of SRI's FASTUS (Appelt et al., I993), an IE system representative of today's IE technology."	"SWIZZLE is a multilingual enhancement of COCKTAIL ( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  ."	"['For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver.', 'SWIZZLE is a multilingual enhancement of COCKTAIL ( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  .', 'When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.', 'When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference.', 'Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents.']"	2
CC8	A00-1020	Multilingual coreference resolution	robust pronoun resolution with limited knowledge	['Ruslan Mitkov']		"Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and time-consuming task. This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the success rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be successfully adapted for other languages with minimum modifications."	"Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #AUTHOR_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) ."	"['Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques.', 'Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge.', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #AUTHOR_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) .', 'For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.', 'For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference.', 'This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).', 'These constraints are implemented as a set of heuristics ordered by their priority.', 'Moreover, the COCKTAILframework uniformly addresses the prob- lem of interaction between different forms of coref- erence, thus making the extension to multilingual coreference very natural.']"	0
CC9	A00-1020	Multilingual coreference resolution	knowledgelean coreference resolution and its relation to textual cohesion and coherence	"['Sanda M Harabagiu', 'Steve J Maiorano']"		"In this paper we present a new empirical method for coreference resolution, implemented in the COCKTAIL system. The resuits of COCKTAIL are used for lightweight abduction of cohesion and coherence structures. We show that referential cohesion can be integrated with lexical cohesion to produce pragmatic knowledge. Upon this knowledge coherence abduction takes place. I M o t i v a t i o n Coreference evaluation was introduced as a new domain-independent task at the 6th Message Understandi~ Conference (MUC-6) in 1995. The task focused on a subset of coreference, namely the ide~tiQ/ coreference, established between nouns, pronouns and noun phrases (including proper names) that refer to the same entity. In d ~ ; , ~ the coreference task (d. (Hirschnum and Chinchor, 1997)) special care was taken to use the coreference output not only for supporting Information Extraction(IE), the central task of the MUCs, but also to create means for re .arch on corefea~mce and discourse p h e n o m ~ independent of IE. Annotated corpora were made available, using SGML tagging with~, the text stream. The annotated texts served as tralz~g examples for a variety. of corderence resolution methods, that had to focus not only on precision and recall, but also on robustness. Two general classes of approaches were distinguished. The first class is characterized by adaptations of previously known reference algon'thms (e.g. (Lappin and Leass, 1994), (Brennan et al., 1987)) the scarce syntactic and semantic knowledge available m an w. system (e.g. (Kameyama, 1997)). The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations (e.g. (Aone and Bennett, 1994) (Kehler, 1997)). * In the past two MUC competitions, the high scoring systems achieved a recall in the high 50's to low 60's and a precision in the low 70's (d. (Hirschman et al., 1998)). A study z of the contribution of each form of coreference to the overall performance shows that generally, proper name anaphora resolution have the highest precision (69%), followed by pronominal reference (62%). T h e worse .precision is obtained by the resolution of d~_ n!te nominals anaphors (46%). However, these results need to be contrasted with the distribution of coreferential links on the tagged corpora. The majority of coreference links (38.42%) connect names Of people, organizations or locations. In addition, 19.68% of the tagged c o ~ c e links are accounted by appositives. Only 16.35% of the tagged coreferences are pronominal. Nominal anaphors account for 25.55% of the coreference links, and their resolution is generally poorly represented in IE systems. Due to the distribution of coreference links in newswire texts, a coreference module that is merely capable of handling recognition of appositives with high precision and incorporates rules of name alias identification can achieve a baseline coreference precision up to 58.1%, without sophisticated syntactic or discourse information. Precision increase is obtained by extending lfigh-performance pronoun resolution methods (e.g. (Lappin and Leass, 1994)) to nominal corderence as well. Such enhancements rely on semantic and discourse knowledge. In this paper we describe COCKTAIL, a highperformance coreference resolution system that operatas on a mixture of heuristics that combine semantic and discourse information. The resulting tThe study, reported in (Kameyama, 1997), was performed on the coreference module of SRI's FASTUS (Appelt et al., I993), an IE system representative of today's IE technology."	Details of the top performing heuristics of COCKTAIL were reported in ( #AUTHOR_TAG ) .	"['The third class of heuristics resolves coreference by coercing nominals.', 'Sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'On other occasions, coercions are obtained as paths of meronyms (e.g. is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'Table 1 lists the top performing heuristics of COCKTAILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table 2.', 'Details of the top performing heuristics of COCKTAIL were reported in ( #AUTHOR_TAG ) .']"	0
CC10	A00-1020	Multilingual coreference resolution	knowledgelean coreference resolution and its relation to textual cohesion and coherence	"['Sanda M Harabagiu', 'Steve J Maiorano']"		"In this paper we present a new empirical method for coreference resolution, implemented in the COCKTAIL system. The resuits of COCKTAIL are used for lightweight abduction of cohesion and coherence structures. We show that referential cohesion can be integrated with lexical cohesion to produce pragmatic knowledge. Upon this knowledge coherence abduction takes place. I M o t i v a t i o n Coreference evaluation was introduced as a new domain-independent task at the 6th Message Understandi~ Conference (MUC-6) in 1995. The task focused on a subset of coreference, namely the ide~tiQ/ coreference, established between nouns, pronouns and noun phrases (including proper names) that refer to the same entity. In d ~ ; , ~ the coreference task (d. (Hirschnum and Chinchor, 1997)) special care was taken to use the coreference output not only for supporting Information Extraction(IE), the central task of the MUCs, but also to create means for re .arch on corefea~mce and discourse p h e n o m ~ independent of IE. Annotated corpora were made available, using SGML tagging with~, the text stream. The annotated texts served as tralz~g examples for a variety. of corderence resolution methods, that had to focus not only on precision and recall, but also on robustness. Two general classes of approaches were distinguished. The first class is characterized by adaptations of previously known reference algon'thms (e.g. (Lappin and Leass, 1994), (Brennan et al., 1987)) the scarce syntactic and semantic knowledge available m an w. system (e.g. (Kameyama, 1997)). The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations (e.g. (Aone and Bennett, 1994) (Kehler, 1997)). * In the past two MUC competitions, the high scoring systems achieved a recall in the high 50's to low 60's and a precision in the low 70's (d. (Hirschman et al., 1998)). A study z of the contribution of each form of coreference to the overall performance shows that generally, proper name anaphora resolution have the highest precision (69%), followed by pronominal reference (62%). T h e worse .precision is obtained by the resolution of d~_ n!te nominals anaphors (46%). However, these results need to be contrasted with the distribution of coreferential links on the tagged corpora. The majority of coreference links (38.42%) connect names Of people, organizations or locations. In addition, 19.68% of the tagged c o ~ c e links are accounted by appositives. Only 16.35% of the tagged coreferences are pronominal. Nominal anaphors account for 25.55% of the coreference links, and their resolution is generally poorly represented in IE systems. Due to the distribution of coreference links in newswire texts, a coreference module that is merely capable of handling recognition of appositives with high precision and incorporates rules of name alias identification can achieve a baseline coreference precision up to 58.1%, without sophisticated syntactic or discourse information. Precision increase is obtained by extending lfigh-performance pronoun resolution methods (e.g. (Lappin and Leass, 1994)) to nominal corderence as well. Such enhancements rely on semantic and discourse knowledge. In this paper we describe COCKTAIL, a highperformance coreference resolution system that operatas on a mixture of heuristics that combine semantic and discourse information. The resulting tThe study, reported in (Kameyama, 1997), was performed on the coreference module of SRI's FASTUS (Appelt et al., I993), an IE system representative of today's IE technology."	"For this research , we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference ."	"['Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques.', 'Traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge.', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf.', '(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).', 'For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.', 'For this research , we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference .', 'This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g.', 'subjects of communication verbs are more likely to refer to the last person mentioned in the text).', 'These constraints are implemented as a set of heuristics ordered by their priority.', 'Moreover, the COCKTAIL framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural.']"	5
CC11	A00-1024	Categorizing unknown words	language identification with confidence limits	['D Elworthy']	experiments	"A statistical classification algorithm and its application to language identification from noisy input are described. The main innovation is to compute confidence limits on the classification, so that the algorithm terminates when enough evidence to make a clear decision has been made, and so avoiding problems with categories that have similar characteristics. A second application, to genre identification, is briefly ex- amined. The results show that some of the problems of other language identification tech- niques can be avoided, and illustrate a more important point: that a statistical language process can be used to provide feedback about its own success rate"	"Each component will return a confidence measure of the reliability of its prediction , c.f. ( #AUTHOR_TAG ) ."	"['To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f. ( #AUTHOR_TAG ) .', 'The results from each component are evaluated to determine the final category of the word.']"	4
CC12	A00-1024	Categorizing unknown words	a stochastic parts program and noun phrase parser for unrestricted text	['K Church']	experiments	A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>	We use an in-house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .	"['The first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns.']"	5
CC13	A00-1024	Categorizing unknown words	detecting and correcting morphosyntactic errors in real texts	['T Vosse']			Research that is more similar in goal to that outlined in this paper is Vosse ( #AUTHOR_TAG ) .	"['Research that is more similar in goal to that outlined in this paper is Vosse ( #AUTHOR_TAG ) .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Capitalization is his sole means of identifying names.', 'However, capitalization information is not available in closed captions.', 'Hence, his system would be ineffective on the closed caption domain with which we are working.', '(Granger, 1983) uses expectations generated by scripts to anMyze unknown words.', 'The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-shore messages.']"	1
CC14	A00-1024	Categorizing unknown words	detecting and correcting morphosyntactic errors in real texts	['T Vosse']	experiments		Corpus frequency : ( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .	"['Corpus frequency : ( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', 'Our corpus frequency variable specifies the frequency of each unknown word in a 2.6 million word corpus of business news closed captions.']"	5
CC15	A00-2028	An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email	automatic detection of poor speech recognition at the dialogue level	"['D J Litman', 'M A Walker', 'M J Kearns']"	conclusion	"The dialogue strategies used by a spoken dialogue system strongly influence performance and user satisfaction. An ideal system would not use a single fixed strategy, but would adapt to the circumstances at hand. To do so, a system must be able to identify dialogue properties that suggest adaptation. This paper focuses on identifying situations where the speech recognizer is performing poorly. We adopt a machine learning approach to learn rules from a dialogue corpus for identifying these situations. Our results show a significant improvement over the baseline and illustrate that both lower-level acoustic features and higher-level dialogue features can affect the performance of the learning algorithm."	Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ) .	"['The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.', 'Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ) .', 'However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans.', 'In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime.']"	2
CC16	A00-2028	An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email	automatic detection of poor speech recognition at the dialogue level	"['D J Litman', 'M A Walker', 'M J Kearns']"		"The dialogue strategies used by a spoken dialogue system strongly influence performance and user satisfaction. An ideal system would not use a single fixed strategy, but would adapt to the circumstances at hand. To do so, a system must be able to identify dialogue properties that suggest adaptation. This paper focuses on identifying situations where the speech recognizer is performing poorly. We adopt a machine learning approach to learn rules from a dialogue corpus for identifying these situations. Our results show a significant improvement over the baseline and illustrate that both lower-level acoustic features and higher-level dialogue features can affect the performance of the learning algorithm."	The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG ) .	"['The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num.confirms), and number of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percentconfirms, percent-subdials).', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG ) .']"	4
CC17	D08-1004	Modeling annotators	thumbs up sentiment classification using machine learning techniques	"['B Pang', 'L Lee', 'S Vaithyanathan']"	method	"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging."	"We use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) ."	"['where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.', 'Thus θ ∈ R 17744 , and positive weights in θ favor class label y = +1 and equally discourage y = −1, while negative weights do the opposite.', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2.']"	5
CC18	D08-1006	Refining generative language models using discriminative learning	classbased ngram models of natural language	"['F Brown', 'Vincent J Della Pietra', 'Peter V deSouza', 'Jenifer C Lai', 'Robert L Mercer']"	conclusion		"In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 ) , class n-grams ( #AUTHOR_TAG ) , grammatical features ( Amaya and Benedy , 2001 ) , etc ' ."	"['The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach.', ""In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 ) , class n-grams ( #AUTHOR_TAG ) , grammatical features ( Amaya and Benedy , 2001 ) , etc ' .""]"	3
CC19	D08-1006	Refining generative language models using discriminative learning	a discriminative language model with pseudonegative samples	"['Daisuke Okanohara', ""Jun'ichi Tsujii""]"			"Unfortunately , as shown in ( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) ."	"['Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.', 'Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data.', 'Đn both cases essentially linear classifiers were used as features.', 'As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples.', 'Unfortunately , as shown in ( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', 'While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data.', 'This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.', 'For this reason we use a different sampling scheme which we refer to as rejection sampling.', ""This allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn't too far removed from the baseline.""]"	4
CC20	D08-1006	Refining generative language models using discriminative learning	a discriminative language model with pseudonegative samples	"['Daisuke Okanohara', ""Jun'ichi Tsujii""]"	experiments		"As shown in ( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences ."	"['For our features we used large-margin classifiers trained using the online algorithm described in (Crammer et al., 2006).', 'The code for the classifier was generously provided by Daisuke Okanohara.', 'This code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'As shown in ( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .', 'Therefore, we used a 3rd order polynomial kernel, which was found to give good results.', 'No special effort was otherwise made in order to optimize the parameters of the classifiers.']"	4
CC21	D08-1006	Refining generative language models using discriminative learning	an empirical study of smoothing techniques for language modeling	['Goodman']	experiments	"We present a tutorial introduction to n-gram models for language modeling and survey the most widely-used smoothing algorithms for such models. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980), Katz (1987), Bell, Cleary, and Witten (1990), Ney, Essen, and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g., Brown versus Wall Street Journal), count cutoffs, and n-gram order (bigram versus trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. Our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance. We introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser-Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.Engineering and Applied Science"	"Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models ."	"['Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .', '2. Sentence representation: Each sentence was represented as the collection of unigrams, bigrams and trigrams it contained.', 'A coordinate was reserved for each such n-gram which appeared in the data, whether real or sampled.', ""The value of the n'th coordinate in the vector representation of Interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0.0001."", 'This indicates that satisfying the constraint (18) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated.']"	5
CC22	D08-1010	Maximum entropy based rule selection model for syntax-based statistical machine translation	treetostring alignment template for statistical machine translation	"['Yang Liu', 'Qun Liu', 'Shouxun Lin']"	experiments	"We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models."	The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG ) .	"['The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG ) .', 'When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.', 'Then we use the toolkit implemented by Zhang (2004) to train MERS models for the ambiguous source syntactic trees separately.', 'We set the iteration number to 100 and Gaussian prior to 1.']"	2
CC23	D08-1016	Dependency parsing by belief propagation	coarsetofine nbest parsing and maxent discriminative reranking	"['E Charniak', 'M Johnson']"	conclusion		"For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) ."	"['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006).']"	3
CC24	D08-1016	Dependency parsing by belief propagation	forest reranking discriminative parsing with nonlocal features	['L Huang']	conclusion	"Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank."	"For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) ."	"['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006).']"	3
CC25	D08-1016	Dependency parsing by belief propagation	integrating graphbased and transitionbased dependency parsers	"['J Nivre', 'R McDonald']"	conclusion		"For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( #AUTHOR_TAG ) ."	"['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( #AUTHOR_TAG ) .', 'We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006).']"	3
CC26	D08-1016	Dependency parsing by belief propagation	probabilistic cfg with latent annotations	"['T Matsuzaki', 'Y Miyao', 'J Tsujii']"	conclusion	"This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine-grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences <= 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection."	"We could also introduce new variables , e.g. , nonterminal refinements ( #AUTHOR_TAG ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) ."	"['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005;Huang, 2008) and history-based parsing (Nivre and McDonald, 2008).', 'We could also introduce new variables , e.g. , nonterminal refinements ( #AUTHOR_TAG ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) .']"	3
CC27	D08-1022	Forest-based translation rule extraction	forestbased translation	"['Haitao Mi', 'Liang Huang', 'Qun Liu']"	introduction		"Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses ."	"['We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists.', 'Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', 'When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date.']"	2
CC28	D08-1022	Forest-based translation rule extraction	forestbased translation	"['Haitao Mi', 'Liang Huang', 'Qun Liu']"	related work		The first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .	"['The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).', 'The first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .', 'This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding.']"	2
CC29	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	using predicateargument structures for information extraction	"['Mihai Surdeanu', 'Sanda Harabagiu', 'John Williams', 'Paul Aarseth']"	introduction	"In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures. We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm. It is based on: (1) an extended set of features; and (2) inductive decision tree learning. The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results."	"Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) ."	"['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']"	0
CC30	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	automatic semantic role labeling for chinese verbs	"['Nianwen Xue', 'Martha Palmer']"	experiments	"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling."	"To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , Xue ( 2008 ) ."	"['To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , Xue ( 2008 ) .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.', 'From the table 6, we can find that our system is better than both of the related systems.', 'Our system has outperformed Xue (2008) with a relative error reduction rate of 9.8%.']"	1
CC31	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	question answering based on semantic structures	"['Srini Narayanan', 'Sanda Harabagiu']"	introduction	The ability to answer complex questions posed in Natural Language depends on (1) the depth of the available semantic representations and (2) the inferential mechanisms they support. In this paper we describe a QA architecture where questions are analyzed and candidate answers generated by 1) identifying predicate argument structures and semantic frames from the input and 2) performing structured probabilistic inference using the extracted relations in the context of a domain and scenario model. A novel aspect of our system is a scalable and expressive representation of actions and events based on Coordinated Probabilistic Relational Models (CPRM). In this paper we report on the ability of the implemented system to perform several forms of probabilistic and temporal inferences to extract answers to complex questions. The results indicate enhanced accuracy over current state-of-the-art Q/A systems.	"Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) ."	"['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']"	0
CC32	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	the specification of the semantic knowledgebase of contemporary chinese	"['Hui Wang', 'Weidong Zhan', 'Shiwen Yu']"			The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG ) .	"['SemCat (semantic category) of predicate, Sem-Cat of first word, SemCat of head word, SemCat of last word, SemCat of predicate + SemCat of first word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word.', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG ) .']"	5
CC33	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	calibrating features for semantic role labeling	"['Nianwen Xue', 'Martha Palmer']"	introduction	"This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited. We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed. We further show that different features are needed for different subtasks. Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis."	#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .	"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', '#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']"	0
CC34	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	annotating the propositions in the penn chinese treebank	"['Nianwen Xue', 'Martha Palmer']"	introduction	"In this paper, we describe an approach to annotate the propositions in the Penn Chinese Treebank. We describe how diathesis alternation patterns can be used to make coarse sense distinctions for Chinese verbs as a necessary step in annotating the predicate-structure of Chinese verbs. We then discuss the representation scheme we use to label the semantic arguments and adjuncts of the predicates. We discuss several complications for this type of annotation and describe our solutions. We then discuss how a lexical database with predicate-argument structure information can be used to ensure consistent annotation. Finally, we discuss possible applications for this resource."	"After the PropBank ( #AUTHOR_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL ."	"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank ( #AUTHOR_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL .', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']"	0
CC35	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	automatic semantic role labeling for chinese verbs	"['Nianwen Xue', 'Martha Palmer']"	introduction	"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling."	"Experiments on Chinese SRL ( #AUTHOR_TAG , Xue 2008 ) reassured these findings ."	"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( #AUTHOR_TAG , Xue 2008 ) reassured these findings .']"	4
CC36	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	semantic argument classification exploiting argument interdependence	"['Zheng Ping Jiang', 'Jia Li', 'Hwee Tou Ng']"		"This paper describes our research on automatic semantic argument classification, using the PropBank data [Kingsbury et al., 2002]. Previous research employed features that were based either on a full parse or shallow parse of a sentence. These features were mostly based on an individual semantic argument and the relation between the predicate and a semantic argument, but they did not capture the interdependence among all arguments of a predicate. In this paper, we propose the use of the neighboring semantic arguments of a predicate as additional features in determining the class of the current semantic argument. Our experimental results show significant improvement in the accuracy of semantic argument classification after exploiting argument interdependence. Argument classification accuracy on the standard Section 23 test set improves to 90.50%, representing a relative error reduction of 18%."	#AUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .	"['#AUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .', 'It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', 'We can use window size to represent the scope of the context.', 'Window size [-m, n] means that, in the sequence that all the arguments has constructed, the features of previous m and following n argu- ments will be utilized for the classification of cur- rent semantic role.', 'There are two kinds of argu- ment sequences in Jiang et al. (2005), and we only test the linear sequence.', 'Take the sentence in fig- ure 1 as an example.', 'The linear sequence of the arguments in this sentence is: ____(until then), ____ (the insurance company), _ (has), _ ____ (for the Sanxia Project), ____ (in- surance services).', 'For the argument _ (has), if the semantic context window size is [-1,2], the seman- tic context features e.g. headword, phrase type and etc. of ____ (the insurance company), __ ___ (for the Sanxia Project) and ____ (insurance services) will be utilized to serve the classification task of _ (has).']"	5
CC37	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	can semantic roles generalize across genres	"['Szu-ting Yi', 'Edward Loper', 'Martha Palmer']"	conclusion	"PropBank has been widely used as training data for Semantic Role Labeling. However, because this training data is taken from the WSJ, the resulting machine learning models tend to overfit on idiosyncrasies of that text's style, and do not port well to other genres. In addition, since PropBank was designed on a verb-by-verb basis, the argument labels Arg2 - Arg5 get used for very diverse argument roles with inconsistent training instances. For example, the verb ""make"" uses Arg2 for the ""Material"" argument; but the verb ""multiply"" uses Arg2 for the ""Extent"" argument. As a result, it can be difficult for automatic classifiers to learn to distinguish arguments Arg2-Arg5. We have created a mapping between PropBank and VerbNet that provides a VerbNet thematic role label for each verb-specific PropBank label. Since VerbNet uses argument labels that are more consistent across verbs, we are able to demonstrate that these new labels are easier to learn."	#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .	"['Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', 'However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank.', 'What if we could extend the idea of hierarchical architecture to the single semantic role level?', 'Would that help the improvement of SRC?']"	1
CC38	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	shallow semantic parsing of chinese	"['Honglin Sun', 'Daniel Jurafsky']"	introduction	"In this paper we address the question of assigning semantic roles to sentences in Chinese. We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set. In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix. Finally, we compare English and Chinese semantic-parsing performance. While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese. We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese."	"Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , Xue and Palmer ( 2005 ) and Xue ( 2008 ) ."	"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , Xue and Palmer ( 2005 ) and Xue ( 2008 ) .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']"	0
CC39	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	labeling chinese predicates with semantic roles	['Nianwen Xue']	experiments	"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1"	"To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #AUTHOR_TAG ."	"['To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #AUTHOR_TAG .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.', 'From the table 6, we can find that our system is better than both of the related systems.', 'Our system has outperformed Xue (2008) with a relative error reduction rate of 9.8%.']"	1
CC40	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	hierarchical semantic role labeling	"['Alessandro Moschitti', 'Ana-Maria Giuglea', 'Bonaventura Coppola', 'Roberto Basili']"		"We present a four-step hierarchical SRL strategy which generalizes the classical two-level approach (boundary detection and classification). To achieve this, we have split the classification step by grouping together roles which share linguistic properties (e.g. Core Roles versus Adjuncts). The results show that the non-optimized hierarchical approach is computationally more efficient than the traditional systems and it preserves their accuracy."	"be found in figure 2 , which is similar with that in #AUTHOR_TAG ."	"['Previous semantic role classifiers always did the classification problem in one-step.', 'However, in this paper, we did SRC in two steps.', 'The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in #AUTHOR_TAG .']"	1
CC41	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	labeling chinese predicates with semantic roles	['Nianwen Xue']		"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1"	"Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #AUTHOR_TAG ."	"['Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #AUTHOR_TAG .']"	5
CC42	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	bilingual framenet dictionaries for machine translation	['Hans C Boas']	introduction	"This paper describes issues surrounding the planning and design of GermanFrameNet (GFN), a counterpart to the English-based FrameNet project. The goals of GFN are (a) to create lexical entries for German nouns, verbs, and adjectives that correspond to existing FrameNet entries, and (b) to link the parallel lexicon fragments by means of common semantic frames and numerical indexing mechanisms. GFN will take a fine-grained approach towards polysemy that seeks to split word senses based on the semantic frames that underlie their analysis. The parallel lexicon fragments represent an important step towards capturing valuable information about the different syntactic realizations of frame semantic concepts across languages, which is relevant for information retrieval, machine translation, and language generation. 1"	"Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #AUTHOR_TAG ) ."	"['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #AUTHOR_TAG ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']"	0
CC43	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	labeling chinese predicates with semantic roles	['Nianwen Xue']	experiments	"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1"	"We use the same data setting with #AUTHOR_TAG , however a bit different from Xue and Palmer ( 2005 ) ."	"['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'to chtb_931.fid,', 'or the first 250K words of the Chinese TreeBank 5.1.', 'For the experiments, the data of PropBank is divided into three parts.', '648 files (from chtb_081 to chtb_899.fid)', 'are used as the training set.', 'The development set includes 40 files, from chtb_041.fid to chtb_080.fid.', 'The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.', 'We use the same data setting with #AUTHOR_TAG , however a bit different from Xue and Palmer ( 2005 ) .']"	5
CC44	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	labeling chinese predicates with semantic roles	['Nianwen Xue']	introduction	"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1"	"Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #AUTHOR_TAG ."	"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #AUTHOR_TAG .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']"	0
CC45	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	labeling chinese predicates with semantic roles	['Nianwen Xue']	introduction	"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1"	"Experiments on Chinese SRL ( Xue and Palmer 2005 , #AUTHOR_TAG ) reassured these findings ."	"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 , #AUTHOR_TAG ) reassured these findings .']"	0
CC46	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	hierarchical semantic role labeling	"['Alessandro Moschitti', 'Ana-Maria Giuglea', 'Bonaventura Coppola', 'Roberto Basili']"	introduction	"We present a four-step hierarchical SRL strategy which generalizes the classical two-level approach (boundary detection and classification). To achieve this, we have split the classification step by grouping together roles which share linguistic properties (e.g. Core Roles versus Adjuncts). The results show that the non-optimized hierarchical approach is computationally more efficient than the traditional systems and it preserves their accuracy."	#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.	"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', '#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']"	0
CC47	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	automatic semantic role labeling for chinese verbs	"['Nianwen Xue', 'Martha Palmer']"	introduction	"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling."	"Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #AUTHOR_TAG and Xue ( 2008 ) ."	"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #AUTHOR_TAG and Xue ( 2008 ) .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']"	0
CC48	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	automatic semantic role labeling for chinese verbs	"['Nianwen Xue', 'Martha Palmer']"	experiments	"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling."	"We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG ."	"['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'to chtb_931.fid,', 'or the first 250K words of the Chinese TreeBank 5.1.', 'For the experiments, the data of PropBank is divided into three parts.', '648 files (from chtb_081 to chtb_899.fid)', 'are used as the training set.', 'The development set includes 40 files, from chtb_041.fid to chtb_080.fid.', 'The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']"	1
CC49	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	shallow semantic parsing of chinese	"['Honglin Sun', 'Daniel Jurafsky']"		"In this paper we address the question of assigning semantic roles to sentences in Chinese. We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set. In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix. Finally, we compare English and Chinese semantic-parsing performance. While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese. We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese."	The candidate feature templates include : Voice from #AUTHOR_TAG .	['The candidate feature templates include : Voice from #AUTHOR_TAG .']	5
CC50	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	the penn chinese treebank phrase structure annotation of a large corpus	"['Nianwen Xue', 'Fei Xia', 'Fu dong Chiou', 'Martha Palmer']"		"With growing interest in Chinese Language Processing, numerous NLP tools (e.g., word segmenters, part-of-speech taggers, and parsers) for Chinese have been developed all over the world. However, since no large-scale bracketed corpora are available to the public, these tools are trained on corpora with different segmentation criteria, part-of-speech tagsets and bracketing guidelines, and therefore, comparisons are difficult. As a first step towards addressing this issue, we have been preparing a large bracketed corpus since late 1998. The first two installments of the corpus, 250 thousand words of data, fully segmented, POS-tagged and syntactically bracketed, have been released to the public via LDC (www.ldc.upenn.edu). In this paper, we discuss several Chinese linguistic issues and their implications for our treebanking efforts and how we address these issues when developing our annotation guidelines. We also describe our engineering strategies to improve speed while ensuring annotation quality."	The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .	"['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure 1 is an example from the PropBank 1 .', 'We put the word-by-word translation and the translation of the whole sentence below the example.', 'It is quite a complex sentence, as there are many semantic roles in it.', 'In this sentence, all the semantic roles of the verb 提供 (provide) are presented in the syntactic tree.', 'We can separate the semantic roles into two groups.']"	5
CC51	D08-1034	Improving Chinese semantic role classification with hierarchical feature selection strategy	automatic labeling of semantic roles	"['Daniel Gildea', 'Daniel Jurafsky']"	introduction	"We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data."	Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .	"['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation (Boas 2002).', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']"	0
CC52	D09-1056	The role of named entities in web people search	automatic entity disambiguation benefits to ner relation extraction link analysis and inference	['Matthias Blume']	related work	"Entity disambiguation resolves the many-to-many correspondence between mentions of entities in text and unique real-world entities. Entity disambiguation can bring to bear global (corpus-level) statistics to improve the performance of named entity recognition systems. More importantly , intelligence analysts are keenly interested in relationships between real-world entities. Entity disambiguation makes possible additional types of relation assertions and affects relation extraction performance assessment. Finally, link analysis and inference inherently operate at the level of entities, not text strings. Thus, entity disambiguation is a prerequisite to carrying out these higher-level operations on information extracted from plain text. This paper describes Fair Isaac's automatic entity disambiguation capability and its performance."	"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) ."	"['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']"	0
CC53	D09-1056	The role of named entities in web people search	cucomsem exploring rich features for unsupervised web personal name disambiguation	"['Ying Chen', 'James H Martin']"	related work		"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) ."	"['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']"	0
CC54	D09-1056	The role of named entities in web people search	disambiguation algorithm for people search on the web in	"['Dmitri V Kalashnikov', 'Stella Chen', 'Rabia Nuray', 'Sharad Mehrotra', 'Naveen Ashish']"	related work		"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) ."	"['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']"	0
CC55	D09-1056	The role of named entities in web people search	unsupervised name disambiguation via social network similarity	['Bradley Malin']	related work	"Though names reference actual entities it is nontrivial to resolve which entity a particular name observation represents. Even when names are devoid of typographical error, the resolution process is confounded by both ambiguity, where the same name correctly references multiple entities, and by variation, when an entity is correctly referenced by multiple names. Thus, before link analysis for surveillance or intelligence-gathering purposes can proceed, it is necessary to ensure vertices and edges of the network are correct. In this paper, we concentrate on ambiguity and investigate unsupervised methods which simultaneously learn 1) the number of entities represented by a particular name and 2) which observations correspond to the same entity. The disambiguation methods leverage the fact that an entity's name can be listed in multiple sources, each with a number of related entity's names, which permits the construction of name-based relational networks. The methods studied in this paper differ based on the type of network similarity exploited for disambiguation. The first method relies upon exact name similarity and employs hierarchical clustering of sources, where each source is considered a local network. In contrast, the second method employs a less strict similarity requirement by using random walks between ambiguous observations on a global social network constructed from all sources, or a community similarity. While both methods provide better than simple baseline results on a subset of the Internet Movie Database, findings suggest methods which measure similarity based on community, rather than exact, similarity provide more robust disambiguation capability."	"Other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) ."	"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']"	0
CC56	D09-1056	The role of named entities in web people search	named entity disambiguation a hybrid statistical and rulebased incremental approach	"['Hien T Nguyen', 'Tru H Cao']"	related work	"The rapidly increasing use of large-scale data on the Web makes named entity disambiguation become one of the main challenges to research in Information Extraction and development of Semantic Web. This paper presents a novel method for detecting proper names in a text and linking them to the right entities in Wikipedia. The method is hybrid, containing two phases of which the first one utilizes some heuristics and patterns to narrow down the candidates, and the second one employs the vector space model to rank the ambiguous cases to choose the right candidate. The novelty is that the disambiguation process is incremental and includes several rounds that filter the candidates, by exploiting previously identified entities and extending the text by those entity attributes every time they are successfully resolved in a round. We test the performance of the proposed method in disambiguation of names of people, locations and organizations in texts of the news domain. The experiment results show that our approach achieves high accuracy and can be used to construct a robust named entity disambiguation system."	"Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process ."	"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']"	0
CC57	D09-1056	The role of named entities in web people search	the semeval2007 weps evaluation establishing a benchmark for the web people search task	"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']"	related work	"This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name."	"It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG ) ."	"['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006) and Cross-document Coreference (CDC) (Bagga and Baldwin, 1998).', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG ) .']"	0
CC58	D09-1056	The role of named entities in web people search	multidocument statistical fact extraction and fusion	['Gideon S Mann']	experiments	"This dissertation presents original techniques for statistical fact extraction and fusion from multiple documents. Fact extraction, or relationship extraction, is a process where natural language text is scanned to find instances of a predetermined class of facts (e.g. birthday(x,y)). A framework for training statistical fact extractors from example is used wherein a set of examples and a target model are used to annotate an automatically collected corpus. This annotation is then used to provide training data for classifiers (Phrase Conditional Likelihood and Native Bayes) or sequence models (Conditional Random Fields).  Fact extractors are used in two information retrieval tasks. In question answering the set of candidate answers is narrowed using fine-grained proper noun ontological facts (is-a(X, Y)) extracted from a corpus by rote classifiers leading to higher performance. Extracted facts are also used for name-referent disambiguation, or cross-document coreference, where one personal name may refer to multiple potential people in the world. The distinguishing biographic facts for each person, such as birthday(x,y) and occupation (x,y), are automatically extracted from plain text and these biographic facts are used along with other statistical methods to distinguish between mentions of each of the referents.  This dissertation presents novel techniques for fusion which integrate facts extracted from multiple sources. For the task of biographic fact extraction, fusion of factual information extracted from multiple documents improves the precision of the resulting information. Further improvements result from cascaded fact extraction, where certain facts are extracted and fused and then these facts are used to extract additional information. The technique of cascaded fact extraction and fusion is also applied to time-bounded facts, where a cascade of fact extractors produce a timeline of corporate management succession.  Collectively, this research demonstrates the utility of multi-document fact extraction and fusion. It shows that facts can serve as a building-block for deeper text processing such as finding coreferent names in a series of documents, finding the answers to questions, and constructing a timeline for time-variable facts. The key aspects to the process are training with minimal supervision, high-performance statistical fact extraction, fusion across multiple sources of information, and cascaded extraction."	"2The WePS-1 corpus includes data from the Web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable ."	"['2The WePS-1 corpus includes data from the Web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .']"	5
CC59	D09-1056	The role of named entities in web people search	crossdocument coreference on a large scale corpus	"['Chung Heong Gooi', 'James Allan']"	related work		"Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) ."	"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) .', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']"	0
CC60	D09-1056	The role of named entities in web people search	a testbed for people searching strategies in the www	"['Javier Artiles', 'Julio Gonzalo', 'Felisa Verdejo']"	related work	This paper describes the creation of a testbed to evaluate people searching strategies on the World-Wide-Web. This task involves resolving person names' ambiguity and locating relevant information characterising every individual under the same name.	"It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 ) ."	"['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006) and Cross-document Coreference (CDC) (Bagga and Baldwin, 1998).', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 ) .']"	0
CC61	D09-1056	The role of named entities in web people search	cucomsem exploring rich features for unsupervised web personal name disambiguation	"['Ying Chen', 'James H Martin']"	related work		"Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - ."	"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']"	0
CC62	D09-1056	The role of named entities in web people search	disambiguation algorithm for people search on the web in	"['Dmitri V Kalashnikov', 'Stella Chen', 'Rabia Nuray', 'Sharad Mehrotra', 'Naveen Ashish']"	related work		"Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ) ."	"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ) .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']"	0
CC63	D09-1056	The role of named entities in web people search	searching for people on web search engines	"['Amanda Spink', 'Bernard Jansen', 'Jan Pedersen']"	introduction	"The Web is a communication and information technology that is often used for the distribution and retrieval of personal information. Many people and organizations mount Web sites containing large amounts of information on individuals, particularly about celebrities. However, limited studies have examined how people search for information on other people, using personal names, via Web search engines. Explores the nature of personal name searching on Web search engines. The specific research questions addressed in the study are: ""Do personal names form a major part of queries to Web search engines?""; ""What are the characteristics of personal name Web searching?""; and ""How effective is personal name Web searching?"". Random samples of queries from two Web search engines were analyzed. The findings show that: personal name searching is a common but not a major part of Web searching with few people seeking information on celebrities via Web search engines; few personal name queries include double quotations or additional identifying terms; and name searches on Alta Vista included more advanced search features relative to those on AlltheWeb.com. Discusses the implications of the findings for Web searching and search engines, and further research."	A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ) .	"['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ) .', 'According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people (Artiles et al., 2005).', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.']"	0
CC64	D09-1056	The role of named entities in web people search	experiments on semanticbased clustering for crossdocument coreference	['Horacio Saggion']	related work		#AUTHOR_TAG compared the performace of NEs versus BoW features .	"['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007)- .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of NEs versus BoW features .', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']"	0
CC65	D09-1056	The role of named entities in web people search	entitybased crossdocument coreferencing using the vector space model	"['Amit Bagga', 'Breck Baldwin']"	related work		"Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ) ."	"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ) .', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']"	0
CC66	D09-1056	The role of named entities in web people search	extended named entity ontology with attribute information	['Satoshi Sekine']	experiments	"Named Entities (NE) are regarded as an important type of semantic knowledge in many natural language processing (NLP) applications. Originally, a limited number of NE categories were proposed. In MUC, it was 7 categories - people, organization, location, time, date, money and percentage expressions. However, it was noticed that such a limited number of NE categories is too small for many applications. The author has proposed Extended Named Entity (ENE), which has about 200 categories (Sekine and Nobata 04). During the development of ENE, we noticed that many ENE categories have specific attributes, and those provide very important information for the entities. For example, rivers have attributes like source location, outflow, and length. Some such information is essential to knowing about the river, while the name is only a label which can be used to refer to the river. Also, such attributes are important information for many NLP applications. In this paper, we report on the design of a set of attributes for ENE categories. We used a bottom up approach to creating the knowledge using a Japanese encyclopedia, which contains abundant descriptions of ENE instances."	It provides a fine grained NE recognition covering 100 different NE types ( #AUTHOR_TAG ) .	"['OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types ( #AUTHOR_TAG ) .', 'Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex.']"	5
CC67	D09-1056	The role of named entities in web people search	a testbed for people searching strategies in the www	"['Javier Artiles', 'Julio Gonzalo', 'Felisa Verdejo']"	introduction	This paper describes the creation of a testbed to evaluate people searching strategies on the World-Wide-Web. This task involves resolving person names' ambiguity and locating relevant information characterising every individual under the same name.	"According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #AUTHOR_TAG ) ."	"['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (Spink et al., 2004).', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #AUTHOR_TAG ) .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.']"	0
CC68	D09-1056	The role of named entities in web people search	large scale named entity disambiguation based on wikipedia data	['Silviu Cucerzan']	related work	"This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles."	"Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process ."	"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']"	0
CC69	D09-1056	The role of named entities in web people search	the semeval2007 weps evaluation establishing a benchmark for the web people search task	"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']"	introduction	"This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name."	"The Web People Search task , as defined in the first WePS evaluation campaign ( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it ."	"['This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in.', 'The user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name.', 'The Web People Search task , as defined in the first WePS evaluation campaign ( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it .']"	0
CC70	D09-1056	The role of named entities in web people search	the semeval2007 weps evaluation establishing a benchmark for the web people search task	"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']"	related work	"This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name."	"We have used the testbeds from WePS-1 ( #AUTHOR_TAG , 2007)2 and WePS-2 (Artiles et al., 2009) evaluation campaigns 3."	"['We have used the testbeds from WePS-1 ( #AUTHOR_TAG , 2007)2 and WePS-2 (Artiles et al., 2009) evaluation campaigns 3.']"	5
CC71	D09-1056	The role of named entities in web people search	irstbp web people search using name entities	"['Octavian Popescu', 'Bernardo Magnini']"	related work		"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) ."	"['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']"	0
CC72	D09-1056	The role of named entities in web people search	entitybased crossdocument coreferencing using the vector space model	"['Amit Bagga', 'Breck Baldwin']"	related work		"The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG ) ."	"['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG ) .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task -Web People Search -on its own (Artiles et al., 2005;Artiles et al., 2007).']"	0
CC73	D09-1056	The role of named entities in web people search	weps 2 evaluation campaign overview of the web people search clustering task	"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']"	related work	"The second WePS (Web People Search) Evaluation cam-paign took place in 2008-2009 with the participation of 19 re-search groups from Europe, Asia and North America. Given the output of a Web Search Engine for a (usually ambiguous) person name as query, two tasks were addressed: a clustering task, which consists of grouping together web pages referring to the same person, and an extraction task, which consists of extracting salient attributes for each of the persons shar-ing the same name. This paper presents the definition, re-sources, methodology and evaluation metrics, participation and comparative results for the clustering task"	"In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG ) ."	"['Among the 16 teams that submitted results for the first WePS campaign, 10 of them 1 used NEs in their document representation.', 'This makes NEs the second most common type of feature; only the BoW feature was more popular.', 'Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc.', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG ) .', 'Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams.', 'But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'In the next Section we describe this dataset and how it has been adapted for our purposes.']"	0
CC74	D09-1056	The role of named entities in web people search	titpi web people search task using semisupervised clustering approach	"['Kazunari Sugiyama', 'Manabu Okumura']"	related work		"Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - ."	"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']"	0
CC75	D09-1056	The role of named entities in web people search	irstbp web people search using name entities	"['Octavian Popescu', 'Bernardo Magnini']"	related work		"Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - ."	"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']"	0
CC76	D09-1067	Improving verb clustering with automatically acquired selectional preferences	discriminative learning of selectional preference from unlabeled text	"['Shane Bergsma', 'Dekang Lin', 'Randy Goebel']"	conclusion	"We present a discriminative method for learning selectional preferences from unlabeled text. Positive examples are taken from observed predicate-argument pairs, while negatives are constructed from unobserved combinations. We train a Support Vector Machine classifier to distinguish the positive from the negative instances. We show how to partition the examples for efficient training with 57 thousand features and 6.5 million training instances. The model outperforms other recent approaches, achieving excellent correlation with human plausibility judgments. Compared to Mutual Information, it identifies 66% more verb-object pairs in unseen text, and resolves 37 % more pronouns correctly in a pronoun resolution experiment."	"Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ) ."	"['In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ) .', 'The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.']"	3
CC77	D09-1067	Improving verb clustering with automatically acquired selectional preferences	a simple similaritybased model for selectional preferences	['Katrin Erk']	conclusion		"Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ) ."	"['In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ) .', 'The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.']"	3
CC78	D09-1087	Self-training PCFG grammars with latent annotations across languages	forest reranking discriminative parsing with nonlocal features	['Liang Huang']	conclusion	"Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank."	"Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training ."	"['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training .', 'Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case.']"	3
CC79	D09-1087	Self-training PCFG grammars with latent annotations across languages	coarsetofine nbest parsing and maxent discriminative reranking	"['Eugene Charniak', 'Mark Johnson']"	conclusion		"Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training ."	"['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training .', 'Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case.']"	3
CC80	D09-1087	Self-training PCFG grammars with latent annotations across languages	sparse multiscale grammars for discriminative latent variable parsing	"['Slav Petrov', 'Dan Klein']"	conclusion	"We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement. The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories). Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars."	"Self-training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case ."	"['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training.', 'Self-training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case .']"	3
CC81	D09-1143	"Convolution kernels on constituent, dependency and sequential structures for relation extraction"	a semantic kernel for predicate argument classification	"['Alessandro Moschitti', 'Cosmin Bejan']"	conclusion	"Automatically deriving semantic structures from text is a challenging task for machine learning. The flat feature representations, usually used in learning models, can only partially describe structured data. This makes difficult the processing of the semantic information that is embedded into parse-trees. In this paper a new kernel for automatic classification of predicate arguments has been designed and experimented. It is based on subparse-trees annotated with predicate argument information from PropBank corpus. This kernel, exploiting the convolution properties of the parse-tree kernel, enables us to learn which syntactic structures can be associated with the arguments defined in PropBank. Support Vector Machines (SVMs) using such a kernel classify arguments with a better accuracy than SVMs based on linear kernel."	"Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) ."	"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']"	3
CC82	D09-1143	"Convolution kernels on constituent, dependency and sequential structures for relation extraction"	semantic role labeling via framenet verbnet and propbank	"['Ana-Maria Giuglea', 'Alessandro Moschitti']"	conclusion	"This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources: FrameNet, VerbNet and PropBank. The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs. We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes. The PropBank corpus, which is tightly connected to the VerbNet lexicon, is used to increase the verb coverage and also to test the effectiveness of our approach. The results indicate that our model is an interesting step towards the design of more robust semantic parsers."	"Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) ."	"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']"	3
CC83	D09-1143	"Convolution kernels on constituent, dependency and sequential structures for relation extraction"	tree kernels for semantic role labeling	"['Alessandro Moschitti', 'Daniele Pighin', 'Roberto Basili']"	conclusion	"The availability of large scale data sets of manually annotated predicate-argument structures has recently favored the use of machine learning approaches to the design of automated semantic role labeling (SRL) systems. The main research in this area relates to the design choices for feature representation and for effective decompositions of the task in different learning models. Regarding the former choice, structural properties of full syntactic parses are largely employed as they represent ways to encode different principles suggested by the linking theory between syntax and semantics. The latter choice relates to several learning schemes over global views of the parses. For example, re-ranking stages operating over alternative predicate-argument sequences of the same sentence have shown to be very effective. In this article, we propose several kernel functions to model parse tree properties in kernel-based machines, for example, perceptrons or support vector machines. In particular, we define different kinds of tree kernels as general approaches to feature engineering in SRL. Moreover, we extensively experiment with such kernels to investigate their contribution to individual stages of an SRL architecture both in isolation and in combination with other traditional manually coded features. The results for boundary recognition, classification, and re-ranking stages provide systematic evidence about the significant impact of tree kernels on the overall accuracy, especially when the amount of training data is small. As a conclusive result, tree kernels allow for a general and easily portable feature engineering method which is applicable to a large family of natural language processing tasks."	"Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG ) ."	"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG ) .']"	3
CC84	D09-1143	"Convolution kernels on constituent, dependency and sequential structures for relation extraction"	effective use of wordnet semantics via kernelbased learning	"['Roberto Basili', 'Marco Cammisa', 'Alessandro Moschitti']"	conclusion	"Research on document similarity has shown that complex representations are not more accurate than the simple bag-of-words. Term clustering, e.g. using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective. In particular, when to extend the similarity function external prior knowledge is used, e.g. WordNet, the retrieval system decreases its performance. The critical issues here are methods and conditions to integrate such knowledge.    In this paper we propose kernel functions to add prior knowledge to learning algorithms for document classification. Such kernels use a term similarity measure based on the WordNet hierarchy. The kernel trick is used to implement such space in a balanced and statistically coherent way. Cross-validation results show the benefit of the approach for the Support Vector Machines when few training data is available."	"Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) ."	"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']"	3
CC85	D09-1160	Polynomial to linear	small statistical models by random feature mixing	"['Kuzman Ganchev', 'Mark Dredze']"	conclusion	The application of statistical NLP systems to resource constrained devices is limited by the need to maintain parameters for a large number of features and an alphabet mapping features to parameters. We introduce random feature mixing to eliminate alphabet storage and reduce the number of parameters without severely impacting model performance.	"When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage ."	"['We plan to apply our method to wider range of classifiers used in various NLP tasks.', 'To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .']"	3
CC86	D10-1052	Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism	ordering phrases with function words	"['Hendra Setiawan', 'Min-Yen Kan', 'Haizhou Li']"		"This paper presents a Function Word centered, Syntax-based (FWS) solution to address phrase ordering in the context of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS approach consistently outperforms the baseline system in ordering function words' arguments and improving translation quality in both perfect and noisy word alignment scenarios."	"The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ) ."	"['The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.', 'This section provides a high level overview of our reordering model, which attempts to leverage this information.']"	2
CC87	D10-1052	Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism	improved word alignment with statistics and linguistic heuristics	['Ulf Hermjakob']	related work	"We present a method to align words in a bitext that combines elements of a tra-ditional statistical approach with linguis-tic knowledge. We demonstrate this ap-proach for Arabic-English, using an align-ment lexicon produced by a statistical word aligner, as well as linguistic re-sources ranging from an English parser to heuristic alignment rules for function words. These linguistic heuristics have been generalized from a development cor-pus of 100 parallel sentences. Our aligner, UALIGN, outperforms both the commonly used GIZA++ aligner and the state-of-the-art LEAF aligner on F-measure and pro-duces superior scores in end-to-end sta-tistical machine translation, +1.3 BLEU points over GIZA++, and +0.7 over LEAF."	"With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #AUTHOR_TAG ) ."	"['Our reordering model is closely related to the model proposed by Zhang and Gildea (2005;2007a), with respect to conditioning the reordering predictions on lexical items.', 'These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words.', 'With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #AUTHOR_TAG ) .', 'However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model.']"	1
CC88	D10-1052	Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism	ordering phrases with function words	"['Hendra Setiawan', 'Min-Yen Kan', 'Haizhou Li']"		"This paper presents a Function Word centered, Syntax-based (FWS) solution to address phrase ordering in the context of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS approach consistently outperforms the baseline system in ordering function words' arguments and improving translation quality in both perfect and noisy word alignment scenarios."	"To model o ( Li , S â T ) , o ( Ri , S â T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #AUTHOR_TAG ."	"['To model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #AUTHOR_TAG .', 'Formally, this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases).', 'In particular, o maps the reordering into one of the following four orientation values (borrowed from Nagata et al. (2006)) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG).', 'The Monotone/Reverse distinction indicates whether the projected order follows the original order, while the Adjacent/Gap distinction indicates whether the pro-This heuristic is commonly used in learning phrase pairs from parallel text.', 'The maximality ensures the uniqueness of L and R.']"	5
CC89	D10-1052	Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism	topological ordering of function words in hierarchical phrasebased translation	"['Hendra Setiawan', 'Min Yen Kan', 'Haizhou Li', 'Philip Resnik']"		"Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotation-driven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance."	"The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) ."	"['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.', 'This section provides a high level overview of our reordering model, which attempts to leverage this information.']"	2
CC90	D10-1052	Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism	topological ordering of function words in hierarchical phrasebased translation	"['Hendra Setiawan', 'Min Yen Kan', 'Haizhou Li', 'Philip Resnik']"		"Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotation-driven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance."	"To model d ( FWi â 1 , S â T ) , d ( FWi +1 , S â T ) , i.e. whether Li , S â T and Ri , S â T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG ."	"['To model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .', 'Taking d(F W i−1,S→T ) as a case in point, this model takes the form']"	5
CC91	D10-1074	What do we know about conversation participants	what do we know about conversation participants experiments on conversation entailment	"['Chen Zhang', 'Joyce Chai']"	related work	"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment."	"In our previous work ( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment ."	"['In our previous work ( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment .', 'We have collected a dataset of 875 instances.', 'Each instance consists of a conversation segment and a hypothesis (as described in Section 1).', 'The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'We developed an approach that is motivated by previous work on textual entailment.', 'We use clauses in the logic-based approaches as the underlying representation of our system.', 'Based on this representation, we apply a two stage entailment process similar to MacCartney et al. (2006) developed for textual entailment: an alignment stage followed by an entailment stage.']"	2
CC92	D10-1074	What do we know about conversation participants	what do we know about conversation participants experiments on conversation entailment	"['Chen Zhang', 'Joyce Chai']"	method	"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment."	"Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 ."	"['Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .']"	2
CC93	D10-1074	What do we know about conversation participants	what do we know about conversation participants experiments on conversation entailment	"['Chen Zhang', 'Joyce Chai']"	experiments	"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment."	"Note that in our original work ( #AUTHOR_TAG ) , only development data were used to show some initial observations."	"['Note that in our original work ( #AUTHOR_TAG ) , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data.']"	1
CC94	D10-1074	What do we know about conversation participants	what do we know about conversation participants experiments on conversation entailment	"['Chen Zhang', 'Joyce Chai']"	method	"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment."	This alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG ) .	"['This string representation of paths is used to capture both the subject consistency and the object consistency.', 'Since they are non-numerical features, and the variability of their values can be extremely large, so we applied an instance-based classification model (e.g., k-nearest neighbor) to determine alignments between verb terms.', 'We measure the distance between two path features by their minimal string edit distance, and then simply use the Euclidean distance to measure the closeness between any two verbs.', 'Again this model is trained from our development data described in Zhang and Chai (2009).', 'Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG ) .']"	5
CC95	D10-1074	What do we know about conversation participants	what do we know about conversation participants experiments on conversation entailment	"['Chen Zhang', 'Joyce Chai']"	introduction	"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment."	"To address this limitation , our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment ."	"['To address this limitation , our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', 'For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', 'While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data.', 'It is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome.']"	2
CC96	D10-1074	What do we know about conversation participants	what do we know about conversation participants experiments on conversation entailment	"['Chen Zhang', 'Joyce Chai']"		"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment."	"In our previous work ( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows ."	"['In our previous work ( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .', 'This is based on a simple as- sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses.']"	2
CC97	D10-1100	Automatic Detection of Micro-Arousals	convolution kernels on constituent dependency and sequential structures for relation extraction	"['Truc-Vien T Nguyen', 'Alessandro Moschitti', 'Giuseppe Riccardi']"		"This paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task. Syntax is derived from constituent and dependency parse trees whereas se-mantics concerns to entity types and lex-ical sequences. We investigate the effec-tiveness of such representations in the au-tomated relation extraction from text. We process the above data by means of Sup-port Vector Machines along with the syn-tactic tree, the partial tree and the word sequence kernels. Our study on the ACE 2004 corpus illustrates that the combina-tion of the above kernels achieves high ef-fectiveness and significantly improves the current state-of-the-art."	"We use the structures previously used by #AUTHOR_TAG , and propose one new structure ."	"['Linear learning machines are one of the most popular machines used for classification problems.', 'The objective of a typical classification problem is to learn a function that separates the data into different classes.', 'The data is usually in the form of features extracted from abstract objects like strings, trees, etc.', 'A drawback of learning by using complex functions is that complex functions do not generalize well and thus tend to over-fit.', 'The research community therefore prefers linear classifiers over other complex classifiers.', 'But more often than not, the data is not linearly separable.', 'It can be made linearly separable by increasing the dimensionality of data but then learning suffers from the curse of dimensionality and classification becomes computationally intractable.', 'This is where kernels come to the rescue.', 'The well-known kernel trick aids us in finding similarity between feature vectors in a high dimensional space without having to write down the expanded feature space.', 'The essence of kernel methods is that they compare two feature vectors in high dimensional space by using a dot product that is a function of the dot product of feature vectors in the lower dimensional space.', 'Moreover, Convolution Kernels (first introduced by Haussler (1999)) can be used to compare abstract objects instead of feature vectors.', 'This is because these kernels involve a recursive calculation over the ""parts"" of a discrete structure.', 'This calculation is usually made computationally efficient using Dynamic Programming techniques.', 'Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data).', 'Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task.', 'Now we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by #AUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', 'All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT).', 'For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).', 'We use the Stanford parser (Klein and Manning, 2003) to get the basic PSTs and DTs.', 'Following are the structures that we refer to in our experiments and results section: PET: This refers to the smallest common phrase structure tree that contains the two target entities.', 'Dependency Words (DW) tree: This is the smallest common dependency tree that contains the two target entities.', 'In Figure 1, since the target entities are at the leftmost and rightmost branch of the depen- 3 We omitted SK6, which is the worst performing sequence kernel in (Nguyen et al., 2009).', 'Grammatical Relation (GR) tree: If we replace the words at the nodes by their relation to their corresponding parent in DW, we get a GR tree.', 'For example, in Figure 1, replacing Toujan Faisal by nsubj, 54 by appos, she by nsubjpass and so on.', 'Grammatical Relation Word (GRW) tree: We get this tree by adding the grammatical relations as separate nodes between a node and its parent.', 'For example, in Figure 1, adding nsubj as a node between T1-Individual and Toujan Faisal, appos as a node between 54 and Toujan Faisal, and so on.', 'Sequence Kernel of words (SK1): This is the sequence of words between the two entities, including their tags.', 'For our example in Figure 1, it would be T1-Individual Toujan Faisal 54 said she was informed of the refusal by an T2-Group Interior Ministry committee.', 'Sequence in GRW tree (SqGRW): This is the new structure that we introduce which, to the best of our knowledge, has not been used before for similar tasks.', 'It is the sequence of nodes from one target to the other in the GRW tree.', 'For example, in Figure 1, this would be Toujan Faisal nsubj T1-Individual said ccomp informed prep by T2-Group pobj committee.']"	5
CC98	D10-1100	Automatic Detection of Micro-Arousals	convolution kernels on constituent dependency and sequential structures for relation extraction	"['Truc-Vien T Nguyen', 'Alessandro Moschitti', 'Giuseppe Riccardi']"	conclusion	"This paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task. Syntax is derived from constituent and dependency parse trees whereas se-mantics concerns to entity types and lex-ical sequences. We investigate the effec-tiveness of such representations in the au-tomated relation extraction from text. We process the above data by means of Sup-port Vector Machines along with the syn-tactic tree, the partial tree and the word sequence kernels. Our study on the ACE 2004 corpus illustrates that the combina-tion of the above kernels achieves high ef-fectiveness and significantly improves the current state-of-the-art."	This revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .	"['In this paper, we have introduced the novel tasks of social event detection and classification.', 'We show that data sampling techniques play a crucial role for the task of relation detection.', 'Through oversampling we achieve an increase in F1-measure of 22.2% absolute over a baseline system.', 'Our experiments show that as a result of how language expresses the relevant information, dependency-based structures are best suited for encoding this information.', 'Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best.', 'This revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .', 'We also introduced a new sequence structure (SqGRW) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks.']"	1
CC99	D10-1100	Automatic Detection of Micro-Arousals	convolution kernels on constituent dependency and sequential structures for relation extraction	"['Truc-Vien T Nguyen', 'Alessandro Moschitti', 'Giuseppe Riccardi']"	experiments	"This paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task. Syntax is derived from constituent and dependency parse trees whereas se-mantics concerns to entity types and lex-ical sequences. We investigate the effec-tiveness of such representations in the au-tomated relation extraction from text. We process the above data by means of Sup-port Vector Machines along with the syn-tactic tree, the partial tree and the word sequence kernels. Our study on the ACE 2004 corpus illustrates that the combina-tion of the above kernels achieves high ef-fectiveness and significantly improves the current state-of-the-art."	"Here , the PET and GR kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where GR performed much worse than PET for ACE data ."	"['Social event detection is the task of detecting if any social event exists between a pair of entities in a sentence.', 'We formulate the problem as a binary classification task by labeling an example that does not have a social event as class -1 and by labeling an example that either has an INR or COG social event as class 1.', 'First we present results for our baseline system.', 'Our baseline system uses various structures and their combinations but without any data balancing.', '1 presents results for our baseline system.', 'Grammatical relation tree structure (GR), a structure derived from dependency tree by replacing the words by their grammatical relations achieves the best precision.', 'This is probably because the clas-sifier learns that if both the arguments of a predicate contain target entities then it is a social event.', 'Among kernels for single structures, the path enclosed tree for PSTs (PET) achieves the best recall.', 'Furthermore, a combination of structures derived from PSTs and DTs performs best.', 'The sequence kernels, perform much worse than SqGRW (F1-measure as low as 0.45).', 'Since it is the same case for all subsequent experiments, we omit them from the discussion.', 'We now turn to experiments involving sampling.', 'Table 2 presents results for under-sampling, i.e. randomly removing examples belonging to the negative class until its size matches the positive class.', 'Table 2 shows a large gain in F1-measure of 9.72% absolute over the baseline system (Table 1).', 'We found that worst performing kernel with under-sampling is SK1 with an F1-measure of 39.2% which is better than the best performance without undersampling.', 'These results make it clear that doing under-sampling greatly improves the performance of the classifier, despite the fact that we are using less training data (fewer negative examples).', 'This is as expected because we are evaluating on F1-measure and the classifier is optimizing for accuracy.', 'absolute.', 'As in the baseline system, a combination of structures performs best.', 'As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall.', 'Here , the PET and GR kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where GR performed much worse than PET for ACE data .', 'This exemplifies the difference in the nature of our event annotations from that of ACE relations.', 'Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger.', 'This means that implicit feature space is much sparser and thus not the best representation.', '4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are ""close"" to the original examples.', 'This method achieves a gain 16.78% over the baseline system.', 'We expected this system to perform better than the over-sampled system but it does not.', 'This suggests that our over-sampled system is not over-fitting; a concern with using oversampling techniques.']"	1
CC100	D10-1101	Extracting Opinion Targets in a Single-and Cross-Domain Setting with Conditional Random Fields	biographies bollywood boomboxes and blenders domain adaptation for sentiment classification	"['John Blitzer', 'Mark Dredze', 'Fernando Pereira']"	experiments	"Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains."	"Our results also confirm the insights gained by #AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial ."	"['Our results also confirm the insights gained by #AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .', 'Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data.']"	1
CC101	D10-1101	Extracting Opinion Targets in a Single-and Cross-Domain Setting with Conditional Random Fields	instance weighting for domain adaptation in nlp	"['Jing Jiang', 'ChengXiang Zhai']"	conclusion	"Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective."	"For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach ."	"['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.', 'Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'Our CRF-based approach also yields promising results in the crossdomain setting.', 'The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach .', 'Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.']"	3
CC102	D10-1101	Extracting Opinion Targets in a Single-and Cross-Domain Setting with Conditional Random Fields	biographies bollywood boomboxes and blenders domain adaptation for sentiment classification	"['John Blitzer', 'Mark Dredze', 'Fernando Pereira']"	conclusion	"Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains."	"For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach ."	"['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.', 'Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'Our CRF-based approach also yields promising results in the crossdomain setting.', 'The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .', 'Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.']"	3
CC103	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	algorithms for deterministic incremental dependency parsing	['J Nivre']	experiments	"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework."	An implementation of the transition-based dependency parsing frame- work ( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in Zhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.	"['An implementation of the transition-based dependency parsing frame- work ( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in Zhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.', 'The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word iden- tity of the syntactic head of the top word on the stack (if available); dependency arc label iden- tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).', 'All feature conjunc- tions are included.']"	5
CC104	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	structured output learning with indirect supervision	"['M Chang', 'D Goldwasser', 'D Roth', 'V Srikumar']"	introduction	"We present a novel approach for structure prediction that addresses the difficulty of obtaining labeled structures for training. We observe that structured output problems often have a companion learning problem of determining whether a given input possesses a good structure. For example, the companion problem for the part-of-speech (POS) tagging task asks whether a given sequence of words has a corresponding sequence of POS tags that is ""legitimate"". While obtaining direct supervision for structures is difficult and expensive, it is often very easy to obtain indirect supervision from the companion binary decision problem.    In this paper, we develop a large margin framework that jointly learns from both direct and indirect forms of supervision. Our experiments exhibit the significant contribution of the easy-to-get indirect binary supervision on three important NLP structure learning problems."	"This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ) ."	"['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'For our setting this would mean using weak application specific signals to improve dependency parsing.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']"	0
CC105	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	kbest spanning tree parsing	['K Hall']	experiments	"This paper introduces a Maximum Entropy dependency parser based on an efficient kbest Maximum Spanning Tree (MST) algorithm. Although recent work suggests that the edge-factored constraints of the MST algorithm significantly inhibit parsing accuracy, we show that generating the 50-best parses according to an edge-factored model has an oracle performance well above the 1-best performance of the best dependency parsers. This motivates our parsing approach, which is based on reranking the kbest parses generated by an edge-factored model. Oracle parse accuracy results are presented for the edge-factored model and 1-best results for the reranker on eight languages (seven from CoNLL-X and English)."	"We use the non-projective k-best MST algorithm to generate k-best lists ( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper ."	"['• Graph-based: An implementation of graphbased parsing algorithms with an arc-factored parameterization (McDonald et al., 2005).', 'We use the non-projective k-best MST algorithm to generate k-best lists ( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper .', 'The graphbased parser features used in the experiments in this paper are defined over a word, w i at position i; the head of this word w ρ(i) where ρ(i) provides the index of the head word; and partof-speech tags of these words t i .', 'We use the following set of features similar to McDonald et al. ( 2005):']"	5
CC106	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	uptraining for accurate deterministic question parsing	"['S Petrov', 'P C Chang', 'M Ringgaard', 'H Alshawi']"	experiments	"It is well known that parsing accuracies drop significantly on out-of-domain data. What is less known is that some parsers suffer more from domain shifts than others. We show that dependency parsers have more difficulty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance."	#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .	"['Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.', 'Consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', 'Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'The question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'Root-F1 scores from Table 2 suggest that one simple question is ""what is the main verb of this sentence?""', 'for sentences that are questions.', 'In most cases this task is straight-forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'We feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data.']"	1
CC107	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	using a dependency parser to improve smt for subjectobjectverb languages in	"['P Xu', 'J Kang', 'M Ringgaard', 'F Och']"	experiments		1Our rules are similar to those from #AUTHOR_TAG .	['1Our rules are similar to those from #AUTHOR_TAG .']	1
CC108	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	dependency treebased sentiment classification using crfs with hidden variables	"['T Nakagawa', 'K Inui', 'S Kurohashi']"	introduction	"In this paper, we present a dependency tree-based method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method. In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features."	"This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( #AUTHOR_TAG ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks ."	"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( #AUTHOR_TAG ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']"	0
CC109	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	online largemargin training of dependency parsers	"['R McDonald', 'K Crammer', 'F Pereira']"	experiments	"We present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements."	An implementation of graph- based parsing algorithms with an arc-factored parameterization ( #AUTHOR_TAG ) .	"['An implementation of graph- based parsing algorithms with an arc-factored parameterization ( #AUTHOR_TAG ) .', 'We use the non-projective k-best MST algorithm to generate k-best lists (Hall, 2007), where k = 8 for the experiments in this paper.', 'The graph- based parser features used in the experiments in this paper are defined over a word, wi at po- sition i; the head of this word w_(i) where _(i) provides the index of the head word; and part- of-speech tags of these words ti.', 'We use the following set of features similar to McDonald et al. (2005):']"	5
CC110	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	reranking and selftraining for parser adaptation	"['D McClosky', 'E Charniak', 'M Johnson']"	related work		"The method is called targeted self-training as it is similar in vein to self-training ( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings ."	"['A recent study by  also investigates the task of training parsers to improve MT reordering.', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training ( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings .', 'Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'This allows us to give guarantees of convergence.', 'Furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system.', 'Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.']"	1
CC111	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	meteor an automatic metric for mt evaluation with improved correlation with human judgments	"['S Banerjee', 'A Lavie']"	experiments	"We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules."	"Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure ."	"['In our experiments we work with a set of English-Japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences.', 'We use a reordering score based on the reordering penalty from the METEOR scoring metric.', 'Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure .', 'Results for three augmented-loss schedules are shown: 0.5 where for every two treebank updates we make one augmented-loss update, 1 is a 1-to-1 mix, and 2 is where we make twice as many augmented-loss updates as treebank updates.']"	4
CC112	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	using a dependency parser to improve smt for subjectobjectverb languages in	"['P Xu', 'J Kang', 'M Ringgaard', 'F Och']"	introduction		"This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( #AUTHOR_TAG ) , and many other tasks ."	"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( #AUTHOR_TAG ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']"	0
CC113	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	a tale of two parsers investigating and combining graphbased and transitionbased dependency parsing	"['Y Zhang', 'S Clark']"	experiments	"Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beam-search. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively."	"• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists."	"['• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', 'The features used by all models are: the part-ofspeech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).', 'All feature conjunctions are included.']"	5
CC114	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	domain adaptation with structural correspondence learning	"['J Blitzer', 'R McDonald', 'F Pereira']"	introduction	"Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger."	"In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) ."	"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']"	0
CC115	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	building a large annotated corpus of english the penn treebank computational linguistics	"['M Marcus', 'B Santorini', 'M A Marcinkiewicz']"	experiments		"In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #AUTHOR_TAG ) ."	"['In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #AUTHOR_TAG ) .', 'We also make use of the Brown corpus, and the Question Treebank (QTB) (Judge et al., 2006']"	5
CC116	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	reranking and selftraining for parser adaptation	"['D McClosky', 'E Charniak', 'M Johnson']"	introduction		"In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) ."	"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']"	0
CC117	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	guiding semisupervision with constraintdriven learning	"['M W Chang', 'L Ratinov', 'D Roth']"	related work		"The work that is most similar to ours is that of #AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) ."	"['The work that is most similar to ours is that of #AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).', 'For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'The augmented-loss algorithm can be viewed as an online version of this algorithm which performs model updates based on the augmented-loss functions directly (rather than adding a set of examples to the training set).', 'Unlike the CODL approach, we do not perform complete optimization on each iteration over the unlabeled dataset; rather, we incorporate the updates in our online learning algorithm.', 'As mentioned earlier, CODL is one example of learning algorithms that use weak supervision, others include Mann and Mc-Callum (2010) and Ganchev et al. (2010).', 'Again, these works are typically interested in using the extrinsic metric -or, in general, extrinsic information -to optimize the intrinsic metric in the absence of any labeled intrinsic data.', 'Our goal is to optimize both simultaneously.']"	1
CC118	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	an endtoend discriminative approach to machine translation	"['P Liang', 'A Bouchard-Ct', 'D Klein', 'B Taskar']"	related work		#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .	"['A recent study by  also investigates the task of training parsers to improve MT reordering.', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'This allows us to give guarantees of convergence.', 'Furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', 'Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.']"	1
CC119	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	training a parser for machine translation reordering	"['J Katz-Brown', 'S Petrov', 'R McDonald', 'D Talbot', 'F Och', 'H Ichikawa', 'M Seno', 'H Kazawa']"	related work	"We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress."	A recent study by #AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .	"['A recent study by #AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'This allows us to give guarantees of convergence.', 'Furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system.', 'Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.']"	1
CC120	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	unsupervised methods for determining object and relation synonyms on the web	"['A Yates', 'O Etzioni']"	experiments	"The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fully-implemented system that runs in O(KN log N) time in the number of extractions, N, and the maximum number of synonyms per word, K. The system, called RESOLVER, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, RESOLVER resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of RESOLVER's probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve F1 by 5%. An extension to the basic RESOLVER system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus."	"Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 ) ."	"['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 ) .']"	0
CC121	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	discriminative training methods for hidden markov models theory and experiments with perceptron algorithms	['M Collins']	introduction	"We describe new algorithms for train-ing tagging models, as an alternative to maximum-entropy models or condi-tional random elds (CRFs). The al-gorithms rely on Viterbi decoding of training examples, combined with sim-ple additive updates. We describe the-ory justifying the algorithms through a modication of the proof of conver-gence of the perceptron algorithm for classi cation problems. We give exper-imental results on part-of-speech tag-ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger."	"Identical to the standard perceptron proof , e.g. , #AUTHOR_TAG , by inserting in loss-separability for normal separability ."	"['A training set D is loss-separable with margin γ > 0 if there exists a vector u with u = 1 such that for all y , y ∈ Y x and (x, y) ∈ D, if L(y , y) < L(y , y), then u•Φ(y )−u•Φ(y ) ≥ γ.', 'Furthermore, let R ≥ ||Φ(y) − Φ(y )||, for all y, y .', 'Assumption 1. Assume training set D is lossseparable with margin γ.', 'Theorem 1.', 'Given Assumption 1.', 'Let m be the number of mistakes made when training the perceptron (Algorithm 2) with inline ranker loss (Algorithm 3) on D, where a mistake occurs for (x, y) ∈ D with parameter vector θ when ∃ŷ j ∈ F k-best θ (x) wherê y j =ŷ 1 and L(ŷ j , y) < L(ŷ 1 , y).', 'If training is run indefinitely, then m ≤ R 2 γ 2 .', 'Proof.', 'Identical to the standard perceptron proof , e.g. , #AUTHOR_TAG , by inserting in loss-separability for normal separability .']"	0
CC122	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	characterizing the errors of datadriven dependency parsing models	"['R McDonald', 'J Nivre']"	experiments	"We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing: global, exhaustive, graph-based models, and local, greedy, transition-based models. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. This analysis leads to new directions for parser development."	"Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010)."	"['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).']"	4
CC123	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	corpus variation and parser performance	['D Gildea']	introduction	"Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might a ect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model."	"In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) ."	"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']"	0
CC124	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	discriminative reranking for natural language parsing	['Michael Collins']	introduction	"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation"	One obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ) .	"['One obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ) .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework).', 'The reranker can then be trained to optimize for the downstream or extrinsic objective.', 'While this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser.']"	0
CC125	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	global learning of focused entailment graphs	"['J Berant', 'I Dagan', 'J Goldberger']"	experiments	"We propose a global algorithm for learning entailment relations between predicates. We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms."	"Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG ) ."	"['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG ) .']"	0
CC126	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	guiding semisupervision with constraintdriven learning	"['M W Chang', 'L Ratinov', 'D Roth']"	introduction		"This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ) ."	"['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'For our setting this would mean using weak application specific signals to improve dependency parsing.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']"	1
CC127	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	algorithms for deterministic incremental dependency parsing	['J Nivre']	experiments	"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework."	An implementation of the transition-based dependency parsing framework ( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .	"['An implementation of the transition-based dependency parsing framework ( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .', 'Beams with varying sizes can be used to produce k-best lists.', 'The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).', 'All feature conjunctions are included.']"	5
CC128	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	generalized expectation criteria for semisupervised learning with weakly labeled data	"['G S Mann', 'A McCallum']"	introduction	"In this paper, we present an overview of generalized expectation criteria (GE), a simple, robust, scalable method for semi-supervised training using weakly-labeled data. GE fits model parameters by favoring models that match certain expectation constraints, such as marginal label distributions, on the unlabeled data. This paper shows how to apply generalized expectation criteria to two classes of parametric models: maximum entropy models and conditional random fields. Experimental results demonstrate accuracy improvements over supervised training and a number of other state-of-the-art semi-supervised learning methods for these models."	"This includes work on generalized expectation ( #AUTHOR_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) ."	"['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( #AUTHOR_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'For our setting this would mean using weak application specific signals to improve dependency parsing.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']"	0
CC129	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	conllx shared task on multilingual dependency parsing	"['S Buchholz', 'E Marsi']"	experiments	"Each year the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?"	For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG ) .	"['In the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented-loss for each one.', 'Augmented-loss learning is then applied to target a downstream task using the loss functions to measure gains.', 'We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score.', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG ) .']"	5
CC130	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	uptraining for accurate deterministic question parsing	"['S Petrov', 'P C Chang', 'M Ringgaard', 'H Alshawi']"	introduction	"It is well known that parsing accuracies drop significantly on out-of-domain data. What is less known is that some parsers suffer more from domain shifts than others. We show that dependency parsers have more difficulty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance."	"In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) ."	"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']"	0
CC131	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	what is the jeopardy model a quasisynchronous grammar for qa	"['M Wang', 'N A Smith', 'T Mitamura']"	introduction	"This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions. Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimentalresultsusing theTRECdataset are shown to significantly outperform strong state-of-the-art baselines."	"This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks ."	"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']"	0
CC132	D11-1138	Training dependency parsers by jointly optimizing multiple objectives	a lightweight evaluation framework for machine translation reordering	"['D Talbot', 'H Kazawa', 'H Ichikawa', 'J Katz-Brown', 'M Seno', 'F Och']"	experiments	Reordering is a major challenge for machine translation between distant languages. Recent work has shown that evaluation metrics that explicitly account for target language word order correlate better with human judgments of translation quality. Here we present a simple framework for evaluating word order independently of lexical choice by comparing the system's reordering of a source sentence to reference reordering data generated from manually word-aligned translations. When used to evaluate a system that performs reordering as a preprocessing step our framework allows the parser and reordering rules to be evaluated extremely quickly without time-consuming end-to-end machine translation experiments. A novelty of our approach is that the translations used to generate the reordering reference data are generated in an alignment-oriented fashion. We show that how the alignments are generated can significantly effect the robustness of the evaluation. We also outline some ways in which this framework has allowed our group to analyze reordering errors for English to Japanese machine translation.	criteria and data used in our experiments are based on the work of #AUTHOR_TAG .	['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']	5
CC133	D12-1037	Discriminative Training for Log-Linear Based SMT	svmknn discriminative nearest neighbor classification for visual category recognition	"['Hao Zhang', 'Alexander C Berg', 'Michael Maire', 'Jitendra Malik']"	introduction	"We consider visual category recognition in the framework of measuring similarities, or equivalently perceptual distances, to prototype examples of categories. This approach is quite flexible, and permits recognition based on color, texture, and particularly shape, in a homogeneous framework. While nearest neighbor classifiers are natural in this setting, they suffer from the problem of high variance (in bias-variance decomposition) in the case of limited sampling. Alternatively, one could use support vector machines but they involve time-consuming optimization and computation of pairwise distances. We propose a hybrid of these two methods which deals naturally with the multiclass setting, has reasonable computational complexity both in training and at run time, and yields excellent results in practice. The basic idea is to find close neighbors to a query sample and train a local support vector machine that preserves the distance function on the collection of neighbors. Our method can be applied to large, multiclass data sets for which it outperforms nearest neighbor and support vector machines, and remains efficient when the problem becomes intractable for support vector machines. A wide variety of distance functions can be used and our experiments show state-of-the-art performance on a number of benchmark data sets for shape and texture classification (MNIST, USPS, CUReT) and object recognition (Caltech- 101). On Caltech-101 we achieved a correct classification rate of 59.05%(+-0.56%) at 15 training images per class, and 66.23%(+-0.48%) at 30 training images."	"In this paper , inspired by KNN-SVM ( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems ."	"['In this paper , inspired by KNN-SVM ( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', 'Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'This online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'Similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'To put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set.']"	4
CC134	D12-1037	Discriminative Training for Log-Linear Based SMT	minimum error rate training in statistical machine translation	['Franz Josef Och']	introduction	"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT."	"Motivated by (#AUTHOR_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\u2212WbII2+ A \ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\u03b1W \u00b7 h(fj, e)] P\u03b1(e|fj; W) = (7) Ee'Ec; exp[\u03b1W \u00b7 h(fj, e')], where \u03b1 > 0 is a real number valued smoother."	"['Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.', ""Motivated by (#AUTHOR_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[αW · h(fj, e)] Pα(e|fj; W) = (7) Ee'Ec; exp[αW · h(fj, e')], where α > 0 is a real number valued smoother."", 'One can see that, in the extreme case, for α —* oc, (6) converges to (5).']"	4
CC135	D12-1037	Discriminative Training for Log-Linear Based SMT	discriminative training and maximum entropy models for statistical machine translation	"['Franz Josef Och', 'Hermann Ney']"	introduction	"We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach."	"#AUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :"	"['#AUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :']"	0
CC136	D12-1037	Discriminative Training for Log-Linear Based SMT	examplebased decoding for statistical machine translation	"['Taro Watanabe', 'Eiichiro Sumita']"	related work		"Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) ."	"['Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) .', 'Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights.']"	1
CC137	D12-1037	Discriminative Training for Log-Linear Based SMT	nearoptimal hashing algorithms for approximate nearest neighbor in high dimensions	"['Alexandr Andoni', 'Piotr Indyk']"	experiments	"We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice"	"Actually , if we use LSH technique ( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data ."	"['Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method 4 .', 'This shows that the local method is efficient.', 'Further, compared to the retrieval, the local training is not the bottleneck.', 'Actually , if we use LSH technique ( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data .']"	3
CC138	D12-1037	Discriminative Training for Log-Linear Based SMT	optimal search for minimum error rate training	"['Michel Galley', 'Chris Quirk']"	related work	"Abstract Minimum error rate training is a crucial component to many state-of-the-art NLP applications, such as machine translation and speech recognition. However, common evaluation functions such as BLEU or word error rate are generally highly non-convex and thus prone to search errors. In this paper, we present LP-MERT, an exact search algorithm for minimum error rate training that reaches the global optimum using a series of reductions to linear programming. Given a set of N -best lists produced from S input sentences, this algorithm finds a linear model that is globally optimal with respect to this set. We find that this algorithm is polynomial in N and in the size of the model, but exponential in S. We present extensions of this work that let us scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm"	"(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it."	"['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza- tion objectives by introducing a margin-based and ranking-based indirect loss functions.']"	1
CC139	D12-1037	Discriminative Training for Log-Linear Based SMT	local learning algorithms	"['L´eon Bottou', 'Vladimir Vapnik']"	introduction	"The goal of digital image processing is to capture, transmit, and display images as efficiently as possible. Such tasks are computationally intensive because an image is digitally represented by large amounts of data. It is possible to render an image by reconstructing it with a subset of the most relevant data. One such procedure used to accomplish this task is commonly referred to as sparse coding. For our purpose, we use images of handwritten digits that are presented to an artificial neural network. The network implements Rozell u27s locally competitive algorithm (LCA) to generate a sparse code. This sparse code is then presented to another neural network, a classifier that attempts to place the image in one of ten categories, each representing one of the digits zero through nine. Furthermore, the LCA approach is unique in that it produces quality sparse codes by utilizing highly parallel architectures. Pattern recognition problems have been of interest by industries that rely heavily on data as a core part of their business. Social networking companies use it to analyze, predict, and even influence user behavior. However, as data becomes more cost-effective to collect, it will be important for companies in other industries to extract useful information from said data. Manufacturing companies use it to analyze the performance of their products and financial service companies use it to flag customers likely to default on their loans. Interestingly, the image processing techniques described above can be generalized for use on data other than image data"	"The local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ) ."	"['The local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .', 'Compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'It is superior to the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992;Zhang et al., 2006).']"	0
CC140	D12-1037	Discriminative Training for Log-Linear Based SMT	a simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters	"['Bing Zhao', 'Shengyuan Chen']"	introduction	"We propose a variation of simplex-downhill algo-rithm specifically customized for optimizing param-eters in statistical machine translation (SMT) de-coder for better end-user automatic evaluation met-ric scores for translations, such as versions of BLEU, TER and mixtures of them. Traditional simplex-downhill has the advantage of derivative-free com-putations of objective functions, yet still gives satis-factory searching directions in most scenarios. This is suitable for optimizing translation metrics as they are not differentiable in nature. On the other hand, Armijo algorithm usually performs line search ef-ficiently given a searching direction. It is a deep hidden fact that an efficient line search method will change the iterations of simplex, and hence the searching trajectories. We propose to embed the Armijo inexact line search within the simplex-downhill algorithm. We show, in our experiments, the proposed algorithm improves over the widely-applied Minimum Error Rate training algorithm for optimizing machine translation parameters."	"Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one ."	"['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']"	0
CC141	D12-1037	Discriminative Training for Log-Linear Based SMT	an empirical study of smoothing techniques for language modeling in	"['Stanley F Chen', 'Joshua Goodman']"	experiments	"We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t..."	"We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( Stolcke , 2002 ) with modified Kneser-Ney smoothing ( #AUTHOR_TAG ) ."	"['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( Stolcke , 2002 ) with modified Kneser-Ney smoothing ( #AUTHOR_TAG ) .', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']"	5
CC142	D12-1037	Discriminative Training for Log-Linear Based SMT	srilm  an extensible language modeling toolkit	['Andreas Stolcke']	experiments	"SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools. 1"	"We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) ."	"['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) .', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']"	5
CC143	D12-1037	Discriminative Training for Log-Linear Based SMT	optimal search for minimum error rate training	"['Michel Galley', 'Chris Quirk']"	introduction	"Abstract Minimum error rate training is a crucial component to many state-of-the-art NLP applications, such as machine translation and speech recognition. However, common evaluation functions such as BLEU or word error rate are generally highly non-convex and thus prone to search errors. In this paper, we present LP-MERT, an exact search algorithm for minimum error rate training that reaches the global optimum using a series of reductions to linear programming. Given a set of N -best lists produced from S input sentences, this algorithm finds a linear model that is globally optimal with respect to this set. We find that this algorithm is polynomial in N and in the size of the model, but exponential in S. We present extensions of this work that let us scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm"	"Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one ."	"['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']"	0
CC144	D12-1037	Discriminative Training for Log-Linear Based SMT	incremental and decremental support vector machine learning	"['G Cauwenberghs', 'T Poggio']"		"An on-line recursive algorithm for training support vector machines, one vector at a time, is presented. Adiabatic increments retain the Kuhn-Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decremental ""unlearning"" offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data."	"In the field of machine learning research , incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation ."	"['Compared with retraining mode, incremental training can improve the training efficiency.', 'In the field of machine learning research , incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .', 'The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'In this section, we will investigate the incremental trainingmethodsinSMTscenario.']"	0
CC145	D12-1037	Discriminative Training for Log-Linear Based SMT	a simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters	"['Bing Zhao', 'Shengyuan Chen']"	related work	"We propose a variation of simplex-downhill algo-rithm specifically customized for optimizing param-eters in statistical machine translation (SMT) de-coder for better end-user automatic evaluation met-ric scores for translations, such as versions of BLEU, TER and mixtures of them. Traditional simplex-downhill has the advantage of derivative-free com-putations of objective functions, yet still gives satis-factory searching directions in most scenarios. This is suitable for optimizing translation metrics as they are not differentiable in nature. On the other hand, Armijo algorithm usually performs line search ef-ficiently given a searching direction. It is a deep hidden fact that an efficient line search method will change the iterations of simplex, and hence the searching trajectories. We propose to embed the Armijo inexact line search within the simplex-downhill algorithm. We show, in our experiments, the proposed algorithm improves over the widely-applied Minimum Error Rate training algorithm for optimizing machine translation parameters."	"( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it ."	"['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']"	1
CC146	D12-1037	Discriminative Training for Log-Linear Based SMT	random restarts in minimum error rate training for statistical machine translation	"['Robert C Moore', 'Chris Quirk']"	related work	"Och's (2003) minimum error rate training (MERT) procedure is the most commonly used method for training feature weights in statistical machine translation (SMT) models. The use of multiple randomized starting points in MERT is a well-established practice, although there seems to be no published systematic study of its benefits. We compare several ways of performing random restarts with MERT. We find that all of our random restart methods outperform MERT without random restarts, and we develop some refinements of random restarts that are superior to the most common approach with regard to resulting model quality and training time."	"( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it ."	"['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']"	1
CC147	D12-1037	Discriminative Training for Log-Linear Based SMT	statistical significance tests for machine translation evaluation	['Philipp Koehn']	experiments	"If two translation systems differ differ in perfor-mance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling meth-ods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real."	The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG ) .	"['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG ) .']"	5
CC148	D12-1037	Discriminative Training for Log-Linear Based SMT	tuning as ranking	"['Mark Hopkins', 'Jonathan May']"		"We offer a simple, effective, and scalable method for statistical machine translation pa-rameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to imple-ment. It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours. We establish PRO's scalability and effective-ness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios."	"(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#AUTHOR_TAG, 2011)."	"['(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#AUTHOR_TAG, 2011).', 'Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as ”•”, and (−1, 0) corresponds to a negative example denoted as ”*”.', 'Since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'However, one can obtain e11 and e21 with weights: (1, 1) and (−1, 1), respectively.']"	0
CC149	D12-1037	Discriminative Training for Log-Linear Based SMT	discriminative training and maximum entropy models for statistical machine translation	"['Franz Josef Och', 'Hermann Ney']"	related work	"We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach."	"( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT."	"['Several works have proposed discriminative techniques to train log-linear model for SMT.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']"	1
CC150	D12-1037	Discriminative Training for Log-Linear Based SMT	examplebased decoding for statistical machine translation	"['Taro Watanabe', 'Eiichiro Sumita']"			"To retrieve translation examples for a test sentence , ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :"	"['The metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence , ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :']"	5
CC151	D12-1037	Discriminative Training for Log-Linear Based SMT	minimum error rate training in statistical machine translation	['Franz Josef Och']	introduction	"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT."	"Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one ."	"['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']"	0
CC152	D12-1037	Discriminative Training for Log-Linear Based SMT	ultraconservative online algorithms for multiclass problems	"['Koby Crammer', 'Yoram Singer']"		"In this paper we study a paradigm to generalize online classification algorithms for binary classification problems to multiclass problems. The particular hypotheses we investigate maintain one prototype vector per class. Given an input instance, a multiclass hypothesis computes a similarity-score between each prototype and the input instance and sets the predicted label to be the index of the prototype achieving the highest similarity. To design and analyze the learning algorithms in this paper we introduce the notion of ultraconservativeness. Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype. We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores. We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm. The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well. We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems. We discuss the form the algorithms take in the binary case and show that all the algorithms from the first family reduce to the Perceptron algorithm while MIRA provides a new Perceptron-like algorithm with a margin-dependent learning rate. We then return to multiclass problems and describe an analogous multiplicative family of algorithms with corresponding mistake bounds. We end the formal part by deriving and analyzing a multiclass version of Li and Long's ROMMA algorithm. We conclude with a discussion of experimental results that demonstrate the merits of our algorithms."	"We employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows ."	"['Following the notations in Algorithm 2, W b is the baseline weight, D i = { f i j , c i j , r i j } K j=1 denotes training examples for t i .', 'For the sake of brevity, we will drop the index i, D i = { f j , c j , r j } K j=1 , in the rest of this paper.', 'Our goal is to find an optimal weight, denoted by W i , which is a local weight and used for decoding the sentence t i .', 'Unlike the global method which performs tuning on the whole development set Dev + D i as in Algorithm 1, W i can be incrementally learned by optimizing on D i based on W b .', 'We employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows .']"	5
CC153	D12-1037	Discriminative Training for Log-Linear Based SMT	minimum error rate training in statistical machine translation	['Franz Josef Och']	related work	"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT."	"( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it ."	"['Several works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']"	1
CC154	D12-1037	Discriminative Training for Log-Linear Based SMT	a hierarchical phrasebased model for statistical machine translation	['David Chiang']	experiments		"We use an in-house developed hierarchical phrase-based translation ( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero ."	"['We use an in-house developed hierarchical phrase-based translation ( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero .', 'To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-    (Koehn et al., 2007).', 'Both of these systems are with default setting.', 'All three systems are trained by MERT with 100 best candidates.']"	5
CC155	D12-1037	Discriminative Training for Log-Linear Based SMT	improved statistical alignment models	"['Franz Josef Och', 'Hermann Ney']"	experiments	"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignmen"	"We run GIZA + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair ."	"['We run GIZA + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']"	5
CC156	D12-1037	Discriminative Training for Log-Linear Based SMT	tuning as ranking	"['Mark Hopkins', 'Jonathan May']"	related work	"We offer a simple, effective, and scalable method for statistical machine translation pa-rameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to imple-ment. It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours. We establish PRO's scalability and effective-ness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios."	"( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions ."	"['Several works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .']"	1
CC157	D12-1037	Discriminative Training for Log-Linear Based SMT	tuning as ranking	"['Mark Hopkins', 'Jonathan May']"	introduction	"We offer a simple, effective, and scalable method for statistical machine translation pa-rameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to imple-ment. It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours. We establish PRO's scalability and effective-ness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios."	"Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one ."	"['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']"	0
CC158	D13-1038	Embodied Collaborative Referring Expression Generation in Situated Human-Robot Interaction	the tunareg challenge 2009 overview and evaluation results	"['Albert Gatt', 'Anja Belz', 'Eric Kow']"	experiments		"Although evaluated on different data sets , this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ) ."	"['Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'Although evaluated on different data sets , this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ) .', ""However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to  the situation where the agent's representation of the shared world is problematic and full of mistakes.""]"	1
CC159	D13-1038	Embodied Collaborative Referring Expression Generation in Situated Human-Robot Interaction	towards mediating shared perceptual basis in situated dialogue	"['Changsong Liu', 'Rui Fang', 'Joyce Y Chai']"		"To enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities. In particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references. Our empirical results have shown that, even when computer vision algorithms produce many errors (e.g. 84.7% of the objects in the environment are mis-recognized), our approach can still achieve 66% accuracy in referential grounding. These results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction."	"Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) ."	"['Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990).', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .', 'For the referring expression generation task here, we also need a lexicon with grounded semantics.']"	2
CC160	D13-1038	Embodied Collaborative Referring Expression Generation in Situated Human-Robot Interaction	towards mediating shared perceptual basis in situated dialogue	"['Changsong Liu', 'Rui Fang', 'Joyce Y Chai']"	introduction	"To enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities. In particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references. Our empirical results have shown that, even when computer vision algorithms produce many errors (e.g. 84.7% of the objects in the environment are mis-recognized), our approach can still achieve 66% accuracy in referential grounding. These results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction."	How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ) .	"['How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ) .', 'In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue.', 'Robots have much lower perceptual capabilities of the environment than humans.', 'How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to?']"	2
CC161	D13-1115	Integrating Theory and Practice: A Daunting Task	combining feature norms and text data with topic models	['Mark Steyvers']	related work	"Many psychological theories of semantic cognition assume that concepts are represented by features. The empirical procedures used to elicit features from humans rely on explicit human judgments which limit the scope of such representations. An alternative computational framework for semantic cognition that does not rely on explicit human judgment is based on the statistical analysis of large text collections. In the topic modeling approach, documents are represented as a mixture of learned topics where each topic is represented as a probability distribution over words. We propose feature-topic models, where each document is represented by a mixture of learned topics as well as predefined topics that are derived from feature norms. Results indicate that this model leads to systematic improvements in generalization tasks. We show that the learned topics in the model play in an important role in the generalization performance by including words that are not part of current feature norms.2009 Elsevier B.V. All rights reserved."	"In a similar vein , #AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task ."	"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein , #AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"	0
CC162	D13-1115	Integrating Theory and Practice: A Daunting Task	training a multilingual sportscaster using perceptual context to learn language	"['David L Chen', 'Joohyun Kim', 'Raymond J Mooney']"	related work	We present a novel framework for learning to interpret and generate language using only perceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.	"Some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies ."	"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter cat- egory, the two most common representations have been association norms, where subjects are given a cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).']"	0
CC163	D13-1115	Integrating Theory and Practice: A Daunting Task	distinctive image features from scaleinvariant keypoints	['David G Lowe']	related work	This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ...	They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .	"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']"	0
CC164	D13-1115	Integrating Theory and Practice: A Daunting Task	describing objects by their attributes	"['Ali Farhadi', 'Ian Endres', 'Derek Hoiem', 'David Forsyth']"	related work	"We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (""spotty dog"", not just ""dog""); to say something about unfamiliar objects (""hairy and four-legged"", not just ""unknown""); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (""spotty"") or discriminative (""dogs have it but sheep do not""). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attributebased framework. 1"	"More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( #AUTHOR_TAG ) , act as excellent substitutes for feature"	"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( #AUTHOR_TAG ) , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']"	0
CC165	D13-1115	Integrating Theory and Practice: A Daunting Task	simple supervised document geolocation with geodesic grids	"['Benjamin Wing', 'Jason Baldridge']"	introduction	"We investigate automatic geolocation (i.e. identification of the location, expressed as latitude/longitude coordinates) of documents. Geolocation can be an effective means of summarizing large document collections and it is an important component of geographic information retrieval. We describe several simple supervised methods for document geolocation using only the document's raw text as evidence. All of our methods predict locations in the context of geodesic grids of varying degrees of resolution. We evaluate the methods on geotagged Wikipedia articles and Twitter feeds. For Wikipedia, our best method obtains a median prediction error of just 11.8 kilometers. Twitter geolocation is more challenging: we obtain a median error of 479 km, an improvement on previous results for the dataset."	"Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 ) ."	"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 ) .']"	0
CC166	D13-1115	Integrating Theory and Practice: A Daunting Task	integrating experiential and distributional data to learn semantic representations	"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']"	introduction	"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved."	"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) ."	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	0
CC167	D13-1115	Integrating Theory and Practice: A Daunting Task	integrating experiential and distributional data to learn semantic representations	"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']"	related work	"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved."	"That is, we simply take the original mLDA model of #AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA."	"['That is, we simply take the original mLDA model of #AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', 'At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi.']"	2
CC168	D13-1115	Integrating Theory and Practice: A Daunting Task	learning language semantics from ambiguous supervision	"['Rohit J Kate', 'Raymond J Mooney']"	introduction	"This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers."	"Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ) ."	"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ) .', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a;Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012).']"	0
CC169	D13-1115	Integrating Theory and Practice: A Daunting Task	integrating experiential and distributional data to learn semantic representations	"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']"	method	"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved."	#AUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .	"['#AUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'In their model, a document consists of a set of (word, feature) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', 'Topics consist of multinomial distributions over words, β k , but are extended to also include multinomial distributions over features, ψ k .', 'The generative process is amended to include these feature distributions:']"	0
CC170	D13-1115	Integrating Theory and Practice: A Daunting Task	webscale kmeans clustering	['D Sculley']	experiments		"The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords ."	"['First, we compute a simple Bag of Visual Words (BoVW) model for our images using SURF keypoints (Bay et al., 2008).', 'SURF is a method for selecting points-of-interest within an image.', 'It is faster and more forgiving than the commonly known SIFT algorithm.', 'We compute SURF keypoints for every image in our data set using Sim-pleCV 3 and randomly sample 1% of the keypoints.', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords .', 'All images for a given word are summed together to provide an average representation for the word.', 'We refer to this representation as the SURF modality.']"	5
CC171	D13-1115	Integrating Theory and Practice: A Daunting Task	grounded models of semantic representation	"['Carina Silberer', 'Mirella Lapata']"	experiments	"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two."	"This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG ) ."	"['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .']"	1
CC172	D13-1115	Integrating Theory and Practice: A Daunting Task	grounding action descriptions in videos	"['Michaela Regneri', 'Marcus Rohrbach', 'Dominikus Wetzel', 'Stefan Thater', 'Bernt Schiele', 'Manfred Pinkal']"	related work	"Recent work has shown that the integration of visual information into text-based models can substantially improve model predictions, but so far only visual information extracted from static images has been used. In this paper, we consider the problem of grounding sentences describing actions in visual information ex-tracted from videos. We present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other. Experimental results demonstrate that a text-based model of similarity between actions improves substan-tially when combined with visual information from videos depicting the described actions."	"Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG ) ."	"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG ) .']"	0
CC173	D13-1115	Integrating Theory and Practice: A Daunting Task	semantic feature production norms for a large set of living and nonliving things	"['Ken McRae', 'George S Cree', 'Mark S Seidenberg', 'Chris McNorgan']"	related work	"Semantic features have provided insight into numerous behavioral phenomena concerning concepts, categorization, and semantic memory in adults, children, and neuropsychological populations. Numerous theories and models in these areas are based on representations and computations involving semantic features. Consequently, empirically derived semantic feature production norms have played, and continue to play, a highly useful role in these domains. This article describes a set of feature norms collected from approximately 725 participants for 541 living (dog) and nonliving (chair) basic-level concepts, the largest such set of norms developed to date. This article describes the norms and numerous statistics associated with them. Our aim is to make these norms available to facilitate other research, while obviating the need to repeat the labor-intensive methods involved in collecting and analyzing such norms. The full set of norms may be downloaded from www.psychonomic.org/archive."	"Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #AUTHOR_TAG ) ."	"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the us- age of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholin- guistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #AUTHOR_TAG ) .']"	0
CC174	D13-1115	Integrating Theory and Practice: A Daunting Task	learning to interpret natural language navigation instructions from observations	"['David L Chen', 'Raymond J Mooney']"	introduction	"The ability to understand natural-language instructions is crit-ical to building intelligent agents that interact with humans. We present a system that learns to transform natural-language navigation instructions into executable formal plans. Given no prior linguistic knowledge, the system learns by simply observing how humans follow navigation instructions. The system is evaluated in three complex virtual indoor environ-ments with numerous objects and landmarks. A previously collected realistic corpus of complex English navigation in-structions for these environments is used for training and test-ing data. By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser, the sys-tem is able to automatically learn to correctly interpret a rea-sonable fraction of the complex instructions in this corpus."	"Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) ."	"['The language grounding problem has come in many different flavors with just as many different ap- proaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .', 'Some efforts have tackled tasks such as automatic image caption generation (Feng and La- pata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identifica- tion of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012).']"	0
CC175	D13-1115	Integrating Theory and Practice: A Daunting Task	im2text describing images using 1 million captioned photographs	"['Vicente Ordonez', 'Girish Kulkarni', 'Tamara L Berg']"	introduction	"We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset - performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning."	"Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; #AUTHOR_TAG ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) ."	"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; #AUTHOR_TAG ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']"	0
CC176	D13-1115	Integrating Theory and Practice: A Daunting Task	models of semantic representation with visual attributes	"['Carina Silberer', 'Vittorio Ferrari', 'Mirella Lapata']"	introduction	We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.	"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; #AUTHOR_TAG ) ."	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; #AUTHOR_TAG ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	0
CC177	D13-1115	Integrating Theory and Practice: A Daunting Task	integrating experiential and distributional data to learn semantic representations	"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']"	experiments	"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved."	"We use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."	"['In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.5', 'That is, for the feature norm modal- ity, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc.', 'The resulting stochastically generated corpus is used in its corresponding experiments.']"	5
CC178	D13-1115	Integrating Theory and Practice: A Daunting Task	relative attributes	"['Devi Parikh', 'Kristen Grauman']"	related work	"Visual attributes are great means of describing images or scenes, in a way both humans and computers understand. In order to establish a correspondence between images and to be able to compare the strength of each property between images, relative attributes were introduced. However, since their introduction, hand-crafted and engineered features were used to learn increasingly complex models for the problem of relative attributes. This limits the applicability of those methods for more realistic cases. We introduce a deep neural network architecture for the task of relative attribute prediction. A convolutional neural network (ConvNet) is adopted to learn the features by including an additional layer (ranking layer) that learns to rank the images based on these features. We adopt an appropriate ranking loss to train the whole network in an end-to-end fashion. Our proposed method outperforms the baseline and state-of-the-art methods in relative attribute prediction on various coarse and fine-grained datasets. Our qualitative results along with the visualization of the saliency maps show that the network is able to learn effective features for each specific attribute. Source code of the proposed method is available at https://github.com/yassersouri/ghiaseddin.Comment: ACCV 201"	"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) ."	"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"	0
CC179	D13-1115	Integrating Theory and Practice: A Daunting Task	integrating experiential and distributional data to learn semantic representations	"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']"	method	"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved."	Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .	"['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'However, prior work using mLDA is limited to two modalities at a time.', 'In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.']"	5
CC180	D13-1115	Integrating Theory and Practice: A Daunting Task	distributional semantics in technicolor	"['Elia Bruni', 'Gemma Boleda', 'Marco Baroni', 'NamKhanh Tran']"	experiments	"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance."	This seems to provide additional evidence of #AUTHOR_TAGb ) 's suggestion that something like a distributional hypothesis of images is plausible .	"['We see that the image modalities are much more useful than they are in compositionality prediction.', 'The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model.', 'Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', ""This seems to provide additional evidence of #AUTHOR_TAGb ) 's suggestion that something like a distributional hypothesis of images is plausible .""]"	1
CC181	D13-1115	Integrating Theory and Practice: A Daunting Task	visual and semantic similarity in imagenet	"['Thomas Deselaers', 'Vittorio Ferrari']"	experiments	"Many computer vision approaches take for granted positive answers to questions such as ""Are semantic categories visually separable?"" and ""Is visual similarity correlated to semantic similarity?"". In this paper, we study experimentally whether these assumptions hold and show parallels to questions investigated in cognitive science about the human visual system. The insights gained from our analysis enable building a novel distance function between images assessing whether they are from the same basic-level category. This function goes beyond direct visual distance as it also exploits semantic similarity measured through ImageNet. We demonstrate experimentally that it outperforms purely visual distances."	"It is frequently used in tasks like scene identification , and #AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet ."	"['We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al., 2009).', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ""gist"" of the whole image.', 'It is frequently used in tasks like scene identification , and #AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']"	4
CC182	D13-1115	Integrating Theory and Practice: A Daunting Task	distributional semantics in technicolor	"['Elia Bruni', 'Gemma Boleda', 'Marco Baroni', 'NamKhanh Tran']"	related work	"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance."	#AUTHOR_TAGa ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .	"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAGa ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']"	0
CC183	D13-1115	Integrating Theory and Practice: A Daunting Task	how many words is a picture worth automatic caption generation for news images	"['Yansong Feng', 'Mirella Lapata']"	introduction	"In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods."	"Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) ."	"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']"	0
CC184	D13-1115	Integrating Theory and Practice: A Daunting Task	the wacky wide web a collection of very large linguistically processed webcrawled corpora language resources and evaluation	"['Marco Baroni', 'Silvia Bernardini', 'Adriano Ferraresi', 'Eros Zanchetta']"	experiments		"For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #AUTHOR_TAG ) containing approximately 1.7 B word tokens ."	"['For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #AUTHOR_TAG ) containing approximately 1.7 B word tokens .', 'We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100.', 'The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens.']"	5
CC185	D13-1115	Integrating Theory and Practice: A Daunting Task	improving video activity recognition using object recognition and text mining	"['Tanvi S Motwani', 'Raymond J Mooney']"	related work	"Abstract. Recognizing activities in real-world videos is a chal-lenging AI problem. We present a novel combination of standard activity classification, object recognition, and text mining to learn effective activity recognizers without ever explicitly labeling train-ing videos. We cluster verbs used to describe videos to automatically discover classes of activities and produce a labeled training set. This labeled data is then used to train an activity classifier based on spatio-temporal features. Next, text mining is employed to learn the correla-tions between these verbs and related objects. This knowledge is then used together with the outputs of an off-the-shelf object recognizer and the trained activity classifier to produce an improved activity rec-ognizer. Experiments on a corpus of YouTube videos demonstrate the effectiveness of the overall approach."	"To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos ."	"['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']"	0
CC186	D13-1115	Integrating Theory and Practice: A Daunting Task	perceptual inference through global lexical similarity	"['Brendan T Johns', 'Michael N Jones']"	related work	"The literature contains a disconnect between accounts of how humans learn lexical semantic representations for words. Theories generally propose that lexical semantics are learned either through perceptual experience or through exposure to regularities in language. We propose here a model to integrate these two information sources. Specifically, the model uses the global structure of memory to exploit the redundancy between language and perception in order to generate inferred perceptual representations for words with which the model has no perceptual experience. We test the model on a variety of different datasets from grounded cognition experiments and demonstrate that this diverse set of results can be explained as perceptual simulation (cf. Barsalou, Simmons, Barbey, & Wilson, 2003) within a global memory model.Copyright (c) 2012 Cognitive Science Society, Inc."	#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .	"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"	0
CC187	D13-1115	Integrating Theory and Practice: A Daunting Task	integrating experiential and distributional data to learn semantic representations	"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']"	experiments	"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved."	"Following #AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks ."	"['Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'Following #AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words.', 'We then compute the percentile ranks of similarity for each word pair, e.g., ""cat"" is more similar to ""dog"" than 97.3% of the rest of the vocabulary.', 'We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once.']"	5
CC188	D13-1115	Integrating Theory and Practice: A Daunting Task	integrating experiential and distributional data to learn semantic representations	"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']"	conclusion	"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved."	"In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #AUTHOR_TAG ."	"['In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #AUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA.', 'SURF features also provided significant gains over text-only LDA in predicting the compositionality of noun compounds.']"	5
CC189	D13-1115	Integrating Theory and Practice: A Daunting Task	modeling the shape of the scene a holistic representation of the spatial envelope	"['Aude Oliva', 'Antonio Torralba']"	experiments	"In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category."	"We also compute GIST vectors ( #AUTHOR_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) ."	"['We also compute GIST vectors ( #AUTHOR_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ""gist"" of the whole image.', 'It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet.', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']"	5
CC190	D13-1115	Integrating Theory and Practice: A Daunting Task	supervised textbased geolocation using language models on an adaptive grid	"['Stephen Roller', 'Michael Speriosu', 'Sarat Rallapalli', 'Benjamin Wing', 'Jason Baldridge']"	introduction	"The geographical properties of words have recently begun to be exploited for geolocating documents based solely on their text, often in the context of social media and online content. One common approach for geolocating texts is rooted in information retrieval. Given training documents labeled with latitude/longitude coordinates, a grid is overlaid on the Earth and pseudo-documents constructed by concatenating the documents within a given grid cell; then a location for a test document is chosen based on the most similar pseudo-document. Uniform grids are normally used, but they are sensitive to the dispersion of documents over the earth. We define an alternative grid construction using k-d trees that more robustly adapts to data, especially with larger training sets. We also provide a better way of choosing the locations for pseudo-documents. We evaluate these strategies on existing Wikipedia and Twitter corpora, as well as a new, larger Twitter corpus. The adaptive grid achieves competitive results with a uniform grid on small training sets and outperforms it on the large Twitter corpus. The two grid constructions can also be combined to produce consistently strong results across all training sets."	"Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG ) ."	"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG ) .']"	0
CC191	D13-1115	Integrating Theory and Practice: A Daunting Task	topics in semantic representation	"['Thomas L Griffiths', 'Mark Steyvers', 'Joshua B Tenenbaum']"	related work	"Accounts of language processing have suggested that it requires retrieving concepts from memory in response to an ongoing stream of information. This can be facilitated by inferring the gist of a sentence, conversation, or document, and using that computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference. This leads us to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics. The topic model performs well in predicting word association and the effects of semantic association and ambiguity on a variety of language processing and memory tasks. It also provides a foundation for developing more richly structured statistical models of language, as the generative process assumed in the topic model can easily be extended to incorporate other kinds of semantic and syntactic structure. Many aspects of perception and cognition can be understood by considering the computational problem that is addressed by a particular human capacity (Andersion, 1990; Marr, 1982). Perceptual capacities such as identifying shape from shading (Freeman, 1994), motion perceptio"	"#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( Deerwester et al. , 1990 ) in the prediction of association norms ."	"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', '#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( Deerwester et al. , 1990 ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"	0
CC192	D13-1115	Integrating Theory and Practice: A Daunting Task	zeroshot learning through crossmodal transfer	"['Richard Socher', 'Milind Ganjoo', 'Hamsa Sridhar', 'Osbert Bastani', 'Christopher D Manning', 'Andrew Y Ng']"	related work		"To name a few examples , Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos ."	"['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .']"	0
CC193	D13-1115	Integrating Theory and Practice: A Daunting Task	imagenet a largescale hierarchical image database	"['Jia Deng', 'Wei Dong', 'Richard Socher', 'Li-Jia Li', 'Kai Li', 'Li Fei-Fei']"	experiments	"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ""ImageNet"", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."	"ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #AUTHOR_TAG ) ."	"['BilderNetle (""little ImageNet"" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.', 'ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #AUTHOR_TAG ) .', 'Multiple synsets exist for each meaning of a word.', 'For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral.', 'This BilderNetle data set provides mappings from German noun types to images of the nouns via ImageNet.']"	5
CC194	D13-1115	Integrating Theory and Practice: A Daunting Task	grounded models of semantic representation	"['Carina Silberer', 'Mirella Lapata']"	related work	"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two."	"#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity ."	"['Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', '#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .']"	0
CC195	D13-1115	Integrating Theory and Practice: A Daunting Task	models of semantic representation with visual attributes	"['Carina Silberer', 'Vittorio Ferrari', 'Mirella Lapata']"	related work	We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.	"More recently , #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition ( Farhadi et al. , 2009 ) , act as excellent substitutes for feature"	"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently , #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition ( Farhadi et al. , 2009 ) , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']"	0
CC196	D13-1115	Integrating Theory and Practice: A Daunting Task	online learning for latent dirichlet allocation	"['Matthew Hoffman', 'David M Blei', 'Francis Bach']"	experiments	"We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time."	"The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #AUTHOR_TAG ."	"['In all settings, we fix all Dirichlet priors at 0.1, use a learning rate 0.7, and use minibatch sizes of 1024 documents.', 'We do not optimize these hyperparameters or vary them over time.', 'The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #AUTHOR_TAG .']"	5
CC197	D13-1115	Integrating Theory and Practice: A Daunting Task	grounded models of semantic representation	"['Carina Silberer', 'Mirella Lapata']"	introduction	"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two."	"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) ."	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	0
CC198	D13-1115	Integrating Theory and Practice: A Daunting Task	distinctive image features from scaleinvariant keypoints	['David G Lowe']	related work	This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ...	"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) ."	"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"	0
CC199	D13-1115	Integrating Theory and Practice: A Daunting Task	grounded models of semantic representation	"['Carina Silberer', 'Mirella Lapata']"	method	"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two."	"Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ) ."	"['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009).', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'However, prior work using mLDA is limited to two modalities at a time.', 'In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.']"	0
CC200	D13-1115	Integrating Theory and Practice: A Daunting Task	combining feature norms and text data with topic models	['Mark Steyvers']	introduction	"Many psychological theories of semantic cognition assume that concepts are represented by features. The empirical procedures used to elicit features from humans rely on explicit human judgments which limit the scope of such representations. An alternative computational framework for semantic cognition that does not rely on explicit human judgment is based on the statistical analysis of large text collections. In the topic modeling approach, documents are represented as a mixture of learned topics where each topic is represented as a probability distribution over words. We propose feature-topic models, where each document is represented by a mixture of learned topics as well as predefined topics that are derived from feature norms. Results indicate that this model leads to systematic improvements in generalization tasks. We show that the learned topics in the model play in an important role in the generalization performance by including words that are not part of current feature norms.2009 Elsevier B.V. All rights reserved."	"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) ."	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	0
CC201	D13-1115	Integrating Theory and Practice: A Daunting Task	learning the abstract motion semantics of verbs from captioned videos	"['Stefan Mathe', 'Afsaneh Fazly', 'Sven Dickinson', 'Suzanne Stevenson']"	related work	"We propose an algorithm for learning the semantics of a (motion) verb from videos depicting the action expressed by the verb, paired with sentences describing the action participants and their roles. Acknowledging that commonalities among example videos may not exist at the level of the input features, our approximation algorithm efficiently searches the space of more abstract features for a common solution. We test our algorithm by using it to learn the semantics of a sample set of verbs; results demonstrate the usefulness of the proposed framework, while identifying directions for further improvement."	"Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 ) ."	"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 ) .']"	0
CC202	D13-1115	Integrating Theory and Practice: A Daunting Task	association norms of german noun compounds	"['Sabine Schulte im Walde', 'Susanne Borgwaldt', 'Ronny Jauch']"	experiments	"This paper introduces association norms of German noun compounds as a lexical-semantic resource for cognitive and computational linguistics research on compositionality. Based on an existing database of German noun compounds, we collected human associations to the compounds and their constituents within a web experiment. The current study describes the collection process and a part-of-speech analysis of the association resource. In addition, we demonstrate that the associations provide insight into the semantic properties of the compounds, and perform a case study that predicts the degree of compositionality of the experiment compound nouns, as relying on the norms. Applying a comparatively simple measure of association overlap, we reach a Spearman rank correlation coefficient of rs = 0.5228, p &lt;.000001, when comparing our predictions with human judgements"	Association Norms ( AN ) is a collection of association norms collected by Schulte im #AUTHOR_TAG .	"['Association Norms ( AN ) is a collection of association norms collected by Schulte im #AUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'After removing responses given only once in the entire study, the data set contains a total of 95,214 cue-response pairs for 1,012 nouns and 5,716 response types.']"	5
CC203	D13-1115	Integrating Theory and Practice: A Daunting Task	latent dirichlet allocation	"['David M Blei', 'Andrew Y Ng', 'Michael I Jordan']"	method	"Topic models (e.g., pLSA, LDA, sLDA) have been widely used for segmenting imagery. However, these models are confined to crisp segmentation, forcing a visual word (i.e., an image patch) to belong to one and only one topic. Yet, there are many images in which some regions cannot be assigned a crisp categorical label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and an associated parameter estimation algorithm. This model can be useful for imagery where a visual word may be a mixture of multiple topics. Experimental results on visual and sonar imagery show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability previous topic modeling methods do not have.Comment: Version 1, Sent for Review. arXiv admin note: substantial text   overlap with arXiv:1511.0282"	"Latent Dirichlet Allocation ( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents ."	"['Latent Dirichlet Allocation ( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', 'LDA assumes every document in the corpus is generated using the fol-lowing generative process:']"	0
CC204	D13-1115	Integrating Theory and Practice: A Daunting Task	learning to parse natural language commands to a robot control system	"['Cynthia Matuszek', 'Evan Herbst', 'Luke Zettlemoyer', 'Dieter Fox']"	related work	"Abstract As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor environment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our formal representation allows a robot to interpret route instructions online while moving through a previously unknown environment."	"Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies ."	"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"	0
CC205	D13-1115	Integrating Theory and Practice: A Daunting Task	what helps where–and why semantic relatedness for knowledge transfer	"['Marcus Rohrbach', 'Michael Stark', 'Gy¨orgy Szarvas', 'Iryna Gurevych', 'Bernt Schiele']"	related work	"Remarkable performance has been reported to recognize  single object classes. Scalability to large numbers of classes  however remains an important challenge for today's recognition  methods. Several authors have promoted knowledge  transfer between classes as a key ingredient to address this  challenge. However, in previous work the decision, which  knowledge to transfer has required either manual supervision  or at least a few training examples limiting the scalability  of these approaches. In this work we explicitly address  the question of how to automatically decide which information  to transfer between classes without the need of any human  intervention. For this we tap into linguistic knowledge  bases to provide the semantic link between sources (what)  and targets (where) of knowledge transfer. We provide a rigorous  experimental evaluation of different knowledge bases  and state-of-the-art techniques from Natural Language Processing  which goes far beyond the limited use of language  in related work. We also give insights into the applicability  (why) of different knowledge sources and similarity measures  for knowledge transfer"	"To name a few examples , #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos ."	"['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .']"	0
CC206	D13-1115	Integrating Theory and Practice: A Daunting Task	learning to interpret natural language navigation instructions from observations	"['David L Chen', 'Raymond J Mooney']"	related work	"The ability to understand natural-language instructions is crit-ical to building intelligent agents that interact with humans. We present a system that learns to transform natural-language navigation instructions into executable formal plans. Given no prior linguistic knowledge, the system learns by simply observing how humans follow navigation instructions. The system is evaluated in three complex virtual indoor environ-ments with numerous objects and landmarks. A previously collected realistic corpus of complex English navigation in-structions for these environments is used for training and test-ing data. By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser, the sys-tem is able to automatically learn to correctly interpret a rea-sonable fraction of the complex instructions in this corpus."	"Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies ."	"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"	0
CC207	D13-1115	Integrating Theory and Practice: A Daunting Task	integrating experiential and distributional data to learn semantic representations	"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']"	related work	"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved."	"#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks ."	"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"	0
CC208	D13-1115	Integrating Theory and Practice: A Daunting Task	distributional semantics from text and images	"['Elia Bruni', 'Giang Binh Tran', 'Marco Baroni']"	introduction	"We present a distributional semantic model combining text- and image-based features. We evaluate this multimodal semantic model on simulating similarity judgments, concept clus-tering and the BLESS benchmark. When inte-grated with the same core text-based model, image-based features are at least as good as further text-based features, and they capture different qualitative aspects of the tasks, sug-gesting that the two sources of information are complementary."	"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) ."	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	0
CC209	D13-1115	Integrating Theory and Practice: A Daunting Task	models of semantic representation with visual attributes	"['Carina Silberer', 'Vittorio Ferrari', 'Mirella Lapata']"	method	We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.	It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ) .	"['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009).', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ) .', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'However, prior work using mLDA is limited to two modalities at a time.', 'In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.']"	0
CC210	D13-1115	Integrating Theory and Practice: A Daunting Task	the story picturing engine—a system for automatic text illustration	"['Dhiraj Joshi', 'James Z Wang', 'Jia Li']"	introduction	"We present an unsupervised approach to automated story picturing. Semantic keywords are extracted from the story, an annotated image database is searched. Thereafter, a novel image ranking scheme automatically determines the importance of each image. Both lexical annotations and visual content play a role in determining the ranks. Annotations are processed using the Wordnet. A mutual reinforcement-based rank is calculated for each image. We have implemented the methods in our Story Picturing Engine (SPE) system. Experiments on large-scale image databases are reported. A user study has been performed and statistical analysis of the results has been presented."	"Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #AUTHOR_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) ."	"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #AUTHOR_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']"	0
CC211	D13-1115	Integrating Theory and Practice: A Daunting Task	learning to parse natural language commands to a robot control system	"['Cynthia Matuszek', 'Evan Herbst', 'Luke Zettlemoyer', 'Dieter Fox']"	introduction	"Abstract As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor environment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our formal representation allows a robot to interpret route instructions online while moving through a previously unknown environment."	"Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ) ."	"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ) .', 'Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a;Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012).']"	0
CC212	D13-1115	Integrating Theory and Practice: A Daunting Task	congruent embodied representations for visually presented actions and linguistic phrases describing actions	"['Lisa Aziz-Zadeh', 'Stephen M Wilson', 'Giacomo Rizzolatti', 'Marco Iacoboni']"	related work	"The thesis of embodied semantics holds that conceptual representations accessed during linguistic processing are, in part, equivalent to the sensory-motor representations required for the enactment of the concepts described . Here, using fMRI, we tested the hypothesis that areas in human premotor cortex that respond both to the execution and observation of actions-mirror neuron areas -are key neural structures in these processes. Participants observed actions and read phrases relating to foot, hand, or mouth actions. In the premotor cortex of the left hemisphere, a clear congruence was found between effector-specific activations of visually presented actions and of actions described by literal phrases. These results suggest a key role of mirror neuron areas in the re-enactment of sensory-motor representations during conceptual processing of actions invoked by linguistic stimuli."	"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG ) ."	"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG ) .']"	0
CC213	D13-1115	Integrating Theory and Practice: A Daunting Task	stochastic variational inference arxiv eprints	"['Matthew Hoffman', 'David M Blei', 'Chong Wang', 'John Paisley']"	method		"To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models ."	"['To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.', 'Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.', 'Online VBI easily scales and quickly converges in all of our experiments.', 'A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source.']"	5
CC214	D13-1115	Integrating Theory and Practice: A Daunting Task	distributional semantics in technicolor	"['Elia Bruni', 'Gemma Boleda', 'Marco Baroni', 'NamKhanh Tran']"	introduction	"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance."	"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) ."	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	0
CC215	D13-1115	Integrating Theory and Practice: A Daunting Task	perceptual inference through global lexical similarity	"['Brendan T Johns', 'Michael N Jones']"	introduction	"The literature contains a disconnect between accounts of how humans learn lexical semantic representations for words. Theories generally propose that lexical semantics are learned either through perceptual experience or through exposure to regularities in language. We propose here a model to integrate these two information sources. Specifically, the model uses the global structure of memory to exploit the redundancy between language and perception in order to generate inferred perceptual representations for words with which the model has no perceptual experience. We test the model on a variety of different datasets from grounded cognition experiments and demonstrate that this diverse set of results can be explained as perceptual simulation (cf. Barsalou, Simmons, Barbey, & Wilson, 2003) within a global memory model.Copyright (c) 2012 Cognitive Science Society, Inc."	"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) ."	"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"	0
CC216	D13-1115	Integrating Theory and Practice: A Daunting Task	integrating experiential and distributional data to learn semantic representations	"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']"	experiments	"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved."	"This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) ."	"['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']"	1
CC217	D13-1115	Integrating Theory and Practice: A Daunting Task	modeling the shape of the scene a holistic representation of the spatial envelope	"['Aude Oliva', 'Antonio Torralba']"	related work	"In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category."	"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) ."	"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"	0
CC218	D13-1115	Integrating Theory and Practice: A Daunting Task	describing objects by their attributes	"['Ali Farhadi', 'Ian Endres', 'Derek Hoiem', 'David Forsyth']"	related work	"We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (""spotty dog"", not just ""dog""); to say something about unfamiliar objects (""hairy and four-legged"", not just ""unknown""); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (""spotty"") or discriminative (""dogs have it but sheep do not""). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attributebased framework. 1"	"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) ."	"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"	0
CC219	D13-1115	Integrating Theory and Practice: A Daunting Task	indexing by latent semantic analysis	"['Scott Deerwester', 'Susan T Dumais', 'George W Furnas', 'Thomas K Landauer', 'Richard Harshman']"	related work	"A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (""semantic structure"") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."	"Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms ."	"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"	0
CC220	D13-1115	Integrating Theory and Practice: A Daunting Task	online learning for latent dirichlet allocation	"['Matthew Hoffman', 'David M Blei', 'Francis Bach']"	method	"We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time."	"To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models ."	"['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.', 'Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.', 'Online VBI easily scales and quickly converges in all of our experiments.', 'A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source.']"	5
CC221	D13-1115	Integrating Theory and Practice: A Daunting Task	how many words is a picture worth automatic caption generation for news images	"['Yansong Feng', 'Mirella Lapata']"	related work	"In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods."	The first work to do this with topic models is #AUTHOR_TAGb ) .	"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAGb ) .', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']"	0
CC222	D13-1115	Integrating Theory and Practice: A Daunting Task	integrating experiential and distributional data to learn semantic representations	"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']"	introduction	"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved."	"This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) ."	"['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', 'While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.', 'We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'Finally, we describe two ways to extend the model by incorporating three or more modalities.', 'We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'We release both our code and data to the community for future research. 1']"	2
CC223	D14-1083	Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates	extralinguistic constraints on stance recognition in ideological debates	"['Kazi Saidul Hasan', 'Vincent Ng']"	experiments		"This choice is motivated by an observation we made previously ( #AUTHOR_TAGa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3"	"['In P2, on the other hand, we recast SC as a se- quence labeling task.', 'In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence.', 'This choice is motivated by an observation we made previously ( #AUTHOR_TAGa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3']"	2
CC224	D14-1083	Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates	extralinguistic constraints on stance recognition in ideological debates	"['Kazi Saidul Hasan', 'Vincent Ng']"	experiments		"Following our previous work on stance classification ( #AUTHOR_TAGc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) ."	"['Frame-semantic features.', 'While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.', 'Following our previous work on stance classification ( #AUTHOR_TAGc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .', 'Frame-word interaction features encode whether two words appear in different elements of the same frame.', 'Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and']"	2
CC225	D14-1130	Human Effort and Machine Learnability in Computer Aided Translation	the efficacy of human postediting for language translation	"['S Green', 'J Heer', 'C D Manning']"	related work		Our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAGa ) comparing scratch translation to post-edit .	"['The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes.', 'However, he used undergraduate, non-professional subjects, and did not consider re-tuning.', 'Our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAGa ) comparing scratch translation to post-edit .', 'Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013).', 'However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature.']"	2
CC226	D14-1157	Staying on Topic: An Indicator of Power in Political Debates	power of confidence how poll scores impact topic dynamics in political debates	"['Vinodkumar Prabhakaran', 'Ashima Arora', 'Owen Rambow']"	method	"In this paper, we investigate how topic dynamics during the course of an interaction correlate with the power differences between its participants. We perform this study on the US presidential debates and show that a candidate's power, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction."	This is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .	"[""Table 1 shows the Pearson's product correlation between each topical feature and candidate's power."", 'We obtain a highly significant (p = 0.002) negative correlation between topic shift tendency of a candidate (PI) and his/her power.', ""In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates' recent poll standings."", 'Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'This is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .', 'It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others.', 'On the other hand, we did not obtain any significant correlation for the features proposed by Nguyen et al. (2013).']"	1
CC227	D14-1222	A Rule-Based System for Unrestricted Bridging Resolution: Recognizing Bridging Anaphora and Finding Links to Antecedents	cascading collective classification for bridging anaphora recognition using a rich linguistic feature set	"['Yufang Hou', 'Katja Markert', 'Michael Strube']"	introduction	"Recognizing bridging anaphora is difficult due to the wide variation within the phenomenon, the resulting lack of easily identifiable surface markers and their relative rarity. We develop linguistically motivated discourse structure, lexico-semantic and genericity detection fea-tures and integrate these into a cascaded mi-nority preference algorithm that models bridg-ing recognition as a subtask of learning fine-grained information status (IS). We substan-tially improve bridging recognition without impairing performance on other IS classes"	We follow our previous work ( #AUTHOR_TAGb ) and restrict bridging to non-coreferential cases .	"['Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993;Löbner, 1998).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference).', 'We follow our previous work ( #AUTHOR_TAGb ) and restrict bridging to non-coreferential cases .', 'We also exclude comparative anaphora (Modjeska et al., 2003).']"	2
CC228	D14-1222	A Rule-Based System for Unrestricted Bridging Resolution: Recognizing Bridging Anaphora and Finding Links to Antecedents	cascading collective classification for bridging anaphora recognition using a rich linguistic feature set	"['Yufang Hou', 'Katja Markert', 'Michael Strube']"	experiments	"Recognizing bridging anaphora is difficult due to the wide variation within the phenomenon, the resulting lack of easily identifiable surface markers and their relative rarity. We develop linguistically motivated discourse structure, lexico-semantic and genericity detection fea-tures and integrate these into a cascaded mi-nority preference algorithm that models bridg-ing recognition as a subtask of learning fine-grained information status (IS). We substan-tially improve bridging recognition without impairing performance on other IS classes"	"mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAGa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection ."	"['7 In ISNotes, 71% of NP antecedents occur in the same or up to two sentences prior to the anaphor.', 'Initial experiments show that increasing the window size more than two sentences decreases the performance. 8', 'To deal with data imbalance, the SVM light parameter is set according to the ratio between positive and negative instances in the training set. 9', 'To compare the learning-based approach to the rulebased system described in Section 3 directly, we report the mlSystem ruleFeats We provide mlSystem ruleFeats with the same knowledge resources as the rule-based system.', 'All rules from the rule-based system are incorporated into mlSystem ruleFeats as the features.', 'mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAGa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection .', 'Some of these features overlap with the atomic features used in the rule-based system.']"	2
CC229	E03-1002	Neural network probability estimation for broad coverage parsing	towards historybased grammars using richer models for probabilistic parsing	"['E Black', 'F Jelinek', 'J Lafferty', 'D Magerman', 'R Mercer', 'S Roukos']"	method	"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."	"in history-based models ( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i ."	"['The probability model we use is generative and history-based.', 'Generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence.', 'At each step, the process chooses a characteristic of the tree or predicts a word in the sentence.', 'This sequence of decisions is the derivation of the tree, which we will denote d1,..., dm .', ""Because there is a one-to-one mapping from phrase structure trees to our derivations, the probability of a derivation P(di,..., dm) is equal to the joint probability of the derivation's tree and the input sentence."", 'The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history-based models ( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .', 'This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions.']"	5
CC230	E03-1002	Neural network probability estimation for broad coverage parsing	a maximumentropyinspired parser	['Eugene Charniak']			"The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ) ."	"['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ) .']"	1
CC231	E03-1002	Neural network probability estimation for broad coverage parsing	headdriven statistical models for natural language parsing	['Michael Collins']	experiments	"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."	7A11 our results are computed with the evalb program following the now-standard criteria in ( #AUTHOR_TAG ) .	['7A11 our results are computed with the evalb program following the now-standard criteria in ( #AUTHOR_TAG ) .']	5
CC232	E03-1002	Neural network probability estimation for broad coverage parsing	towards historybased grammars using richer models for probabilistic parsing	"['E Black', 'F Jelinek', 'J Lafferty', 'D Magerman', 'R Mercer', 'S Roukos']"	introduction	"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."	"Many statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model ( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse ."	"['Many statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model ( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001;Henderson, 2000).', 'Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'The resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state-of-the-art for parsing the Penn Treebank.']"	0
CC233	E03-1002	Neural network probability estimation for broad coverage parsing	headdriven statistical models for natural language parsing	['Michael Collins']		"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."	"The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 ) ."	"['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 ) .']"	1
CC234	E03-1002	Neural network probability estimation for broad coverage parsing	discriminative reranking for natural language parsing	['Michael Collins']		"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation"	"Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #AUTHOR_TAG ) ."	"['The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features.', 'The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand.', 'One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms.', 'We replace Chomsky adjunction structures (i.e.', 'structures of the form [X [X ...] [Y ...]]) with a special ""modifier"" link in the tree (becoming [X ... [mod Y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'We also compiled some frequent chains of non-branching nodes (such as [S [VP ...1]) into a single node with a new label (becoming [S-VP ...]).', 'These transforms are undone before any evaluation is performed on the output trees.', 'We do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #AUTHOR_TAG ) .""]"	0
CC235	E03-1002	Neural network probability estimation for broad coverage parsing	pcfg models of linguistic tree representations	['Mark Johnson']		"The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus. This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today. This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases. The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process."	"For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #AUTHOR_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) ."	"['The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality.', 'For this reason, D(top) includes nodes which are structurally local to top,.', ""These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any)."", 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #AUTHOR_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) .', 'Because these inputs include the history features of both the left- corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i � 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.', 'Thus this model is making no a priori hard independence assumptions, just a priori soft biases.']"	0
CC236	E03-1002	Neural network probability estimation for broad coverage parsing	new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron	"['Michael Collins', 'Nigel Duffy']"		"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data."	"#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( Collins , 2000 ) ."	"['The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features.', 'The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand.', 'One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms.', 'We replace Chomsky adjunction structures (i.e.', 'structures of the form [X [X ...] [Y ...]]) with a special ""modifier"" link in the tree (becoming [X ... [mod Y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'We also compiled some frequent chains of non-branching nodes (such as [S [VP ...1]) into a single node with a new label (becoming [S-VP ...]).', 'These transforms are undone before any evaluation is performed on the output trees.', 'We do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( Collins , 2000 ) .""]"	0
CC237	E03-1002	Neural network probability estimation for broad coverage parsing	a maximum entropy model for partofspeech tagging	['Adwait Ratnaparkhi']	experiments		We used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system .	"['In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus.', 'We used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system .']"	5
CC238	E03-1002	Neural network probability estimation for broad coverage parsing	a maximumentropyinspired parser	['Eugene Charniak']	experiments		"The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) ."	"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']"	1
CC239	E03-1002	Neural network probability estimation for broad coverage parsing	headdriven statistical models for natural language parsing	['Michael Collins']	introduction	"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."	"Many statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse ."	"['Many statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001;Henderson, 2000).', 'Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'The resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state-of-the-art for parsing the Penn Treebank.']"	0
CC240	E03-1002	Neural network probability estimation for broad coverage parsing	efficient probabilistic topdown and leftcorner parsing	"['Brian Roark', 'Mark Johnson']"			"For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #AUTHOR_TAG ) ."	"['The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality.', 'For this reason, D(top) includes nodes which are structurally local to top,.', ""These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any)."", 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #AUTHOR_TAG ) .', 'Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i -1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.', 'Thus this model is making no a priori hard independence assumptions, just a priori soft biases.']"	0
CC241	E03-1002	Neural network probability estimation for broad coverage parsing	what is the minimal set of fragments that achieves maximal parse accuracy	['Rens Bod']	experiments	"We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank (a precision of 90.8% and a recall of 90.6%). We isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy."	"The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) ."	"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']"	1
CC242	E03-1002	Neural network probability estimation for broad coverage parsing	discriminative reranking for natural language parsing	['Michael Collins']	experiments	"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation"	"The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) ."	"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']"	1
CC243	E03-1002	Neural network probability estimation for broad coverage parsing	a maximumentropyinspired parser	['Eugene Charniak']			"Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) ."	"['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000).', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999).', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', 'We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.', 'These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features.']"	1
CC244	E03-1005	An efficient implementation of a new DOP model	a maximumentropyinspired parser	['E Charniak']	conclusion		This is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .	"['Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12.', 'But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', 'Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.', 'While SL-DOP and LS-DOP have been compared before in']"	1
CC245	E03-1005	An efficient implementation of a new DOP model	new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron	"['M Collins', 'N Duffy']"	introduction	"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data."	"#AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ ."	"['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', ""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999;Chappelier et al. 2002)."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", 'Goodman (1996Goodman ( , 1998 developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998b showed that DOP1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'Cross-validation is needed to avoid this problem.', 'But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).', ""Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', ""Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."", ""#AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ ."", ""Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.""]"	0
CC246	E03-1005	An efficient implementation of a new DOP model	a new statistical parser based on bigram lexical dependencies	['M Collins']	introduction	"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil.."	"But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG ."	"[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'Bod (2001Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]"	0
CC247	E03-1005	An efficient implementation of a new DOP model	discriminative reranking for natural language parsing	['M Collins']	conclusion	"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation"	"Compared to the reranking technique in #AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction ."	"['Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12.', 'But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', ""This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bod's PCFG-reduction reported in Table 1."", 'Compared to the reranking technique in #AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .', 'While SL-DOP and LS-DOP have been compared before in']"	1
CC248	E03-1005	An efficient implementation of a new DOP model	discriminative reranking for natural language parsing	['M Collins']	introduction	"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation"	"But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) ."	"[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'Bod (2001Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .""]"	0
CC249	E03-1005	An efficient implementation of a new DOP model	a dop model for semantic interpretation	"['R Bonnema', 'R Bod', 'R Scha']"		"In data-oriented language processing, an annotated language corpus is used as a stochastic grammar. The most probable analysis of a new sentence is constructed by combining fragments from the corpus in the most probable way. This approach has been successfully used for syntactic analysis, using corpora with syntactic annotations such as the Penn Tree-bank. If a corpus with semantically annotated sentences is used, the same approach can also generate the most probable semantic interpretation of an input sentence. The present paper explains this semantic interpretation method. A data-oriented semantic interpretation algorithm was tested on two semantically annotated corpora: the English ATIS corpus and the Dutch OVIS corpus. Experiments show an increase in semantic accuracy if larger corpus-fragments are taken into consideration."	"Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , #AUTHOR_TAG , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."	"[""Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , #AUTHOR_TAG , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']"	0
CC250	E03-1005	An efficient implementation of a new DOP model	a maximumentropyinspired parser	['E Charniak']	experiments		"Collins 1996 , Charniak 1997 , Collins 1999 and #AUTHOR_TAG ) ."	"['4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and #AUTHOR_TAG ) .', 'As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'This corresponds to a speedup of over 60 times.', 'It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.', 'In the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001).']"	1
CC251	E03-1005	An efficient implementation of a new DOP model	building a large annotated corpus of english the penn treebank	"['M Marcus', 'B Santorini', 'M Marcinkiewicz']"	experiments	"Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."	"For our experiments we used the standard division of the WSJ ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx ."	"['For our experiments we used the standard division of the WSJ ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.', 'Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing).', 'We employed the same unknown (category) word model as in Bod (2001), based on statistics on word-endings, hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 -87).', 'We used ""evalb"" 4 to compute the standard PARSEVAL scores for our results (Manning & Schiitze 1999).', 'We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems.']"	5
CC252	E03-1005	An efficient implementation of a new DOP model	a new statistical parser based on bigram lexical dependencies	['M Collins']	experiments	"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil.."	"#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) ."	"['4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', '#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', 'As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'This corresponds to a speedup of over 60 times.', 'It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.', 'In the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001).']"	1
CC253	E03-1005	An efficient implementation of a new DOP model	a maximumentropyinspired parser	['E Charniak']	introduction		"But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) ."	"[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'Bod (2001Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .""]"	0
CC254	E03-1005	An efficient implementation of a new DOP model	discriminative reranking for natural language parsing	['M Collins']	introduction	"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation"	"And #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) ."	"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial (e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	0
CC255	E03-1005	An efficient implementation of a new DOP model	new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron	"['M Collins', 'N Duffy']"		"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data."	"Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."	"[""Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."", 'We will refer to these models as Likelihood- DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']"	0
CC256	E03-1005	An efficient implementation of a new DOP model	efficient algorithms for parsing the dop model	['J Goodman']		"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers."	"Most DOP models , such as in Bod ( 1993 ) , #AUTHOR_TAG , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."	"[""Most DOP models , such as in Bod ( 1993 ) , #AUTHOR_TAG , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']"	0
CC257	E03-1005	An efficient implementation of a new DOP model	treegram parsing lexical dependencies and structural relations	"[""K Sima'an""]"		"This paper explores the kinds of probabilistic relations that are important in syntactic disambiguation. It proposes that two widely used kinds of relations, lexical dependencies and structural relations, have complementary disambiguation capabilities. It presents a new model based on structural relations, the Tree-gram model, and reports experiments showing that structural relations should benefit from enrichment by lexical dependencies."	"Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , #AUTHOR_TAG and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."	"['Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , #AUTHOR_TAG and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']"	0
CC258	E03-1005	An efficient implementation of a new DOP model	efficient algorithms for parsing the dop model	['J Goodman']	introduction	"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers."	"Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #AUTHOR_TAG , 2002 ) ."	"['Thus DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees.', 'A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', 'Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #AUTHOR_TAG , 2002 ) .', 'Here we will only sketch this PCFG-reduction, which is heavily based on Goodman (2002).']"	0
CC259	E03-1005	An efficient implementation of a new DOP model	a new statistical parser based on bigram lexical dependencies	['M Collins']	introduction	"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil.."	"This approach has now gained wide usage , as exemplified by the work of #AUTHOR_TAG , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others ."	"['Waegner 1992; Pereira and Schabes 1992).', 'The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of #AUTHOR_TAG , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others .']"	4
CC260	E03-1005	An efficient implementation of a new DOP model	a maximumentropyinspired parser	['E Charniak']	introduction		The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG ; Goodman 1998 ) .	"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include non- lexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG ; Goodman 1998 ) .', 'And Collins (2000) argues for ""keeping track of counts of arbitrary fragments within parse trees"", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992).']"	0
CC261	E03-1005	An efficient implementation of a new DOP model	Motivation	new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron	"['M Collins', 'N Duffy']"	"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data."	"And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) ."	"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial (e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"	4
CC262	E03-1005	An efficient implementation of a new DOP model	efficient algorithms for parsing the dop model	['J Goodman']	introduction	"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers."	"And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) ."	"['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', ""Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree."", 'But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constituents parse.', ""Moreover, Goodman's PCFG reduction may also be used to estimate the most probable parse by Viterbi n-best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this, Goodman's method can do the same job with a more compact grammar.""]"	0
CC263	E03-1005	An efficient implementation of a new DOP model	efficient algorithms for parsing the dop model	['J Goodman']	introduction	"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers."	"#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar ."	"['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', ""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999;Chappelier et al. 2002)."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998b showed that DOP1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'Cross-validation is needed to avoid this problem.', 'But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).', ""Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', ""Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."", ""Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees, reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ."", ""Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.""]"	0
CC264	E12-1068	Modeling Inflection and Word-Formation in SMT	srilm  an extensible language modeling toolkit	['Andreas Stolcke']	experiments	"SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools. 1"	The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #AUTHOR_TAG ) .	"['To evaluate our end-to-end system, we perform the well-studied task of news translation, using the Moses SMT package.', 'We use the English/German data released for the 2009 ACL Workshop on Machine Translation shared task on translation. 7', 'There are 82,740 parallel sentences from news-commentary09.de-en and 1,418,115 parallel sentences from europarl-v4.de-en.', 'The monolingual data contains 9.8 M sentences. 8', 'o build the baseline, the data was tokenized using the Moses tokenizer and lowercased.', 'We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4. We symmetrize using the ""grow-diag-final-and"" heuristic.', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #AUTHOR_TAG ) .', 'We run MERT separately for each system.', 'The recaser used is the same for all systems.', 'It is the standard recaser supplied with Moses, trained on all German training data.', 'The dev set is wmt-2009-a and the test set is wmt-2009-b, and we report end-to-end case sensitive BLEU scores against the unmodified reference SGML file.', 'The blind test set used is wmt-2009-blind (all lines).']"	5
CC265	E12-1068	Modeling Inflection and Word-Formation in SMT	productive generation of compound words in statistical machine translation	"['Sara Stymne', 'Nicola Cancedda']"	experiments	"In this article we investigate statistical machine translation (SMT) into Germanic languages, with a focus on compound processing. Our main goal is to enable the generation of novel compounds that have not been seen in the training data. We adopt a split-merge strategy, where compounds are split before training the SMT system, and merged after the translation step. This approach reduces sparsity in the training data, but runs the risk of placing translations of compound parts in non-consecutive positions. It also requires a postprocessing step of compound merging, where compounds are reconstructed in the translation output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order and show that it can lead to improvements both by direct inspection and in terms of standard translation evaluation metrics. We also propose several new methods for compound merging, based on heuristics and machine learning, which outperform previously suggested algorithms. These methods can produce novel compounds and a translation with at least the same overall quality as the baseline. For all subtasks we show that it is useful to include part-of-speech based information in the translation process, in order to handle compounds. 1"	"Following the work of #AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these ."	"['After translation, compound parts have to be resynthesized into compounds before inflection.', 'Two decisions have to be taken: i) where to merge and ii) how to merge.', 'Following the work of #AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', 'It only proposes merging decisions, merging itself uses a list extracted from the monolingual data (Popovic et al., 2006).']"	5
CC266	E12-1068	Modeling Inflection and Word-Formation in SMT	how to avoid burning ducks combining linguistic analysis and corpus statistics for german compound processing	"['Fabienne Fritzinger', 'Alexander Fraser']"	related work	"Compound splitting is an important problem in many NLP applications which must be solved in order to address issues of data sparsity. Previous work has shown that linguistic approaches for German compound splitting produce a correct splitting more often, but corpus-driven approaches work best for phrase-based statistical machine translation from German to English, a worrisome contradiction. We address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance."	"For compound splitting , we follow #AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies ."	"['For compound splitting , we follow #AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne ( 2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)).', 'Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow Stymne and Cancedda (2011), for compound merging.', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.']"	5
CC267	E12-1068	Modeling Inflection and Word-Formation in SMT	factored translation models	"['Philipp Koehn', 'Hieu Hoang']"	introduction	"We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level -- may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence."	"#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models ."	"['We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', '#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .']"	0
CC268	E12-1068	Modeling Inflection and Word-Formation in SMT	productive generation of compound words in statistical machine translation	"['Sara Stymne', 'Nicola Cancedda']"	related work	"In many languages the use of compound words is very productive. A common practice to reduce sparsity consists in splitting compounds in the training data. When this is done, the system incurs the risk of translating components in non-consecutive positions, or in the wrong order. Furthermore, a post-processing step of compound merging is required to reconstruct compound words in the output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order. We also propose new heuristic methods for merging components that outperform all known methods, and a learning-based method that has similar accuracy as the heuristic method, is better at producing novel compounds, and can operate with no background linguistic resources."	"We follow #AUTHOR_TAG , for compound merging ."	"['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the ge- ometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging ap- proach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow #AUTHOR_TAG , for compound merging .', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets.']"	5
CC269	E12-1068	Modeling Inflection and Word-Formation in SMT	failures in englishczech phrasebased mt	"['Ondˇrej Bojar', 'Kamil Kos']"	related work		#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .	"['Given a stem such as brother, Toutanova et.', 'al\'s system might generate the ""stem and inflection"" corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., adjectives) separating his and brother.', 'This required mapping is a significant problem for generalization.', 'We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).', 'We apply a ""split in preprocessing and resynthesize in postprocessing"" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et.', 'al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored SMT.', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .', 'Both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex context features.']"	1
CC270	E12-1068	Modeling Inflection and Word-Formation in SMT	empirical methods for compound splitting	"['Philipp Koehn', 'Kevin Knight']"	related work	"Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.Comment: 8 pages, 2 figures. Published at EACL 200"	"Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) ."	"['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) .', 'Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow Stymne and Cancedda (2011), for compound merging.', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.']"	1
CC271	E12-1068	Modeling Inflection and Word-Formation in SMT	agreement constraints for statistical machine translation into german	"['Philip Williams', 'Philipp Koehn']"	related work	"Languages with rich inflectional morphology pose a difficult challenge for statistical machine translation. To address the problem of morphologically inconsistent output, we add unification-based constraints to the target-side of a string-to-tree model. By integrating constraint evaluation into the decoding process, implausible hypotheses can be penalised or filtered out during search. We use a simple heuristic process to extract agreement constraints for German and test our approach on an English-German system trained on WMT data, achieving a small improvement in translation accuracy as measured by BLEU."	#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.	"['Given a stem such as brother, Toutanova et. al�s system might generate the �stem and inflection� corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a map- ping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., ad- jectives) separating his and brother.', 'This required mapping is a significant problem for generaliza- tion.', 'We view this issue as a different sort of prob- lem entirely, one of word-formation (rather than inflection).', 'We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored SMT.', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to in- flected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex con- text features.']"	1
CC272	E12-1068	Modeling Inflection and Word-Formation in SMT	efficient parsing of highly ambiguous contextfree grammars with bit vectors	['Helmut Schmid']	introduction	An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one.	"The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG ) ."	"['The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG ) .']"	5
CC273	E12-1068	Modeling Inflection and Word-Formation in SMT	how to avoid burning ducks combining linguistic analysis and corpus statistics for german compound processing	"['Fabienne Fritzinger', 'Alexander Fraser']"	experiments	"Compound splitting is an important problem in many NLP applications which must be solved in order to address issues of data sparsity. Previous work has shown that linguistic approaches for German compound splitting produce a correct splitting more often, but corpus-driven approaches work best for phrase-based statistical machine translation from German to English, a worrisome contradiction. We address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance."	"We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG ."	"['We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', 'Training data is then stemmed as described in Section 2.3.', 'The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb.', 'In these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization.']"	5
CC274	E12-1068	Modeling Inflection and Word-Formation in SMT	combining morphemebased machine translation with postprocessing morpheme prediction	"['Ann Clifton', 'Anoop Sarkar']"	related work		"Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation ."	"['We have directly addressed the question as to whether inflection should be predicted using surface forms as the target of the prediction, or whether linguistic features should be predicted, along with the use of a subsequent generation step.', 'The direct prediction of surface forms is limited to those forms observed in the training data, which is a significant limitation.', 'However, it is reasonable to expect that the use of features (and morphological generation) could also be problematic as this requires the use of morphologically-aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation.', 'Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT.', 'This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing.', 'As parsing performance improves, the performance of linguistic-feature-based approaches will increase.', 'Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .', 'However, this does not deal directly with linguistic features marked by inflection.', 'In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.', 'So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.']"	1
CC275	E12-1068	Modeling Inflection and Word-Formation in SMT	experiments in morphosyntactic processing for translating to and from german	['Alexander Fraser']	related work	We describe two shared task systems and associated experiments. The German to English system used reordering rules ap-plied to parses and morphological split-ting and stemming. The English to Ger-man system used an additional translation step which recreated compound words and generated morphological inflection	#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .	"['Given a stem such as brother, Toutanova et.', 'al\'s system might generate the ""stem and inflection"" corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., adjectives) separating his and brother.', 'This required mapping is a significant problem for generalization.', 'We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).', 'We apply a ""split in preprocessing and resynthesize in postprocessing"" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et.', 'al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored SMT.', 'We use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex context features.']"	1
CC276	E12-1068	Modeling Inflection and Word-Formation in SMT	factored translation models	"['Philipp Koehn', 'Hieu Hoang']"	related work	"We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level -- may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence."	#AUTHOR_TAG introduced factored SMT .	"['Given a stem such as brother, Toutanova et. al�s system might generate the �stem and inflection� corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., adjectives) separating his and brother.', 'This required mapping is a significant problem for generalization.', 'We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).', 'We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', '#AUTHOR_TAG introduced factored SMT .', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex context features.']"	1
CC277	E12-1068	Modeling Inflection and Word-Formation in SMT	syntaxtomorphology mapping in factored phrasebased statistical machine translation from english to turkish	"['Reyyan Yeniterzi', 'Kemal Oflazer']"	related work	"We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets."	"Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , #AUTHOR_TAG and others ."	"['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , #AUTHOR_TAG and others .', 'Toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'Using additional source side information beyond the markup did not produce a gain in performance.']"	1
CC278	E12-1068	Modeling Inflection and Word-Formation in SMT	enriching morphologically poor languages for statistical machine translation	"['Eleftherios Avramidis', 'Philipp Koehn']"	related work	"We address the problem of translating from morphologically poor to morphologically rich languages by adding per-word linguistic information to the source language. We use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly. In experiments, we show improved performance for translating from English into Greek and Czech. For English-Greek, we reduce the error on the verb conjugation from 19% to 5.4% and noun case agreement from 9% to 6%."	"Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , Yeniterzi and Oflazer ( 2010 ) and others ."	"['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , Yeniterzi and Oflazer ( 2010 ) and others .', 'Toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'Using additional source side information beyond the markup did not produce a gain in performance.']"	1
CC279	E12-1068	Modeling Inflection and Word-Formation in SMT	german compounds in factored statistical machine translation	['Sara Stymne']	related work	"Abstract. An empirical method for splitting German compounds is explored by varying it in a number of ways to investigate the consequences for factored statistical machine translation between English and German in both directions. Compound splitting is incorporated into translation in a preprocessing step, performed on training data and on German translation input. For translation into German, compounds are merged based on part-of-speech in a postprocessing step. Compound parts are marked, to separate them from ordinary words. Translation quality is improved in both translation directions and the number of untranslated words in the English output is reduced. Different versions of the splitting algorithm performs best in the two different translation directions."	"Other approaches use less deep linguistic resources ( e.g. , POS-tags #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) ."	"['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) .', 'Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow Stymne and Cancedda (2011), for compound merging.', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.']"	1
CC280	E14-1023	Frame Semantic Tree Kernels for Social Network Extraction from Text	automatic detection and classification of social events	"['Apoorv Agarwal', 'Owen Rambow']"	related work	"In this paper we introduce the new task of social event extraction from text. We distinguish two broad types of social events depending on whether only one or both parties are aware of the social contact. We annotate part of Automatic Content Extraction (ACE) data, and perform experiments using Support Vector Machines with Kernel methods. We use a combination of structures derived from phrase structure trees and dependency trees. A characteristic of our events (which distinguishes them from ACE events) is that the participating entities can be spread far across the parse trees. We use syntactic and semantic insights to devise a new structure derived from dependency trees and show that this plays a role in achieving the best performing system for both social event detection and classification tasks. We also use three data sampling approaches to solve the problem of data skewness. Sampling methods improve the F1-measure for the task of relation detection by over 20% absolute over the baseline."	"Our approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) ."	"['Our approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) .', 'Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', 'Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003;Culotta and Sorensen, 2004).', 'To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005).', 'Harabagiu et al. (2005) propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees.', 'They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees.', 'We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by Harabagiu et al. (2005).']"	2
CC281	E99-1022	Selective magic HPSG parsing	the logic of typed feature structures  with applications to unification grammars logic programs and constraint resolution	['Bob Carpenter']			2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ) .	"['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ) .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects.""]"	0
CC282	E99-1022	Selective magic HPSG parsing	offline compilation for efficient processing with constraintlogic grammars	['Guido Minnen']	introduction		I A more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG ) .	['I A more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG ) .']	0
CC283	E99-1022	Selective magic HPSG parsing	ale — the attribute logic engine users guide version 202	"['Bob Carpenter', 'Gerald Penn']"		"ale 3.0 is completely compatible with ale 2.0 grammars, and adds the following new features: * A semantic-head-driven generator, based on the algorithm presented in Shieber et al. (1990). The generator was adapted to the logic of typed feature structures by Octav Popescu in his Carnegie Mellon Master's Thesis, Popescu (1996). Octav also wrote most of the generation code for this release. Grammars can be compiled for parsing only, generation only, or both. Some glue-code is also available from the ale homepage, to parse and generate with different grammars through a unix pipe. * A source-level debugger with a graphical XEmacs interface. This debugger works only with SICStus Prolog 3.0.6 and higher. A debugger with reduced functionality will be made available to SWI Prolog users in a later release. This debugger builds on, and incorporates the functionality of the code for the SICStus source-level debugger, written by Per Mildner at Uppsala University. * a /1 atoms. There are now an infinite number of atoms (types with no appropriate features), implicitly declared in every signature. These atoms can be arbitrary Prolog terms, including unbound variables, and can be used wherever normal ale types can, e.g., f:(a p(3.7)). a /1 atoms are extensional as Prolog terms, i.e., are taken to be identical according to the Prolog predicate , ==/2. In particular, this means that ground atoms behave exactly as ale extensional types. * Optional edge subsumption checking. For completeness of parsing, one only needs to ensure that, for every pair of nodes in the chart, the most general feature structure spanning those nodes is stored in the chart. This can reduce the number of edges in many domains. * An autonomous intro/2 operator. Features can now be declared on their own in a separate part of the grammar. * Default specifications for types. These are NOT default types. If a type appears on the right-hand side of a sub/2 or intro/2 specification, but not on iii iv CONTENTS the left-hand side of one, ale will assume this type is maximal, i.e., assume the specification, T ype sub []. Similarly, if it occurs on a left-hand side, but not on a right-hand side, ale will assume the type is immediately subsumed by bot, the most general type. In both cases, ale will announce these assumptions during compilation. * Several bug corrections and more compile-time warning and error messages. * An SWI Prolog 2.9.7 ..."	"Proceedings of EACL '99 example , the ALE parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."	"['Combining control strategies depends on a way to differentiate between types of constraints.', ""Proceedings of EACL '99 example , the ALE parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", 'In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1° All types in the type hierarchy can be used as parse types.', 'This way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'However, in the remainder we will concentrate on a specific class of parse types: We assume the specification of type sign and its subtypes as parse types.', '11 This choice is based on the observation that the constraints on type sign and its sub-types play an important guiding role in the parsing process and are best interpreted bottom-up given the lexical orientation of I-IPSG.', ""The parsing process corresponding to such a parse type specification is represented schematically in figure 8. Starting from the lexical entries, i. e., word word word Figure 8: Schematic representation of the selective magic parsing process the :r~'L definite clauses that specify the word objects in the grammar, phrases are built bottomup by matching the parse type literals of the definite clauses in the grammar against the edges in the table."", 'The non-parse type literals are processed according to the top-down control strategy 1°The notion of a parse type literal is closely related to that of a memo literal as in (Johnson and DSrre, 1995).']"	0
CC284	E99-1022	Selective magic HPSG parsing	magic for filter optimization in dynamic bottomup processing	['Guido Minnen']	introduction		"As shown in ( #AUTHOR_TAG ) â¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 ."	"['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See, among others, (Ramakrishnan et al. 1992).', 'As shown in ( #AUTHOR_TAG ) â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .']"	0
CC285	E99-1022	Selective magic HPSG parsing	headdriven phrase structure grammar	"['Carl Pollard', 'Ivan Sag']"	introduction		Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG )	"['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG )', '3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992).', ""Typed feature structures as normal form ir~'~E terms are merely syntactic objects.""]"	0
CC286	E99-1022	Selective magic HPSG parsing	headdriven phrase structure grammar	"['Carl Pollard', 'Ivan Sag']"			Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #AUTHOR_TAG ) .	"['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #AUTHOR_TAG ) .', '3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into TIT definite clauses which are used to restrict lexical entries.', '(GStz and Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.4', 'Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in (GStz and Meurers, 1997b) implements the above men- tioned techniques for compiling an HPSG theory into typed feature grammars.']"	0
CC287	E99-1022	Selective magic HPSG parsing	efficient bottomup evaluation of logic programs	"['Raghu Ramakrishnan', 'Divesh Srivastava', 'S Sudarshan']"	introduction	"In recent years, much work has been directed towards evaluating logic programs and queries on deductive databases by using an iterative bottom-up fixpoint computation. The resulting techniques offer an attractive alternative to Prolog-style top-down evaluation in several situations. They are sound and complete for positive Horn clause programs, are well-suited to applications with large volumes of data (facts), and can support a variety of extensions to the standard logic programming paradigm."	"See , among others , ( #AUTHOR_TAG ) ."	"['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others , ( #AUTHOR_TAG ) .', 'As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv�:; GStz, 1995).', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) as discussed in (GStz and Meurers, 1997a) and (Meurers and Minnen, 1997).', 'Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (GStz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation.']"	0
CC288	E99-1022	Selective magic HPSG parsing	interleaving universal principles and relational constraints over typed feature logic	"['Thilo Gotz', 'Detmar Meurers']"		"We introduce a typed feature logic system providing both universal implicational principles as well as definite clauses over feature terms. We show that such an architecture supports a modular encoding of linguistic theories and allows for a compact representation using underspecification. The system is fully implemented and has been used as a workbench to develop and test large HPSG grammars. The techniques described in this paper are not restricted to a specific implementation, but could be added to many current feature-based grammar development systems."	The ConTroll grammar development system as described in ( #AUTHOR_TAGb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .	"['aSee (King, 1994) for a discussion of the appropriateness of T~-£: for HPSG and a comparison with other feature logic approaches designed for HPSG.', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in ( #AUTHOR_TAGb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .']"	0
CC289	E99-1022	Selective magic HPSG parsing	prologii manuel de reference et modele theorique	['Alain Colmerauer']			"See also ( #AUTHOR_TAG ; Naish , 1986 ) ."	"['Coroutining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking.', 'See also ( #AUTHOR_TAG ; Naish , 1986 ) .']"	0
CC290	E99-1022	Selective magic HPSG parsing	ale — the attribute logic engine users guide version 202	"['Bob Carpenter', 'Gerald Penn']"	introduction	"ale 3.0 is completely compatible with ale 2.0 grammars, and adds the following new features: * A semantic-head-driven generator, based on the algorithm presented in Shieber et al. (1990). The generator was adapted to the logic of typed feature structures by Octav Popescu in his Carnegie Mellon Master's Thesis, Popescu (1996). Octav also wrote most of the generation code for this release. Grammars can be compiled for parsing only, generation only, or both. Some glue-code is also available from the ale homepage, to parse and generate with different grammars through a unix pipe. * A source-level debugger with a graphical XEmacs interface. This debugger works only with SICStus Prolog 3.0.6 and higher. A debugger with reduced functionality will be made available to SWI Prolog users in a later release. This debugger builds on, and incorporates the functionality of the code for the SICStus source-level debugger, written by Per Mildner at Uppsala University. * a /1 atoms. There are now an infinite number of atoms (types with no appropriate features), implicitly declared in every signature. These atoms can be arbitrary Prolog terms, including unbound variables, and can be used wherever normal ale types can, e.g., f:(a p(3.7)). a /1 atoms are extensional as Prolog terms, i.e., are taken to be identical according to the Prolog predicate , ==/2. In particular, this means that ground atoms behave exactly as ale extensional types. * Optional edge subsumption checking. For completeness of parsing, one only needs to ensure that, for every pair of nodes in the chart, the most general feature structure spanning those nodes is stored in the chart. This can reduce the number of edges in many domains. * An autonomous intro/2 operator. Features can now be declared on their own in a separate part of the grammar. * Default specifications for types. These are NOT default types. If a type appears on the right-hand side of a sub/2 or intro/2 specification, but not on iii iv CONTENTS the left-hand side of one, ale will assume this type is maximal, i.e., assume the specification, T ype sub []. Similarly, if it occurs on a left-hand side, but not on a right-hand side, ale will assume the type is immediately subsumed by bot, the most general type. In both cases, ale will announce these assumptions during compilation. * Several bug corrections and more compile-time warning and error messages. * An SWI Prolog 2.9.7 ..."	As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #AUTHOR_TAG ) .	"['The proposed parser is related to the so-called Lemma Table deduction system (Johnson and DSrre, 1995) which allows the user to specify whether top-down sub-computations are to be tabled.', ""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #AUTHOR_TAG ) .', 'Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered.', 'feature grammars on the basis of an example and introduce a dynamic bottom-up interpreter that can be used for goM-directed interpretation of magic-compiled typed feature grammars.']"	1
CC291	E99-1022	Selective magic HPSG parsing	interleaving universal principles and relational constraints over typed feature logic	"['Thilo Gotz', 'Detmar Meurers']"	introduction	"We introduce a typed feature logic system providing both universal implicational principles as well as definite clauses over feature terms. We show that such an architecture supports a modular encoding of linguistic theories and allows for a compact representation using underspecification. The system is fully implemented and has been used as a workbench to develop and test large HPSG grammars. The techniques described in this paper are not restricted to a specific implementation, but could be added to many current feature-based grammar development systems."	"Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAGa ) and ( Meurers and Minnen , 1997 ) ."	"['magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv£:;GStz, 1995).', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAGa ) and ( Meurers and Minnen , 1997 ) .', 'Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (GStz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation.']"	2
CC292	E99-1022	Selective magic HPSG parsing	typed feature structures as descriptions	['Paul King']		"A description is an entity that can be interpreted as true or false of an object, and using feature structures as descriptions accrues several computational benefits. In this paper, I create an explicit interpretation of a typed feature structure used as a description, define the notion of a satisfiable feature structure, and create a simple and effective algorithm to decide if a feature structure is satisfiable.Comment: COLING 94 reserve paper, 5 pages, LaTeX (no .sty exotica"	` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .	"['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in (GStz and Meurers, 1997b) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars.']"	0
CC293	J00-1004	Learning Dependency Translation Models as Collections of Finite-State Head Transducers	machine translation divergences a formal description and proposed solution	['B J Dorr']	method	"There are many cases in which the natural translation of one language into another results in a very different form than that of the original. The existence of translation divergences (i.e., crosslinguistic distinctions) makes the straightforward transfer from source structures into target structures impractical. Many existing translation systems have mechanisms for handling divergent structures but do not provide a general procedure that takes advantage of takes advantage of the systematic relation between lexical-semantic structure and syntactic structure. This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved. This formalization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system."	"This contrasts with one of the traditional approaches ( e.g. , #AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language ."	"['It is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages.', 'Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus.', 'For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.', 'This contrasts with one of the traditional approaches ( e.g. , #AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .']"	1
CC294	J00-1004	Learning Dependency Translation Models as Collections of Finite-State Head Transducers	englishtomandarin speech translation with head transducers	"['Hiyan Alshawi', 'Fei Xia']"		"We describe the head transducer model used in an experimental English-toMandarin speech translation system. Head transduction is a translation method in which weighted finite state transducers are associated with sourcetarget word pairs. The method is suitable for speech translation because it allows efficient bottom up processing. The head transducers in the experimental system have a wider range of output positions than input positions. This asymmetry is motivated by a tradeoff between model complexity and search efficiency. 1 I n t r o d u c t i o n In this paper we describe the head transducer model used for translation in an experimental English-to-Mandarin speech translation system. Head transducer models consist of collections of weighted finite state transducers associated with pairs of lexical items in a bilingual lexicon. Head transducers operate ""outwards"" from the heads of phrases; they convert the left and right dependents of a source word into the left and right dependents of a corresponding target word. The transducer model can be characterized as a statistical translation model, but unlike the models proposed by Brown et al. (1990, 1993), these models have non-uniform linguistically motivated structure, at present coded by hand. The underlying linguistic structure of these models is similar to dependency grammar (Hudson 1984), although dependency representations are not explicitly constructed in our approach to translation. The original motivation for the head transducer models was Fei Xia D e p a r t m e n t of C o m p u t e r and I n f o r m a t i o n Science Univers i ty of P e n n s y l v a n i a Ph i l ade lph ia , PA 19104, USA fx i aQc i s .upenn . edu that they are simpler and more amenable to automatic model structure acquisition as compared with earlier transfer models. We first describe the head transduction approach in general in Section 2. In Section 3 we explain properties of the particular head transducers used in the experimental English-to-Mandarin speech translator. In Section 4, we explain how head transducers help satisfy the requirements of the speech translation application, and we conclude in Section 5. 2 B i l i n g u a l H e a d T r a n s d u c t i o n 2.1 Bilingual Head Transducers A head transducer M is a finite state machine associated with a pair of words, a source word w and a target word v. In fact, w is taken from the set V1 consisting of the source language vocabulary augmented by the ""empty word"" e, and v is taken from V~, the target language vocabulary augmented with e. A head transducer reads from a pair of source sequences, a left source sequence L1 and a right source sequence Rt; it writes to a pair of target sequences, a left target sequence L2 and a right target sequence R2 (Figure 1). Head transducers were introduced in Alshawi 1996b, where the symbols in the source and target sequences are source and target words respectively. In the model described in this paper, the symbols written are dependency relation symbols, or the empty symbol e. The use of relation symbols here is a result of the historical development of the system from an earlier transfer model. A conceptually simpler translator can be built using head transducer models with only lexical items, in which case the distinction between different dependents is implicit in the state of a transducer. In head transducer models, the use of relations corresponds to a type of class-based model (cf Je-"	"In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions ."	"['In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .']"	5
CC295	J00-1004	Learning Dependency Translation Models as Collections of Finite-State Head Transducers	a statistical approach to machine translation	"['P J Brown', 'J Cocke', 'S A Della Pietra', 'V J Della Pietra', 'J Lafferty', 'R L Mercer', 'P Rossin']"	conclusion	"Statistical Machine Translation has successfully been used for translation between many language pairs contributing to its popularity in recent years. It has however not been used for the  English/Persian language pair. This paper presents the first such attempt and describes the problems faced in creating a corpus and building a base line system. Our experience with  the construction of a parallel corpus during this ongoing study and the problems encountered especially with the process of alignment are discussed in this paper. The prototype  constructed and its evaluation using the BiLingual Evaluation Understudy (BLEU) is briefly described and results are analyzed. In the final part of the paper, conclusions are drawn  and work planned for the future is discussed"	"At the same time , we believe our method has advantages over the approach developed initially at IBM ( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically ."	"['At the same time , we believe our method has advantages over the approach developed initially at IBM ( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .', 'One advantage is that our method attempts to model the natural decomposition of sentences into phrases.', 'Another is that the compilation of this decomposition into lexically anchored finite-state head transducers produces implementations that are much more efficient than those for the IBM model.', 'In particular, our search algorithm finds optimal transductions of test sentences in less than ""real time"" on a 300MHz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application.']"	1
CC296	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	planning text for advisory dialogues	"['Johanna D Moore', 'Cecile Paris']"		"Explanation is an interactive process requiring a dialogue between advice-giver and advice-seeker. In this paper, we argue that in order to participate in a dialogue with its users, a generation system must be capable of reasoning about its own utterances and therefore must maintain a rich representation of the responses it produces. We present a text planner that constructs a detailed text plan, containing the intentional, attentional, and rhetorical structures of the text it generates."	"1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( #AUTHOR_TAG , 203 ) ."	"['IGEN constructs its plans using a hierarchical planning algorithm (Nilsson 1980).', 'The planner first checks all of its top-level plans to see which have effects that match the goal.', ""Each matching plan's preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan's body."", ""1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( #AUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987)."", 'In IGEN, the plans can involve any goals or actions that could be achieved via communication.']"	0
CC297	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	building another bridge over the generation gap	['Leo Wanner']		"In this paper, we address one of the central problems in text generation: the missing link (&quot;the generation gap&quot; in Meteer&apos;s terms) between the global discourse organization as often provided by text planning modules and the linguistic realization of this organiza- tion. We argue that the link should be established by the lexical choice process using resources derived from Mel&apos;iuk&apos;s ezicel Functions (LFs). In particular, we demonstrate that sequences of LFs may well serve as lexical discourse structure relations which link up to global discourse relations in the output of a Rhelorical Structure Theory style text planner"	"McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #AUTHOR_TAG ) ."	"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary; they have been called ""strategic"" and ""tactical"" components (e.g., McKeown 1985;Thompson 1977;Danlos 1987) 1, ""planning"" and""realization"" (e.g., McDonald 1983;Hovy 1988a), or simply ""what to say"" versus ""how to say it"" (e.g., Danlos 1987;Reithinger 1990).', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #AUTHOR_TAG ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']"	0
CC298	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	has a consensus nl generation architecture appeared and is it psycholinguistically plausible	['Ehud Reiter']		"I survey some recent applications-oriented  NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations  these modules perform, and the way the modules interact with each other. I also  compare this &apos;consensus architecture&apos; among  applied NLG systems with psycholinguistic  knowledge about how humans speak, and argue  that at least some aspects of the consensns  architecture seem to be in agreement  with what is known about human language production, despite the fact that psycholinguistic  plausibility was not in general a goal  of the developers of the surveyed systems"	"In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ) ."	"['The opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'Whatever problems result will be handled as best they can, on a case-by-case basis.', 'This approach is the one taken (implicitly or explicitly) in the majority of generators.', 'In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ) .', 'While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', ""Certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity.""]"	0
CC299	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	generating event descriptions with sage a simulation and generation environment	['Marie W Meteer']		"The SAGE system (Simulation and Generation Environment) was developed to address issues at the interface between conceptual modelling and natural language generation. In this paper, I describe SAGE and its components in the context of event descriptions. I show how kinds of information, such as the Reichenbachian temporal points and event structure, which are usually treated as unified systems, are often best represented at multiple levels in the overall system. SAGE is composed of a knowledge representation language and simulator, which form the underlying model and constitute the ""speaker""; a graphics component, which displays the actions of the simulator and provides an anchor for locative and deictic relations; and the generator SPOKESMAN, which produces a textual narration of events."	"McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) ."	"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary; they have been called ""strategic"" and ""tactical"" components (e.g., McKeown 1985;Thompson 1977;Danlos 1987) 1, ""planning"" and""realization"" (e.g., McDonald 1983;Hovy 1988a), or simply ""what to say"" versus ""how to say it"" (e.g., Danlos 1987;Reithinger 1990).', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']"	0
CC300	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	generating natural language linder pragmatic constraints lawrence erlbaum	['Eduard H Hovy']			"Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #AUTHOR_TAGa ) ."	"['The point here is not just that IGEN can produce different lexical realizations for a particular concept.', 'If that were the only goal, we could dispense with the feedback mechanism and simply design some sort of discrimination network (or similar device) to test various features of the information being expressed.', 'The planner could supply whatever information is needed to drive the network.', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #AUTHOR_TAGa ) .']"	0
CC301	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	an overview of the nigel text generation grammar	['William C Mann']		"Research on the text generation task has led to creation of a large systemic grammar of English, Nigel, which is embedded in a computer program. The grammar and the systemic framework have been extended by addition of a semantic stratum. The grammar generates sentences and other units under several kinds of experimental control.This paper describes augmentations of various precedents in the systemic framework. The emphasis is on developments which control the text to fulfill a purpose, and on characteristics which make Nigel relatively easy to embed in a larger experimental program."	"These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."	"['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", 'All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '5']"	0
CC302	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	telegram a grammar formalism for language planning	['Douglas E Appelt']			"These include devices such as interleaving the components ( McDonald 1983 ; #AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."	"['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; #AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", 'All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.5']"	0
CC303	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	intentions structure and expression in multilingual instructions	"['C6cile L Paris', 'Donia R Scott']"		"Instructional tex-ts have been the object of many studies recently, motivated by the increased need to produce manuals (especially multilingual manuals) coupled with the cost of translators and technical writers. Because these studies concentrate on aspects other than the linguistic realismion of instructions for example, the integration of text and graphi c s they all generate a sequence of steps required to achieve a task, using imperatives. Our research so flushows, however, that manuals can iu fact have different styles, i.e., not all instructions are stated using a sequence of imperatives, and that, furthermore, different parts of manuals often use different styles. In this paper, we present our preliminary results from an analysis of over 30 user guides/manuals for consumer appliances and discuss some of the implications."	"This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and , at least implicitly , in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost ."	"['One possible response would be to abandon the separation; the generator could be a single component that handles all of the work.', 'This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and , at least implicitly , in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost .']"	0
CC304	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	generating natural language linder pragmatic constraints lawrence erlbaum	['Eduard H Hovy']			"These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAGa , 1988c ) ."	"['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAGa , 1988c ) ."", 'All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '5']"	0
CC305	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	generating natural language linder pragmatic constraints lawrence erlbaum	['Eduard H Hovy']			Hovy has described another text planner that builds similar plans ( #AUTHOR_TAGb ) .	"['Hovy has described another text planner that builds similar plans ( #AUTHOR_TAGb ) .', 'This system, however, starts with a list of information to be expressed and merely arranges it into a coherent pattern; it is thus not a planner in the sense used here (as Hovy makes clear).', '10 Since text planning was not the primary focus of this work, IGEN is designed to simply assume that any false preconditions are unattainable.', ""IGEN's planner divides the requirements of a plan into two parts: the preconditions, which are not planned for, and those in the plan body, which are."", 'This has no .']"	0
CC306	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	has a consensus nl generation architecture appeared and is it psycholinguistically plausible	['Ehud Reiter']		"I survey some recent applications-oriented  NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations  these modules perform, and the way the modules interact with each other. I also  compare this &apos;consensus architecture&apos; among  applied NLG systems with psycholinguistic  knowledge about how humans speak, and argue  that at least some aspects of the consensns  architecture seem to be in agreement  with what is known about human language production, despite the fact that psycholinguistic  plausibility was not in general a goal  of the developers of the surveyed systems"	Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG ) .	"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary; they have been called ""strategic"" and ""tactical"" components (e.g., McKeown 1985;Thompson 1977;Danlos 1987) 1, ""planning"" and""realization"" (e.g., McDonald 1983;Hovy 1988a), or simply ""what to say"" versus ""how to say it"" (e.g., Danlos 1987;Reithinger 1990).', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994).', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG ) .']"	0
CC307	J00-2001	Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator	generating natural language linder pragmatic constraints lawrence erlbaum	['Eduard H Hovy']			"The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) ."	"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994; Panaget 1994; Wanner 1994). Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']"	0
CC308	J00-3002	Incremental Processing and Acceptability	geometry of lexicosyntactic interaction	['Glyn Morrill']			"Surveys and articles on the topic include Lamarche and Retord ( 1996 ) , de Groote and Retord ( 1996 ) , and #AUTHOR_TAG ."	"['Surveys and articles on the topic include Lamarche and Retord ( 1996 ) , de Groote and Retord ( 1996 ) , and #AUTHOR_TAG .', 'Still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained.']"	0
CC309	J00-3002	Incremental Processing and Acceptability	parsing and derivational equivalence	"['Mark Hepple', 'Glyn Morrill']"		"It is a tacit assumption of much linguistic inquiry that all distinct derivations of a string should assign distinct meanings. But despite the tidiness of such derlvational uniqueness, there seems to be no a priori reason to assume that a gramma r must have this property. If a grammar exhibits derivational equivalence, whereby distinct derivations of a string assign the same meanings, naive exhaustive search for all derivations will be redundant, and quite possibly intractable. In this paper we show how notions of derivation-reduction and normal form can be used to avoid unnecessary work while parsing with grammars exhibiting derivational equivalence. With grammar regarded as analogous to logic, derivations are proofs; what we are advocating is proof-reduction, and normal form proof; the invocation of these logical techniques adds a further paragraph to the story of parsing-as-deduction"	"An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction ."	"['(17) By a result of Zielonka (1981), the Lambek calculus is not axiomatizable by any finite set of combinatory schemata, so no such combinatory presentation can constitute the logic of concatenation in the sense of Lambek calculus.', 'Combinatory categorial grammar does not concern itself with the capture of all (or only) the concatenatively valid combinatory schemata, but rather with incrementality, for example, on a shiftreduce design.', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .']"	0
CC310	J00-3002	Incremental Processing and Acceptability	parsing as natural deduction	['Esther Konig']		"The logic behind parsers for categorial grammars can be formalized in several different ways. Lambek Calculus (LC) constitutes an example for a natural deduction1 style parsing method.In natural language processing, the task of a parser usually consists in finding derivations for all different readings of a sentence. The original Lambek Calculus, when it is used as a parser/theorem prover, has the undesirable property of allowing for the derivation of more than one proof for a reading of a sentence, in the general case.In order to overcome this inconvenience and to turn Lambek Calculus into a reasonable parsing method, we show the existence of ""relative"" normal form proof trees and make use of their properties to constrain the proof procedure in the desired way."	"One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) ."	"['One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .', 'Each sequent has a distinguished category formula (underlined) on which rule applications are keyed: In the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i.e., provided .L is not needed, F ~ A is a theorem of the Lambek calculus iff F ~ A is a theorem of the regulated calculus.', 'However, apart from the issue regarding .L, there is a general cause for dissatisfaction with this approach: it assumes the initial presence of the entire sequent to be proved, i.e., it is in principle nonincremental; on the other hand, allowing incrementality on the basis of Cut would reinstate with a vengeance the problem of spurious ambiguity, for then what are to be the Cut formulas?', 'Consequently, the sequent approach is ill-equipped to address the basic asymmetry of language--the asymmetry of its processing in time---and has never been forwarded in a model of the kind of processing phenomena cited in the introduction.']"	0
CC311	J00-3003	Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech	empirical studies on the disambiguation of cue phrases	"['Julia B Hirschberg', 'Diane J Litman']"		"Cue phrases are linguistic expressions such as now and well that function as explicit indicators of the structure of a discourse. For example, now may signal the beginning of a subtopic or a return to a previous topic, while well may mark subsequent material as a response to prior material, or as an explanatory comment. However, while cue phrases may convey discourse structure, each also has one or more alternate uses. While incidentally may be used sententially as an adverbial, for example, the discourse use initiates a digression. Although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse, the question of how speakers and hearers accomplish this disambiguation is rarely addressed. This paper reports results of empirical studies on discourse and sentential uses of cue phrases, in which both text-based and prosodic features were examined for disambiguating power. Based on these studies, it is proposed that discourse versus sentential usage may be distinguished by intonational features, specifically, pitch accent and prosodic phrasing. A prosodic model that characterizes these distinctions is identified. This model is associated with features identifiable from text analysis, including orthography and part of speech, to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech"	It is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .	"['DA classification using words is based on the observation that different DAs use distinctive word strings.', 'It is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .', 'Similarly, we find distinctive correlations between certain phrases and DA types.', 'For example, 92.4% of the uh-huh\'s occur in BACKCHANNELS, and 88.4% of the trigrams ""<start> do you"" occur in YES-NO-QUESTIONS.', 'To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type.', '5.1.1', 'Classification from True Words.', 'Assuming that the true (hand-transcribed) words of utterances are given as evidence, we can compute word-based likelihoods P(WIU ) in a straightforward way, by building a statistical language model for each of the 42 DAs.', 'All DAs of a particular type found in the training corpus were pooled, and a DA-specific trigram model was estimated using standard techniques (Katz backoff [Katz 1987] with Witten-Bell discounting [Witten and Bell 1991]).']"	4
CC312	J00-3003	Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech	automatic grammar induction and parsing free text a transformationbased approach	['Eric Brill']		"In this paper we describe a new technique for parsing free text: a transformational grammar1 is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction."	"A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( #AUTHOR_TAG ) ."	"['All the work mentioned so far uses statistical models of various kinds.', 'As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way.', 'However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.', 'A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( #AUTHOR_TAG ) .', 'Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (Munk 1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which further techniques could be borrowed.']"	1
CC313	J00-3003	Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech	a stochastic parts program and noun phrase parser for unrestricted text	['Kenneth Ward Church']		A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>	"The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #AUTHOR_TAG ) ."	"['The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #AUTHOR_TAG ) .', 'It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.', 'We can compute the per-utterance posterior DA probabilities by summing:']"	1
CC314	J00-3003	Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech	automatic stochastic tagging of natural language texts	"['Evangelos Dermatas', 'George Kokkinakis']"		"Five language and tagset independent stochastic taggers, handling morphological and contextual information, are presented and tested in corpora of seven European languages (Dutch, English, French, German, Greek, Italian and Spanish), using two sets of grammatical tags; a small set containing the eleven main grammatical classes and a large set of grammatical categories common to all languages. The unknown words are tagged using an experimentally proven stochastic hypothesis that links the stochastic behavior of the unknown words with that of the less probable known words. A fully automatic training and tagging program has been implemented on an IBM PC-compatible 80386-based computer. Measurements of error rate, time response, and memory requirements have shown that the taggers' performance is satisfactory, even though a small training text is available. The error rate is improved when new texts are used to update the stochastic model parameters."	"It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) ."	"['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.', 'We can compute the per-utterance posterior DA probabilities by summing:']"	0
CC315	J00-4002	Bidirectional Contextual Resolution	categorial semantics and scoping	['Fernando C N Pereira']		"Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings."	This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #AUTHOR_TAG .	"[""This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #AUTHOR_TAG ."", 'It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations.']"	1
CC316	J00-4002	Bidirectional Contextual Resolution	monotonic semantic interpretation	"['Hiyan Alshawi', 'Richard Crouch']"		"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved. 1. INTRODUCTION  The monotonicity property of unification based grammar formalisms is perhaps the most important factor in their widespread use for grammatical description and parsing. Monotonicity guarantees that the grammatical analysis of a sentence can proceed incrementally by combining information from rules and lexical entries in a nondestructive way. By contrast, aspects of semantic interpretation, such as reference and quantifier scope resolution, are often realised by non-mo.."	"These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) ."	"['What is required is that QLFs are, as here, expressed in a typed higher-order logic, augmented with constructs representing the interpretation of context-dependent elements (pronouns, ellipsis, focus, etc.).', 'These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .', 'Syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at QLF (again unlike standard QLFs) but are assumed to be available as components of the linguistic context.', '~']"	0
CC317	J00-4002	Bidirectional Contextual Resolution	monotonic semantic interpretation	"['Hiyan Alshawi', 'Richard Crouch']"		"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved."	"In the CLE-QLF approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules."	"[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE)."", 'In the CLE-QLF approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']"	1
CC318	J00-4002	Bidirectional Contextual Resolution	monotonic semantic interpretation	"['Hiyan Alshawi', 'Richard Crouch']"		"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved."	A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #AUTHOR_TAG .	"['A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #AUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints.', 'The role of resolution rules (for perfectly good presentational reasons) is completely ignored by their treatment.', 'However, it is really the case that in giving the semantics of a QLF, one is interested only in the set of RQLFs that are obtainable from it under closure of the resolution rules.', 'Ideally, therefore, we would like a formal reconstruction of resolution rules as well.', 'This is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they express are an important object of study in their own right.', 'Anyone who has built a wide-coverage system knows that the range of context-dependent phenomena encountered in real life is a lot wider than the preoccupations of many linguists might suggest.', 'In the CLE, for example, contextual resolution forms a larger part of the system than do syntactic and semantic processing.', 'Unfortunately, in the CLE there is no formal theory of resolution rules, and thus no prospect of capturing their role in assigning a semantics to QLFs.']"	1
CC319	J00-4002	Bidirectional Contextual Resolution	monotonic semantic interpretation	"['Hiyan Alshawi', 'Richard Crouch']"	introduction	"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved."	"We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language  approach of Dalrymple et al. ( 1996 ) ."	"['We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language  approach of Dalrymple et al. ( 1996 ) .']"	1
CC320	J00-4002	Bidirectional Contextual Resolution	resolving quasi logical forms	['Hiyan Alshawi']		"The paper describes intermediate and resolved logical form representations of sentences involving referring expressions and a reference resolution process for mapping between these representations. The intermediate representation, Quasi Logical Form (or QLF), may contain unresolved terms corresponding to anaphoric noun phrases covering bound variable anaphora, reflexives, and definite descripitions. Implict relations arising in constructs such as compound nominals appear in QLF as unresolved formulae. The QLF representation is also neutral with respect to ambiguities corresponding to quantifier scope and the collective/distributive distinction, the latter being treated as quantifier resolution. Reference candidates are proposed according to an ordered set of ""reference resolution rules"" producing possible resolved logical forms to which linguistic and pragmatic constraints are then applied."	"The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."	"[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."", 'In the CLE-QLF approach, as ra-tionally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994), the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']"	1
CC321	J00-4002	Bidirectional Contextual Resolution	resolving quasi logical forms	['Hiyan Alshawi']		"The paper describes intermediate and resolved logical form representations of sentences involving referring expressions and a reference resolution process for mapping between these representations. The intermediate representation, Quasi Logical Form (or QLF), may contain unresolved terms corresponding to anaphoric noun phrases covering bound variable anaphora, reflexives, and definite descripitions. Implict relations arising in constructs such as compound nominals appear in QLF as unresolved formulae. The QLF representation is also neutral with respect to ambiguities corresponding to quantifier scope and the collective/distributive distinction, the latter being treated as quantifier resolution. Reference candidates are proposed according to an ordered set of ""reference resolution rules"" producing possible resolved logical forms to which linguistic and pragmatic constraints are then applied."	"We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity."	"['We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.']"	0
CC322	J00-4002	Bidirectional Contextual Resolution	training and scaling preference functions for disambiguation	"['Hiyan Alshawi', 'David M Carter']"		"We present an automatic method for weighting the contributions of preference functions used in disambiguation. Initial scaling factors are derived as the solution to a least squares minimization problem, and improvements are then made by hill climbing. The method is applied to disambiguating sentences in the Air Travel Information System corpus, and the performance of the resulting scaling factors is compared with hand-tuned factors. We then focus on one class of preference function, those based on semantic lexical collocations. Experimental results are presented showing that such functions vary considerably in selecting correct analyses. In particular, we define a function that performs significantly better than ones based on mutual information and likelihood ratios of lexical associations."	"The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #AUTHOR_TAG ) , with the resolution process as described here ."	"['There are several stategies that might be pursued.', 'One is to adopt Pinkal\'s ""radical underspecification"" approach (Pinkal 1995) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #AUTHOR_TAG ) , with the resolution process as described here ."", 'Alternatively, I believe it is worth exploring the approach to disambiguation described in Pulman (2000), which would mesh nicely with the theory presented here.']"	3
CC323	J00-4002	Bidirectional Contextual Resolution	categorial semantics and scoping	['Fernando C N Pereira']		"Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings."	"The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG , 1991 ) , with some differences that are commented on below ."	"['We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG , 1991 ) , with some differences that are commented on below .', ""Like Pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin."", 'We analyze quantified NPs at the QLF level as illustrated in the QLF for: We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any (Alshawi 1990), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '6']"	1
CC324	J00-4002	Bidirectional Contextual Resolution	categorial semantics and scoping	['Fernando C N Pereira']		"Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings."	"It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and #AUTHOR_TAG , 1991 ) ."	"['It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and #AUTHOR_TAG , 1991 ) .', 'Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a ""free variable"" of type e is introduced in the NP position, with an associated ""quantifier assumption,"" which is added as a kind of premise.', 'At a later stage the quantifier assumption is ""discharged,"" capturing all occurrences of the free variable.', 'Thus their analysis of something like every manager disappeared would proceed as follows:']"	1
CC325	J00-4002	Bidirectional Contextual Resolution	an algorithm for generating quantifier scopings	"['Jerry R Hobbs', 'Stuart M Shieber']"		"The syntactic structure of a sentence often manifests quite clearly the predicate-argument structure and relations of grammatical subordination. But scope dependencies are not so transparent. As a result, many systems for representing the semantics of sentences have ignored scoping or generating scoping mechanisms that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow. In this paper, we present an algorithm, along with proofs of some of its important properties, that generates scoped semantic forms from unscoped expressions encoding predicate-argument structure. The algorithm is not profligate as are those based on permutation of quantifiers, and it can provide a solid foundation for computational solutions where completeness is sacrificed for efficiency and heuristic efficacy.Engineering and Applied Science"	"only the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â¢ the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ; â¢ partial scopings are permitted ( see Reyle [ 19961 ) â¢ scoping can be freely interleaved with other types of reference resolution ; â¢ unscoped or partially scoped forms are available for inference or for generation at every stage ."	"['only the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â\x80¢ the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ; â\x80¢ partial scopings are permitted ( see Reyle [ 19961 ) â\x80¢ scoping can be freely interleaved with other types of reference resolution ; â\x80¢ unscoped or partially scoped forms are available for inference or for generation at every stage .']"	0
CC326	J00-4002	Bidirectional Contextual Resolution	on reasoning with ambiguities	['Uwe Reyle']		"The paper adresses the problem of reasoning with ambiguities. Semantic representations are presented that leave scope relations between quantifiers and/or other operators unspecified. Truth conditions are provided for these representations and different consequence relations are judged on the basis of intuitive correctness. Finally inference patterns are presented that operate directly on these underspecified structures, i.e. do not rely on any translation into the set of their disambiguations.Comment: EACL 199"	"But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG ) work to our own framework ."	"['Developing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG ) work to our own framework .', 'Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.', 'His example is:']"	5
CC327	J00-4002	Bidirectional Contextual Resolution	monotonic semantic interpretation	"['Hiyan Alshawi', 'Richard Crouch']"		"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved."	"#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise ."	"['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']"	0
CC328	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	cogniac high precision coreference with limited knowledge and linguistic resources	['Breck Baldwin']		"This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns. It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution. The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied. The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient. Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension. The system has been evaluated in two distinct experiments which support the overall validity of the approach."	"A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG ; Mitkov 1996 , 1998b ) ."	"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG ; Mitkov 1996 , 1998b ) .']"	0
CC329	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	centeringinthelarge computing referential discourse segments	"['Udo Hahn', 'Michael Strube']"		"We specify an algorithm that builds up a hierarchy of referential discourse segments from local centering data. The spatial extension and nesting of these discourse segments constrain the reachability of potential antecedents of an anaphoric expression beyond the local level of adjacent center pairs. Thus, the centering model is scaled up to the level of the global referential structure of discourse. An empirical evaluation of the algorithm is supplied."	"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #AUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) ."	"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #AUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"	0
CC330	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	never look back an alternative to centering	['Michael Strube']		"I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list). The ordering among the elements of the S-list covers also the function of the backward-looking center in the centering model. The ranking criteria for the S-list are based on the distinction between hearer-old and hearer-new discourse entities and incorporate preferences for inter- and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word."	"Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( #AUTHOR_TAG ) ."	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( #AUTHOR_TAG ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']"	0
CC331	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	evaluating anaphora resolution approaches	['Ruslan Mitkov']			"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAGa , 2001b ) ."	"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAGa , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"	0
CC332	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	anaphora for everyone pronominal anaphora resolution without a parser	"['Christopher Kennedy', 'Branimir Boguraev']"		"We present an algorithm for anaphora resolution which is a modified and extended version of that developed by (Lappin and Leass, 1994). In contrast to that work, our algorithm does not require in-depth, full, syntactic parsing of text. Instead, with minimal compromise in output quality, the modifications enable the resolution process to work from the output of a part of speech tagger, enriched only with annotations of grammatical function of lexical items in the input text stream. Evaluation of the results of our implementation demonstrates that accurate anaphora resolution can be realized within natural language processing frameworks which do not---or cannot--- employ robust and reliable parsing components."	"A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) ."	"['Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']"	0
CC333	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	functional centering	"['Michael Strube', 'Udo Hahn']"		"Based on empirical evidence from a free word order language (German) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering."	"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #AUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) ."	"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #AUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .']"	0
CC334	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	probabilistic coreference in information extraction	['Andrew Kehler']		"Certain applications require that the out-put of an information extraction system be probabilistic, so that a downstream sys-tem can reliably fuse the output with pos-sibly contradictory information from other sources. In this paper we consider the problem of assigning a probability distri-bution to alternative sets of coreference re-lationships among entity descriptions. We present the results of initial experiments with several approaches to estimating such distributions in an application using SRI's FASTUS information extraction system."	"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #AUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) ."	"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #AUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"	0
CC335	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	multilingual anaphora resolution	['Ruslan Mitkov']		"This paper presents amultilingual robust, knowledge-poor approach to resolvingpronouns in technical manuals. This approach is a modification of the practicalapproach (Mitkov 1998a) and operates on texts pre-processed by apart-of-speech tagger. Input is checked against agreementand a number of antecedent indicators. Candidates are assigned scores by eachindicator and the candidate with the highest aggregate score isreturned as the antecedent. We propose this approach as aplatform for multilingual pronoun resolution. The robust approach was initiallydeveloped and tested for English, but we have also adaptedand tested it for Polish and Arabic. For bothlanguages, we found that adaptation required minimummodification and that further, even if used unmodified, the approachdelivers acceptable success rates. Preliminary evaluation reports high successrates in the range of over 90%."	"Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) ."	"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"	0
CC336	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	robust method of pronoun resolution using fulltext information	['Tetsuya Nasukawa']		"A consistent text contains rich information for resolving ambiguities within its sentences. Even simple syntactic information such as word occurrence and collocation patterns, which can be extracted from the text without deep discourse analysis, improves the accuracy of sentence analysis. Pronoun resolution is a typical proceeding that utilizes this information. Through the use of this information, along with information on the syntactic position of each candidate, 93.8% of pronoun references were resolved correctly in an experiment on computer manuals."	"A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) ."	"['Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']"	0
CC337	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	an architecture for anaphora resolution	"['Elaine Rich', 'Susann LuperFoy']"		"In this paper, we describe the pronominal anaphora resolution module of Lucy, a porta.ble English understanding system. The design.of thi.s module was motivated by the observation that, al- though there exist many theories of anaphora resolution, no one of these theories is complete. Thus we have implemented a blackboard-like architecture in which individual partial theories can be encoded as separate modules that can interact to propose .can- didate antecedents and to evaluate each other&apos;s proposals"	"Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input ."	"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']"	0
CC338	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	improving pronoun resolution in two languages by means of bilingual corpora	"['Ruslan Mitkov', 'Catalina Barbu']"			"Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) ."	"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"	0
CC339	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	outstanding issues in anaphora resolution	['Ruslan Mitkov']			"The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAGa ) ."	"['The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAGa ) .', 'A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are.', 'In particular, more research should be carried out on the factors influencing the performance of these algorithms.', 'One of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links.', 'More work toward the proposal of consistent and comprehensive evaluation is necessary; so too is work in multilingual contexts.', 'Some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future.']"	3
CC340	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	analysis of syntaxbased pronoun resolution methods	['Joel Tetreault']		"This paper presents a pronoun resolution algorithm that adheres to the constraints and rules of Centering Theory (Grosz et al., 1995) and is an alternative to Brennan et al.'s 1987 algorithm. The advantages of this new model, the Left-Right Centering Algorithm (LRC), lie in its incremental processing of utterances and in its low computational overhead. The algorithm is compared with three other pronoun resolution methods: Hobbs' syntax-based algorithm, Strube's S-list approach, and the BFP Centering algorithm. All four methods were implemented in a system and tested on an annotated subset of the Treebank corpus consisting of 2026 pronouns. The noteworthy results were that Hobbs and LRC performed the best."	"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) ."	"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"	0
CC341	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	anaphora resolution a multistrategy approach	"['Jaime Carbonell', 'Ralf Brown']"			"Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input ."	"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']"	0
CC342	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	description of the university of pennsylvania system used for muc6	"['Breck Baldwin', 'Jeff Reynar', 'Mike Collins', 'Jason Eisner', 'Adwait Ratnaparki', 'Joseph Rosenzweig', 'Anoop Sarkar', 'Srivinas Bangalore']"			"The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) ."	"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"	0
CC343	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	multilingual coreference resolution	"['Sanda Harabagiu', 'Steven Maiorano']"		"The current work investigates the problems that occur when coreference resolution is considered as a multilingual task. We assess the issues that arise when a framework using the mention-pair coreference resolution model and memory-based learning for the resolution process are used. Along the way, we revise three essential subtasks of coreference resolution: mention detection, mention head detection and feature selection. For each of these aspects we propose various multilingual solutions including both heuristic, rule-based and machine learning methods. We carry out a detailed analysis that includes eight different languages (Arabic, Catalan, Chinese, Dutch, English, German, Italian and Spanish) for which datasets were provided by the only two multilingual shared tasks on coreference resolution held so far: SemEval-2 and CoNLL-2012. Our investigation shows that, although complex, the coreference resolution task can be targeted in a multilingual and even language independent way. We proposed machine learning methods for each of the subtasks that are affected by the transition, evaluated and compared them to the performance of rule-based and heuristic approaches. Our results confirmed that machine learning provides the needed flexibility for the multilingual task and that the minimal requirement for a language independent system is a part-of-speech annotation layer provided for each of the approached languages. We also showed that the performance of the system can be improved by introducing other layers of linguistic annotations, such as syntactic parses (in the form of either constituency or dependency parses), named entity information, predicate argument structure, etc. Additionally, we discuss the problems occurring in the proposed approaches and suggest possibilities for their improvement"	"Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) ."	"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"	0
CC344	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	pronoun resolution the practical alternative presented at the discourse anaphora and anaphor resolution colloquium daarc1	['Ruslan Mitkov']			"A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; #AUTHOR_TAG , 1998b ) ."	"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; #AUTHOR_TAG , 1998b ) .']"	0
CC345	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	resolving pronoun references	['Jerry Hobbs']		"Two approaches to the problem of resolving pronoun references are presented. The first is a naive algorithm that works by traversing the surface parse trees of the sentences of the text in a particular order looking for noun phrases of the correct gender and number. The algorithm clearly does not work in all cases, but the results of an examination of several hundred examples from published texts show that it performs remarkably well. In the second approach, it is shown how pronoun solution can be handled in a comprehensive system for semantic analysis of English texts. The system is described, and it is shown in a detailed treatment of several examples how semantic analysis locates the antecedents of most pronouns as a by-product. Included are the classic examples of Winograd and Charniak."	"Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."	"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']"	0
CC346	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	robust reference resolution with limited knowledge high precision genrespecific approach for english and polish	"['Ruslan Mitkov', 'Malgorzata Stys']"		"Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labourand time-consuming task. This paper presents a robust, knowledgepoor approach to resolving pronouns in technical manuals in both English and Polish. This approach is a modification of the practical approach reported in [Mitkov 97] and operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and tested for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Preliminary evaluation reports precision of over 90%."	"Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) ."	"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"	0
CC347	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	recognizing referential links an information extraction perspective	['Megumi Kameyama']		"We present an efficient and robust reference resolution algorithm in an end-to-end state-of-the-art information extraction system, which must work with a considerably impoverished syntactic analysis of the input sentences. Considering this disadvantage, the basic setup to collect, filter, then order by salience does remarkably well with third-person pronouns, but needs more semantic and discourse information to improve the treatments of other expression types."	"The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG ."	"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"	0
CC348	J01-4001	Introduction to the Special Issue on Computational Anaphora Resolution	toward a computational theory of definite anaphora comprehension in english	['Candace Sidner']		"Abstract : This report investigates the process of focussing as a description and explanation of the comprehension of certain anaphoric expressions in English discourse. The investigation centers on the interpretation of definite anaphora, that is, on the personal pronouns, and noun phrases used with a definite article the, this, or that. Focussing is formalized as a process in which a speaker centers attention on a particular aspect of the discourse. An algorithmic description specifies what the speaker can focus on and how the speaker may change the focus of the discourse as the discourse unfolds. The algorithm allows for a simple focussing mechanism to be constructed: an element in focus, an ordered collection of alternate foci, and a stack of old foci. The data structure for the element in focus is a representation which encodes a limited set of associations between it and other elements from the discourse as well as from general knowledge. This report also establishes other constraints which are needed for the successful comprehension of anaphoric expressions. The focussing mechanism is designed to take advantage of syntactic and semantic information encoded as constraints on the choice of anaphora interpretation. These constraints are due to the work of language researchers; and the focussing mechanism provides a principled means for choosing when to apply the constraints in the comprehension process."	"Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input ."	"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']"	0
CC349	J02-3002	"Periods, Capitalized Words, etc."	overview of muc7”	['Nancy Chinchor']			Proper names are the main concern of the named-entity recognition subtask ( #AUTHOR_TAG 1998) of information extraction.	"['Proper names are the main concern of the named-entity recognition subtask ( #AUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', 'There the disambiguation of the first word in a sentence (and in other ambiguous positions) is one of the central problems: about 20% of named entities occur in ambiguous positions.', 'For instance, the word Black in the sentenceinitial position can stand for a person�s surname but can also refer to the color.', 'Even in multiword capitalized phrases, the first word can belong to the rest of the phrase or can be just an external modifier.', 'In the sentence Daily, Mason and Partners lost their court case, it is clear that Daily, Mason and Partners is the name of a company.', 'In the sentence Unfortunately, Mason and Partners lost their court case, the name of the company does not include the word Unfortunately, but the word Daily is just as common a word as Unfortunately.']"	0
CC350	J02-3002	"Periods, Capitalized Words, etc."	robust partofspeech tagging using a hidden markov model computer speech and language	['Julian Kupiec']			"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."	"['In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'There is, however, one difference in the implementation of such a tagger.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'At the same time since this token is unambiguous, it is not affected by the history.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']"	1
CC351	J02-3002	"Periods, Capitalized Words, etc."	a knowledgefree method for capitalized word disambiguation	['Andrei Mikheev']			"This is implemented as a cascade of simple strategies , which were briefly described in #AUTHOR_TAG ."	"['The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign.', 'Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'This is implemented as a cascade of simple strategies , which were briefly described in #AUTHOR_TAG .']"	5
CC352	J02-3002	"Periods, Capitalized Words, etc."	frequency analysis of english usage lexicon and grammar	"['W Nelson Francis', 'Henry Kucera']"			"There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) ."	"['There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'Altogether there are about 500 documents in the Brown corpus, with an average length of 2,300 word tokens.']"	5
CC353	J02-3002	"Periods, Capitalized Words, etc."	language independent named entity recognition combining morphological and contextual evidence”	"['Silviu Cucerzan', 'David Yarowsky']"		"Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications. This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchicaily smoothed trie models. The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools"	"Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #AUTHOR_TAG ) ."	"['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #AUTHOR_TAG ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']"	0
CC354	J02-3002	"Periods, Capitalized Words, etc."	some applications of treebased modeling to speech and language indexing”	['Michael D Riley']			"Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) ."	"['Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']"	0
CC355	J02-3002	"Periods, Capitalized Words, etc."	adaptive sentence boundary disambiguation”	"['David D Palmer', 'Marti A Hearst']"		"Labeling of sentence boundaries is a necessary prerequisite for many natural language processing tasks, including part-of-speech tagging and sentence alignment. End-of-sentence punctuation marks are ambiguous; to disambiguate them most systems use brittle, special-purpose regular expression grammars and exception rules. As an alternative, we have developed an efficient, trainable algorithm that uses a lexicon with part-of-speech probabilities and a feed-forward neural network. After training for less than one minute, the method correctly labels over 98.5 % of sentence boundaries in a corpus of over 27,000 sentence-boundary marks. We show the method to be efficient and easily adaptable to different text genres, including single-case texts.Comment: This is a Latex version of the previously submitted ps file   (formatted as a uuencoded gz-compressed .tar file created by csh script). The   software from the work described in this paper is available by contacting   dpalmer@cs.berkeley.ed"	"Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) ."	"['Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']"	0
CC356	J02-3002	"Periods, Capitalized Words, etc."	transformationbased errordriven learning and natural language parsing a case study in partofspeech tagging	['Eric Brill']			"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #AUTHOR_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."	"['In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'There is, however, one difference in the implementation of such a tagger.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #AUTHOR_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'At the same time since this token is unambiguous, it is not affected by the history.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']"	1
CC357	J02-3002	"Periods, Capitalized Words, etc."	a maximum entropy model for partofspeech tagging”	['Adwait Ratnaparkhi']			"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."	"['In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'There is, however, one difference in the implementation of such a tagger.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'At the same time since this token is unambiguous, it is not affected by the history.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']"	1
CC358	J02-3002	"Periods, Capitalized Words, etc."	adaptive multilingual sentence boundary disambiguation	"['David D Palmer', 'Marti A Hearst']"		"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German."	The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .	"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']"	1
CC359	J02-3002	"Periods, Capitalized Words, etc."	mitre description of the alembic system used for muc6”	"['John S Aberdeen', 'John D Burger', 'David S Day', 'Lynette Hirschman', 'Patricia Robinson', 'Marc Vilain']"			"For instance , the Alembic workbench ( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex ."	"['There exist two large classes of SBD systems: rule based and machine learning.', 'The rule-based systems use manually built rules that are usually encoded in terms of regular-expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'To put together a few rules is fast and easy, but to develop a rule-based system with good performance is quite a labor-consuming enterprise.', 'For instance , the Alembic workbench ( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', 'Another well-acknowledged shortcoming of rule-based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains.']"	0
CC360	J02-3002	"Periods, Capitalized Words, etc."	adaptive multilingual sentence boundary disambiguation	"['David D Palmer', 'Marti A Hearst']"	conclusion	"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German."	On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .	"['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate).', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .', 'Although these error rates seem to be very small, they are quite significant.', 'Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).', 'This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level.', 'On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus.']"	1
CC361	J02-3002	"Periods, Capitalized Words, etc."	transformationbased errordriven learning and natural language parsing a case study in partofspeech tagging	['Eric Brill']			"This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #AUTHOR_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."	"['We decided to train the tagger with the minimum of preannotated resources.', 'First, we used 20,000 tagged words to ""bootstrap"" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision.', 'We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%.', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #AUTHOR_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", 'Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'Periods as many other closed-class words cannot be successfully covered by such technique.']"	1
CC362	J02-3002	"Periods, Capitalized Words, etc."	tagging sentence boundaries”	['Andrei Mikheev']		In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts.	"In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #AUTHOR_TAG ."	"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #AUTHOR_TAG .', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']"	1
CC363	J02-3002	"Periods, Capitalized Words, etc."	a stochastic parts program and nounphrase parser for unrestricted text”	['Kenneth Church']		A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>	"As #AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not ."	"['Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'As #AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.', ""For example, the capitalized word 'Acts' is found twice in the Brown Corpus, both times as a proper noun (in a title)."", 'It would be misleading to infer from this evidence that the word \'Acts\' is always a proper noun.""']"	1
CC364	J02-3002	"Periods, Capitalized Words, etc."	statistical inference for probabilistic functions of finite markov chains	"['Leonard E Baum', 'Ted Petrie']"			"This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."	"['We decided to train the tagger with the minimum of preannotated resources.', 'First, we used 20,000 tagged words to ""bootstrap"" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision.', 'We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%.', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", 'Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'Periods as many other closed-class words cannot be successfully covered by such technique.']"	1
CC365	J02-3002	"Periods, Capitalized Words, etc."	russian morphology an engineering approach	"['Andrei Mikheev', 'Liubov Liubushkina']"	experiments	"Morphological analysis, which is at the heart of the processing of natural language requires computationally effective morphological processors. In this paper an approach to the organization of an inflectional morphological model and its application for the Russian language are described. The main objective of our morphological processor is not the classification of word constituents, but rather an efficient computational recognition of morpho-syntactic features of words and the generation of words according to requested morpho-syntactic features. Another major concern that the processor aims to address is the ease of extending the lexicon. The templated word-paradigm model used in the system has an engineering flavour: paradigm formation rules are of a bottom-up (word specific) nature rather than general observations about the language, and word formation units are segments of words rather than proper morphemes. This approach allows us to handle uniformly both general cases and exceptions, and requires extremely simple data structures and control mechanisms which can be easily implemented as a finite-state automata. The morphological processor described in this paper is fully implemented for a substantial subset of Russian (more then 1,500,000 word-tokens - 95,000 word paradigms) and provides an extensive list of morpho-syntactic features together with stress positions for words utilized in its lexicon. Special dictionary management tools were built for browsing, debugging and extension of the lexicon. The actual implementation was done in C and C++, and the system is available for the MS-DOS, MS-Windows and UNIX platforms."	"Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. ."	"['Since, unlike English, Russian is a highly inflected language, we had to deal with the case normalization issue.', 'Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', 'For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms.', 'Since the documents in the BBC news corpus were rather short, we applied the cache module, as described in Section 11.1.', 'This allowed us to reuse information across the documents.']"	5
CC366	J02-3002	"Periods, Capitalized Words, etc."	one sense per collocation”	['David Yarowsky']		"Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a polysemous word has one sense per discourse. In this paper we show that for certain definitions of collocation, a polysemous word exhibits essentially only one sense per collocation. We test his empirical hypothesis for several definitions of sense and collocation, and discover that it holds with 90-99 % accuracy for binary ambiguities. We utilize this property in a disambiguation algorithm that achieves precision of 92 % using combined models of very local context. 1"	"This is similar to ""one sense per collocation"" idea of #AUTHOR_TAG ."	"['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999).', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of #AUTHOR_TAG .']"	1
CC367	J02-3002	"Periods, Capitalized Words, etc."	celex a guide for users centre for lexical information	['Gavin Burnage']			"A variety of such lists for many languages are already available ( e.g. , #AUTHOR_TAG ) ."	"['The first list on which our method relies is a list of common words.', 'This list includes common words for a given language, but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. , #AUTHOR_TAG ) .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method).', 'We do not have to rely on pre-existing resources, however.', 'A list of common words can be easily obtained automatically from a raw (unannotated in any way) text collection by simply collecting and counting lowercased words in it.', 'We generated such list from the NYT collection.', 'To account for potential spelling and capitalization errors, we included in the list of common words only words that occurred lower-cased at least three times in the NYT texts.', 'The list of common words that we developed from the NYT collection contained about 15,000 English words.']"	0
CC368	J02-3002	"Periods, Capitalized Words, etc."	some applications of treebased modeling to speech and language indexing”	['Michael D Riley']	conclusion		The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .	"['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate).', 'Although these error rates seem to be very small, they are quite significant.', 'Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).', 'This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level.', 'On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus.']"	1
CC369	J02-3002	"Periods, Capitalized Words, etc."	nymble a high performance learning namefinder”	"['Daniel Bikel', 'Scott Miller', 'Richard Schwartz', 'Ralph Weischedel']"		"This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach."	In some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ) .	"['In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998).', 'In some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ) .', 'The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data.', 'Also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port.']"	0
CC370	J02-3002	"Periods, Capitalized Words, etc."	overview of muc7”	['Nancy Chinchor']			"For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG ) ."	"['• abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts.', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG ) .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', 'Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain.']"	5
CC371	J02-3002	"Periods, Capitalized Words, etc."	some applications of treebased modeling to speech and language indexing”	['Michael D Riley']			"The best performance on the Brown corpus , a 0.2 % error rate , was reported by #AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus ."	"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by #AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']"	1
CC372	J02-3002	"Periods, Capitalized Words, etc."	what is a word what is a sentence problems of tokenization”	"['Gregory Grefenstette', 'Pasi Tapanainen']"		"Any linguistic treatment of freely occurring text must provide an answer to what is considered as a token. In arti cial languages, the de nition of what is considered as a token can be precisely and unambiguously de ned. Natural languages, on the other hand, display such a rich variety that there are many ways to decide upon what will be considered as a unit for a computational approach to text. Here we will discuss tokenization as a problem for computational lexicography. Our discussion will cover the aspects of what is usually considered preprocessing of text in order to prepare it for some automated treatment. We present the roles of tokenization, methods of tokenizing, grammars for recognizing acronyms, abbreviations, and regular expressions such as numbers and dates. We present the problems encountered and discuss the e ects of seemingly innocent choices."	"One of the better-known approaches is described in #AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing ."	"['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in #AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', 'This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', 'Park and Byrd (2001) recently described a hybrid method for finding abbreviations and their definitions.', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'Therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'Candidates then are filtered through a set of known common words and proper names.', 'At the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document.']"	0
CC373	J02-3002	"Periods, Capitalized Words, etc."	unsupervised word sense disambiguation rivaling supervised methods”	['David Yarowsky']		"This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%"	"Since then this idea has been applied to several tasks , including word sense disambiguation ( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) ."	"['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']"	0
CC374	J02-3002	"Periods, Capitalized Words, etc."	adaptive multilingual sentence boundary disambiguation	"['David D Palmer', 'Marti A Hearst']"	introduction	"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German."	A detailed introduction to the SBD problem can be found in #AUTHOR_TAG .	"['Another important task of text normalization is sentence boundary disambiguation (SBD) or sentence splitting.', 'Segmenting text into sentences is an important aspect in developing many applications: syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. Sentence splitting in most cases is a simple matter: a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop).', 'A detailed introduction to the SBD problem can be found in #AUTHOR_TAG .']"	0
CC375	J02-3002	"Periods, Capitalized Words, etc."	tagging sentence boundaries”	['Andrei Mikheev']	conclusion	In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts.	"This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage ."	"['With all its strong points, there are a number of restrictions to the proposed approach.', 'First, in its present form it is suitable only for processing of reasonably ""wellbehaved"" texts that consistently use capitalization (mixed case) and do not contain much noisy data.', 'Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts.', 'We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']"	1
CC376	J02-3002	"Periods, Capitalized Words, etc."	identifying unknown proper names in newswire text”	"['Inderjeet Mani', 'T Richard MacMillan']"		"The identification of unknown proper names in text is a significant challenge for NLP systems operating on unrestricted text. A system which indexes documents according to name references can be useful for information retrieval or as a preprocessor for more knowledge intensive tasks such as database extraction. This paper describes a system which uses text skimming techniques for deriving proper names and their semantic attributes automatically from newswire text, without relying on any listing of name elements. In order to identify new names, the system treats proper names as (potentially) context-dependent linguistic expressions. In addition to using information in the local context, the system exploits a computational model of discourse which identifies individuals based on the way they are described in the text, instead of relying on their description in a pre-existing knowledge base."	#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .	"['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', '#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999).', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']"	5
CC377	J02-3002	"Periods, Capitalized Words, etc."	tagging sentence boundaries”	['Andrei Mikheev']		In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts.	"Unlike other POS taggers , this POS tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries ."	"['To test our hypothesis that DCA can be used as a complement to a local-context approach, we combined our main configuration (evaluated in row D of Table 4) with a POS tagger.', 'Unlike other POS taggers , this POS tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries .']"	5
CC378	J02-3002	"Periods, Capitalized Words, etc."	mitre description of the alembic system used for muc6”	"['John S Aberdeen', 'John D Burger', 'David S Day', 'Lynette Hirschman', 'Patricia Robinson', 'Marc Vilain']"			The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG ) : a 0.5 % error rate .	"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG ) : a 0.5 % error rate .', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']"	1
CC379	J02-3002	"Periods, Capitalized Words, etc."	adaptive multilingual sentence boundary disambiguation	"['David D Palmer', 'Marti A Hearst']"	conclusion	"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German."	"This is where robust syntactic systems like SATZ ( #AUTHOR_TAG ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage ."	"['With all its strong points, there are a number of restrictions to the proposed approach.', 'First, in its present form it is suitable only for processing of reasonably ""wellbehaved"" texts that consistently use capitalization (mixed case) and do not contain much noisy data.', 'Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts.', 'We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( #AUTHOR_TAG ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']"	1
CC380	J02-3002	"Periods, Capitalized Words, etc."	language model adaptation using mixtures and an exponentially decaying cache”	"['Philip Clarkson', 'Anthony J Robinson']"		"Presents two techniques for language model adaptation. The first is based on the use of mixtures of language models: the training text is partitioned according to topic, a language model is constructed for each component and, at recognition time, appropriate weightings are assigned to each component to model the observed style of language. The second technique is based on augmenting the standard trigram model with a cache component in which the words' recurrence probabilities decay exponentially over time. Both techniques yield a significant reduction in perplexity over the baseline trigram language model when faced with a multi-domain test text, the mixture-based model giving a 24% reduction and the cache-based model giving a 14% reduction. The two techniques attack the problem of adaptation at different scales, and as a result can be used in parallel to give a total perplexity reduction of 30%."	"#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"	"['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999).', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']"	2
CC381	J02-3002	"Periods, Capitalized Words, etc."	eagle an extensible architecture for general linguistic engineering”	"['Breck Baldwin', 'Christine Doran', 'Jeffrey Reynar', 'Michael Niv', 'Bangalore Srinivas', 'Mark Wasson']"		"Over the course of two summer projects, we developed a general purpose natural language system which advances the state-of-the-art in several areas. The system contains demonstrated advancements in part-of-speech tagging, end-of-sentence detection, and coreference resolution. In addition, we believe that we have strong maximal noun phrase detection, and subject-verb-object recognition and a pat tern matching language well suited to a range of tasks. Other features of the system include modularity and interchangeability of components, rapid component integration and a debugging environment."	The description of the EAGLE workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .	"['The description of the EAGLE workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .', 'This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.', 'It is quite similar to our method for capitalized-word disambiguation.', 'The description of the EAGLE case normalization module provided by Baldwin et al. is, however, very brief and provides no performance evaluation or other details.']"	1
CC382	J02-3002	"Periods, Capitalized Words, etc."	a maximum entropy approach to identifying sentence boundaries”	"['Jeffrey C Reynar', 'Adwait Ratnaparkhi']"		"We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of ., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman-alphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.Comment: 4 pages, uses aclap.sty and covingtn.st"	"Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) ."	"['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']"	0
CC383	J02-3002	"Periods, Capitalized Words, etc."	one term or two” in	['Kenneth Church']	introduction	"Objective SYNTAX score II (SSII) is a long-term mortality prediction model to guide the decision making of the heart-team between coronary artery bypass grafting or percutaneous coronary intervention (PCI) in patients with left main or three-vessel coronary artery disease. This study aims to investigate the long-term predictive value of SSII for all-cause mortality in patients with one- or two-vessel disease undergoing PCI. Methods A total of 628 patients (76% men, mean age: 61+-10 years) undergoing PCI due to stable angina pectoris (43%) or acute coronary syndrome (57%), included between January 2008 and June 2013, were eligible for the current study. SSII was calculated using the original SYNTAX score website (www.syntaxscore.com). Cox regression analysis was used to assess the association between continuous SSII and long-term all-cause mortality. The area under the receiver-operating characteristic curve was used to assess the performance of SSII. Results SSII ranged from 6.6 to 58.2 (median: 20.4, interquartile range: 16.1-26.8). In multivariable analysis, SSII proved to be an independent significant predictor for 4.5-year mortality (hazard ratio per point increase: 1.10; 95% confidence interval: 1.07-1.13; p<0.001). In terms of discrimination, SSII had a concordance index of 0.77. Conclusion In addition to its established value in patients with left main and three-vessel disease, SSII may also predict long-term mortality in PCI-treated patients with one- or two-vessel disease"	"#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"	"['Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', 'In mixed-case texts capitalized words usually denote proper names (names of organizations, locations, people, artifacts, etc.), but there are special positions in the text where capitalization is expected.', 'Such mandatory positions include the first word in a sentence, words in titles with all significant words capitalized or table entries, a capitalized word after a colon or open quote, and the first word in a list entry, among others.', 'Capitalized words in these and some other positions present a case of ambiguity: they can stand for proper names, as in White later said . . .', ', or they can be just capitalized common words, as in White elephants are . . . .', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']"	0
CC384	J02-3002	"Periods, Capitalized Words, etc."	hybrid text mining for finding abbreviations and their definitions”	"['Youngja Park', 'Roy J Byrd']"		"We present a hybrid text mining method for finding abbreviations and their definitions in free format texts. To deal with the problem, this method employs pattern-based abbreviation rules in addition to text markers and cue words. The pattern-based rules describe how abbreviations are formed from definitions. Rules can be generated automatically and/or manually and can be augmented when the system processes new documents. The proposed method has the advantages of high accuracy, high flexibility, wide coverage, and fast recognition."	#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .	"['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing.', 'This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'Therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'Candidates then are filtered through a set of known common words and proper names.', 'At the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document.']"	0
CC385	J02-3002	"Periods, Capitalized Words, etc."	adaptive multilingual sentence boundary disambiguation	"['David D Palmer', 'Marti A Hearst']"	conclusion	"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German."	"For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words ."	"['A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time."", 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']"	1
CC386	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	inducing translation templates for examplebased machine translation	['Michael Carl']	introduction	"This paper describes an example-based machine translation (EBMT) system which relays on various knowledge resources. Morphologic analyses abstract the surface forms of the languages to be translated. A shallow syntactic rule formalism is used to percolate features in derivation trees. Translation examples serve the decomposition of the text to be translated and determine the transfer of lexical values into the target language. Translation templates determine the word order of the target language and the type of phrases (e.g. noun phrase, prepositional phase, ...) to be generated in the target language. An induction mechanism generalizes translation templates from translation examples. The paper outlines the basic idea underlying the EBMT system and investigates the possibilities and limits of the translation template induction process."	"Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #AUTHOR_TAG ) .7 As an example , consider the translation into French of the house collapsed ."	"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #AUTHOR_TAG ) .7 As an example , consider the translation into French of the house collapsed .']"	0
CC387	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	three heads are better than one	"['Robert Frederking', 'Sergei Nirenburg']"	conclusion	"With the stress of ongoing budget cuts librarians are tempted to hunker down and focus exclusively on their clients, their college, department or assigned area. But collaboration across campus, within new areas, with different faculty, and different students, can be beneficial to both student and faculty learning. Students often have research needs which cannot be answered by one faculty member or librarian. Cross disciplinary collaboration between multiple librarians and faculty is key to providing the best service to these students. In this case study a team of agribusiness students need help in preparing for a competition on food distribution. During the contest the students play the role of consultants, listen to a client's problem, research the industry and possible solutions, and then present a solution to the client. This competition requires research on commodities, government policies for food safety, food distribution, economics, management, marketing, and merchandising. A team was formed of an agriculture librarian, business librarian, and an agribusiness faculty advisor in order to cover all the elements required for student success. Each person played a specific role in preparing the students for the competition. The business librarian taught a selection of databases and online resources, the agriculture librarian taught agriculture resources and created a LibGuide specific to the contest, and the faculty advisor gave real world examples about the competition and best practices for their presentations. Outcomes of this collaboration included the sharing of knowledge about the research process, building bonds between faculty and librarians, knowledge transfer between the librarians, and successfully preparing the team of students for their competition"	"These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) ."	"['These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .']"	1
CC388	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	finding terminology translations from nonparallel corpora	"['Pascale Fung', 'Kathleen McKeown']"	introduction	"We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance."	"#AUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora ."	"['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', '#AUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']"	0
CC389	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	the selfextending phrasal lexicon	"['Uri Zernik', 'Michael Dyer']"	introduction		"â¢ Learnability ( #AUTHOR_TAG ) â¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( Rayner and Carter 1997 ) â¢ Localization ( Sch Â¨ aler 1996 )"	"['â\x80¢ Learnability ( #AUTHOR_TAG ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']"	0
CC390	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	a languageneutral sparsedata algorithm for extracting translation patterns	"['Kevin McTait', 'Arturo Trujillo']"	introduction		"Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #AUTHOR_TAG , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia ."	"['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #AUTHOR_TAG , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .']"	0
CC391	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	generating language with a phrasal lexicon	['Edward Hovy']	introduction	"In this paper, we ask: How should language be represented in a generator program? In particular, how do the concepts the generator must express, the grammar it is to use, and the words and phrases with which it must express them, relate? The answer presented here is that all linguistic knowledge -- all language -- should be contained in the lexicon. The argument is the following: A generator performs three types of task to produce text (deciding what material to include; ordering the parts within paragraphs and sentences; and expressing the parts as appropriate phrases and parts of speech). It gets the information it requires to do these tasks from three sources: from the grammar, from partially frozen phrases (including multi-predicate phrasal patterns), and from certain words. In a functionally organized system, there is no reason why an a priori distinction should be made between the contents of the lexicon and the contents of the grammar. From the generator's perspective, the difference between these sources is not important. Rules of grammar, multi-predicate phrases, and phrasal and verb predicate patterns can all be viewed as phrases, frozen to a greater or lesser degree, and should all be part of the lexicon. Some such ""phrases"" can be quite complex, prescribing a series of actions and tests to perform the three tasks: these can be thought of as specialist procedures. Others can be very simple: templates. This paper also describes the elements that constitute the lexicon of a phrasal generator program and the way the elements are used."	"â¢ Learnability ( Zernik and Dyer 1987 ) â¢ Text generation ( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( Rayner and Carter 1997 ) â¢ Localization ( Sch Â¨ aler 1996 )"	"['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']"	0
CC392	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	toward memorybased translation	"['Satoshi Sato', 'Makoto Nagao']"	introduction		"Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7"	"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7']"	0
CC393	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	what’s been forgotten in translation memory in envisioning machine translation in the information future	"['Elliott Macklovitch', 'Graham Russell']"	introduction		"From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #AUTHOR_TAG ) ."	"[""In quite a short space of time, translation memory (TM) systems have become a very useful tool in the translator's armory."", 'TM systems store a set of source, target translation pairs in their databases.', 'If a new input string cannot be found exactly in the translation database, a search is conducted for close (or ""fuzzy"") matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the final, output translation.', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #AUTHOR_TAG ) .']"	0
CC394	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	aligning clauses in parallel texts	"['Sotiris Boutsis', 'Stelios Piperidis']"	introduction	"This paper describes a method for the automatic alignment of parallel texts at clause level. The method features statistical techniques coupled with shallow linguistic processing. It presupposes a parallel bilingual corpus and identifies alignments between the clauses of the source and target language sides of the corpus. Parallel texts are first statistically aligned at sentence level and then tagged with their part-of-speech categories. Regular grammars functioning on tags, recognize clauses on both sides of the parallel text. A probabilistic model is applied next, operating on the basis of word occurrence and co-occurrence probabilities and character lengths. Depending on sentence size, possible alignments arc fed into a dynamic progranuning framework or a simulated annealing system in order to find or approxim~te the best alignment. 1he method has been tested on a Small Eng~ lish-Greek corpus consisting of texts relevant to software systems and has produced promising results in terms of correctly identified clause alignments."	#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .	"['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', '#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']"	0
CC395	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	gaijin a bootstrapping templatedriven approach to examplebased machine translation	"['Tony Veale', 'Andy Way']"	introduction		"â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; #AUTHOR_TAG ; Gough , Way , and Hearne 2002 )"	"['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; #AUTHOR_TAG ; Gough , Way , and Hearne 2002 )']"	0
CC396	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	the necessity of syntax markers two experiments with artificial languages	['Thomas Green']	introduction		"â¢ language learning ( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )"	"['â\x80¢ language learning ( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"	0
CC397	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	integrating translations from multiple sources with the pangloss mark iii machine translation system	"['Robert Frederking', 'Sergei Nirenburg', 'David Farwell', 'Steven Helmreich', 'Eduard Hovy', 'Kevin Knight', 'Stephen Beale', 'Constantin Domashnev', 'Donna Attardo', 'Dean Grannes', 'Ralf Brown']"	conclusion	"Since MT systems, whatever translation method they employ, do not reach an optimum output on free text; each method handles some problems better than others. The PANGLOSS Mark III system is an MT environment that uses the best results from a variety of independent MT systems or engines working simultaneously within a single framework on the same text. This paper describes the method used to combine the outputs of the engines into a single text."	"Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by Frederking and Nirenburg ( 1994 ) , #AUTHOR_TAG , and Hogan and Frederking ( 1998 ) ."	"['Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by Frederking and Nirenburg ( 1994 ) , #AUTHOR_TAG , and Hogan and Frederking ( 1998 ) .']"	1
CC398	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	subsentential exploitation of translation memories	"['Michel Simard', 'Philippe Langlais']"	introduction	Laboratoire de recherche appliquee en linguistique informatique (RALI) Departement d&apos;Informatique et recherche operationnelle Universite de Montrea	"More recently , #AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment ."	"['More recently , #AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .', 'This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments, and from there they suggest that �it is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those described in [Carl and Way 2003].�']"	0
CC399	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	further experiments in bilingual text alignment	['Harold Somers']	introduction	"We describe and experimentally evaluate an alternative algorithm for aligning and extracting vocabulary from parallel texts using recency vectors and a similarity measure based on Levenshtein distance. The work is largely inspired by Fung and McKeown 's DK-vec, though we use a simpler algorithm. The technique is tested on two sets of parallel corpora involving English, French, German, Dutch, Spanish, and Japanese. We attempt to evaluate the importance of parameters such as frequency of words chosen as candidates, the effect of different language pairings, and differences between the two corpora."	#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .	"['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']"	0
CC400	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	a psycholinguistic approach to corpusbased machine translation	['Patrick Juola']	introduction		"#AUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs ."	"['#AUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs .', 'However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides.', 'That is, for a rule in the Penn Treebank VP −→ VBG, NP, PP, we are certain (if the annotators have done their job correctly) that the first word in each of the strings corresponding to this right-hand side is a VBG, that is, a present participle.', 'Given this information, in such cases we tag such words with the <LEX> tag.', 'Taking expanding the board to 14 members −→ augmente le conseilà 14 membres as an example, we extract the chunks in ( 24 We ignore here the trivially true lexical chunk ""<QUANT> 14 : 14.""']"	1
CC401	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	the weighted majority algorithm	"['Nick Littlestone', 'Manfred Warmuth']"	introduction	"AbstractWe study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes. We are interested in the case where the learner has reason to believe that one of some pool of known algorithms will perform well, but the learner does not know which one. A simple and effective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. We call this method the Weighted Majority Algorithm. We show that this algorithm is robust in the presence of errors in the data. We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool. For example, given a sequence of trials, if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c(log |A| + m) mistakes on that sequence, where c is fixed constant"	"We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG ) ."	"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf.', 'Sato and Nagao 1990;Veale and Way 1997;Carl 1999). 7', 's an example, consider the translation into French of the house collapsed.', 'Assume the conditional probabilities in ( 33  These mistranslations are all caused by boundary friction.', 'Each of the translations in ( 37) and ( 38) would be output with an associated weight and ranked by the system.', 'We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word-level lexicon into generalized marker chunks.', 'That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons.', 'We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG ) .']"	3
CC402	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	gaijin a bootstrapping templatedriven approach to examplebased machine translation	"['Tony Veale', 'Andy Way']"	introduction		"Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG ; Carl 1999 ) .7"	"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG ; Carl 1999 ) .7']"	0
CC403	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	the world wide web as a resource for examplebased machine translation tasks	['Gregory Grefenstette']	experiments	"The WWW is two orders of magnitude larger than the largest corpora. Although noisy, web text presents language as it is used, and statistics derived from the Web can have practical uses in many NLP applications. For this reason, the WWW should be seen and studied as any other computationally available linguistic resource. In this article, we illustrate this by showing that an Example-Based approach to lexical choice for machine translation can use the Web as an adequate and free resource."	"However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #AUTHOR_TAG ."	"['The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.', 'However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #AUTHOR_TAG .', 'Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.', 'Rather than search for competing candidates, we select the ""best"" translation and have its morphological variants searched for on-line.', ""In the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le/la/l'ordinateurs personnels."", 'Interestingly, using Lycos, and setting the search language to French, the correct form les ordinateurs personnels is uniquely preferred over the other alternatives, as it is found 2,454 times, whereas the others are not found at all.', 'In this case, this translation overrides the highest-ranked translation (50) and is output as the final translation.', 'In fact, in checking the translations obtained for NPs using system combination ABC, we noted that 251 NPs out of the test set of 500 could be improved.', 'Of these 251, 207 (82.5%) were improved post hoc via the Web, with no improvement for the remaining 43 cases.', 'We consider this to be quite a significant result.']"	5
CC404	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	a psycholinguistic approach to corpusbased machine translation	['Patrick Juola']	introduction		"#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â > French and English â > Urdu ."	"['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', 'For the English −→ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English −→ Urdu, Juola (1997, page 213) notes that ""the system learned the original training corpus . . .', 'perfectly and could reproduce it without errors""; that is, it scored 100% accuracy when tested against the training corpus.', 'On novel test sentences, he gives results of 72% correct translation.', 'In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English −→ German on a test set of 791 sentences from CorelDRAW manuals.']"	0
CC405	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	a psycholinguistic approach to corpusbased machine translation	['Patrick Juola']	introduction		"â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( #AUTHOR_TAG ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )"	"['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( #AUTHOR_TAG ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"	0
CC406	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	examplebased incremental synchronous interpretation	['Hans-Ulrich Block']	introduction	"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output."	"In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG ."	"['In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG .', ""In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment.']"	5
CC407	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	examplebased incremental synchronous interpretation	['Hans-Ulrich Block']	introduction	"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output."	"Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #AUTHOR_TAG to permit a limited form of insertion in the translation process ."	"['• language learning (Green 1979;Mori and Moeser 1983;Morgan, Meier, and Newport 1989) • monolingual grammar induction (Juola 1998) • grammar optimization (Juola 1994) • insights into universal grammar (Juola 1998) • machine translation (Juola 1994(Juola , 1997Veale and Way 1997;Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.', ""Green's (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist."", 'The research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis.', 'Juola\'s (1994Juola\'s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to ""marker-normal form.""', 'However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', 'Nevertheless, Juola (1998, page 23) observes that ""a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.""', 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #AUTHOR_TAG to permit a limited form of insertion in the translation process .', 'As a byproduct of the chosen methodology, we also derive a standard ""word-level"" translation lexicon.', 'These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.']"	5
CC408	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	gaijin a bootstrapping templatedriven approach to examplebased machine translation	"['Tony Veale', 'Andy Way']"	introduction		"In their Gaijin system , #AUTHOR_TAG give a result of 63 % accurate translations obtained for English â > German on a test set of 791 sentences from CorelDRAW manuals ."	"['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', 'Juola (1994Juola ( , 1997 conducts some small experiments using his METLA system to show the viability of this approach for English −→ French and English −→ Urdu.', 'For the English −→ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English −→ Urdu, Juola (1997, page 213) notes that ""the system learned the original training corpus . . .', 'perfectly and could reproduce it without errors""; that is, it scored 100% accuracy when tested against the training corpus.', 'On novel test sentences, he gives results of 72% correct translation.', 'In their Gaijin system , #AUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals .']"	1
CC409	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	the role of syntax markers and semantic referents in learning an artificial language	"['Kazuo Mori', 'Shannon Moeser']"	introduction		"â¢ language learning ( Green 1979 ; #AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )"	"['â\x80¢ language learning ( Green 1979 ; #AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"	0
CC410	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	on psycholinguistic grammars	['Patrick Juola']	introduction	"It has long been known that language acquisition is only possible if information is available above and beyond the mere presence of a set of strings in the language. One commonly postulated source of such information is a (possibly innate) constraint on the syntactic forms that a grammar can take. This paper develops and presents a set of formalisms based on the Marker Hypothesis that natural languages are ""marked"" for complex syntactic structure at surface form. It further compares the expressivity and restrictedness of these formalisms and shows that, first, not all constraints are actually restrictive, and second, that the Marker Hypothesis, and its implicit function/content word distinction, provide strong restrictions on the form of allowable grammars. These restrictions may in turn provide evidence about its actual psychological reality and salience. In particular, the class of strongly marked languages can be demonstrated not to admit all finite languages and thus not be subject to the hangman's noose of Gold's learnability proofs, and it is conjectured that these languages may provide a computable method of inferring human-like languages."	"â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( #AUTHOR_TAG ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )"	"['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( #AUTHOR_TAG ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"	0
CC411	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	corpusbased acquisition of transfer functions using psycholinguistic principles	['Patrick Juola']	introduction		"For English â\x88\x92 > Urdu , #AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus ."	"['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of _source, target_ chunks.', 'Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English __ French and English __ Urdu.', 'For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English â\x88\x92 > Urdu , #AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus .', 'On novel test sentences, he gives results of 72% correct translation.', 'In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English __ German on a test set of 791 sentences from CorelDRAW manuals.']"	0
CC412	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	inducing translation templates for examplebased machine translation	['Michael Carl']	introduction	"This paper describes an example-based machine translation (EBMT) system which relays on various knowledge resources. Morphologic analyses abstract the surface forms of the languages to be translated. A shallow syntactic rule formalism is used to percolate features in derivation trees. Translation examples serve the decomposition of the text to be translated and determine the transfer of lexical values into the target language. Translation templates determine the word order of the target language and the type of phrases (e.g. noun phrase, prepositional phase, ...) to be generated in the target language. An induction mechanism generalizes translation templates from translation examples. The paper outlines the basic idea underlying the EBMT system and investigates the possibilities and limits of the translation template induction process."	"Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , #AUTHOR_TAG , and Brown ( 2000 ) , inter alia ."	"['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , #AUTHOR_TAG , and Brown ( 2000 ) , inter alia .']"	0
CC413	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	examplebased incremental synchronous interpretation	['Hans-Ulrich Block']	introduction	"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output."	"This is then generalized , following a methodology based on #AUTHOR_TAG , to generate the `` generalized marker lexicon . ''"	"['In this section, we describe how the memory of our EBMT system is seeded with a set of translations obtained from Web-based MT systems.', 'From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems.', 'First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', ""This is then generalized , following a methodology based on #AUTHOR_TAG , to generate the `` generalized marker lexicon . ''"", 'Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.up.univ-mrs.fr/˜veronis/biblios/ptp.htm.', 'methodology chosen, we automatically derive a fourth resource, namely, a ""word-level lexicon.""']"	5
CC414	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	hybrid language processing in the spoken language translator	"['Manny Rayner', 'David Carter']"	introduction	"We present an overview of the Spoken Language Translator (SLT) system's hybrid language-processing architecture, focusing on the way in which rule-based and statistical methods are combined to achieve robust and efficient performance within a linguistically motivated framework. In general, we argue that rules are desirable in order to encode domain-independent linguistic constraints and achieve high-quality grammatical output, while corpus-derived statistics are needed if systems are to be efficient and robust; further, that hybrid architectures are superior from the point of view of portability to architectures which only make use of one type of information. We address the topics of ""multi-engine"" strategies for robust translation; robust bottom-up parsing using pruning and grammar specialization; rational development of linguistic rule-sets using balanced domain corpora; and efficient supervised training by interactive disambiguation. All work described is fully implemented in the current version of the SLT-2 system."	"â¢ Learnability ( Zernik and Dyer 1987 ) â¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( #AUTHOR_TAG ) â¢ Localization ( Sch Â¨ aler 1996 )"	"['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( #AUTHOR_TAG ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']"	0
CC415	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	the necessity of syntax markers two experiments with artificial languages	['Thomas Green']	introduction		"Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''"	"[""Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.', 'That is, a basic phrase-level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment.']"	5
CC416	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	examplebased incremental synchronous interpretation	['Hans-Ulrich Block']	introduction	"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output."	"That is, where #AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag."	"['That is, where #AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme.', 'We thus cluster on marker words to improve the coverage of our system (see Section 5 for results that show exactly how clustering on marker words helps); others (notably Brown [2000, 2003]) use clustering techniques to determine equivalence classes of individual words that can occur in the same context, and in so doing derive translation templates from individual translation examples.']"	1
CC417	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	on psycholinguistic grammars	['Patrick Juola']	introduction	"It has long been known that language acquisition is only possible if information is available above and beyond the mere presence of a set of strings in the language. One commonly postulated source of such information is a (possibly innate) constraint on the syntactic forms that a grammar can take. This paper develops and presents a set of formalisms based on the Marker Hypothesis that natural languages are ""marked"" for complex syntactic structure at surface form. It further compares the expressivity and restrictedness of these formalisms and shows that, first, not all constraints are actually restrictive, and second, that the Marker Hypothesis, and its implicit function/content word distinction, provide strong restrictions on the form of allowable grammars. These restrictions may in turn provide evidence about its actual psychological reality and salience. In particular, the class of strongly marked languages can be demonstrated not to admit all finite languages and thus not be subject to the hangman's noose of Gold's learnability proofs, and it is conjectured that these languages may provide a computable method of inferring human-like languages."	"Nevertheless , #AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"	"['• language learning (Green 1979;Mori and Moeser 1983;Morgan, Meier, and Newport 1989) • monolingual grammar induction (Juola 1998) • grammar optimization (Juola 1994) • insights into universal grammar (Juola 1998) • machine translation (Juola 1994(Juola , 1997Veale and Way 1997;Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.', ""Green's (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist."", 'The research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis.', 'Juola\'s (1994Juola\'s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to ""marker-normal form.""', 'However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', ""Nevertheless , #AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"", 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process.', 'As a byproduct of the chosen methodology, we also derive a standard ""word-level"" translation lexicon.', 'These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.']"	0
CC418	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	automated generalization of translation examples	['Ralf Brown']	introduction	"Previous work has shown that adding generalization of the examples in the corpus of an example-based machine translation (EBMT) system can reduce the required amount of pretranslated example text by as much as an order of magnitude for Spanish-English and FrenchEnglish EBMT. Using word clustering to automatically generalize the example corpus can provide the majority of this improvement for French-English with no manual intervention; the prior work required a large bilingual dictionary tagged with parts of speech and the manual creation of grammar rules. By seeding the clustering with a small amount of manuallycreated information, even better performance can be achieved. This paper describes a method whereby bilingual word clustering can be performed using standard monolingual document clustering techniques, and its effectiveness at reducing the size of the example corpus required.  1 Introduction  Example-Based Machine Translation (EBMT) relies on a collection of textual units (usuall.."	"Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and #AUTHOR_TAG , inter alia ."	"['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and #AUTHOR_TAG , inter alia .']"	0
CC419	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	the phrasal lexicon	['Joseph Becker']	introduction	"Theoretical linguists have in recent years concentrated their attention on the productive aspect of language, wherein utterances are formed combinatorically from units the size of words or smaller. This paper will focus on the contrary aspect of language, wherein utterances are formed by repetition, modification, and concatenation of previously-known phrases consisting of more than one word. I suspect that we speak mostly by stitching together swatches of text that we have heard before; productive processes have the secondary role of adapting the old phrases to the new situation. The advantage of this point of view is that it has the potential to account for the observed linguistic behavior of native speakers, rather than discounting their actual behavior as irrelevant to their language. In particular, this point of view allows us to concede that most utterances are produced in stereotyped social situations, where the communicative and ritualistic functions of language demand not novelty, but rather an appropriate combination of formulas, cliches, idioms, allusions, slogans, and so forth. Language must have originated in such constrained social contexts, and they are still the predominant arena for language production. Therefore an understanding of the use of phrases is basic to the understanding of language as a whole.You are currently reading a much-abridged version of a paper that will be published elsewhere later."	"More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :"	"['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']"	0
CC420	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	a framework of a mechanical translation between japanese and english by analogy principle	['Makoto Nagao']	introduction	"Problems inherent in current machine translation systems have been reviewed and have been shown to be inherently inconsistent. The present paper defines a model based on a series of human language processing and in particular the use of analogical thinking. Machine translation systems developed so far have a kind of inherent contradiction in themselves. The more detailed a system has become by the additional improvements, the clearer the limitation and the boundary will be for the translation ability. To break through this difficulty we have to think about the mechanism of human translation, and have to build a model based on the fundamental function of language processing in the human brain. The following is an attempt to do this based on the ability of analogy finding in human beings."	"All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext ."	"['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']"	0
CC421	J03-3004	wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web	a method for extracting translation patterns from translation examples	['Hideo Watanabe']	introduction		#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .	"['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Güvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.']"	0
CC422	J04-3001	Sample Selection for Statistical Parsing	elements of information theory	"['Thomas M Cover', 'Joy A Thomas']"		"Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."	"Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #AUTHOR_TAG ) ."	"['where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.', 'Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #AUTHOR_TAG ) .', 'Determining the parse tree for a sentence from a set of possible parses can be viewed as assigning a value to a random variable.', 'Thus, a direct application of the entropy definition to the probability distribution of the parses for sentence w in G computes its tree entropy, TE(w, G), the expected number of bits needed to encode the distribution of possible parses for w.', 'However, we may not wish to compare two sentences with different numbers of parses by their entropy directly.', 'If the parse probability distributions for both sentences are uniform, the sentence with more parses will have a higher entropy.', 'Because longer sentences typically have more parses, using entropy directly would result in a bias toward selecting long sentences.', 'To normalize for the number of parses, the uncertainty-based evaluation function, f unc , is defined as a measurement of similarity between the actual probability distribution of the parses and a hypothetical uniform distribution for that set of parses.', 'In particular, we divide the tree entropy by the log of the number of parses: 10']"	0
CC423	J04-3001	Sample Selection for Statistical Parsing	applying cotraining methods to statistical parsing	['Anoop Sarkar']	related work	"We propose a novel Co-Training method for statistical parsing. The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text. The algorithm iteratively labels the entire data set with parse trees. Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly out-performs training only on the labeled data."	"The work of #AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing ."	"['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'The work of #AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', 'Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.', 'Similar approaches are being explored for parsing Hwa et al. 2003).']"	0
CC424	J04-3001	Sample Selection for Statistical Parsing	heterogeneous uncertainty sampling for supervised learning	"['David D Lewis', 'Jason Catlett']"	related work	"Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger."	"Some examples include text categorization ( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) ."	"['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']"	0
CC425	J04-3001	Sample Selection for Statistical Parsing	combining labeled and unlabeled data with cotraining	"['Avrim Blum', 'Tom Mitchell']"	related work		"Another technique for making better use of unlabeled data is cotraining ( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another ."	"['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining ( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another .', 'The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.', 'Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.', 'Similar approaches are being explored for parsing Hwa et al. 2003).']"	0
CC426	J04-3001	Sample Selection for Statistical Parsing	stochastic lexicalized contextfree grammar	"['Yves Schabes', 'Richard Waters']"		"Stochastic lexicalized context-free grammar (SLCFG) is an attractive compromise between the parsing efficiency of stochastic context-free grammar (SCFG) and the lexical sensitivity of stochastic lexicalized tree-adjoining grammar (SLTAG) . SLCFG is a restricted form of SLTAG that can only generate context-free languages and can be parsed in cubic time. However, SLCFG retains the lexical sensitivity of SLTAG and is therefore a much better basis for capturing distributional information about words than SCFG."	"Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."	"[""In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates."", 'Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.', 'In this section, we investigate whether these observations hold true for training statistical parsing models as well.', ""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers, their learning algorithms are different.', 'The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.', ""In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data.""]"	5
CC427	J04-3001	Sample Selection for Statistical Parsing	prepositional phrase attachment through a backedoff model	"['Michael Collins', 'James Brooks']"		"Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events -- ignoring events which occur less than 5 times in training data reduces performance to 81.6%."	"Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) ."	"['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.', 'We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier.']"	0
CC428	J04-3001	Sample Selection for Statistical Parsing	scaling to very very large corpora for natural language disambiguation	"['Michele Banko', 'Eric Brill']"	related work	"The amount of readily available online  text has reached hundreds of  billions of words and continues to  grow. Yet for most core natural  language tasks, algorithms continue  to be optimized, tested and compared  after training on corpora consisting  of only one million words or less. In  this paper, we evaluate the  performance of different learning  methods on a prototypical natural  language disambiguation task,  confusion set disambiguation, when  trained on orders of magnitude more  labeled data than has previously been  used. We are fortunate that for this  particular application, correctly  labeled training data is free. Since  this will often not be the case, we  examine methods for effectively  exploiting very large corpora when  labeled data comes at a cost"	"Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 ) ."	"['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 ) .']"	0
CC429	J04-3001	Sample Selection for Statistical Parsing	the estimation of stochastic contextfree grammars using the insideoutside algorithm computer speech and language	"['Karim A Lari', 'Steve J Young']"			"Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) ."	"['Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) .', 'Similarly, the algorithm can be modified to compute the quantity']"	5
CC430	J04-3001	Sample Selection for Statistical Parsing	insideoutside reestimation from partially bracketed corpora	"['Fernando C N Pereira', 'Yves Schabes']"		"The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences."	Our algorithm is similar to the approach taken by #AUTHOR_TAG for inducing PCFG parsers .	"['In the first experiment, we use an induction algorithm (Hwa 2001a) based on the expectation-maximization (EM) principle that induces parsers for PLTIGs.', ""The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.', 'For example, the sentence Several fund managers expect a rough market this morning before prices stabilize.', 'would be labeled as ""((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)""', 'Our algorithm is similar to the approach taken by #AUTHOR_TAG for inducing PCFG parsers .']"	1
CC431	J04-3001	Sample Selection for Statistical Parsing	insideoutside reestimation from partially bracketed corpora	"['Fernando C N Pereira', 'Yves Schabes']"	introduction	"The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences."	"For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #AUTHOR_TAG ) ."	"['Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #AUTHOR_TAG ) .', 'Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).', 'However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.', 'For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.', 'Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor.', ""The goal of this work is to minimize a system's reliance on annotated training data.""]"	0
CC432	J04-3001	Sample Selection for Statistical Parsing	rule writing or annotation costefficient resource usage for base noun phrase chunking	"['Grace Ngai', 'David Yarowsky']"	related work	"This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive real-time human annotation. Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored. Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment."	"Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) ."	"['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']"	0
CC433	J04-3001	Sample Selection for Statistical Parsing	an empirical evaluation of probabilistic lexicalized tree insertion grammars	['Rebecca Hwa']		"We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural-language processing. Comparing the performance of PLTIGs, with non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-grams, and improved parsing performance over its nonlexicalized counterpart. Furthermore, training of PLTIGs displays faster convergence than PCFGs."	"Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) ."	"[""In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates."", 'Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.', 'In this section, we investigate whether these observations hold true for training statistical parsing models as well.', ""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers, their learning algorithms are different.', 'The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.', ""In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data.""]"	5
CC434	J04-3001	Sample Selection for Statistical Parsing	limitations of cotraining for natural language learning from large datasets	"['David Pierce', 'Claire Cardie']"	related work	"Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data. This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data. This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels. Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between co-trained classifiers and fully supervised classifiers trained on a labeled version of all available data. However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement. To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling. Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks."	"#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training ."	"['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.', '#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .', 'Similar approaches are being explored for parsing Hwa et al. 2003).']"	0
CC435	J04-3001	Sample Selection for Statistical Parsing	headdriven statistical models for natural language parsing	['Michael Collins']	introduction	"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."	"Current state-of-the-art statistical parsers ( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) ."	"['Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992).', 'Current state-of-the-art statistical parsers ( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .', 'However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.', 'For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.', 'Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor.', ""The goal of this work is to minimize a system's reliance on annotated training data.""]"	0
CC436	J04-3001	Sample Selection for Statistical Parsing	a rule based approach to pp attachment disambiguation	"['Eric Brill', 'Philip S Resnik']"			"Some well-known approaches include rule-based models ( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) ."	"['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.', 'We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier.']"	0
CC437	J04-3001	Sample Selection for Statistical Parsing	learning probabilistic lexicalized grammars for natural language processing	['Rebecca Hwa']		"A good representation of language is essential to building natural language processing (NLP) applications. In recent years, the growing availability of machine-readable text corpora has popularized the use of corpus-trained probabilistic grammars to represent languages in NLP systems. Although automatically inducing grammars from large corpora is an appealing idea, it faces several challenges. First, the trained grammar must capture the complexity and ambiguities inherent in human languages. Second, to be useful in practical applications, the grammar must be computationally tractable. Third, although there exists an abundance of raw text, the induction of high-quality grammars depends on manually annotated training data, which are scarce; therefore, the learning algorithm must be able to generalize well. Finally, there are inherent trade-offs in attempting to meet all three challenges; a meta-level challenge is to find a good compromise between the competing factors.  To address these challenges, this thesis presents a partially supervised induction algorithm based on the Expectation-Maximization principle for the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism. Using the lexical properties of the PLTIG formalism in the learning algorithm, we show that it is possible to automatically induce a grammar for a natural language that adequately resolves ambiguities and manages domain complexity at a tractable computational cost. By augmenting the basic learning algorithm with training techniques such as grammar adaptation and sample-selection, we show that the induction's dependency on annotated training data can be significantly reduced. Our empirical studies indicate that even with a 36% reduction in annotated training data, the learning algorithm can nonetheless induce grammars without degrading their quality."	"In the first experiment , we use an induction algorithm ( #AUTHOR_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs ."	"['In the first experiment , we use an induction algorithm ( #AUTHOR_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', ""The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.', 'For example, the sentence Several fund managers expect a rough market this morning before prices stabilize.', 'would be labeled as ""((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)""', 'Our algorithm is similar to the approach taken by Pereira and Schabes (1992) for inducing PCFG parsers.']"	5
CC438	J04-3001	Sample Selection for Statistical Parsing	heterogeneous uncertainty sampling for supervised learning	"['David D Lewis', 'Jason Catlett']"		"Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger."	"That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #AUTHOR_TAG ) ."	"['Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #AUTHOR_TAG ) .', 'The underlying assumption is that an uncertain output is likely to be wrong.']"	0
CC439	J04-3001	Sample Selection for Statistical Parsing	corrected cotraining for statistical parsers	"['Rebecca Hwa', 'Miles Osborne', 'Anoop Sarkar', 'Mark Steedman']"	related work		"Similar approaches are being explored for parsing ( Steedman , #AUTHOR_TAG ; Hwa et al. 2003 ) ."	"['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.', 'Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.', 'Similar approaches are being explored for parsing ( Steedman , #AUTHOR_TAG ; Hwa et al. 2003 ) .']"	0
CC440	J04-3001	Sample Selection for Statistical Parsing	natural language parsing as statistical pattern recognition	['David Magerman']		"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%."	The head words can be automatically extracted using a heuristic table lookup in the manner described by #AUTHOR_TAG .	"['The Collins-Brooks PP-attachment classification algorithm.', 'preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.', 'For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).', 'The head words can be automatically extracted using a heuristic table lookup in the manner described by #AUTHOR_TAG .', 'For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n.', 'In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples.', 'A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition.', 'Each training example forms eight characteristic tuples:']"	5
CC441	J04-3001	Sample Selection for Statistical Parsing	statistical models for unsupervised prepositional phrase attachment	['Adwait Ratnaparkhi']		"We present several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task. Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information. It is therefore less resource-intensive and more portable than previous corpus-based algorithms proposed for this task. We present results for prepositional phrase attachment in both English and Spanish.Comment: uses colacl.st"	"Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #AUTHOR_TAG ) ."	"['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #AUTHOR_TAG ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.', 'We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier.']"	0
CC442	J04-3001	Sample Selection for Statistical Parsing	the estimation of stochastic contextfree grammars using the insideoutside algorithm computer speech and language	"['Karim A Lari', 'Steve J Young']"			We follow the notation convention of #AUTHOR_TAG .	"['For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities).', 'We follow the notation convention of #AUTHOR_TAG .']"	5
CC443	J04-3001	Sample Selection for Statistical Parsing	selective sampling using the query by committee algorithm	"['Yoav Freund', 'H Sebastian Seung', 'Eli Shamir', 'Naftali Tishby']"		"We analyze the ""query by committee"" algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons."	"The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) ."	"['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']"	0
CC444	J05-3003	Gaussian coordinates and the large scale universe	duden—das stilworterbuch duden—the style dictionary	['editor Dudenredaktion']			"She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG ) ."	"['Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'Their system recognizes 15 frames, and these do not contain details of subcategorizedfor prepositions.', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. Sarkar and Zeman (2000) evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88%.', 'However, their evaluation does not examine the extracted subcategorization frames but rather the argument-adjunct distinctions posited by their system.', 'The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG ) .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']"	0
CC445	J05-3003	Gaussian coordinates and the large scale universe	automatic extraction of subcategorization from corpora	"['Edward Briscoe', 'John Carroll']"		"We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1."	"#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX ."	"['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall.', '#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']"	1
CC446	J05-3003	Gaussian coordinates and the large scale universe	natural language parsing as statistical pattern recognition	['David Magerman']	related work	"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%."	"The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and Collins ( 1997 ) ."	"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and Collins ( 1997 ) .', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']"	0
CC447	J05-3003	Gaussian coordinates and the large scale universe	tree adjoining grammars in	['Aravind Joshi']	introduction	"The VERBMOBIL project is developing a translation system that can assist a face-to-face dialogue between two non-native english speakers. Instead of having continiously speak english, the dialogue partners have the option to switch to their respective mother tongues (currently german or japanese) in cases where they can't find the required word, phrase or sentence. In such situations, the users activate VERBMOBIL to translate their utterances into english.  A very important requirement for such a system is realtime processing. Realtime processing is essentially necessary, if such a system is to be smoothly integrated into an ongoing communication. This can be achieved by the use of anytime processing, which always provides a result. The quality of the result however, depends on the computation time given to the system. Early interruptions can only produce shallow results. Aiming at such a processing mode, methods for fast but preliminary translation must be integrated into the system assisted by others that refine these results. In this case we suggest structural translation with Synchronous Tree Adjoining Grammars (S-TAGs), which can serve as a fast and shallow realisation of all steps necessary during translation, i.e. analysis, transfer and generation, in a system capable of running anytime methods. This mode is especially adequate for standardized speech acts and simple sentences. Furthermore, it provides a result for early interruptions of the translation process. By building an explicit linguistic structure, methods for refining the result can rearrange the structure in order to increase the quality of the translation given extended execution time. This paper describes the formalism of S-TAGs and the parsing algorithm implemented in VERBMOBIL. Furthermore the language covered by the german grammar is described. Finally we list examples together with the execution time required for their processing"	"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."	"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"	0
CC448	J05-3003	Gaussian coordinates and the large scale universe	lexical functional grammar volume 34 of syntax and semantics	['Mary Dalrymple']			"While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level ."	"['While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'LFG argues that subcategorization requirements are best stated at the f-structure level, in functional rather than phrasal terms.', 'This is because of the assumption that abstract grammatical functions are primitive concepts as opposed to derivatives of phrase structural position.', 'In LFG, the subcategorization requirements of a particular predicate are expressed by its semantic form: FOCUS (↑ SUBJ)(↑ OBL on ) in Figure 1.']"	0
CC449	J05-3003	Gaussian coordinates and the large scale universe	automatic acquisition of a large subcategorisation dictionary from corpora	['Christopher Manning']	related work		#AUTHOR_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", '#AUTHOR_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"	0
CC450	J05-3003	Gaussian coordinates and the large scale universe	automatic extraction of subcategorization from corpora	"['Edward Briscoe', 'John Carroll']"	experiments	"We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1."	"#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames ."	"['Applying an absolute threshold of five occurrences, we still generate 162 frame types  from Penn-II and 221 from Penn-III.', '#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames .']"	0
CC451	J05-3003	Gaussian coordinates and the large scale universe	extracting tree adjoining grammars from bracketed corpora	['Fei Xia']	related work	"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG."	"Unlike our approach , those of #AUTHOR_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees ."	"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach , those of #AUTHOR_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']"	1
CC452	J05-3003	Gaussian coordinates and the large scale universe	automated extraction of tags from the penn treebank	"['John Chen', 'K Vijay-Shanker']"	introduction	"The accuracy of statistical parsing models can be improved with the use of lexical information. Statistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accuracy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English."	"Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) ."	"['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'However, our approach also generalizes to CFG category-based approaches.', 'In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate.', 'Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002;Cahill, McCarthy, et al. 2004).', 'Our technique requires a treebank annotated with LFG functional schemata.', 'In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.', 'If the f-structures are of high quality, reliable LFG semantic forms can be generated quite simply by recursively reading off the subcategorizable grammatical functions for each local PRED value at each level of embedding in the f-structures.', 'The work reported in van Genabith, Sadler, and Way (1999) was small scale (100 trees) and proof of concept and required considerable manual annotation work.', 'It did not associate frames with probabilities, discriminate between frames for active and passive constructions, properly reflect the effects of long-distance dependencies (LDDs), or include CFG category information.', 'In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in Cahill et al. (2002) and Cahill, McCarthy, et al. (2004).', 'More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']"	0
CC453	J05-3003	Gaussian coordinates and the large scale universe	automatic extraction of subcategorization from corpora	"['Edward Briscoe', 'John Carroll']"	related work	"We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1."	"#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection ."	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', '#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"	0
CC454	J05-3003	Gaussian coordinates and the large scale universe	the tiger treebank	"['Sabine Brants', 'Stefanie Dipper', 'Silvia Hansen', 'Wolfgang Lezius', 'George Smith']"		"Proceedings of the 16th Nordic Conference   of Computational Linguistics NODALIDA-2007.  Editors: Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit.  University of Tartu, Tartu, 2007.  ISBN 978-9985-4-0513-0 (online)  ISBN 978-9985-4-0514-7 (CD-ROM)  pp. 81-88"	"We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004)."	"['We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', 'The lexical resources, however, have not yet been evaluated. This, and much else, has to await further research.']"	5
CC455	J05-3003	Gaussian coordinates and the large scale universe	lexical functional grammar a formal system for grammatical representation	"['Ronald Kaplan', 'Joan Bresnan']"		In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation	The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .	['The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .']	0
CC456	J05-3003	Gaussian coordinates and the large scale universe	lexical functional grammar volume 34 of syntax and semantics	['Mary Dalrymple']			Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ) is a member of the family of constraint-based grammars .	"['Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries.', 'F-structure is produced from functional annotations on the nodes of the c-structure and implemented in terms of recursive feature structures (attribute-value matrices).', 'This is exemplified by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using the grammar in Figure 1, which results in the annotated c-structure and f-structure in Figure 2.']"	0
CC457	J05-3003	Gaussian coordinates and the large scale universe	lexicalfunctional syntax	['Joan Bresnan']			Lexical functional grammar ( Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .	"['Lexical functional grammar ( Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries.', 'F-structure is produced from functional annotations on the nodes of the c-structure and implemented in terms of recursive feature structures (attribute-value matrices).', 'This is exemplified by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using the grammar in Figure 1, which results in the annotated c-structure and f-structure in Figure 2.']"	0
CC458	J05-3003	Gaussian coordinates and the large scale universe	lexical functional grammar volume 34 of syntax and semantics	['Mary Dalrymple']			"According to #AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ ."	"['The value of the PRED attribute in an f-structure is a semantic form Π gf 1 , gf 2 , . . .', ', gf n , where Π is a lemma and gf a grammatical function.', 'The semantic form provides an argument list gf 1 ,gf 2 , . . .', ',gf n specifying the governable grammatical functions (or arguments) required by the predicate to form a grammatical construction.', 'In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS (↑ SUBJ)(↑ OBL on ) .', 'The argument list can be empty, as in the PRED value for judge in Figure 1.', 'According to #AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list of grammatical functions is divided into governable (subcategorizable) grammatical functions (arguments) and nongovernable (nonsubcategorizable) grammatical functions (modifiers/adjuncts), as summarized in Table 1.']"	0
CC459	J05-3003	Gaussian coordinates and the large scale universe	automatic extraction of subcategorization frames for czech	"['Anoop Sarkar', 'Daniel Zeman']"		"We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88 % precision on unseen parsed text."	#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .	"['Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions.', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3.', '#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', 'However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem.', 'The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']"	1
CC460	J05-3003	Gaussian coordinates and the large scale universe	the automatic acquisition of frequencies of verb subcategorization frames from tagged corpora	"['Akira Ushioda', 'David Evans', 'Ted Gibson', 'Alex Waibel']"	method	"We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a finear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation."	"Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #AUTHOR_TAG ) ."	"['The syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2. However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #AUTHOR_TAG ) .', 'With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail.', 'For example, to identify a that-clause, we use']"	0
CC461	J05-3003	Gaussian coordinates and the large scale universe	statistical decision tree models for parsing	['David Magerman']	related work	"Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing {$n$}-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86 % precision, 86 % recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91 % precision, 90 % recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.Comment: uses aclap.sty, psfig.tex (v1.9), postscript figure"	The extraction procedure utilizes a head percolation table as introduced by #AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct .	"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by #AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']"	0
CC462	J05-3003	Gaussian coordinates and the large scale universe	from grammar to lexicon unsupervised learning of lexical syntax	['Michael Brent']	related work	"Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words---it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation."	"The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #AUTHOR_TAG ."	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #AUTHOR_TAG .', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"	0
CC463	J05-3003	Gaussian coordinates and the large scale universe	lexical functional grammar a formal system for grammatical representation	"['Ronald Kaplan', 'Joan Bresnan']"	introduction	In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation	"We applied lexical-redundancy rules ( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects."	"['We applied lexical-redundancy rules ( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects.', 'The resulting precision was very high (from 72.3% to 80.2%), and there was the expected drop in recall when prepositional details were included (from 54.7% to 29.3%).']"	5
CC464	J05-3003	Gaussian coordinates and the large scale universe	a comparison of evaluation metrics for a broad coverage parser	"['Richard Crouch', 'Ron Kaplan', 'Tracy King', 'Stefan Riezler']"	method		Both use the evaluation software and triple encoding presented in #AUTHOR_TAG .	"['In order to ensure the quality of the semantic forms extracted by our method, we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in #AUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures']"	5
CC465	J05-3003	Gaussian coordinates and the large scale universe	the parc 700 dependency bank	"['Tracy Holloway King', 'Richard Crouch', 'Stefan Riezler', 'Mary Dalrymple', 'Ronald Kaplan']"	method	"An automatic method for annotating the Penn-II Treebank (Marcus et al., 1994) with high-level Lexical Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001; Dalrymple, 2001) f-structure representations is described in (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004). The annotation algorithm and the automatically-generated f-structures are the basis for the automatic acquisition of wide-coverage and robust probabilistic approximations of LFG grammars (Cahill et al., 2002; Cahill et al., 2004a) and for the induction of LFG semantic forms (O'Donovan et al., 2004). The quality of the annotation algorithm and the f-structures it generates is, therefore, extremely important. To date, annotation quality has been measured in terms of precision and recall against the DCU 105. The annotation algorithm currently achieves an f-score of 96.57% for complete f-structures and 94.3% for preds-only f-structures. There are a number of problems with evaluating against a gold standard of this size, most notably that of overfitting. There is a risk of assuming that the gold standard is a complete and balanced representation of the linguistic phenomena in a language and basing design decisions on this. It is, therefore, preferable to evaluate against a more extensive, external standard. Although the DCU 105 is publicly available, 1 a larger well-established external standard can provide a more widely-recognised benchmark against which the quality of the f-structure annotation algorithm can be evaluated. For these reasons, we present an evaluation of the f-structure annotation algorithm of (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004) against the PARC 700 Dependency Bank (King et al., 2003). Evaluation against an external gold standard is a non-trivial task as linguistic analyses may differ systematically between the gold standard and the output to be evaluated as regards feature geometry and nomenclature. We present conversion software to automatically account for many (but not all) of the systematic differences. Currently, we achieve an f-score of 87.31% for the f-structures generated from the original Penn-II trees and an f-score of 81.79% for f-structures from parse trees produced by Charniak's (2000) parser in our pipeline parsing architecture against the PARC 700"	"More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators ."	"['from Section 23 of the Penn Treebank as described in Cahill, McCarthy, et al. (2004).', 'For the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%.', 'There is, however, a risk of overfitting when evaluation is limited to a gold standard of this size.', 'More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'They report precision of over 88.5% and recall of over 86% (Table 2).', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, and feature geometry.', 'Some, but not all, of these differences are captured by automatic conversion software.', 'A detailed discussion of the issues inherent in this process and a full analysis of results is presented in Burke, Cahill, et al. (2004a).', 'Results broken down by grammatical function for the DCU 105 evaluation are presented in Table 3. OBL (prepositional phrase) arguments are traditionally difficult to annotate reliably.', 'The results show, however, that with respect to obliques, the annotation algorithm, while slightly conservative (recall of 82%), is very accurate: 96% of the time it annotates an oblique, the annotation is correct.']"	0
CC466	J05-3003	Gaussian coordinates and the large scale universe	natural language parsing as statistical pattern recognition	['David Magerman']	related work	"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%."	"Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category."	"['Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�']"	5
CC467	J05-3003	Gaussian coordinates and the large scale universe	on the order of words	"['Anthony Ades', 'Mark Steedman']"	introduction	"Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn't matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn't. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-advComment: ACL 202"	"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ #AUTHOR_TAG ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."	"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ #AUTHOR_TAG ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"	0
CC468	J05-3003	Gaussian coordinates and the large scale universe	automatic fstructure annotation of treebank trees	['Anette Frank']	method		"Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept ."	"['The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information.', 'F-structures are attribute-value structures which represent abstract syntactic information, approximating to basic predicate-argument-modifier structures.', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .', 'However, more recent work (Cahill et al. 2002;Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (Marcus et al. 1994), containing more than 1,000,000 words and 49,000 sentences.']"	0
CC469	J05-3003	Gaussian coordinates and the large scale universe	subcategorization acquisition as	['Anna Korhonen']	related work	"Evaluation of word sense disambiguation (WSD) systems is often based on machine-readable dictionaries (MRDs). Such evaluation typically employs a set of fine-grained dictionary senses and considers them all to be equally important. In this paper, we propose a novel evaluation method for WSD systems in the context of automatic subcategorization acquisition. Building on an extant subcategorization acquisition system, we show that the system would benefit from WSD and propose modifications which allow it to make use of WSD. The enhanced subcategorization acquisition system can then be used as a task-based evaluation method for WSD systems where both the notion of sense and the sense's relevance to the evaluation process is determined by the application itself. 1"	Recent work by #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"	0
CC470	J05-3003	Gaussian coordinates and the large scale universe	compacting the penn treebank grammar	"['Alexander Krotov', 'Mark Hepple', 'Robert Gaizauskas', 'Yorick Wilks']"		"Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar off the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more treebanked text would be required to obtain a complete grammar, if one exists at some limit. However, we offer an alternative explanation in terms of the underspecification of structures within the treebank. This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules - rules whose right hand sides can be parsed by other rules. The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance figures. A version of the compaction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall, but a loss in precision."	"In Charniak ( 1996 ) and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank ."	"['The rate of accession may also be represented graphically.', 'In Charniak ( 1996 ) and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', 'Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).', 'Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count.', 'The first part of the graph (up to 1,004,414 words)']"	0
CC471	J05-3003	Gaussian coordinates and the large scale universe	three generative lexicalised models for statistical parsing	['Michael Collins']	related work	"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97"	"Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category."	"['Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�']"	5
CC472	J05-3003	Gaussian coordinates and the large scale universe	identifying verb arguments and their syntactic function in the penn treebank	"['Alexandra Kinyon', 'Carlos Prolo']"	related work	"In this paper, we present a tool that allows one to automatically extract verb argument-structure from the Penn Treebank as well as from other corpora annotated with the Penn Treebank release 2 conventions. More specifically, we examine each possible sequence of tags, both functional and categorial and determine whether such a sequence indicates an obligatory argument, an optional argument or a modifier. We argue that this approach is more fine-grained and thus more satisfactory than the existing approaches which have aimed at determining argumenthood in the Penn Treebank. The goal of this work is to provide a set of sufficiently general and fine-grained rules as well as an implementation which will be reusable and freely available to the research community. 1"	#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .	"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', '#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', 'Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 1998).', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', 'Marinov and Hemming (2004) present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank (Simov, Popova, and Osenova 2002).', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']"	0
CC473	J05-3003	Gaussian coordinates and the large scale universe	automatic extraction of subcategorization frames from the bulgarian tree bank	"['Svetoslav Marinov', 'Cecilia Hemming']"	related work	"(1) a. Teodora opened the door. b. *Arto looked the door. In (1-a) the verb open takes as an obligatory argument an NP and therefore differs from look in (1-b), which is an ill-formed sentence, because look require"	"#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) ."	"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', 'Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', 'Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 1998).', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', '#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']"	0
CC474	J05-3003	Gaussian coordinates and the large scale universe	extracting tree adjoining grammars from bracketed corpora	['Fei Xia']	related work	"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG."	#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .	"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', '#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']"	0
CC475	J05-3003	Gaussian coordinates and the large scale universe	lexical functional grammar a formal system for grammatical representation	"['Ronald Kaplan', 'Joan Bresnan']"	introduction	In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation	"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."	"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"	0
CC476	J05-3003	Gaussian coordinates and the large scale universe	how verb subcategorization frequencies are affected by corpus choice	"['Douglas Roland', 'Daniel Jurafsky']"		"The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers, and in psychological theories of language processing. But these probabilities are computed in very different ways by the two sets of researchers. Computational linguists compute verb subcategorization probabilities from large corpora while psycholinguists compute them from psychological studies (sentence production and completion tasks). Recent studies have found differences between corpus frequencies and psycholinguistic measures. We analyze subcategorization frequencies from four different corpora: psychological sentence production data (Connine et al. 1984), written text (Brown and WSJ), and telephone conversation data (Switchboard). We find two different sources for the differences. Discourse influence is a result of how verb use is affected by different discourse types such as narrative, connected discourse, and single sentence productions. Semantic influence is a result of different corpora using different senses of verbs, which have different subcategorization frequencies. We conclude that verb sense and discourse type play an important role in the frequencies observed in different experimental and corpus based sources of verb subcategorization frequencies."	It has been shown ( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .	"['Recently we have applied our methodology to the Penn-III Treebank, a more balanced corpus resource with a number of text genres.', 'Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus.', 'The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor.', 'It has been shown ( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .', 'Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur.', 'The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus.', 'The most important of these was the way in which we distinguish between oblique and adjunct.', 'We noted in Section 4 that our method of assigning an oblique annotation in Penn-II was precise, albeit conservative.', 'Because of a change of annotation policy in Penn-III, the -CLR tag (indicating a close relationship between a PP and the local syntactic head), information which we had previously exploited, is no longer used.', 'For Penn-III the algorithm annotates all PPs which do not carry a Penn adverbial functional tag (such as -TMP or -LOC) and occur as the sisters of the verbal head of a VP as obliques.']"	4
CC477	J05-3003	Gaussian coordinates and the large scale universe	extracting tree adjoining grammars from bracketed corpora	['Fei Xia']	introduction	"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG."	"Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) ."	"['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'However, our approach also generalizes to CFG category-based approaches.', 'In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate.', 'Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002;Cahill, McCarthy, et al. 2004).', 'Our technique requires a treebank annotated with LFG functional schemata.', 'In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.', 'If the f-structures are of high quality, reliable LFG semantic forms can be generated quite simply by recursively reading off the subcategorizable grammatical functions for each local PRED value at each level of embedding in the f-structures.', 'The work reported in van Genabith, Sadler, and Way (1999) was small scale (100 trees) and proof of concept and required considerable manual annotation work.', 'It did not associate frames with probabilities, discriminate between frames for active and passive constructions, properly reflect the effects of long-distance dependencies (LDDs), or include CFG category information.', 'In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in Cahill et al. (2002) and Cahill, McCarthy, et al. (2004).', 'More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']"	0
CC478	J05-3003	Gaussian coordinates and the large scale universe	the derivation of a grammatically indexed lexicon from the longman dictionary of contemporary english	"['Branimir Boguraev', 'Edward Briscoe', 'John Carroll', 'David Carter', 'Claire Grover']"	related work	"We describe a methodology and associated software system for the construction of a large lexicon from an existing machine-readable (published) dictionary. The lexicon serves as a component of an English morphological and syntactic analyser and contains entries with grammatical definitions compatible with the word and sentence grammar employed by the analyser. We describe a software system with two integrated components. One of these is capable of extracting syntactically rich, theory-neutral lexical templates from a suitable machine-readable source. The second supports interactive and semi-automatic generation and testing of target lexical entries in order to derive a sizeable, accurate and consistent lexicon from the source dictionary which contains partial (and occasionally in-accurate) information. Finally, we evaluate the utility of the Longman Dictionary of Contemporary English as a suitable source dictionary for the target lexicon."	"Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection ."	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"	0
CC479	J05-3003	Gaussian coordinates and the large scale universe	natural language parsing as statistical pattern recognition	['David Magerman']	method	"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%."	"The annotation procedure is dependent on locating the head daughter , for which an amended version of #AUTHOR_TAG is used ."	"['We utilize the automatic annotation algorithm of Cahill et al. (2002) and Cahill, McCarthy, et al. (2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations.', 'The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information.', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of #AUTHOR_TAG is used .', 'The head is annotated with the LFG equation ↑=↓.', 'Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads.', ""To give a simple example, the rightmost NP to the left of a VP head under an S is likely to be the subject of the sentence (↑ SUBJ =↓), while the leftmost NP to the right of the V head of a VP is most probably the verb's object (↑ OBJ =↓)."", 'Cahill, McCarthy, et al. (2004) provide four classes of annotation principles: one for noncoordinate configurations, one for coordinate configurations, one for traces (long-distance dependencies), and a final ""catch all and clean up"" phase.']"	5
CC480	J05-3003	Gaussian coordinates and the large scale universe	lexical functional grammar volume 34 of syntax and semantics	['Mary Dalrymple']	introduction		"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."	"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"	0
CC481	J05-3003	Gaussian coordinates and the large scale universe	the penn treebank annotating predicate argument structure	"['Mitchell Marcus', 'Grace Kim', 'Mary Ann Marcinkiewicz', 'Robert MacIntyre', 'Mark Ferguson', 'Karen Katz', 'Britta Schasberger']"	method	"The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as ""underlying"" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles."	"However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences ."	"['The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information.', 'F-structures are attribute-value structures which represent abstract syntactic information, approximating to basic predicate-argument-modifier structures.', 'Most of the early work on automatic f-structure annotation (e.g., van Genabith, Way, and Sadler 1999;Frank 2000;Sadler, van Genabith, and Way 2000) was applied only to small data sets (fewer than 200 sentences) and was largely proof of concept.', 'However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences .']"	0
CC482	J05-3003	Gaussian coordinates and the large scale universe	headdriven phrase structure grammar	"['Carl Pollard', 'Ivan Sag']"	introduction		"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."	"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"	0
CC483	J05-3003	Gaussian coordinates and the large scale universe	extracting tree adjoining grammars from bracketed corpora	['Fei Xia']		"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG."	"Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #AUTHOR_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ ."	"['In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced.', 'This can be expressed as a measure of the coverage of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #AUTHOR_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lexicon from Section 23.', 'Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only.', 'There is a corresponding semantic form in the reference lexicon for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not appear in the reference lexicon.', 'Within this group, we can distinguish between known words, which have an entry in the reference lexicon, and unknown words, which do not exist at all in the reference lexicon.', 'In the same way we make the distinction  between known frames and unknown frames.', 'There are, therefore, four different cases in which an entry may not appear in the reference lexicon.', 'Table 27 shows that the most common case is that of known verbs occurring with a different, although known, subcategorization frame (7.85%).']"	5
CC484	J05-3003	Gaussian coordinates and the large scale universe	from grammar to lexicon unsupervised learning of lexical syntax	['Michael Brent']	related work	"Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words---it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation."	#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', '#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"	0
CC485	J05-3003	Gaussian coordinates and the large scale universe	lexical functional grammar a formal system for grammatical representation	"['Ronald Kaplan', 'Joan Bresnan']"		In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation	Lexical functional grammar ( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .	"['Lexical functional grammar ( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries.', 'F-structure is produced from functional annotations on the nodes of the c-structure and implemented in terms of recursive feature structures (attribute-value matrices).', 'This is exemplified by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using the grammar in Figure 1, which results in the annotated c-structure and f-structure in Figure 2.']"	0
CC486	J05-3003	Gaussian coordinates and the large scale universe	automatic extraction of subcategorization frames for czech	"['Anoop Sarkar', 'Daniel Zeman']"	related work	"We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88 % precision on unseen parsed text."	#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic	"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', 'Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', '#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', 'Marinov and Hemming (2004) present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank (Simov, Popova, and Osenova 2002).', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']"	0
CC487	J05-3003	Gaussian coordinates and the large scale universe	three generative lexicalised models for statistical parsing	['Michael Collins']	related work	"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97"	"The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #AUTHOR_TAG ."	"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #AUTHOR_TAG .', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']"	0
CC488	J05-3003	Gaussian coordinates and the large scale universe	lexical functional grammar volume 34 of syntax and semantics	['Mary Dalrymple']	method		"#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization ."	"['In order to capture CFG-based categorial information, we add a CAT feature to the f-structures automatically generated from the Penn-II and Penn-III Treebanks.', 'Its value is the syntactic category of the lexical item whose lemma gives rise to the PRED value at that particular level of embedding.', 'This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight.', 'With this, the output for the verb impose in Figure 4 is impose (v,[subj, obj, obl:on]).', 'For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4).', 'As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details.', '#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', 'In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function.']"	4
CC489	J05-3003	Gaussian coordinates and the large scale universe	from treebank to propbank	"['Paul Kingsbury', 'Martha Palmer']"			"In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #AUTHOR_TAG ) ."	"['We have presented an algorithm for the extraction of semantic forms (or subcategorization frames) from the Penn-II and Penn-III Treebanks, automatically annotated with LFG f-structures.', 'In contrast to many other approaches, ours does not predefine the subcategorization frames we extract.', ""We have applied the algorithm to the WSJ sections of Penn-II (50,000 trees) (O' Donovan et al. 2004) and to the parse-annotated Brown corpus of Penn-III (almost 25,000 additional trees)."", 'We extract syntactic-function-based subcategorization frames (LFG semantic forms) and traditional CFG category-based frames, as well as mixed-function-category-based frames.', 'Unlike many other approaches to subcategorization frame extraction, our system properly reflects the effects of long-distance dependencies.', 'Also unlike many approaches, our method distinguishes between active and passive frames.', 'Finally, our system associates conditional probabilities with the frames we extract.', 'Making the distinction between the behavior of verbs in active and passive contexts is particularly important for the accurate assignment of probabilities to semantic forms.', 'We carried out an extensive evaluation of the complete induced lexicon against the full COMLEX resource.', 'To our knowledge, this is the most extensive qualitative evaluation of subcategorization extraction in English.', 'The only evaluation of a similar scale is that carried out by Schulte im Walde (2002b) for German.', ""The results reported here for Penn-II compare favorably against the baseline and, in fact, are an improvement on those reported in O' Donovan et al. (2004)."", 'The results for the larger, more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15% above the baseline.', 'We believe our semantic forms are fine-grained, and by choosing to evaluate against COMLEX, we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other earlier evaluations.', 'Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX.', 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #AUTHOR_TAG ) .']"	3
CC490	J05-3003	Gaussian coordinates and the large scale universe	how verb subcategorization frequencies are affected by corpus choice	"['Douglas Roland', 'Daniel Jurafsky']"		"The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers, and in psychological theories of language processing. But these probabilities are computed in very different ways by the two sets of researchers. Computational linguists compute verb subcategorization probabilities from large corpora while psycholinguists compute them from psychological studies (sentence production and completion tasks). Recent studies have found differences between corpus frequencies and psycholinguistic measures. We analyze subcategorization frequencies from four different corpora: psychological sentence production data (Connine et al. 1984), written text (Brown and WSJ), and telephone conversation data (Switchboard). We find two different sources for the differences. Discourse influence is a result of how verb use is affected by different discourse types such as narrative, connected discourse, and single sentence productions. Semantic influence is a result of different corpora using different senses of verbs, which have different subcategorization frequencies. We conclude that verb sense and discourse type play an important role in the frequencies observed in different experimental and corpus based sources of verb subcategorization frequencies."	"As noted above , it is well documented ( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains ."	"['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above , it is well documented ( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains .', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall.', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']"	4
CC491	J05-3003	Gaussian coordinates and the large scale universe	automated extraction of tags from the penn treebank	"['John Chen', 'K Vijay-Shanker']"	related work	"The accuracy of statistical parsing models can be improved with the use of lexical information. Statistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accuracy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English."	#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .	"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', '#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']"	0
CC492	J05-3003	Gaussian coordinates and the large scale universe	treebank grammars in	['Eugene Charniak']			"In #AUTHOR_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank ."	"['The rate of accession may also be represented graphically.', 'In #AUTHOR_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', 'Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).', 'Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count.', 'The first part of the graph (up to 1,004,414 words)']"	0
CC493	J05-3003	Gaussian coordinates and the large scale universe	automatic acquisition of a large subcategorisation dictionary from corpora	['Christopher Manning']	introduction		"#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature ."	"['One important type of lexical information is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction).', 'Lexicons, including subcategorization details, were traditionally produced by hand.', 'However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component.', 'In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998).', '#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.']"	4
CC494	J05-3003	Gaussian coordinates and the large scale universe	lexicalfunctional syntax	['Joan Bresnan']	introduction		"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."	"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"	0
CC495	J05-3003	Gaussian coordinates and the large scale universe	the automatic acquisition of frequencies of verb subcategorization frames from tagged corpora	"['Akira Ushioda', 'David Evans', 'Ted Gibson', 'Alex Waibel']"	related work	"We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a finear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation."	#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .	"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']"	0
CC496	J05-3003	Gaussian coordinates and the large scale universe	from dictionary to corpus to selforganizing dictionary learning valency associations in the face of variation and change	['Edward Briscoe']		"ing over specific lexically-governed particles and prepositions and specific predicate selectional preferences, but including some `derived' / `alternant' semi-productive, and therefore only semipredictable, bounded dependency constructions, such as particle or dative movement, there are at least 163 valency frames associated with verbal predicates in (current) English (Briscoe, 2000). In this paper, I will review the work that my colleagues and I have done to learn (semi-)automatically this very large number of associations between individual verbal predicates and valency frames. Access to a comprehensive and accurate valency lexicon is critical for the development of robust and accurate parsing technology capable of recovering predicate-argument relations (and thus logical forms) from free text or transcribed speech. Without this information it is possible to `chunk' input into phrases but not to distinguish arguments from adjuncts or resolve most phrasal attachment ambiguities. Furthermore, for statistical parsers it is not enough to know the associations of predicates to valency frames, it is also critical to know the relative frequency of such associations given a specific predicate. Such information is a core component of that required to `lexicalize' a probabilistic parser, and it is now well-established that lexicalization is essential for accurate disambiguation (e.g. Collins, 1997, Carroll et al, 1998). While state-of-the-art wide-coverage grammars of English, capable of recovering predicateargument structure and expressed as a unification-based phrase structure grammar, have on the order of 1000 rules, it is clear that the number of associations between valency frames and predicates needed in a lexicon for such a grammar will be much higher."	"As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall ."	"['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']"	0
CC497	J05-4005	Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach	a stochastic finitestate wordsegmentation algorithm for chinese	"['Richard Sproat', 'Chilin Shih', 'William Gale', 'Nancy Chang']"	related work		"A previous work along this line is #AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) ."	"['We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is #AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', 'Our approach is similarly motivated but is based on a different mechanism: linear mixture models.', 'As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information.', ""Many types of OOV words that are not covered in Sproat's system can be dealt with in our system."", 'The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and have been recently introduced into NLP tasks by Collins and Duffy (2001).', 'Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (Xue 2003) and conditional random fields (Peng, Feng, and McCallum 2004).', 'They also use a unified approach to word breaking and OOV identification.']"	1
CC498	J06-2002	Generating Referring Expressions that Involve Gradable Properties	computational interpretations of the gricean maximes in the generation of referring expressions	"['Robbert Dale', 'Ehud Reiter']"	experiments	"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system."	"#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) ."	"['Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', '#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.', 'In one experiment, for example, 34 students at the University of Brighton were shown six pieces of paper, each of which showed two isosceles and approximately equilateral triangles.', 'Triangles of three sizes were shown, with bases of 5, 8, and 16 mm respectively.', 'On each sheet, one of the two triangles had been circled with a pencil.', 'We asked subjects to imagine themselves on the phone to someone who held a copy of the same sheet, but not necessarily with the same orientation (e.g., possibly upside down), and to complete the answers in the following: Q: Which triangle on this sheet was circled?', 'A: The ............ triangle.']"	0
CC499	J06-2002	Generating Referring Expressions that Involve Gradable Properties	situations and attitudes	"['Jon Barwise', 'John Perry']"	introduction	"In this provocative book, Barwise and Perry tackle the slippery subject of ""meaning, "" a subject that has long vexed linguists, language philosophers, and logicians."	"Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" ( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations ."	"['Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" ( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .']"	1
CC500	J06-2002	Generating Referring Expressions that Involve Gradable Properties	projecting the adjective the syntax and semantics of gradability and comparison	['Christopher Kennedy']	introduction		"Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #AUTHOR_TAG ) ."	"['Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #AUTHOR_TAG ) .', 'The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000;DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large.', 'Our own proposal will abstract away from the effects of linguistic context.', 'We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of']"	0
CC501	J06-2002	Generating Referring Expressions that Involve Gradable Properties	welfare economics and social choice theory	['Allan M Feldman']	experiments	"Preferences and Utility.- Barter Exchange.- Welfare Properties of Market Exchange.- Welfare Properties of ""Jungle"" Exchange.- Economies with Production.- Uncertainty in Exchange.- Externalities.- Public Goods.- Compensation Criteria.- Fairness and the Rawls Criterion.- Life and Death Choices.- Majority Voting.- Arrow's Impossibility Theorem.- Dominant-Strategy Implementation.- Nash Implementation.- Bayesian Implementation.- Epilogue."	"Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that"	"['Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that']"	0
CC502	J06-2002	Generating Referring Expressions that Involve Gradable Properties	ordering among premodifiers	"['James Shaw', 'Vasileios Hatzivassiloglou']"		"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus."	"One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #AUTHOR_TAG ; Malouf 2000 ) ."	"['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #AUTHOR_TAG ; Malouf 2000 ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', 'Quirk et al. (1985), for example, report that ""adjectives denoting size, length, and height normally precede other nonderived adjectives"" (e.g., the small round table is usually preferred to the round small table).', 'Semantically, this does not come as a surprise.', 'In a noun phrase of the form the three small (-est) [N], for example, the words preceding N select the three smallest elements of [N].', 'It follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables.', 'The latter would mean something else, namely, the three round ones among the n small(est) tables (where n is not specified).', ""It actually seems quite possible to say this, but only when some set of small tables is contextually salient (e.g., I don't mean those small tables, I mean the three round ones)."", 'Given that n is unspecified, the noun phrase would tend to be very unclear in any other context.']"	0
CC503	J06-2002	Generating Referring Expressions that Involve Gradable Properties	modern engineering mathematics second edition	"['Glyn James', 'David Burley', 'Dick Clements', 'Phil Dyke', 'John Searl', 'Jerry Wright']"	introduction		"3 The degree of precision of the measurement ( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size ."	"['3 The degree of precision of the measurement ( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .']"	0
CC504	J06-2002	Generating Referring Expressions that Involve Gradable Properties	efficient contextsensitive generation of referring expressions	"['Emiel Krahmer', 'Mari¨et Theune']"		3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11	"Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #AUTHOR_TAG ) ."	"['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #AUTHOR_TAG ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', 'Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm .', 'If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm).', 'Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably.']"	1
CC505	J06-2002	Generating Referring Expressions that Involve Gradable Properties	sorites paradox	['Dominic Hyde']			"This is the strongest version of the sorites paradox ( e.g. , #AUTHOR_TAG ) ."	"['NLG has to do more than select a distinguishing description (i.e., one that unambiguously denotes its referent; Dale 1989): The selected expression should also be felicitous.', 'Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between ""observationally indifferent"" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not?', 'A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox ( e.g. , #AUTHOR_TAG ) .']"	0
CC506	J06-2002	Generating Referring Expressions that Involve Gradable Properties	efficient contextsensitive generation of referring expressions	"['Emiel Krahmer', 'Mari¨et Theune']"	introduction	3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11	"We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #AUTHOR_TAG ) ."	"['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #AUTHOR_TAG ) .', 'Before we do this, consider the tractability of the original IA.', 'If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes.', 'This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002).', 'As for the new algorithm, we focus on the crucial phases 2, 4, and 5.']"	0
CC507	J06-2002	Generating Referring Expressions that Involve Gradable Properties	what do we mean by ‘usually’	['John H Toogood']			"This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) ."	"['Sometimes we are forced to be vague because the information we have (e.g., based on perception or verbal reports) is itself inexact.', 'Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input.', 'We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .', 'We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions.', 'Here, the context tends to obliterate the vagueness associated with the adjective.', ""Suppose you enter a vet's surgery in the company of two dogs: a big one on a leash, and a tiny one in your arms."", 'The vet asks ""Who\'s the patient?"", and you answer ""the big dog.""', 'This answer will allow the vet to pick out the patient just as reliably as if you had said ""the one on the leash""; the fact that big is a vague term is irrelevant.', 'You omit the exact size of the dog, just like some of its other properties (e.g., the leash), because they do not improve the description.', 'This shows how vague properties can contribute to the precise task of identifying a referent.']"	0
CC508	J06-2002	Generating Referring Expressions that Involve Gradable Properties	psychologie der objektbenennung	"['Tony Hermann', 'Roland Deutsch']"	introduction		"#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking ."	"['Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely.', 'Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x.', 'Which of these should come first?', '#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', ""In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say 'the tall candle' when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to 'the fat candle.'"", ""Hermann and Deutsch's findings may be implemented as follows."", 'First, the Values of the different Attributes should be normalized to make them comparable.', 'Second, preference order should be calculated dynamically (i.e., based on the current value of C, and taking the target into account), preferring larger gaps over smaller ones.', '(It is possible, e.g., that width is most suitable for singling out a black cat, but height for singling out a white cat.)', 'The rest of the algorithm remains unchanged.']"	0
CC509	J06-2002	Generating Referring Expressions that Involve Gradable Properties	the effects of redundant communications on listeners when more is less child development	['Susan Sonnenschein']	experiments		"While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."	"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"	0
CC510	J06-2002	Generating Referring Expressions that Involve Gradable Properties	principles of categorization	['Eleanor Rosch']	introduction		"The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #AUTHOR_TAG ) ."	"[""FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #AUTHOR_TAG ) .', 'IA Plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}.']"	0
CC511	J06-2002	Generating Referring Expressions that Involve Gradable Properties	efficient contextsensitive generation of referring expressions	"['Emiel Krahmer', 'Mari¨et Theune']"	experiments	3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11	"#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."	"['We shall see that a natural treatment of salience falls automatically out of our treatment of vague descriptions.', 'As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous.', '9.4.1 A New Perspective on Salience.', ""#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", 'In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.', 'Now suppose we let GRE treat salience just like other gradable Attributes.', 'Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience.', 'Then our algorithm might generate this list of properties: L = mouse, black, salience > 4 .', 'This is a distinguishing description for the black mouse whose salience is 5: the most salient black mouse.', 'The simpler description the black mouse can be derived by stipulating that the property of being most salient can be left implicit in English.', 'The salience Attribute has to be taken into account by CD, however, and this can be ensured in various ways.', 'For example, instead of testing whether C ∩']"	0
CC512	J06-2002	Generating Referring Expressions that Involve Gradable Properties	children’s use of context in interpreting ”big” and ”little” child development	"['K S Ebeling', 'S A Gelman']"			Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #AUTHOR_TAG ) .	"['Gradability is especially widespread in adjectives.', 'A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable.', 'Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #AUTHOR_TAG ) .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible).']"	0
CC513	J06-2002	Generating Referring Expressions that Involve Gradable Properties	fitting words vague language in context linguistics and philosophy	"['Alice Kyburg', 'Michael Morreau']"	introduction		"The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large ."	"['Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.', 'We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of']"	0
CC514	J06-2002	Generating Referring Expressions that Involve Gradable Properties	object reference in a shared domain of conversation pragmatics and cognition	"['Robbert-Jan Beun', 'Anita Cremers']"	introduction		"We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #AUTHOR_TAG , Krahmer and Theune 2002 ) ."	"['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #AUTHOR_TAG , Krahmer and Theune 2002 ) .', 'Before we do this, consider the tractability of the original IA.', 'If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes.', 'This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002).', 'As for the new algorithm, we focus on the crucial phases 2, 4, and 5.']"	0
CC515	J06-2002	Generating Referring Expressions that Involve Gradable Properties	speaking from intention to articulation	['William J M Levelt']	introduction	"In Speaking, Willem ""Pim"" Levelt, Director of the Max-Planck-Institut fur Psycholinguistik, accomplishes the formidable task of covering the entire process of speech production, from constraints on conversational appropriateness to articulation and self-monitoring of speech. Speaking is unique in its balanced coverage of all major aspects of the production of speech, in the completeness of its treatment of the entire speech process, and in its strategy of exemplifying rather than formalizing theoretical issues."	"Hermann and Deutsch ( 1976 ; also reported in #AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking ."	"['Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely.', 'Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x.', 'Which of these should come first?', 'Hermann and Deutsch ( 1976 ; also reported in #AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking .', ""In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say 'the tall candle' when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to 'the fat candle.'"", ""Hermann and Deutsch's findings may be implemented as follows."", 'First, the Values of the different Attributes should be normalized to make them comparable.', 'Second, preference order should be calculated dynamically (i.e., based on the current value of C, and taking the target into account), preferring larger gaps over smaller ones.', '(It is possible, e.g., that width is most suitable for singling out a black cat, but height for singling out a white cat.)', 'The rest of the algorithm remains unchanged.']"	0
CC516	J06-2002	Generating Referring Expressions that Involve Gradable Properties	computational interpretations of the gricean maximes in the generation of referring expressions	"['Robbert Dale', 'Ehud Reiter']"	experiments	"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system."	"The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #AUTHOR_TAG ) ."	"['The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #AUTHOR_TAG ) .', 'But IA may be replaced by any other reasonable GRE algorithm, for example, one that always exactly minimizes the number of properties expressed, or one that always ""greedily"" selects the property that removes the maximum number of distractors.', 'Let G be any such GRE algorithm, then we can proceed as follows:']"	0
CC517	J06-2002	Generating Referring Expressions that Involve Gradable Properties	how to refer with vague descriptions	['Manfred Pinkal']		"This paper deals with the question how reference with vague descriptions should be analysed formally, and gives a proposal for a solution. More precisely, it suggests a formal treatment for the reference identifying function of a special type of vague expressions, as they are used in a special type of definite descriptions. The results of the analysis, however, seem to be of more general importance. The paper consists of:    (i)    some remarks on vagueness and the formal treatment of one type of vagueness;          (ii)    some remarks on definite descriptions and the formal treatment of one kind of descriptions;          (iii)    a short outline of the formal frame which is used to describe both phenomena;          (iv)    an exemplary interpretation of one sentence containing a definite description with a vague predicate within this frame;          (v)    some remarks on the results of the formal analysis concerning reference, vagueness and their mutual relations1."	"Following #AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole ."	"['Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees.', 'This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program.', 'Following #AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective.', 'Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness.', 'Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean.']"	5
CC518	J06-2002	Generating Referring Expressions that Involve Gradable Properties	achieving incremental semantic interpretation through contextual representation	"['Julie Sedivy', 'Michael Tanenhaus', 'Craig Chambers', 'Gregory Carlson']"	experiments	"While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information."	"A similar problem is discussed in the psycholinguistics of interpretation ( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known ."	"['While IA is generally thought to be consistent with findings on human language production (Hermann and Deutsch 1976;Levelt 1989;Pechmann 1989;Sonnenschein 1982), the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates.', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation ( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"	1
CC519	J06-2002	Generating Referring Expressions that Involve Gradable Properties	data structures and algorithms	"['Alfred V Aho', 'John E Hopcroft', 'Jeffrey D Ullman']"	introduction	"Multi-adaptive Galerkin methods are extensions of the standard continuous and discontinuous Galerkin methods for the numerical solution of initial value problems for ordinary or partial differential equations. In particular, the multi-adaptive methods allow individual and adaptive time steps to be used for different components or in different regions of space. We present algorithms for efficient multi-adaptive time-stepping, including the recursive construction of time slabs and adaptive time step selection. We also present data structures for efficient storage and interpolation of the multi-adaptive solution. The efficiency of the proposed algorithms and data structures is demonstrated for a series of benchmark problems.Comment: ACM Transactions on Mathematical Software 35(3), 24 pages (2008"	"Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG ] Chapter 8 ) ."	"['If computing the intersection of two sets takes constant time then this makes the complexity of interpreting non-vague descriptions linear: O(nd), where nd is the number of properties used.', 'In a vague description, the property last added to the description is context dependent.', 'Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG ] Chapter 8 ) .', 'Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions.']"	0
CC520	J06-2002	Generating Referring Expressions that Involve Gradable Properties	logic and conversation	['Paul Grice']	experiments	"Die Studie befasst sich mit dem Unterschied zwischen dem sozialen Alltagsgesprach und der experimentellen Befragungssituation. Es wird angenommen, dass das Alltagsgesprach nach bestimmten Prinzipien ('co-operative' and 'relevance' principles) ablauft, die fur beide Gesprachspartner in verbindlicher Weise einen relevanten, wahrheitsgetreuen und gehaltvollen Informationsaustausch garantieren. Diese Prinzipien werden im experimentellen Gesprach durch den Forscher verletzt. Zwei Experimente, die mit 44 Studenten an der Universitat Heidelberg und 48 Studenten der University of Illinois durchgefuhrt wurden, sollten beweisen, dass Versuchspersonen, denen weder informative noch relevante Inhalte vermittelt wurden, sich bei ihrem Antwortverhalten von den durch Psychologen vermittelten personlichen Informationen und nicht von statistischen Informationen leiten liessen, selbst wenn die statistischen Daten einen hoheren diagnostischen Wert besassen. Dies entspricht ganz den Regeln der sozialen Gesprachsfuhrung (denn nur Psychologen, nicht Computer vermogen sich den Konversationsregeln anzupassen) und kann nur in der Weise interpretiert werden, dass Versuchspersonen der ihnen dargebotenen Information eine Relevanz aus dem sozialen Kontext der Situation heraus zuordnen. (ML)'According to the co-operative principle of social discourse, listeners expect speakers to be relevant, truthful, and informative. The apparent overreliance of individuals on non-diagnostic person information at the expense of base-rate information is shown to be, in part, due to the violation of this principle in experiments on judgmental biases. In these experiments, subjects are presented information that is neither informative nor relevant, in a communicative context that suggests otherwise. Accordingly, subjects relied more on individuating personality information and less on base-rate information when the personality information was presented by a psychologist rather than compiled by a computer, presumably because a human communicator but not a computer is supposed to conform to conversational norms. Moreover, subjects relied more on individuating information when the framing of the task implied that psychologists provided correct estimates than when it implied that statisticians provided correct estimates; and when the individuating rather than the base-rate information was varied as a within subjects factor.' (author's abstract"	Common sense ( as well as the Gricean maxims ; #AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .	"['Common sense ( as well as the Gricean maxims ; #AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .', 'We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', 'Dale and Reiter (1995), for example, discussed the transcripts of a dialogue between people who assemble a piece of garden furniture (originally recorded by Candy Sidner).', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.', 'In one experiment, for example, 34 students at the University of Brighton were shown six pieces of paper, each of which showed two isosceles and approximately equilateral triangles.', 'Triangles of three sizes were shown, with bases of 5, 8, and 16 mm respectively.', 'On each sheet, one of the two triangles had been circled with a pencil.', 'We asked subjects to imagine themselves on the phone to someone who held a copy of the same sheet, but not necessarily with the same orientation (e.g., possibly upside down), and to complete the answers in the following: Q: Which triangle on this sheet was circled?', 'A: The ............ triangle.']"	0
CC521	J06-2002	Generating Referring Expressions that Involve Gradable Properties	two theories about adjectives	['Hans Kamp']	experiments		Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG ) .	"['). Multidimensionality can also slip in through the backdoor.', 'Consider big, for example, when applied to 3D shapes.', 'If there exists a formula for mapping three dimensions into one (e.g., length × width × height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG ) .']"	0
CC522	J06-2002	Generating Referring Expressions that Involve Gradable Properties	the semantics of gradation	['Manfred Bierwisch']	introduction	"The meaning of a complex expression is a function of the lexical meanings of its components and the syntactic structure of the whole. Regularity of semantic composition The meaning of a syntactically regular expression derives from the meanings of its components in a regular way. PoC&gt; The doctrine&gt; Sub-compositionality&gt; Verb gradation&gt; Semantics&gt; Cognition&gt; Life 2 1.2 Syntactic composition and semantic composition Syntactic composition (in terms of constituency or in terms of dependency, or both) follows grammatical rules. * The rules of syntactic composition are in terms of syntactic types (""syntactic categories"") of expressions to be composed, and of the results. * The rules are constrained by principles which, at least partly, may be assumed to apply to syntactic composition only: constraints due to the requirements such as linearization, parsability, syntactic interpretability. (Other constraints, such as economy and faithfulness apply more generally."	"For some adjectives , including the ones that #AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate ."	"['What we said above has also disregarded elements of the ""global"" (i.e., not immediately available) context.', 'For some adjectives , including the ones that #AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.', 'For example (after Bierwisch 1989),']"	0
CC523	J06-2002	Generating Referring Expressions that Involve Gradable Properties	logic and conversation	['Paul Grice']		"Die Studie befasst sich mit dem Unterschied zwischen dem sozialen Alltagsgesprach und der experimentellen Befragungssituation. Es wird angenommen, dass das Alltagsgesprach nach bestimmten Prinzipien ('co-operative' and 'relevance' principles) ablauft, die fur beide Gesprachspartner in verbindlicher Weise einen relevanten, wahrheitsgetreuen und gehaltvollen Informationsaustausch garantieren. Diese Prinzipien werden im experimentellen Gesprach durch den Forscher verletzt. Zwei Experimente, die mit 44 Studenten an der Universitat Heidelberg und 48 Studenten der University of Illinois durchgefuhrt wurden, sollten beweisen, dass Versuchspersonen, denen weder informative noch relevante Inhalte vermittelt wurden, sich bei ihrem Antwortverhalten von den durch Psychologen vermittelten personlichen Informationen und nicht von statistischen Informationen leiten liessen, selbst wenn die statistischen Daten einen hoheren diagnostischen Wert besassen. Dies entspricht ganz den Regeln der sozialen Gesprachsfuhrung (denn nur Psychologen, nicht Computer vermogen sich den Konversationsregeln anzupassen) und kann nur in der Weise interpretiert werden, dass Versuchspersonen der ihnen dargebotenen Information eine Relevanz aus dem sozialen Kontext der Situation heraus zuordnen. (ML)'According to the co-operative principle of social discourse, listeners expect speakers to be relevant, truthful, and informative. The apparent overreliance of individuals on non-diagnostic person information at the expense of base-rate information is shown to be, in part, due to the violation of this principle in experiments on judgmental biases. In these experiments, subjects are presented information that is neither informative nor relevant, in a communicative context that suggests otherwise. Accordingly, subjects relied more on individuating personality information and less on base-rate information when the personality information was presented by a psychologist rather than compiled by a computer, presumably because a human communicator but not a computer is supposed to conform to conversational norms. Moreover, subjects relied more on individuating information when the framing of the task implied that psychologists provided correct estimates than when it implied that statisticians provided correct estimates; and when the individuating rather than the base-rate information was varied as a within subjects factor.' (author's abstract"	"In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #AUTHOR_TAG ) ."	"['Minimality.', 'Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #AUTHOR_TAG ) .']"	0
CC524	J06-2002	Generating Referring Expressions that Involve Gradable Properties	understanding shortcuts in nlg systems	['Chris Mellish']	experiments		"In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #AUTHOR_TAG ) ."	"['If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #AUTHOR_TAG ) .', 'The only practical alternative is to provide the generator with ""crisp"" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database.', 'It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE.', 'Far from being a peculiarity of a few adjectives, vagueness is widespread.', 'We believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms (Section 9.3); r nouns that allow different degrees of strictness (Section 9.5); r degrees of salience (Section 9.4); and r imprecise pointing (Section 9.5).']"	4
CC525	J06-2002	Generating Referring Expressions that Involve Gradable Properties	efficient contextsensitive generation of referring expressions	"['Emiel Krahmer', 'Mari¨et Theune']"	experiments	3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11	"CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #AUTHOR_TAG , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed ."	"['Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed.', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #AUTHOR_TAG , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .', 'Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d2 in the following situation (contains-a=b means that a is contained by b):']"	0
CC526	J06-2002	Generating Referring Expressions that Involve Gradable Properties	incremental speech production and referential overspecification	['Thomas Pechmann']	experiments	"Speech is often produced incrementally: speakers start to articulate an utterance before knowing exactly what they are going to say. The time devoted to articulating first parts of an utterance is simultaneously used to process further information which is to be incorporated into that utterance. Empirical evidence is presented which supports the assumption that referential noun phrases are indeed often produced incrementally. This assumption explains several phenomena: first, speakers' production of overspecified ('redundant') utterances; second, irregularities concerning the ordering ofprenominal adjectives; and third, the way acoustic stress is assigned in referential noun phrases. The basic problem that a speaker faces in what is usually called 'referential communication' is that he has to refer unambiguously to a target object in the context of other objects in such a way that a listener is able to single it out from all relevant alternatives. In order to characterize a target object unambiguously in the context of others, the speaker must determine some appropriate set of distinguishing features, that is, those features that distinguish the target from all relevant alternatives. Consider the simple example depicted in Figure 1. Suppose a referential domain consists of three objects: a black and white triangle and a white circle. Suppose the white circle is the target object on which the speaker wants to focus the listener's attention. In this case the distinguishing feature is the object class. The information 'circle' is sufficient for the listener to know which object is meant by the speaker. In contrast, color would be nondistinguishing information in this case, since color does not help the listener to differentiate the circle from the white triangle. Research on referential communication has primarily been oriented toward developmental questions (for overviews, see Dickson 1981; Glucksberg et al. 1975; Shatz 1978). For most researchers, success or failure in the child's referential-communication tasks has served as an Linguistics 27 (1989), 89-110 0024-3949/89/0027-0089 $2.00 (c) Mouton de Gruyter, Amsterdam"	"While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."	"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"	0
CC527	J06-2002	Generating Referring Expressions that Involve Gradable Properties	games and information an introduction to game theory	['Eric Rasmusen']	experiments	"List of Figures. List of Tables. List of Games. Preface. Contents and Purpose. Changes in the Second Edition (1994). Changes in the Third Edition (2001). Changes in the Fourth Edition (2006). Using the Book. The Level of Mathematics. Other Books. Contact Information. Acknowledgements. Introduction. History. Game Theory's Method. Exemplifying Theory. This Book's Style. Notes. PART 1: GAME THEORY. 1. The Rules of the Game. Definitions. Dominated and Dominant Strategies: The Prisoner's Dilemma. Iterated Dominance: The Battle of the Bismarck Sea. Nash Equilibrium: Boxed Pigs, The Battle of the Sexes and Ranked Coordination. Focal Points. Notes. Problems. Classroom Game. 2. Information. The Strategic and Extensive Forms of a Game. Information Sets. Perfect, Certain, Symmetric, and Complete Information. The Harsanyi Transformation and Bayesian Games. Example: The Png Settlement Game. Notes. Problems. Classroom Game. 3. Mixed and Continuous Strategies. Mixed Strategies: The Welfare Game. The Payoff-equating Method and Games of Timing. Mixed Strategies with General Parameters and N Players: The Civic Duty Game. Randomizing is not Always Mixing: The Auditing Game. Continuous Strategies: The Cournot Game. Continuous Strategies: The Bertrand Game, Strategic Complements, and Strategic. Substitutes. Existence of Equilibrium. Notes. Problems. Classroom Game. 4. Dynamic Games with Symmetric Information. Subgame Perfectness. An Example of Perfectness: Entry Deterrence I. Credible Threats, Sunk Costs, and the Open-Set Problem in the Game of Nuisance Suits. Recoordination to Pareto-dominant Equilibria in Subgames: Pareto Perfection. Notes. Problems. Classroom Game. 5. Reputation and Repeated Games with Symmetric Information. Finitely Repeated Games and the Chainstore Paradox. Infinitely Repeated Games, Minimax Punishments, and the Folk Theorem. Reputation: The One-sided Prisoner's Dilemma. Product Quality in an Infinitely Repeated Game. Markov Equilibria and Overlapping Generations: Customer Switching Costs. Evolutionary Equilibrium: The Hawk-Dove Game. Notes. Problems. Classroom Game. 6. Dynamic Games with Incomplete Information. Perfect Bayesian Equilibrium: Entry Deterrence II and III. Refining Perfect Bayesian Equilibrium in the Entry Deterrence and PhD Admissions Games. The Importance of Common Knowledge: Entry Deterrence IV and V. Incomplete Information in the Repeated Prisoner's Dilemma: The Gang of Four Model. The Axelrod Tournament. Credit and the Age of the Firm: The Diamond Model. Notes. Problems. Classroom Game. PART 2: ASYMMETRIC INFORMATION. 7. Moral Hazard: Hidden Actions. Categories of Asymmetric Information Models. A Principal-agent Model: The Production Game. The Incentive Compatibility and Participation Constraints. Optimal Contracts: The Broadway Game. Notes. Problems. Classroom Game. 8. Further Topics in Moral Hazard. Efficiency Wages. Tournaments. Institutions and Agency Problems. Renegotiation: The Repossession Game. State-space Diagrams: Insurance Games I and II. Joint Production by Many Agents: The Holmstrom Teams Model. The Multitask Agency Problem. Notes. Problems. Classroom Game. 9. Adverse Selection. Introduction: Production Game VI. Adverse Selection under Certainty: Lemons I and II. Heterogeneous Tastes: Lemons III and IV. Adverse Selection under Uncertainty: Insurance Game III. Market Microstructure. A Variety of Applications. Adverse Selection and Moral Hazard Combined: Production Game VII. Notes. Problems. Classroom Game. 10. Mechanism Design and Postcontractual Hidden Knowledge. Mechanisms, Unravelling, Cross Checking, and the Revelation Principle. Myerson Mechanism Design. An Example of Postcontractual Hidden Knowledge: The Salesman Game. The Groves Mechanism. Price Discrimination. Rate-of-return Regulation and Government Procurement. Notes. Problems. Classroom Game. 11. Signalling. The Informed Player Moves First: Signalling. Variants on the Signalling Model of Education. General Comments on Signalling in Education. The Informed Player Moves Second: Screening. Two Signals: The Game of Underpricing New Stock Issues. Signal Jamming and Limit Pricing. Countersignalling. Notes. Problems. Classroom Game. PART 3: APPLICATIONS. 12. Bargaining. The Basic Bargaining Problem: Splitting a Pie. The Nash Bargaining Solution. Alternating Offers over Finite Time. Alternating Offers over Infinite Time. Incomplete Information. Setting Up a Way to Bargain: The Myerson-Satterthwaite Mechanism. Notes. Problems. Classroom Game. 13. Auctions. Values Private and Common, Continuous and Discrete. Optimal Strategies under Different Rules in Private-value Auctions. Revenue Equivalence, Risk Aversion, and Uncertainty. Reserve Prices and the Marginal Revenue Approach. Common-value Auctions and the Winner's Curse. Asymmetric Equilibria, Affiliation, and Linkage: The Wallet Game. Notes. Problems. Classroom Game. 14. Pricing. Quantities as Strategies: Cournot Equilibrium Revisited. Capacity Constraints: The Edgeworth Paradox. Location Models. Comparative Statics and Supermodular Games. Vertical Differentiation. Durable Monopoly. Notes. Problems. Classroom Game. Mathematical Appendix. Notation. The Greek Alphabet. Glossary. Formulas and Functions. Probability Distributions. Supermodularity. Fixed Point Theorems. Genericity. Discounting. Risk. References and Name Index. Subject Index"	"When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #AUTHOR_TAG ) ."	"['Generalizations to complex Boolean descriptions involving negation and disjunction (van Deemter 2004) appear to be largely straightforward, except for issues to do with opposites and markedness.', 'For example, the generator will have to decide whether to say the patients that are old or the patients that are not young.', '9.3 Multidimensionality 9.3.1 Combinations of Adjectives.', 'When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #AUTHOR_TAG ) .', 'Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective.', 'The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r ∈ C has a Pareto-optimal combination of Values V iff there is no other x ∈ C such that 1. ∃V i ∈ V : V i (x) > V i (r) and 2. ¬∃V j ∈ V : V j (x) < V j (r)']"	0
CC528	J06-2002	Generating Referring Expressions that Involve Gradable Properties	speaking from intention to articulation	['William J M Levelt']	experiments	"In Speaking, Willem ""Pim"" Levelt, Director of the Max-Planck-Institut fur Psycholinguistik, accomplishes the formidable task of covering the entire process of speech production, from constraints on conversational appropriateness to articulation and self-monitoring of speech. Speaking is unique in its balanced coverage of all major aspects of the production of speech, in the completeness of its treatment of the entire speech process, and in its strategy of exemplifying rather than formalizing theoretical issues."	"While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."	"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"	0
CC529	J06-2002	Generating Referring Expressions that Involve Gradable Properties	achieving incremental semantic interpretation through contextual representation	"['Julie Sedivy', 'Michael Tanenhaus', 'Craig Chambers', 'Gregory Carlson']"	experiments	"While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information."	"(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #AUTHOR_TAG 1999, discussed in Section 7.2)."	"['We could require that the referent of an evaluative description fall into the correct segment of the relevant dimension.', '(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #AUTHOR_TAG 1999, discussed in Section 7.2).', 'Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary.']"	0
CC530	J06-2002	Generating Referring Expressions that Involve Gradable Properties	cooking up referring expressions	['Robert Dale']		"This paper describes the referring expression generation mechanisms used in EPICURE, a computer program which produces natural language descriptions of cookery recipes. Major features of the system include: an underlying ontology which permits the representation of non-singular entities; a notion of discriminatory power, to determine what properties should be used in a description; and a PATR-like unification grammar to produce surface linguistic strings."	"NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #AUTHOR_TAG ) : The selected expression should also be felicitous ."	"['NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #AUTHOR_TAG ) : The selected expression should also be felicitous .', 'Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between ""observationally indifferent"" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not?', 'A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox (e.g., Hyde 2002).']"	0
CC531	J06-2002	Generating Referring Expressions that Involve Gradable Properties	the order of prenominal adjectives in natural language generation	['Rob Malouf']		The order of prenominal adjectival modifiers in English is governed by complex and difficult to describe constraints which straddle the boundary between competence and performance. This paper describes and compares a number of statistical and machine learning techniques for ordering sequences of adjectives in the context of a natural language generation system.	"One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #AUTHOR_TAG ) ."	"['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #AUTHOR_TAG ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', 'Quirk et al. (1985), for example, report that ""adjectives denoting size, length, and height normally precede other nonderived adjectives"" (e.g., the small round table is usually preferred to the round small table).', 'Semantically, this does not come as a surprise.', 'In a noun phrase of the form the three small (-est) [N], for example, the words preceding N select the three smallest elements of [N].', 'It follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables.', 'The latter would mean something else, namely, the three round ones among the n small(est) tables (where n is not specified).', ""It actually seems quite possible to say this, but only when some set of small tables is contextually salient (e.g., I don't mean those small tables, I mean the three round ones)."", 'Given that n is unspecified, the noun phrase would tend to be very unclear in any other context.']"	0
CC532	J06-2002	Generating Referring Expressions that Involve Gradable Properties	computational interpretations of the gricean maximes in the generation of referring expressions	"['Robbert Dale', 'Ehud Reiter']"	introduction	"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system."	"4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #AUTHOR_TAG ) ."	"['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #AUTHOR_TAG ) .', 'VAGUE uses both of these devices.']"	0
CC533	J06-2002	Generating Referring Expressions that Involve Gradable Properties	understanding complex visually referring utterances	"['Peter Gorniak', 'Deb Roy']"	experiments	"We propose a computational model of visually-grounded spatial language understanding, based on a study of how people verbally describe objects in visual scenes. We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework. The implemented system selects the correct referents in response to a broad range of referring expressions for a large percentage of test cases. In an analysis of the system's successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account."	"The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #AUTHOR_TAG ; Thorisson 1994 , for other plans ) ."	"['In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe.', 'It seems likely, however, that people use doubly graded descriptions more liberally.', 'For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe.', 'Many alternative strategies are possible.', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #AUTHOR_TAG ; Thorisson 1994 , for other plans ) .']"	0
CC534	J06-2002	Generating Referring Expressions that Involve Gradable Properties	interpreting vague utterances in context	"['David DeVault', 'Matthew Stone']"	introduction	"We use the interpretation of vague scalar predicates  like small as an illustration of how systematic  semantic models of dialogue context enable  the derivation of useful, fine-grained utterance  interpretations from radically underspecified  semantic forms. Because dialogue context  suffices to determine salient alternative scales  and relevant distinctions along these scales,  we can infer implicit standards of comparison  for vague scalar predicates through completely  general pragmatics, yet closely constrain the intended  meaning to within a natural range"	"The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large ."	"['Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.', 'We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of']"	0
CC535	J06-2002	Generating Referring Expressions that Involve Gradable Properties	computational interpretations of the gricean maximes in the generation of referring expressions	"['Robbert Dale', 'Ehud Reiter']"	introduction	"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system."	"Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #AUTHOR_TAG ) ."	"['The representation of inequalities is not entirely trivial.', 'For one thing, it is convenient to view properties of the form size(x) < α as belonging to a different Attribute than those of the form size(x) > α, because this causes the Values of an Attribute to be linearly ordered: Being larger than 12 cm implies being larger than 10 cm, and so on.', 'More importantly, it will now become normal for an object to have many Values for the same Attribute; c 4 , for example, has the Values > 6 cm, > 10 cm, and > 12 cm.', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #AUTHOR_TAG ) .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen."", 'Thus, size(x) > m is preferred over size(x) > n iff m > n; conversely, size(x) < m is preferred over size(x) < n iff m < n.)', 'This is reflected by the order in which the properties are listed above: Once a sizerelated property is selected, later size-related properties do not remove any distractors and will therefore not be included in the description.']"	0
CC536	J06-2002	Generating Referring Expressions that Involve Gradable Properties	generating referring expressions containing relations	"['Robbert Dale', 'Nickolas Haddock']"	experiments	"Recent work on the Generation of Referring Expressions has increased the generating capability of algorithms in this area. This paper asks whether the models underlying these proposals can still be used if even more complex referring expressions are generated. To discuss this issue, we will investigate a variety of referring expressions that pose difficulties to current generation algorithms. In particular, we will discuss the difficulties associated with quantified referring expressions (such as 'those women who have fewer than two children', or 'the people who work for exactly 2 employers') and explain how they can be generated by extending the inference-based approach described in (Varges, 2004)."	"For example , consider a relational description ( cfXXX , #AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed ."	"['Some generalizations of our method are fairly straightforward.', 'For example , consider a relational description ( cfXXX , #AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed .', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and Theune 2002, Section 8.6.2):', 'Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed.', ""Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d 2 in the following situation (contains-a=b means that a is contained by b): In other words, the dog in the large shed denotes 'the dog such that there is no other shed that is equally large or larger and that contains a dog'."", 'Note that it would be odd, in the above-sketched situation, to say the dog in the largest shed.']"	0
CC537	J06-2002	Generating Referring Expressions that Involve Gradable Properties	the bargaining problem	['John Nash']	experiments	"AbstractConsider the problem of partitioning n items among d players where the utility of each player for bundles of items is additive; so, player r has utility vri for item i and the utility of that player for a bundle of items is the sum of the vir's over the items i in his/her bundle. Each partition S of the items is then associated with a d-dimensional utility vector VS whose coordinates are the utilities that the players assign to the bundles they get under S. Also, lotteries over partitions are associated with the corresponding expected utility vectors. We model the problem as a Nash bargaining game over the set of lotteries over partitions and provide methods for computing the corresponding Nash solution, to prescribed accuracy, with effort that is polynomial in n. In particular, we show that points in the pareto-optimal set of the corresponding bargaining set correspond to lotteries over partitions under which each item, with the possible exception of at most d(d-1)/2 items, is assigned in the same way"	"The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) ."	"['In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe.', 'It seems likely, however, that people use doubly graded descriptions more liberally.', 'For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe.', 'Many alternative strategies are possible.', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .']"	0
CC538	J06-2002	Generating Referring Expressions that Involve Gradable Properties	building natural language generation systems	"['Ehud Reiter', 'Robert Dale']"	experiments	"This book explains how to build Natural Language Generation (NLG) systems - computer software systems which use techniques from artificial intelligence and computational linguistics to automatically generate understandable texts in English or other human languages, either in isolation or as part of multimedia documents, Web pages, and speech output systems"	"The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #AUTHOR_TAG ) ."	"['The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #AUTHOR_TAG ) .', 'An example of such an inference rule is the one that transforms a list of the form mouse, >10 cm into one of the form mouse, size(x) = max 2 if only two mice are larger than 10 cm.', 'The same issues also make it difficult to interleave CD and linguistic realization as proposed by various authors, because properties may need to be combined before they are expressed.']"	0
CC539	J06-2002	Generating Referring Expressions that Involve Gradable Properties	should corpora texts be gold standards for nlg	"['Ehud Reiter', 'Somayajulu Sripada']"			"A more flexible approach is used by #AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on ."	"['Some NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).', 'FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead.', 'A more flexible approach is used by #AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .', 'A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997).', 'To determine, for example, whether one of Mozart�s piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number y of CD recordings of each of Mozart�s sonatas.', 'The sonata was called a famous sonata if x >> y. Like DYD, the work reported in this article will abandon the use of fixed boundary values for gradable adjectives, letting these values depend on the context in which the adjective is used.']"	0
CC540	J06-2002	Generating Referring Expressions that Involve Gradable Properties	textual economy through close coupling of syntax and semantics	"['Matthew Stone', 'Bonnie Webber']"		"We focus on the production of efficient descriptions of objects, actions and events. We define a type of  efficiency, textual economy, that exploits the hearer&apos;s recognition of inferential links to material elsewhere  within a sentence. Textual economy leads to efficient descriptions because the material that supports such  inferences has been included to satisfy independent communicative goals, and is therefore overloaded  in the sense of Pollack [18]. We argue that achieving textual economy imposes strong requirements  on the representation and reasoning used in generating sentences. The representation must support the  generator&apos;s simultaneous consideration of syntax and semantics. Reasoningmust enable the generator  to assess quickly and reliably at any stage how the hearer will interpret the current sentence, with its  &apos;-(inc6mplete)syntax and&apos;semantics. We show that these representational and reasoning requirements are  met in the SPUD system for sentence planning and realization"	"Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG ; Krahmer and Theune 2002 ) ."	"['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', 'Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm .', 'If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm).', 'Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably.']"	1
CC541	J06-2002	Generating Referring Expressions that Involve Gradable Properties	psychologie der objektbenennung	"['Tony Hermann', 'Roland Deutsch']"	experiments		"While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."	"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"	0
CC542	J06-2002	Generating Referring Expressions that Involve Gradable Properties	basic color terms	"['Brent Berlin', 'Paul Kay']"	experiments		"A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) ."	"['Color terms are a case apart.', 'If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue).', 'This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3).', '(The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green.)', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .', '(Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.', 'Red as in red hair, e.g., differs from red as in red chair.)', 'Different attitudes towards multidimensionality are possible.', 'One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense.', 'In this case, the program should limit the use of vague descriptions to situations where there exists a referent that has a Pareto-optimal combination of Values.', 'Alternatively, one could allow referring expressions to be ambiguous.', 'It would be consistent with this attitude, for example, to map multiple dimensions into one overall dimension, perhaps by borrowing from principles applied in perceptual grouping, where different perceptual dimensions are mapped into one (e.g., Thorisson 1994).', 'The empirical basis of this line of work, however, is still somewhat weak, so the risk of referential unclarity looms large.', 'Also, this attitude would go against the spirit of GRE, where referring expressions have always been assumed to be distinguishing.']"	0
CC543	J06-2002	Generating Referring Expressions that Involve Gradable Properties	achieving incremental semantic interpretation through contextual representation	"['Julie Sedivy', 'Michael Tanenhaus', 'Craig Chambers', 'Gregory Carlson']"	experiments	"While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information."	#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .	"['It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', 'Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key).', ""Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied 'intrinsically' to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations."", 'The time subjects took before looking at the target for the first time was measured, and although these latency times were somewhat greater when the referent were not intrinsically tall than when they were, the average difference was tiny at 554 versus 538 miliseconds.', 'Since latency times are thought to be sensitive to most of the problems that hearers may have in processing a text, these results suggest that, for dimensional adjectives, it is forgivable to disregard global context.']"	0
CC544	J06-2002	Generating Referring Expressions that Involve Gradable Properties	efficient contextsensitive generation of referring expressions	"['Emiel Krahmer', 'Mari¨et Theune']"	experiments	3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11	"It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) ."	"['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).', 'Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking.', 'Let us see how things would work if they were ranked more highly.']"	1
CC545	J06-2002	Generating Referring Expressions that Involve Gradable Properties	two theories about adjectives	['Hans Kamp']	introduction		"2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) ."	"['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']"	0
CC546	J06-2002	Generating Referring Expressions that Involve Gradable Properties	situations and attitudes	"['Jon Barwise', 'John Perry']"	introduction	"In this provocative book, Barwise and Perry tackle the slippery subject of ""meaning, "" a subject that has long vexed linguists, language philosophers, and logicians."	"In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) ."	"['IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', 'In this way, IA could never describe the same animal as the large chihuahua and the small brown dog, for example.', 'This approach does not do justice to gradable adjectives, whether they are used in the base form, the superlative, or the comparative.', 'Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it.', 'Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"	0
CC547	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	the wellbuilt clinical question a key to evidencebased decisions	"['W Scott Richardson', 'Mark C Wilson', 'James Nishikawa', 'Robert S Hayward']"			The following four components have been identified as the key elements of a question related to patient care ( #AUTHOR_TAG ) :	"['The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care ( #AUTHOR_TAG ) :']"	0
CC548	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	modern information retrieval	"['Ricardo Baeza-Yates', 'Berthier Ribeiro-Neto']"		"Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships"	Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ).	"['Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ).', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.']"	5
CC549	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	formulating the question	['Andrew Booth']	related work	To know something -- as the first part of our inquiry showed -- is to designate facts by means of judgments in such a way as to obtain a unique correlation while using the smallest possible number of concepts.	"Although originally developed as a tool to assist in query formulation , #AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision ."	"['The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new.', 'Based on analyses of 4,000 MEDLINE citations, Mendonça and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology.', 'The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al. 1994;Wilczynski, McKibbon, and Haynes 2001).', 'Cimino and Mendonça reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis.', 'Although originally developed as a tool to assist in query formulation , #AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', 'PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989).', 'The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision.']"	0
CC550	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	a comparative study on feature selection in text categorization	"['Yiming Yang', 'Jan O Pedersen']"		This paper is a comparative study of feature selection methods in statistical learning of text categorization The focus is on aggres sive dimensionality reduction Five meth ods were evaluated including term selection based on document frequency DF informa tion gain IG mutual information MI a test CHI and term strength TS We found IG and CHI most e ective in our ex periments Using IG thresholding with a k nearest neighbor classi er on the Reuters cor pus removal of up to removal of unique terms actually yielded an improved classi cation accuracy measured by average preci sion DF thresholding performed similarly Indeed we found strong correlations between the DF IG and CHI values of a term This suggests that DF thresholding the simplest method with the lowest cost in computation can be reliably used instead of IG or CHI when the computation of these measures are too expensive TS compares favorably with the other methods with up to vocabulary reduction but is not competitive at higher vo cabulary reduction levels In contrast MI had relatively poor performance due to its bias towards favoring rare terms and its sen sitivity to probability estimation errors	"We first identified the most informative unigrams and bigrams using the information gain measure ( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) ."	"['The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features.', 'We first identified the most informative unigrams and bigrams using the information gain measure ( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', 'Diseasespecific terms, such as rheumatoid arthritis, were then manually removed.', 'Finally, the list of features was revised by the registered nurse who participated in the annotation effort.', 'This classifier also outputs the probability of a class assignment.']"	5
CC551	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	customization in a unified framework for summarizing medical literature	"['Noemie Elhadad', 'Min-Yen Kan', 'Judith Klavans', 'Kathleen McKeown']"	related work	"Objective: We present the summarization system in the PErsonalized Retrieval and Summarization of Images, Video and Language (PERSIVAL) medical digital library. Although we discuss the context of our summarization research within the PERSIVAL platform, the primary focus of this article is on strategies to define and generate customized summaries. Methods and material: Our summarizer employs a unified user model to create a tailored summary of relevant documents for either a physician or lay person. The approach takes advantage of regularities in medical literature text structure and content to fulfill identified user needs. Results: The resulting summaries combine both machine-generated text and extracted text that comes from multiple input documents. Customization includes both group-based modeling for two classes of users, physician and lay person, and individually driven models based on a patient record. Conclusions: Our research shows that customization is feasible in a medical digital library"	"The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #AUTHOR_TAG ) ."	"['In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #AUTHOR_TAG ) ."", 'Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine.', 'Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project.']"	1
CC552	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	generative content models for structural analysis of medical abstracts	"['Jimmy Lin', 'Damianos Karakos', 'Dina Demner-Fushman', 'Sanjeev Khudanpur']"	related work	"The ability to accurately model the content structure of text is important for many natural language processing applications. This paper describes experiments with generative models for analyzing the discourse structure of medical abstracts, which generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"". We demonstrate that Hidden Markov Models are capable of accurately capturing the structure of such texts, and can achieve classification accuracy comparable to that of discriminative techniques. In addition, generative approaches provide advantages that may make them preferable to discriminative techniques such as Support Vector Machines under certain conditions. Our work makes two contributions: at the application level, we report good performance on an interesting task in an important domain; more generally, our results contribute to an ongoing discussion regarding the tradeoffs between generative and discriminative techniques."	"For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #AUTHOR_TAG ) ."	"['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #AUTHOR_TAG ) .', 'Tbahriti et al. (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance.', 'Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering.', 'In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering.']"	0
CC553	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	automatically evaluating answers to definition questions	"['Jimmy Lin', 'Dina Demner-Fushman']"		"Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called Pourpre, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system's response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that Pourpre outperforms direct application of existing metrics."	"We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAGb ) , but these features are also beyond the capabilities of current summarization systems ."	"['The most important characteristic of answers, as recommended by Ely et al. (2005) in their study of real-world physicians, is that they focus on bottom-line clinical advice-information that physicians can directly act on.', 'Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences.', 'The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion-it need not be repeated unless the physician wishes to ""drill down""; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations.', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAGb ) , but these features are also beyond the capabilities of current summarization systems .']"	1
CC554	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	using argumentation to retrieve articles with similar citations an inquiry into improving related articles search in the medline digital library	"['Imad Tbahriti', 'Christine Chichester', 'Fr´ed´erique Lisacek', 'Patrick Ruch']"	related work	"The aim of this study is to investigate the relationships between citations and the scientific argumentation found abstracts. We design a related article search task and observe how the argumentation can affect the search results. We extracted citation lists from a set of 3200 full-text papers originating from a narrow domain. In parallel, we recovered the corresponding MEDLINE records for analysis of the argumentative moves. Our argumentative model is founded on four classes: PURPOSE, METHODS, RESULTS and CONCLUSION. A Bayesian classifier trained on explicitly structured MEDLINE abstracts generates these argumentative categories. The categories are used to generate four different argumentative indexes. A fifth index contains the complete abstract, together with the title and the list of Medical Subject Headings (MeSH) terms. To appraise the relationship of the moves to the citations, the citation lists were used as the criteria for determining relatedness of articles, establishing a benchmark; it means that two articles are considered as ""related"" if they share a significant set of co-citations. Our results show that the average precision of queries with the PURPOSE and CONCLUSION features is the highest, while the precision of the RESULTS and METHODS features was relatively low. A linear weighting combination of the moves is proposed, which significantly improves retrieval of related articles."	#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .	"['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al. 2006).', '#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', 'Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering.', 'In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering.']"	0
CC555	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	answering clinical questions	"['M Lee Chambliss', 'Jennifer Conley']"	introduction	"The combination of recent developments in question-answering research and the availability of unparalleled resources developed specifically for automatic semantic processing of text in the medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine. This article presents a system designed to satisfy the information needs of physicians practicing evidence-based medicine. We have developed a series of knowledge extractors, which employ a combination of knowledge-based and statistical techniques, for automatically identifying clinically relevant aspects of MEDLINE abstracts. These extracted elements serve as the input to an algorithm that scores the relevance of citations with respect to structured representations of information needs, in accordance with the principles of evidencebased medicine. Starting with an initial list of citations retrieved by PubMed, our system can bring relevant abstracts into higher ranking positions, and from these abstracts generate responses that directly answer physicians ' questions. We describe three separate evaluations: one focused on the accuracy of the knowledge extractors, one conceptualized as a document reranking task, and finally, an evaluation of answers by two physicians. Experiments on a collection of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline. 1"	"However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG ) ."	"['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians' questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003)."", 'However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG ) .', 'Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']"	0
CC556	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	the nlm indexing initiative’s medical text indexer	"['Alan R Aronson', 'James G Mork', 'Clifford W Gay', 'Susanne M Humphrey', 'Willie J Rogers']"			"Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #AUTHOR_TAG ) ."	"['Additional metadata are associated with each MEDLINE citation.', 'The most important of these is the controlled vocabulary terms assigned by human indexers.', ""NLM's controlled vocabulary thesaurus, Medical Subject Headings (MeSH), 2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a separate thesaurus."", ""Indexing is performed by approximately 100 indexers with at least bachelor's degrees in life sciences and formal training in indexing provided by NLM."", 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #AUTHOR_TAG ) .', 'Nevertheless, the indexing process remains firmly human-centered.']"	0
CC557	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	what makes a good answer the role of context in question answering	"['Jimmy Lin', 'Dennis Quan', 'Vineet Sinha', 'Karun Bakshi', 'David Huynh', 'Boris Katz', 'David R Karger']"	conclusion	"Question answering systems have proven to be helpful to users because they can provide succinct answers that do not require users to wade through a large number of documents. However, despite recent advances in the underlying question answering technology, the problem of designing effective interfaces has been largely unexplored. We conducted a user study to investigate this area and discovered that, overall, users prefer paragraph-sized chunks of text over just an exact phrase as the answer to their questions. Furthermore, users generally prefer answers embedded in context, regardless of the perceived reliability of the source documents. When researching a topic, increasing the amount of text returned to users significantly decreases the number of queries that they pose to the system, suggesting that users utilize supporting text to answer related questions. We believe that these results can serve to guide future developments in question answering interfaces."	"Previously , a user study ( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance ."	"['The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface.', 'Previously , a user study ( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .', 'We have argued that a query interface based on structured PICO frames will yield better-formulated queries, although it is unclear whether physicians would invest the upfront effort necessary to accomplish this.', 'Issuing extremely short queries appears to be an ingrained habit of information seekers today, and the dominance of World Wide Web searches reinforce this behavior.', 'Given these trends, physicians may actually prefer the rapid back-and-forth interaction style that comes with short queries.', 'We believe that if systems can produce noticeably better results with richer queries, users will make more of an effort to formulate them.', 'This, however, presents a chicken-and-egg problem: One possible solution is to develop models that can automatically fill query frames given a couple of keywords-this would serve to kick-start the query generation process.']"	1
CC558	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	towards a medical questionanswering system a feasibility study	"['Pierre Jacquemart', 'Pierre Zweigenbaum']"	related work		"The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 ."	"['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']"	0
CC559	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	the interaction of domain knowledge and linguistic structure in natural language processing interpreting hypernymic propositions in biomedical text	"['Thomas C Rindflesch', 'Marcelo Fiszman']"	introduction	"Interpretation of semantic propositions in free-text documents such as MEDLINE citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. In this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. In order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the Unified Medical Language System (UMLS). After introducing the syntactic processing on which our system depends, we focus on the UMLS knowledge that supports interpretation of hypernymic propositions. We first use semantic groups from the Semantic Network to ensure that the two concepts involved are compatible; hierarchical information in the Metathesaurus then determines which concept is more general and which more specific. A preliminary evaluation of a sample based on the semantic group Chemicals and Drugs provides 83% precision. An error analysis was conducted and potential solutions to the problems encountered are presented. The research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. Additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. The approach has the potential to support a range of applications, including information retrieval and ontology engineering."	"Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG ) extracts relations between the concepts ."	"['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']"	0
CC560	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	issues in stacked generalization	"['Kai Ming Ting', 'Ian H Witten']"		"Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a 'black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones.    We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging."	"The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #AUTHOR_TAG ) , which can be described by the following equation:"	"['We attempted two approaches for assigning these weights. The first method relied on ad hoc weight selection based on intuition.', 'The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #AUTHOR_TAG ) , which can be described by the following equation:']"	5
CC561	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	analysis of semantic classes in medical text for question answering	"['Yun Niu', 'Graeme Hirst']"	related work	"To answer questions from clinical-evidence texts, we identify occurrences of the semantic classes -- disease, medication, patient outcome -- that are candidate elements of the answer, and the relations among them. Additionally, we determine whether an outcome is positive or negative."	The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #AUTHOR_TAG .	"['The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #AUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope).', 'Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general).', 'Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach.']"	1
CC562	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	effective mapping of biomedical text to the umls metathesaurus the metamap program	['Alan R Aronson']	introduction	"The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library"	"Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts ."	"['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']"	0
CC563	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	improving fulltext precision on short queries using simple constraints	['Marti A Hearst']	related work	"We show that two simple constraints, when applied to short user queries (on the order of 5{10 words) can yield precision scores comparable to or better than those achieved using long queries (50{85 words) at low document cuto levels. These constraints are meant to detect documents that have subtopic passages that includes the most important components of the query. The constraints are: (i) a simple Boolean constraint which requires the user to specify the query as a list of topics; this list is converted into a conjunct of disjuncts by the system, and (ii) a subtopic-sized proximity constraint imposed over the Boolean constraint. The vector space model is used to rank the documents that satisfy both constraints. Experiments run over 45 TREC queries show signi cant, almost consistent improvements over rankings that use no constraints. These results have important rami cations for interactive systems intended for casual users, such as those searching on the World Wide Web."	The work of #AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .	"['The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new.', 'Based on analyses of 4,000 MEDLINE citations, Mendonça and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology.', 'The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al. 1994;Wilczynski, McKibbon, and Haynes 2001).', 'Cimino and Mendonça reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis.', 'Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision.', 'PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989).', 'The work of #AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .']"	0
CC564	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	automatically identifying health outcome information in medline records	"['Dina Demner-Fushman', 'Barbara Few', 'Susan E Hauser', 'George Thoma']"		"Objective: Understanding the effect of a given intervention on the patient's health outcome is one of the key elements in providing optimal patient care. This study presents a methodology for automatic identification of outcomes-related information in medical text and evaluates its potential in satisfying clinical information needs related to health care outcomes. Design: An annotation scheme based on an evidence-based medicine model for critical appraisal of evidence was developed and used to annotate 633 MEDLINE citations. Textual, structural, and meta-information features essential to outcome identification were learned from the created collection and used to develop an automatic system. Accuracy of automatic outcome identification was assessed in an intrinsic evaluation and in an extrinsic evaluation, in which ranking of MEDLINE search results obtained using PubMed Clinical Queries relied on identified outcome statements. Measurements: The accuracy and positive predictive value of outcome identification were calculated. Effectiveness of the outcome-based ranking was measured using mean average precision and precision at rank 10. Results: Automatic outcome identification achieved 88% to 93% accuracy. The positive predictive value of individual sentences identified as outcomes ranged from 30% to 37%. Outcome-based ranking improved retrieval accuracy, tripling mean average precision and achieving 389% improvement in precision at rank 10. Conclusion: Preliminary results in outcome-based document ranking show potential validity of the evidence-based medicine-model approach in timely delivery of information critical to clinical decision support at the point of service"	"The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG ) ."	"['The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG ) .', 'As will be described herein, the population, problem, and the intervention extractors are based largely on recognition of semantic types and a few manually constructed rules; the outcome extrac-tor, in contrast, is implemented as an ensemble of classifiers trained using supervised machine learning techniques ).', 'These two very different approaches can be attributed to differences in the nature of the frame elements: Whereas problems and interventions can be directly mapped to UMLS concepts, and populations easily mapped to patterns that include UMLS concepts, outcome statements follow no predictable pattern.', 'The initial goal of the annotation effort was to identify outcome statements in abstract text.', 'A physician, two registered nurses, and an engineering researcher manually identified sentences that describe outcomes in 633 MEDLINE abstracts; a post hoc analysis demonstrates good agreement (κ = 0.77).', 'The annotated abstracts were retrieved using PubMed and attempted to model different user behaviors ranging from naive to expert (where advanced search features were employed).', 'With the exception of 50 citations retrieved to answer a question about childhood immunization, the rest of the results were retrieved by querying on a disease, for example, diabetes.', 'Of the 633 citations, 100 abstracts were also fully annotated with population, problems, and interventions.', 'These 100 abstracts were set aside as a held-out test set.', 'Of the remaining citations, 275 were used for training and rule derivation, as described in the following sections.']"	5
CC565	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	analysis of questions asked by family doctors regarding patient care	"['John W Ely', 'Jerome A Osheroff', 'Mark H Ebell', 'George R Bergus', 'Barcey T Levy', 'M Lee Chambliss', 'Eric R Evans']"	introduction	"Abstract Objectives: To characterise the information needs of family doctors by collecting the questions they asked about patient care during consultations and to classify these in ways that would be useful to developers of knowledge bases. Design: Observational study in which investigators visited doctors for two half days and collected their questions. Taxonomies were developed to characterise the clinical topic and generic type of information sought for each question. Setting: Eastern Iowa. Participants: Random sample of 103 family doctors. Main outcome measures: Number of questions posed, pursued, and answered; topic and generic type of information sought for each question; time spent pursuing answers; information resources used. Results: Participants asked a total of 1101 questions. Questions about drug prescribing, obstetrics and gynaecology, and adult infectious disease were most common and comprised 36% of all questions. The taxonomy of generic questions included 69 categories; the three most common types, comprising 24% of all questions, were ""What is the cause of symptom X?"" ""What is the dose of drug X?"" and ""How should I manage disease or finding X?"" Answers to most questions (702, 64%) were not immediately pursued, but, of those pursued, most (318, 80%) were answered Doctors spent an average of less than 2 minutes pursuing an answer, and they used readily available print and human resources Only two questions led to a formal literature search. Conclusions: Family doctors in this study did not pursue answers to most of their questions. Questions about patient care can be organised into a limited number of generic types, which could help guide the efforts of knowledge base developers. Key messages Questions that doctors have about the care of their patients could help guide the content of medical information sources and medical training In this study of US family doctors, participants frequently had questions about patient care but did not pursue answers to most questions (64%) On average, participants spent less than 2 minutes seeking an answer to a question The most common resources used to answer questions included textbooks and colleagues; formal literature searches were rarely performed The most common generic questions were ""What is the cause of symptom X?"" ""What is the dose of drug X?"" and ""How should I manage disease or finding X?"""	"Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG , 2005 ) ."	"['Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG , 2005 ) .', ""MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians' questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003)."", 'However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).', 'Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']"	0
CC566	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	answer extraction semantic clustering and extractive summarization for clinical question answering	"['Dina Demner-Fushman', 'Jimmy Lin']"		"This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval. We tackle a frequently-occurring class of questions that takes the form ""What is the best drug treatment for X?"" Starting from an initial set of MEDLINE citations, our system first identifies the drugs under study. Abstracts are then clustered using semantic classes from the UMLS ontology. Finally, a short extractive summary is generated for each abstract to populate the clusters. Two evaluations---a manual one focused on short answers and an automatic one focused on the supporting abstract---demonstrate that our system compares favorably to PubMed, the search system most widely used by physicians today."	"We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #AUTHOR_TAG and Lin (2006)."	"['Finally, answer generation remains an area that awaits further exploration, although we would have to first define what a good answer should be.', 'We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #AUTHOR_TAG and Lin (2006).', 'Furthermore, the current answer generator does not handle complex issues such as contradictory and inconsistent statements.', 'To address these very difficult challenges, finer-grained semantic analysis of medical texts is required.']"	0
CC567	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	automatically identifying health outcome information in medline records	"['Dina Demner-Fushman', 'Barbara Few', 'Susan E Hauser', 'George Thoma']"		"Objective: Understanding the effect of a given intervention on the patient's health outcome is one of the key elements in providing optimal patient care. This study presents a methodology for automatic identification of outcomes-related information in medical text and evaluates its potential in satisfying clinical information needs related to health care outcomes. Design: An annotation scheme based on an evidence-based medicine model for critical appraisal of evidence was developed and used to annotate 633 MEDLINE citations. Textual, structural, and meta-information features essential to outcome identification were learned from the created collection and used to develop an automatic system. Accuracy of automatic outcome identification was assessed in an intrinsic evaluation and in an extrinsic evaluation, in which ranking of MEDLINE search results obtained using PubMed Clinical Queries relied on identified outcome statements. Measurements: The accuracy and positive predictive value of outcome identification were calculated. Effectiveness of the outcome-based ranking was measured using mean average precision and precision at rank 10. Results: Automatic outcome identification achieved 88% to 93% accuracy. The positive predictive value of individual sentences identified as outcomes ranged from 30% to 37%. Outcome-based ranking improved retrieval accuracy, tripling mean average precision and achieving 389% improvement in precision at rank 10. Conclusion: Preliminary results in outcome-based document ranking show potential validity of the evidence-based medicine-model approach in timely delivery of information critical to clinical decision support at the point of service"	"After much exploration , #AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues ."	"['After much exploration , #AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .', 'Consider the following segment:']"	0
CC568	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	feature selection for unbalanced class distribution and naive bayes	"['Dunja Mladenic', 'Marko Grobelnik']"		"This paper describes an approach to feature subset selection that takes into account problem speciics and learning algorithm characteristics. It is developed for the Naive Bayesian classiier applied on text data, since it combines well with the addressed learning problems. We focus on domains with many features that also have a highly unbalanced class distribution and asymmetric misclassii-cation costs given only implicitly in the problem. By asymmetric misclassiication costs we mean that one of the class values is the target class value for which we want to get predictions and we prefer false positive over false negative. Our example problem is automatic document categorization using machine learning, where we want to identify documents relevant for the selected category. Usually, only about 1%-10% of examples belong to the selected category. Our experimental comparison of eleven feature scoring measures show that considering domain and algorithm characteristics signiicantly improves the results of classiication."	"We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #AUTHOR_TAG ) ."	"['The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features.', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #AUTHOR_TAG ) .', 'Diseasespecific terms, such as rheumatoid arthritis, were then manually removed.', 'Finally, the list of features was revised by the registered nurse who participated in the annotation effort.', 'This classifier also outputs the probability of a class assignment.']"	5
CC569	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	answer extraction semantic clustering and extractive summarization for clinical question answering	"['Dina Demner-Fushman', 'Jimmy Lin']"		"This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval. We tackle a frequently-occurring class of questions that takes the form ""What is the best drug treatment for X?"" Starting from an initial set of MEDLINE citations, our system first identifies the drugs under study. Abstracts are then clustered using semantic classes from the UMLS ontology. Finally, a short extractive summary is generated for each abstract to populate the clusters. Two evaluations---a manual one focused on short answers and an automatic one focused on the supporting abstract---demonstrate that our system compares favorably to PubMed, the search system most widely used by physicians today."	"Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #AUTHOR_TAG ."	"['It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies.', 'For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objectives, and results; the closest analogous task in computational linguistics-redundancy detection for multi-document summarization-seems easy by comparison.', 'Furthermore, it is unclear if textual strings make ""good answers.""', 'Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004).', 'Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #AUTHOR_TAG .']"	3
CC570	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	overview of the trec	['Ellen M Voorhees']	related work		"Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( #AUTHOR_TAG ) ."	"['Finally, the evaluation of answers to complex questions remains an open research problem.', 'Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems.', 'In Sections 9 and 10, we have discussed many of these issues.', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( #AUTHOR_TAG ) ."", 'A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b).', 'However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken.']"	0
CC571	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	the role of knowledge in conceptual retrieval a study in the domain of clinical medicine	"['Jimmy Lin', 'Dina Demner-Fushman']"	related work	"Despite its intuitive appeal, the hypothesis that retrieval at the level of ""concepts"" should outperform purely term-based approaches remains unverified empirically. In addition, the use of ""knowledge"" has not consistently resulted in performance gains. After identifying possible reasons for previous negative results, we present a novel framework for ""conceptual retrieval"" that articulates the types of knowledge that are important for information seeking. We instantiate this general framework in the domain of clinical medicine based on the principles of evidence-based medicine (EBM). Experiments show that an EBM-based scoring algorithm dramatically outperforms a state-of-the-art baseline that employs only term statistics. Ablation studies further yield a better understanding of the performance contributions of different components. Finally, we discuss how other domains can benefit from knowledge-based approaches."	"In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #AUTHOR_TAGa ) for a brief overview ."	"['Clinical question answering is an emerging area of research that has only recently begun to receive serious attention.', 'As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated.', 'In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems.', 'For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999).', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #AUTHOR_TAGa ) for a brief overview .']"	0
CC572	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	automatically evaluating answers to definition questions	"['Jimmy Lin', 'Dina Demner-Fushman']"	related work	"Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called Pourpre, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system's response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that Pourpre outperforms direct application of existing metrics."	"A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #AUTHOR_TAGa , 2006b ) ."	"['Finally, the evaluation of answers to complex questions remains an open research problem.', 'Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems.', 'In Sections 9 and 10, we have discussed many of these issues.', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', 'Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and ""other"" questions (Voorhees 2003).', 'A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #AUTHOR_TAGa , 2006b ) .', 'However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken.']"	0
CC573	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	strength of recommendation taxonomy sort a patientcentered approach to grading evidence in the medical literature	"['Mark H Ebell', 'Jay Siwek', 'Barry D Weiss', 'Steven H Woolf', 'Jeffrey Susman', 'Bernard Ewigman', 'Marjorie Bowman']"		"A large number of taxonomies are used to rate the quality of an individual study and the strength of a recommendation based on a body of evidence. We have developed a new grading scale that will be used by several family medicine and primary care journals (required or optional), with the goal of allowing readers to learn one taxonomy that will apply to many sources of evidence. Our scale is called the Strength of Recommendation Taxonomy. It addresses the quality, quantity, and consistency of evidence and allows authors to rate individual studies or bodies of evidence. The taxonomy is built around the information mastery framework, which emphasizes the use of patient-oriented outcomes that measure changes in morbidity or mortality. An A-level recommendation is based on consistent and good-quality patient-oriented evidence; a B-level recommendation is based on inconsistent or limited-quality patient-oriented evidence; and a C-level recommendation is based on consensus, usual practice, opinion, disease-oriented evidence, or case series for studies of diagnosis, treatment, prevention, or screening. Levels of evidence from 1 to 3 for individual studies also are defined. We hope that consistent use of this taxonomy will improve the ability of authors and readers to communicate about the translation of research into practice."	Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #AUTHOR_TAG ) .	"['The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.', 'Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #AUTHOR_TAG ) .']"	5
CC574	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	firstyear medical students’ information needs and resource selection responses to a clinical scenario	"['Keith W Cogdill', 'Margaret E Moore']"	introduction	"Etude ayant pour but une meilleure comprehension des besoins en information des etudiants en medecine de premiere annee, et de leurs perceptions des ressources appropriees( ressources telles que livres , MEDLINE ... )"	"MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) ."	"['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) ."", 'However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).', 'Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']"	0
CC575	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	the trec8 question answering track evaluation	"['Ellen M Voorhees', 'Dawn M Tice']"	related work		"For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #AUTHOR_TAG ) ."	"['Clinical question answering is an emerging area of research that has only recently begun to receive serious attention.', 'As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated.', 'In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems.', 'For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #AUTHOR_TAG ) .', 'In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview.']"	0
CC576	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	evidencebased medicine how to practice and teach ebm second edition churchill livingstone	"['David L Sackett', 'Sharon E Straus', 'W Scott Richardson', 'William Rosenberg', 'R Brian Haynes']"	introduction		"Third , the paradigm of evidence-based medicine ( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process ."	"['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts.', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third , the paradigm of evidence-based medicine ( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process .', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']"	0
CC577	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	natural language question answering the view from here	"['Lynette Hirschman', 'Robert Gaizauskas']"	experiments	"As users struggle to navigate the wealth of on-line information now available, the need for automated question answering systems becomes more urgent. We need systems that allow a user to ask a question in everyday language and receive an answer quickly and succinctly, with sufficient context to validate the answer. Current search engines can return ranked lists of documents, but they do not deliver answers to the user. Question answering systems address this problem. Recent successes have been reported in a series of question-answering evaluations that started in 1999 as part of the Text Retrieval Conference (TREC). The best systems are now able to answer more than two thirds of factual questions in this evaluation."	"As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #AUTHOR_TAG ) ."	"['Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors).', 'However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results.', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #AUTHOR_TAG ) .']"	1
CC578	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	the trec8 question answering track evaluation	"['Ellen M Voorhees', 'Dawn M Tice']"	experiments		"As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) ."	"['Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors).', 'However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results.', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) .']"	1
CC579	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	semantic characteristics of medline citations useful for therapeutic decisionmaking	"['Charles Sneiderman', 'Dina Demner-Fushman', 'Marcelo Fiszman', 'Thomas C Rindflesch']"		MEDLINE retrieval using several information retrieval algorithms was characterized for relevance to point-of-care therapeutic decisions for a sample of clinical queries in family practice. Evaluation methodology is described and preliminary results are presented.	"For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #AUTHOR_TAG ."	"['Although our problem extractor returns a list of clinical problems, we only evaluate performance on identification of the primary problem.', ""For some abstracts, MeSH headings can be used as ground truth, because one of the human indexers' tasks in assigning terms is to identify the main topic of the article (sometimes a disorder)."", 'For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #AUTHOR_TAG .']"	5
CC580	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	answering questions in the genomics domain	"['Fabio Rinaldi', 'James Dowdall', 'Gerold Schneider', 'Andreas Persidis']"	related work	"In this paper we describe current efforts aimed at adapting an existing Question Answering system to a new document set, namely research papers in the genomics domain. The system has been originally developed for another restricted domain, however it has already proved its portability. Nevertheless, the process is not painless, and the specific purpose of  this paper is to describe the problems encountered."	"The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 ."	"['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']"	0
CC581	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	effective mapping of biomedical text to the umls metathesaurus the metamap program	['Alan R Aronson']		"The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library"	"Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus ."	"['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).', 'An additional feature we take advantage of (when present) is explicit section markers present in some abstracts.', 'These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical Literature (1987) to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes.', 'These abstracts loosely adhere to the introduction, methods, results, and conclusions format common in scientific writing, and delineate a study using explicitly marked sections with variations of the above headings.', 'Although many core clinical journals require structured abstracts, there is a great deal of variation in the actual headings.', 'Even when present, the headings are not organized in a manner focused on patient care.', 'In addition, abstracts of much high-quality work remain unstructured.', 'For these reasons, explicit section markers are not entirely reliable indicators for the various semantic elements we seek to extract, but must be considered along with other sources of evidence.']"	5
CC582	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	knowledge extraction for clinical question answering preliminary results	"['Dina Demner-Fushman', 'Jimmy Lin']"		"The combination of recent developments in question an-swering research and the unparalleled resources devel-oped specifically for automatic semantic processing of text in the medical domain provides a unique opportu-nity to explore complex question answering in the clin-ical domain. In this paper, we attempt to operationalize major aspects of evidence-based medicine in the form of knowledge extractors that serve as the fundamental building blocks of a clinical question answering sys-tem. Our evaluations demonstrate that domain-specific knowledge can be effectively leveraged to extract PICO frame elements from MEDLINE abstracts. Clinical in-formation systems in support of physicians ' decision-making process have the potential to improve the qual-ity of patient care in real-world settings"	"This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence ."	"['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', 'Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.']"	2
CC583	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	the nlm indexing initiative’s medical text indexer	"['Alan R Aronson', 'James G Mork', 'Clifford W Gay', 'Susanne M Humphrey', 'Willie J Rogers']"			"Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) ."	"['The function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']"	3
CC584	J07-1005	Answering Clinical Questions with Knowledge-Based and Statistical Techniques	the wellbuilt clinical question a key to evidencebased decisions	"['W Scott Richardson', 'Mark C Wilson', 'James Nishikawa', 'Robert S Hayward']"	introduction		The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .	"['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts.', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']"	0
CC585	J08-1003	Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation	speed and accuracy in shallow and deep stochastic parsing	"['Ron Kaplan', 'Stefan Riezler', 'Tracy Holloway King', 'John T Maxwell', 'Alexander Vasserman', 'Richard Crouch']"	introduction	"Abstract : This paper reports some experiments that Compare the accuracy and performance of two stochastic parsing systems. The currently popular Collins parser is a shallow parser whose output contains more detailed semantically relevant information than other such parsers. The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a log- linear disambiguation component and provides much richer representations theory. We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times. We found the deep-parsing system to be more accurate than the Collins parser with only a slight reduction in parsing speed."	"Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; #AUTHOR_TAG ; Miyao and Tsujii 2004 ) ."	"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; #AUTHOR_TAG ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']"	4
CC586	J08-1003	Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation	the parc 700 dependency bank	"['Tracy Holloway King', 'Richard Crouch', 'Stefan Riezler', 'Mary Dalrymple', 'Ron Kaplan']"	introduction	"An automatic method for annotating the Penn-II Treebank (Marcus et al., 1994) with high-level Lexical Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001; Dalrymple, 2001) f-structure representations is described in (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004). The annotation algorithm and the automatically-generated f-structures are the basis for the automatic acquisition of wide-coverage and robust probabilistic approximations of LFG grammars (Cahill et al., 2002; Cahill et al., 2004a) and for the induction of LFG semantic forms (O'Donovan et al., 2004). The quality of the annotation algorithm and the f-structures it generates is, therefore, extremely important. To date, annotation quality has been measured in terms of precision and recall against the DCU 105. The annotation algorithm currently achieves an f-score of 96.57% for complete f-structures and 94.3% for preds-only f-structures. There are a number of problems with evaluating against a gold standard of this size, most notably that of overfitting. There is a risk of assuming that the gold standard is a complete and balanced representation of the linguistic phenomena in a language and basing design decisions on this. It is, therefore, preferable to evaluate against a more extensive, external standard. Although the DCU 105 is publicly available, 1 a larger well-established external standard can provide a more widely-recognised benchmark against which the quality of the f-structure annotation algorithm can be evaluated. For these reasons, we present an evaluation of the f-structure annotation algorithm of (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004) against the PARC 700 Dependency Bank (King et al., 2003). Evaluation against an external gold standard is a non-trivial task as linguistic analyses may differ systematically between the gold standard and the output to be evaluated as regards feature geometry and nomenclature. We present conversion software to automatically account for many (but not all) of the systematic differences. Currently, we achieve an f-score of 87.31% for the f-structures generated from the original Penn-II trees and an f-score of 81.79% for f-structures from parse trees produced by Charniak's (2000) parser in our pipeline parsing architecture against the PARC 700"	"Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #AUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) ."	"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #AUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']"	4
CC587	J08-1003	Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation	using grammatical relations to compare parsers	['Judita Preiss']	introduction	"We use the grammatical relations (GRs) described in Carroll et al. (1998) to compare a number of parsing algorithms. A first ranking of the parsers is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences: this required an implementation of GR extraction software for Penn Treebank style parsers. In addition, we perform an experiment using the extracted GRs as input to the Lappin and Leass (1994) anaphora resolution algorithm. This produces a second ranking of the parsers, and we investigate the number of errors that are caused by the incorrect 'GRs."	"Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; #AUTHOR_TAG ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) ."	"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; #AUTHOR_TAG ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']"	4
CC588	J08-2002	A Global Joint Model for Semantic Role Labeling	discriminative reranking for natural language parsing	['Michael Collins']	introduction	"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation"	"Our re-ranking approach , like the approach to parse re-ranking of #AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes ."	"['Such a rich graphical model can represent many dependencies but there are two dangers-one is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohibitive, and the other is that if too many dependencies are encoded, the model will over-fit the training data and will not generalize well.', 'We propose a model which circumvents these two dangers and achieves significant performance gains over a similar local model that does not add any dependency arcs among the random variables.', 'To tackle the efficiency problem, we adopt dynamic programming and re-ranking algorithms.', 'To avoid overfitting we encode only a small set of linguistically motivated dependencies in features over sets of the random variables.', 'Our re-ranking approach , like the approach to parse re-ranking of #AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .', 'The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings.']"	1
CC589	J08-2002	A Global Joint Model for Semantic Role Labeling	discriminative reranking for natural language parsing	['Michael Collins']		"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation"	"Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions ."	"['Motivation for Re-Ranking.', 'For argument identification, the number of possible assignments for a parse tree with n nodes is 2 n .', 'This number can run into the hundreds of billions for a normal-sized tree.', 'For argument labeling, the number of possible assignments is ≈ 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments.', 'Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions.', 'Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.', 'As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance.', 'We used a value of n = 10 for training.', 'In Figure 8(a) we can see that if we could pick, using an oracle, the best assignment out of the top 10 assignments according to the local model, we would achieve an F-Measure of 97.3 on all arguments.', 'Increasing the number of n to 30 results in a very small gain in the upper bound on performance and a large increase in memory requirements.', 'We therefore selected n = 10 as a good compromise.']"	5
CC590	J09-1003	Evaluating Centering for Information Ordering Using Corpora	stochastic text structuring using the principle of continuity	"['Nikiforos Karamanis', 'Hisar Maruli Manurung']"		"This paper explores the feasibility of implementing an evolutionary algorithm for text structuring using the heuristic of continuity as a fitness function, chosen over other more complicated metrics of text coherence. Using MCGONAGALL (Manurung et al., 2000) as our experimental platform, we show that by employing an elitist strategy for stochastic search it is possible to quickly reach the global optimum of minimal violations of continuity."	"Following our previous work ( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists ."	"['Following our previous work ( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .', 'A set of candidate orderings is produced by creating different permutations of these lists.', 'A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9', ""A wide range of metrics of coherence can be defined in centering's terms, simply on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5, is a candidate ordering."", 'Table 6 summarizes the NOCBs, the violations of COHERENCE, SALIENCE, and CHEAPNESS, and the centering transitions for this ordering. 10', 'he candidate ordering contains two NOCBs in sentences (3e) and (3f).', 'Its score according to M.NOCB, the metric used by Karamanis and Manurung (2002) and Althaus, Karamanis, and Koller (2004), is 2. Another ordering with fewer NOCBs (should such an ordering exist) will be preferred over this candidate as the selected output of information ordering if M.NOCB is used to guide this process.', 'M.NOCB relies only on CONTINUITY.', 'Because satisfying this principle is a prerequisite for the computation of every other centering feature, M.NOCB is the simplest possible centering-based metric and will be used as the baseline in our experiments.']"	2
CC591	J09-1005	Unsupervised Type and Token Identification of Idiomatic Expressions	automatic identification of noncompositional phrases	['Dekang Lin']			"Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG ."	"['Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG .', 'We use the automatically built thesaurus of Lin (1998) to find words similar to each constituent, in order to automatically generate variants.', 'Variants are generated by replacing either the noun or the verb constituent of a pair with a semantically (and syntactically) similar word. 3', 'xamples of automatically generated variants for the pair spill, bean are pour, bean , stream, bean , spill, corn , and spill, rice .']"	4
CC592	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	decision graphs—an extension of decision trees	['J J Oliver']		"In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain."	"We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #AUTHOR_TAG ) ."	"['In principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases.', 'We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #AUTHOR_TAG ) .']"	0
CC593	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	knowledge harvesting articulation and delivery the hewlettpackard journal	"['K A Delic', 'D Lahaix']"			"Such technologies require significant human input , and are difficult to create and maintain ( #AUTHOR_TAG ) ."	"['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997).', 'Such technologies require significant human input , and are difficult to create and maintain ( #AUTHOR_TAG ) .', 'In contrast, the techniques examined in this article are corpus-based and data-driven.', 'The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.']"	0
CC594	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	bridging the lexical chasm statistical approaches to answerfinding	"['A Berger', 'R Caruana', 'D Cohn', 'D Freitag', 'V Mittal']"	method		"â¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG ) ."	"['â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG ) .']"	1
CC595	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	an introduction to modern information retrieval	"['G Salton', 'M J McGill']"	method	"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."	"This method follows a traditional Information Retrieval paradigm ( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query ."	"['As stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.', '(Doc-Ret).', 'This method follows a traditional Information Retrieval paradigm ( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .', 'In our case, the query is a new request e-mail to be addressed by the system, and we have considered three views of the documents in the corpus:']"	5
CC596	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	an introduction to modern information retrieval	"['G Salton', 'M J McGill']"	method	"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."	"For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) ."	"['We carried out a preliminary experiment in order to compare the three variants of the Doc-Ret method.', 'The evaluation is performed by considering each request e-mail in turn, removing it and its response from the corpus, carrying out the retrieval process, and then comparing the retrieved response with the actual response (if there are several similar responses in the corpus, an appropriate response can still be retrieved).', 'The results of this experiment are shown in Table 1.', 'The first column shows which document retrieval variant is being evaluated.', 'The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold).', 'We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents.', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .', 'The third column in Table 1 shows the proportion of requests for which this similarity is non-zero.', 'Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses.', 'The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place.', 'Here too the third variant yields the best similarity score (0.52).']"	5
CC597	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	an introduction to modern information retrieval	"['G Salton', 'M J McGill']"		"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."	"This situation suggests a response-automation approach that follows the document retrieval paradigm ( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) ."	"['The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.', 'This situation suggests a response-automation approach that follows the document retrieval paradigm ( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) .', 'However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively.']"	0
CC598	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	expert systems a technology before its time ai expert available at wwwstanfordedugroup scipavsgtexpertsystemsaiexperthtml	"['A Barr', 'S Tessler']"	introduction		It is therefore no surprise that early attempts at response automation were knowledge-driven ( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .	"['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']"	0
CC599	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	statistical and inductive inference by minimum message length	['C S Wallace']			We then use the program Snob ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) to cluster these experiences .	"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']"	5
CC600	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	expert systems a technology before its time ai expert available at wwwstanfordedugroup scipavsgtexpertsystemsaiexperthtml	"['A Barr', 'S Tessler']"			"The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) ."	"['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', 'In contrast, the techniques examined in this article are corpus-based and data-driven.', 'The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.']"	1
CC601	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	libsvm a library for support vector machines software available at httpwwwcsie ntuedutw∼cjlinlibsvm	"['C C Chang', 'C J Lin']"	method		7 We employed the LIBSVM package ( #AUTHOR_TAG ) .	"[""We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests. 7"", 'A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request.', 'During the For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results.', '7 We employed the LIBSVM package ( #AUTHOR_TAG ) .', 'prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.', 'We then apply the following steps.']"	5
CC602	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	statistical and inductive inference by minimum message length	['C S Wallace']	method		"In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) ."	"[""The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected."", 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) .', 'We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).', 'The input to Snob is a set of binary vectors, one vector per response document.', 'The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).', 'The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle.', 'The Decision Graph is trained on unigram and bigram lemmas in the request as input features, 5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'The model predicts which response cluster is most suitable for a given request, and returns the probability that this prediction is correct.', 'This probability is our indicator of whether the Doc-Pred method can address a new request.', 'As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3).']"	5
CC603	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	an introduction to modern information retrieval	"['G Salton', 'M J McGill']"	method	"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."	We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) .	"['We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) .', 'Precision measures how much of the information in an automatically generated response is correct (i.e., appears in the model response), and F-score measures the overall similarity between the automatically generated response and the model response.', 'F-score is the harmonic mean of precision and recall, which measures how much of the information in the model response appears in the generated response.', 'We consider precision separately because it does not penalize missing information, enabling us to better assess our sentence-based methods.', 'Precision, recall, and F-score are calculated as follows using a word-by-word comparison (stop-words are excluded). 13', 'ecision =']"	5
CC604	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	applying casebased reasoning techniques for enterprise systems	['I Watson']		"Chapter 1: What Is Case-Based Reasoning? Chapter 2: Understanding CBR Chapter 3: The Application of CBR Chapter 4: Industrial Applications of CBR Chapter 5: CBR and Customer Service Chapter 6: CBR Software Tools Chapter 7: Building a Diagnostic Case-Base Chapter 8: Building, Testing, and Maintaining Case-Bases Chapter 9: Conclusion Chapter 10: Bibliography"	"The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( #AUTHOR_TAG ) ."	"['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( #AUTHOR_TAG ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', 'In contrast, the techniques examined in this article are corpus-based and data-driven.', 'The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.']"	1
CC605	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	mercure towards an automatic email followup system	"['G Lapalme', 'L Kosseim']"			"#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering ."	"['There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000;Lapalme and Kosseim 2003;Bickel and Scheffer 2004;Malik, Subramaniam, and Kaushik 2007).', 'eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user.', 'If the user is unsatisfied with this list, an operator is asked to generate a new response.', 'The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.', 'However, there is no attempt to automatically generate a single response.', 'Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses.', 'Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.', 'Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.', 'The generated response is the answer that is closest to the centroid of the cluster.', ""Bickel and Scheffer's results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred."", 'However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.', 'Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.', 'This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences.', 'In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates.', '#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .', 'Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.', 'The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.', 'However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).']"	1
CC606	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	the design implementation and use of computational linguistics volume 35 number 4 the ngram statistics package in cicling	"['S Banerjee', 'T Pedersen']"	method		"5 Significant bigrams are obtained using the n-gram statistics package NSP ( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) ."	"['5 Significant bigrams are obtained using the n-gram statistics package NSP ( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .']"	5
CC607	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	mercure towards an automatic email followup system	"['G Lapalme', 'L Kosseim']"	introduction		"Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) ."	"['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', 'An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.', 'This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary.', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.', 'Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.', 'Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.', 'Observations from this corpus have led us to consider several methods that implement different types of corpus-based strategies.', 'Specifically, we have investigated two types of methods (retrieval and prediction) applied at two levels of granularity (document and sentence).', 'In this article, we present these methods and compare their performance.']"	0
CC608	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	automatic question answering using the web beyond the factoid	"['R Soricut', 'E Brill']"		"In this paper we describe and evaluate a Question Answering (QA) system that goes beyond answering factoid questions. Our approach to QA assumes no restrictions on the type of questions that are handled, and no assumption that the answers to be provided are factoids. We present an unsupervised approach for collecting question and answer pairs from FAQ pages, which we use to collect a corpus of 1 million question/answer pairs from FAQ pages available on the Web. This corpus is used to train various statistical models employed by our QA system: a statistical chunker used to transform a natural language-posed question into a phrase-based query to be submitted for exact match to an off-the-shelf search engine; an answer/question translation model, used to assess the likelihood that a proposed answer is indeed an answer to the posed question; and an answer language model, used to assess the likelihood that a proposed answer is a well-formed answer. We evaluate our QA system in a modular fashion, by comparing the performance of baseline algorithms against our proposed algorithms for various modules in our QA system. The evaluation shows that our system achieves reasonable performance in terms of answer accuracy for a large variety of complex, non-factoid questions."	"Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;  #AUTHOR_TAG ) ."	"['Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;  #AUTHOR_TAG ) .', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']"	1
CC609	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	mercure towards an automatic email followup system	"['G Lapalme', 'L Kosseim']"	method		â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #AUTHOR_TAG ; Roy and Subramaniam 2006 ) .	['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #AUTHOR_TAG ; Roy and Subramaniam 2006 ) .']	1
CC610	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	assessing agreement on classification tasks the kappa statistic	['J Carletta']	method	"Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as a field adopting techniques from content analysis."	"Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG ) ."	"['Each evaluation set contained 80 cases, randomly selected from the corpus in proportion to the size of each data set, where a case contained a request e-mail, the model response, and the responses generated by the two methods being compared.', 'Each judge was given of these cases, and was asked to assess the generated responses on the four criteria listed previously. 14', 'e maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges.', ""In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable."", 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG ) .', 'However, it is still necessary to We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur.']"	5
CC611	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	multidocument summarization by sentence extraction	"['J Goldstein', 'V Mittal', 'J Carbonell', 'M Kantrowitz']"	method		"In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) ."	"['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'This task can be cast as extractive multi-document summarization.', 'Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .']"	0
CC612	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	hybrid recommender systems user modeling and useradapted interaction	['R Burke']			A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #AUTHOR_TAG ) .	"['A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #AUTHOR_TAG ) .', 'However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred).', 'Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence.', 'Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance.', 'In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience.', 'These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and Japkowicz 2007).']"	1
CC613	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	queryrelevant summarization using faqs	"['A Berger', 'V Mittal']"		"This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments---on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies---suggest the plausibility of learning for summarization."	"Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) ( #AUTHOR_TAG; Soricut and Brill 2006)."	"['Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) ( #AUTHOR_TAG; Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']"	1
CC614	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	building effective question answering characters	"['A Leuski', 'R Patel', 'D Traum', 'B Kennedy']"	method	"In this paper, we describe methods for building and evaluation of limited domain question-answering characters. Several classification techniques are tested, including text classification using support vector machines, language-model based retrieval, and cross-language information retrieval techniques, with the latter having the highest success rate. We also evaluated the effect of speech recognition errors on performance with users, finding that retrieval is robust until recognition reaches over 50 % WER."	"â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #AUTHOR_TAG ) ."	"['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #AUTHOR_TAG ) .', 'The representativeness of the sample size was not discussed in any of these studies.']"	1
CC615	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	queryrelevant summarization using faqs	"['A Berger', 'V Mittal']"	method	"This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments---on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies---suggest the plausibility of learning for summarization."	"â¢ Only an automatic evaluation was performed , which relied on having model responses ( #AUTHOR_TAG ; Berger et al. 2000 ) ."	"['In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( #AUTHOR_TAG ; Berger et al. 2000 ) .']"	1
CC616	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	evaluation of a largescale email response system	"['Y Marom', 'I Zukerman']"	method		In #AUTHOR_TAGa ) we identified several systems that resemble ours in that they provide answers to queries .	"['In #AUTHOR_TAGa ) we identified several systems that resemble ours in that they provide answers to queries .', 'These systems addressed the evaluation issue as follows.', 'r Only an automatic evaluation was performed, which relied on having model responses .']"	0
CC617	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	an information measure for classification	"['C S Wallace', 'D M Boulton']"	method	"1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application"	"In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #AUTHOR_TAG ; Wallace 2005 ) ."	"[""The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected."", 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #AUTHOR_TAG ; Wallace 2005 ) .', 'We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).', 'The input to Snob is a set of binary vectors, one vector per response document.', 'The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).', 'The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle.', 'The Decision Graph is trained on unigram and bigram lemmas in the request as input features, 5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'The model predicts which response cluster is most suitable for a given request, and returns the probability that this prediction is correct.', 'This probability is our indicator of whether the Doc-Pred method can address a new request.', 'As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3).']"	5
CC618	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	queryrelevant summarization using faqs	"['A Berger', 'V Mittal']"		"This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments---on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies---suggest the plausibility of learning for summarization."	"In FAQs , #AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document ."	"['In FAQs , #AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', 'Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval.', 'Two significant differences between help-desk and FAQs are the following.']"	0
CC619	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	an intelligent discussionbot for answering student queries in threaded discussions	"['D Feng', 'E Shaw', 'J Kim', 'E Hovy']"	method	"This paper describes a discussion-bot that provides answers to students' discussion board questions in an unobtrusive and human-like way. Using information retrieval and natural language processing techniques, the discussion-bot identifies the questioner's interest, mines suitable answers from an annotated corpus of 1236 archived threaded discussions and 279 course documents and chooses an appropriate response. A novel modeling approach was designed for the analysis of archived threaded discussions to facilitate answer extraction. We compare a self-out and an all-in evaluation of the mined answers. The results show that the discussion-bot can begin to meet students' learning requests. We discuss directions that might be taken to increase the effectiveness of the question matching and answer extraction algorithms. The research takes place in the context of an undergraduate computer science course."	"â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( #AUTHOR_TAG ; Leuski et al. 2006 ) ."	"['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( #AUTHOR_TAG ; Leuski et al. 2006 ) .', 'The representativeness of the sample size was not discussed in any of these studies.']"	1
CC620	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	learning from message pairs for automatic email answering	"['S Bickel', 'T Scheffer']"	introduction	"We consider the problem of learning a mapping from question to answer messages. The training data for this problem consist of pairs of messages that have been received and sent in the past. We formulate the problem setting, discuss appropriate performance metrics, develop a solution and describe two baseline methods for comparison. We present a case study based on emails received and answered by the service center of a large online store."	"Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) ."	"['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', 'An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.', 'This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary.', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', 'A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.', 'Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.', 'Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.', 'Observations from this corpus have led us to consider several methods that implement different types of corpus-based strategies.', 'Specifically, we have investigated two types of methods (retrieval and prediction) applied at two levels of granularity (document and sentence).', 'In this article, we present these methods and compare their performance.']"	0
CC621	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	statistical learning theory	['V N Vapnik']	method	"The traditional approach of statistical physics to supervised learning routinely assumes unrealistic generative models for the data: usually inputs are independent random variables, uncorrelated with their labels. Only recently, statistical physicists started to explore more complex forms of data, such as equally-labelled points lying on (possibly low dimensional) object manifolds. Here we provide a bridge between this recently-established research area and the framework of statistical learning theory, a branch of mathematics devoted to inference in machine learning. The overarching motivation is the inadequacy of the classic rigorous results in explaining the remarkable generalization properties of deep learning. We propose a way to integrate physical models of data into statistical learning theory, and address, with both combinatorial and statistical mechanics methods, the computation of the Vapnik-Chervonenkis entropy, which counts the number of different binary classifications compatible with the loss class. As a proof of concept, we focus on kernel machines and on two simple realizations of data structure introduced in recent physics literature: $k$-dimensional simplexes with prescribed geometric relations and spherical manifolds (equivalent to margin classification). Entropy, contrary to what happens for unstructured data, is nonmonotonic in the sample size, in contrast with the rigorous bounds. Moreover, data structure induces a novel transition beyond the storage capacity, which we advocate as a proxy of the nonmonotonicity, and ultimately a cue of low generalization error. The identification of a synaptic volume vanishing at the transition allows a quantification of the impact of data structure within replica theory, applicable in cases where combinatorial methods are not available, as we demonstrate for margin learning.Comment: 19 pages, 3 figure"	"Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs ( #AUTHOR_TAG ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) ."	"['The focus of our work is on the general applicability of the different response automa- tion methods, rather than on comparing the performance of particular implementa- tion techniques.', 'Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs ( #AUTHOR_TAG ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']"	5
CC622	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	eventbased extractive summarization	"['E Filatova', 'V Hatzivassiloglou']"	method		"After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #AUTHOR_TAG to penalize redundant sentences in cohesive clusters ."	"['Removing redundant sentences.', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #AUTHOR_TAG to penalize redundant sentences in cohesive clusters .', 'This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented).', 'Specifically, given a sentence s k in cluster SC l which contains a sentence with a higher or equal score, the contribution of SC l to Score(s k ) (= Pr(SC l ) × Pr(s k |SC l )) is subtracted from Score(s k ).', 'After applying these penalties, we retain only the sentences whose adjusted score is greater than zero (for a highly cohesive cluster, typically only one sentence remains).']"	5
CC623	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	decision graphs—an extension of decision trees	['J J Oliver']	method	"In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain."	"The predictive model is a Decision Graph ( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle ."	"['The idea behind the Doc-Pred method is similar to Bickel and Scheffer�s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request�s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected.', 'In our case, the clustering is performed by the program Snob, which implements mixture model- ing combined with model selection based on the Minimum Message Length (MML) criterion (Wallace and Boulton 1968; Wallace 2005).', 'We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).', 'The input to Snob is a set of binary vectors, one vector per response document.', 'The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency).4', 'The predictive model is a Decision Graph ( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle .', 'The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct.', 'This probability is our indicator of whether the Doc-Pred method can address a new request.', 'As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3).']"	5
CC624	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	knowledge harvesting articulation and delivery the hewlettpackard journal	"['K A Delic', 'D Lahaix']"	introduction		It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #AUTHOR_TAG ) .	"['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #AUTHOR_TAG ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']"	0
CC625	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	mercure towards an automatic email followup system	"['G Lapalme', 'L Kosseim']"			"There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) ."	"['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user.', 'If the user is unsatisfied with this list, an operator is asked to generate a new response.', 'The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.', 'However, there is no attempt to automatically generate a single response.', 'Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses.', 'Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.', 'Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.', 'The generated response is the answer that is closest to the centroid of the cluster.', ""Bickel and Scheffer's results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred."", 'However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.', 'Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.', 'This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences.', 'In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates.', 'Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering.', 'Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.', 'The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.', 'However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).']"	1
CC626	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	learning from message pairs for automatic email answering	"['S Bickel', 'T Scheffer']"		"We consider the problem of learning a mapping from question to answer messages. The training data for this problem consist of pairs of messages that have been received and sent in the past. We formulate the problem setting, discuss appropriate performance metrics, develop a solution and describe two baseline methods for comparison. We present a case study based on emails received and answered by the service center of a large online store."	"There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) ."	"['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', 'eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user.', 'If the user is unsatisfied with this list, an operator is asked to generate a new response.', 'The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.', 'However, there is no attempt to automatically generate a single response.', 'Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses.', 'Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.', 'Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.', 'The generated response is the answer that is closest to the centroid of the cluster.', ""Bickel and Scheffer's results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred."", 'However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.', 'Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.', 'This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences.', 'In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates.', 'Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering.', 'Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.', 'The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.', 'However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).']"	1
CC627	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	automatic question answering using the web beyond the factoid	"['R Soricut', 'E Brill']"		"In this paper we describe and evaluate a Question Answering (QA) system that goes beyond answering factoid questions. Our approach to QA assumes no restrictions on the type of questions that are handled, and no assumption that the answers to be provided are factoids. We present an unsupervised approach for collecting question and answer pairs from FAQ pages, which we use to collect a corpus of 1 million question/answer pairs from FAQ pages available on the Web. This corpus is used to train various statistical models employed by our QA system: a statistical chunker used to transform a natural language-posed question into a phrase-based query to be submitted for exact match to an off-the-shelf search engine; an answer/question translation model, used to assess the likelihood that a proposed answer is indeed an answer to the posed question; and an answer language model, used to assess the likelihood that a proposed answer is a well-formed answer. We evaluate our QA system in a modular fashion, by comparing the performance of baseline algorithms against our proposed algorithms for various modules in our QA system. The evaluation shows that our system achieves reasonable performance in terms of answer accuracy for a large variety of complex, non-factoid questions."	"#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval ."	"['In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', '#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .', 'Two significant differences between help-desk and FAQs are the following.']"	1
CC628	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	automating helpdesk responses a comparative study of informationgathering approaches	"['Y Marom', 'I Zukerman']"	method		"6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results ."	"[""We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests. 7"", 'A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request.', '6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results .', '7 We employed the LIBSVM package (Chang and Lin 2001).', 'prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.', 'We then apply the following steps.']"	5
CC629	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	hybrid recommender systems user modeling and useradapted interaction	['R Burke']			"They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #AUTHOR_TAG as follows ."	"['In addition to the different response-generation methods, we have proposed a metalevel strategy to combine them.', 'This kind of meta-learning is referred to as stacking by the Data Mining community (Witten and Frank 2000).', 'Lekakos and Giaglis (2007) implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version.', 'They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #AUTHOR_TAG as follows .', ""The merging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke's feature combination, cascade, feature augmentation, and meta-level sub-categories)."", ""The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke's weighted, switching, and mixed sub-categories).""]"	0
CC630	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	evaluation of a largescale email response system	"['Y Marom', 'I Zukerman']"	method		"In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #AUTHOR_TAGb ) ."	"['Our experimental set-up is designed to evaluate the ability of the different responsegeneration methods to address unseen request e-mails.', 'In particular, we want to determine the applicability of our methods to different situations, namely, whether different requests are addressed only by some methods, or whether there is a significant overlap between the methods.', 'Our evaluation is performed by measuring the quality of the generated responses.', 'Quality is a subjective measure, which is best judged by the users of the system (i.e., the help-desk customers or operators).', 'In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #AUTHOR_TAGb ) .', 'However, our more comprehensive evaluation is an automatic one that treats the responses generated by the help-desk operators as model responses, and performs text-based comparisons between the model responses and the automatically generated ones.']"	5
CC631	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	applying casebased reasoning techniques for enterprise systems	['I Watson']	introduction	"Chapter 1: What Is Case-Based Reasoning? Chapter 2: Understanding CBR Chapter 3: The Application of CBR Chapter 4: Industrial Applications of CBR Chapter 5: CBR and Customer Service Chapter 6: CBR Software Tools Chapter 7: Building a Diagnostic Case-Base Chapter 8: Building, Testing, and Maintaining Case-Bases Chapter 9: Conclusion Chapter 10: Bibliography"	It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #AUTHOR_TAG ; Delic and Lahaix 1998 ) .	"['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #AUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']"	0
CC632	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	bridging the lexical chasm statistical approaches to answerfinding	"['A Berger', 'R Caruana', 'D Cohn', 'D Freitag', 'V Mittal']"			#AUTHOR_TAG compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .	"['In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', '#AUTHOR_TAG compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', 'Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval.', 'Two significant differences between help-desk and FAQs are the following.']"	0
CC633	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	a hybrid approach for improving predictive accuracy of collaborative filtering algorithms user modeling and useradapted interaction	"['G Lekakos', 'G M Giaglis']"			"Following #AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases ."	"['Following #AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', 'However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision).', 'Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned.', 'Instead, we adopt an unsupervised approach that finds patterns in the data-confidence values coupled with performance scores (Section 6.1)-and then attempts to fit unseen data to these patterns (Section 6.2).', 'Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3).', 'In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process.']"	1
CC634	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	automatic generation of domain models for callcenters from noisy transcriptions	"['S Roy', 'L V Subramaniam']"	method	"Call centers handle customer queries from various domains such as computer sales and support, mobile phones, car rental, etc. Each such domain generally has a domain model which is essential to handle customer complaints. These models contain common problem categories, typical customer issues and their solutions, greeting styles. Currently these models are manually created over time. Towards this, we propose an unsupervised technique to generate domain models automatically from call transcriptions. We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers, which still results in high word error rates (40%) and show that even from these noisy transcriptions of calls we can automatically build a domain model. The domain model is comprised of primarily a topic taxonomy where every node is characterized by topic(s), typical Questions-Answers (Q&As), typical actions and call statistics. We show how such a domain model can be used for topic identification of unseen calls. We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model."	â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .	['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']	1
CC635	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	sentence fusion for multidocument news summarization	"['R Barzilay', 'K R McKeown']"	method	"A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources."	"In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) ."	"['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'This task can be cast as extractive multi-document summarization.', 'Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']"	0
CC636	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	detection of questionanswer pairs in email conversations	"['L Shrestha', 'K R McKeown']"			"Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006)."	"['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']"	1
CC637	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	decision graphs—an extension of decision trees	['J J Oliver']	method	"In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain."	"Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) ."	"['The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']"	5
CC638	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	automatic evaluation of summaries using ngram cooccurrence statistics	"['C Y Lin', 'E H Hovy']"	method		"13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures ."	"['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']"	5
CC639	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	an information measure for classification	"['C S Wallace', 'D M Boulton']"		"1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application"	We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .	"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']"	5
CC640	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	evaluation of a largescale email response system	"['Y Marom', 'I Zukerman']"	method		"In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) ."	"['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.', 'Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.']"	5
CC641	J09-4010	An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain	in question answering two heads are better than one	"['J Chu-Carroll', 'K Czuba', 'J M Prager', 'A Ittycheriah']"		"Motivated by the success of ensemble methods  in machine learning and other areas of natural  language processing, we developed a multistrategy  and multi-source approach to question  answering which is based on combining the results  from different answering agents searching  for answers in multiple corpora. The answering  agents adopt fundamentally different strategies,  one utilizing primarily knowledge-based  mechanisms and the other adopting statistical  techniques. We present our multi-level answer  resolution algorithm that combines results from  the answering agents at the question, passage,  and/or answer levels. Experiments evaluating  the effectiveness of our answer resolution algorithm  show a 35.0% relative improvement over  our baseline system in the number of questions  correctly answered, and a 32.8% improvement  according to the average precision metric"	"The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."	"['Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis."", ""A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning."", 'Instead it uses a voting mechanism to select the answer given by the majority of methods.', ""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", 'Because the results of all the methods are comparable, no learning is required: At each stage of the ""cascade of methods,"" the method that performs best is selected.', 'In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.', 'Therefore, we need to learn from experience when to use each method.']"	1
CC642	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	paraphrasing with bilingual parallel corpora	"['Colin Bannard', 'Chris Callison-Burch']"	introduction	"Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments."	"But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) ."	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"	4
CC643	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	nonlinear programming 2nd edition athena scientific	['Dimitri P Bertsekas']			"Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable ."	"['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here η is an optimization precision, α is a step size chosen with the strong Wolfe's rule (Nocedal and Wright 1999)."", 'Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']"	5
CC644	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	wordbased alignment phrasebased translation whats the link	"['Adam Lopez', 'Philip Resnik']"			See #AUTHOR_TAG for further discussion .	"['We now investigate whether our alignments produce improvements in an end-to-end phrase-based machine translation system.', 'We use a state-of-the-art machine translation system,5 and follow the experimental setup used for the 2008 shared task on machine translation (ACL 2008 Third Workshop on Statistical Machine Translation).', 'The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build phrase table; (6) tune weights for the phrase table.', 'For more details consult the shared task description.6', 'To evaluate the quality of the produced alignments, we keep the pipeline unchanged, and use the models described earlier to generate the word alignments in Step 3.', 'For Step 4, we use the soft union symmetrization heuristic.', 'Symmetrization has almost no effect on alignments produced by S-HMM, but we use it for uniformity in the experiments.', 'We tested three values of the threshold (0.2, 0.4, 0.6) which try to capture different tradeoffs of precision vs. recall, and pick the best according to the translation performance on development data.', 'Table 2 summarizes the results for the different corpora.', 'For refer- ence we include IBM Model 4 as suggested in the task description.', 'PR training always outperforms EM training and outperforms IBM Model 4 in all but one experiment.', 'Differences in BLEU range from 0.2 to 0.9.', 'The two constraints help to a different extent for different corpora and translation directions, in a somewhat unpredictable manner.', 'In general our impression is that the connection between alignment quality and BLEU scores is complicated, and changes are difficult to explain and justify.', 'The number of iterations for MERT optimization to converge varied from 2 to 28; and the best choice of threshold on the development set did not always correspond to the best on the test set.', 'Contrary to conventional wisdom in the MT community, bigger phrase tables did not always perform better.', 'In 14 out of 18 cases, the threshold picked was 0.4 (medium size phrase tables) and the other four times 0.2 was picked (smaller phrase tables).', 'When we include only high confidence alignments, more phrases are extracted but many of these are erroneous.', 'Potentially this leads to a poor estimate of the phrase probabilities.', 'See #AUTHOR_TAG for further discussion .']"	0
CC645	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	a systematic comparison of various statistical alignment models	"['Franz Josef Och', 'Hermann Ney']"	introduction	"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."	"Many researchers use the GIZA + + software package ( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency ."	"['IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation.', 'IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package ( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .', 'All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word).', 'Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86-98%).', 'This leads to the common practice of post-processing heuristics for intersecting directional alignments to produce nearly bijective and symmetric results (Koehn, Och, and Marcu 2003).']"	0
CC646	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	numerical optimization	"['Jorge Nocedal', 'Stephen J Wright']"		"Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side."	"Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG ) ."	"['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG ) ."", 'Here, β∇(λ) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when λ = 0, the objective is not differentiable.', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']"	5
CC647	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	getting the structure right for word alignment leaf	"['Alexander Fraser', 'Daniel Marcu']"		"Word alignment is the problem of annotating parallel text with translational correspondence. Previous generative word alignment models have made structural assumptions such as the 1-to-1, 1-to-N, or phrase-based consecutive word assumptions, while previous discriminative models have either made such an assumption directly or used features derived from a generative model making one of these assumptions. We present a new generative alignment model which avoids these structural limitations, and show that it is effective when trained using both unsupervised and semi-supervised training methods."	"This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #AUTHOR_TAG ) ."	"['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #AUTHOR_TAG ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).', 'Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'For example, enforcing that each hidden state of an HMM model should be used at most once per sentence would break the Markov property and make the model intractable.', 'In contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'The underlying model remains unchanged, but the learning method changes.', 'During learning, our method is similar to the EM algorithm with the addition of solving an optimization problem similar to a maximum entropy problem inside the E Step.', 'The following subsections present the Posterior Regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of Section 2.']"	1
CC648	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	moses open source toolkit for statistical machine translation	"['Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Burch', 'Richard Zens', 'Rwth Aachen', 'Alexandra Constantin', 'Marcello Federico', 'Nicola Bertoldi', 'Chris Dyer', 'Brooke Cowan', 'Wade Shen', 'Christine Moran', 'Ondrej Bojar']"		"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks."	5 The open source Moses ( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/ .	['5 The open source Moses ( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/ .']	5
CC649	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	building a multilingual parallel subtitle corpus	['J¨org Tiedemann']		"In this paper on-going work of creating an extensive multilingual parallel corpus of movie subtitles is presented. The corpus currently contains roughly 23,000 pairs of aligned subtitles covering about 2,700 movies in 29 languages. Subtitles mainly consist of transcribed speech, sometimes in a very condensed way. Insertions, deletions and paraphrases are very frequent which makes them a challenging data set to work with especially when applying automatic sentence alignment. Standard alignment approaches rely on translation consistency either in terms of length or term translations or a combination of both. In the paper, we show that these approaches are not applicable for subtitles and we propose a new alignment approach based on time overlaps specifically designed for subtitles. In our experiments we obtain a significant improvement of alignment accuracy compared to standard length-based"	"results are based on a corpus of movie subtitles ( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) ."	"['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We consider a parse tree on the source language as a set of dependency edges to be transferred.', 'For each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'These edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles ( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']"	5
CC650	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	the hiero machine translation system extensions evaluation and analysis	"['David Chiang', 'Adam Lopez', 'Nitin Madnani', 'Christof Monz', 'Philip Resnik', 'Michael Subotin']"	introduction	"Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems."	"Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #AUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006)."	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #AUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001;Hwa et al. 2005;Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']"	0
CC651	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	minimum bayesrisk word alignments of bilingual texts	"['Shankar Kumar', 'William Byrne']"	introduction	"We present Minimum Bayes-Risk word alignment for machine translation. This statistical, model-based approach attempts to minimize the expected risk of alignment errors under loss functions that measure alignment quality. We describe various loss functions, including some that incorporate linguistic analysis as can be obtained from parse trees, and show that these approaches can improve alignments of the English-French Hansards."	"Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) ."	"['Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', 'Using this decoding we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold.', 'This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link.', 'The threshold is tuned on some small amount of labeled data-in our case the development set-to minimize some loss.', 'Kumar and Byrne (2002) study different loss functions that incorporate linguistic knowledge, and show significant improvement over likelihood decoding.', 'Note that this could potentially result in an alignment having zero probability under the model, as many-to-many alignments can be produced in this way.', 'MBR decoding has several advantages over Viterbi decoding.', 'First, independently of the particular choice of the loss function, by picking a specific threshold we can trade off precision and recall of the predicted word alignments.', 'In fact, in this work when comparing different alignment sets we do not commit to any loss function but instead compare precision vs recall curves, by generating alignments for different thresholds (0..1).', 'Second, with this method we can ignore the null word probabilities, which tend to be poorly estimated.']"	5
CC652	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	a systematic comparison of various statistical alignment models	"['Franz Josef Och', 'Hermann Ney']"		"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."	"We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves ."	"['The intent of this experimental section is to evaluate the gains from using constraints during learning, hence the main comparison is between HMM trained with normal EM vs. trained with PR plus constraints.', 'We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference.', 'However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference.', 'Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements.', 'We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .', 'We trained IBM Model 4 using the default configuration of the']"	5
CC653	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	unsupervised multilingual learning for morphological segmentation	"['Benjamin Snyder', 'Regina Barzilay']"	introduction	"For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme patterns, or abstract morphemes. We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family."	"But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #AUTHOR_TAG ) ."	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #AUTHOR_TAG ) .']"	4
CC654	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	a systematic comparison of various statistical alignment models	"['Franz Josef Och', 'Hermann Ney']"		"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."	The standard approach is to train two models independently and then intersect their predictions ( #AUTHOR_TAG ) .	"['The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations.', 'In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1).', 'The standard approach is to train two models independently and then intersect their predictions ( #AUTHOR_TAG ) .', 'However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.', 'Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source).', 'We suppress dependence on x and y for brevity.', 'Define z to range over the union of all possible directional alignments − → Z ∪ ← − Z .', 'We define a mixture model p']"	1
CC655	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	a new view of the em algorithm that justifies incremental sparse and other variants	"['Radford M Neal', 'Geoffrey E Hinton']"	introduction	"The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible."	"EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #AUTHOR_TAG ) :"	"['Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #AUTHOR_TAG ) :']"	5
CC656	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	a systematic comparison of various statistical alignment models	"['Franz Josef Och', 'Hermann Ney']"		"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."	"This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) ."	"['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).', 'Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'For example, enforcing that each hidden state of an HMM model should be used at most once per sentence would break the Markov property and make the model intractable.', 'In contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'The underlying model remains unchanged, but the learning method changes.', 'During learning, our method is similar to the EM algorithm with the addition of solving an optimization problem similar to a maximum entropy problem inside the E Step.', 'The following subsections present the Posterior Regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of Section 2.']"	1
CC657	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	hmm word and phrase alignment for statistical machine translation	"['Yonggang Deng', 'William Byrne']"	related work	"Estimation and alignment procedures for word and phrase alignment hidden Markov models (HMMs) are developed for the alignment of parallel text. The development of these models is motivated by an analysis of the desirable features of IBM Model 4, one of the original and most effective models for word alignment. These models are formulated to capture the desirable aspects of Model 4 in an HMM alignment formalism. Alignment behavior is analyzed and compared to human-generated reference alignments, and the ability of these models to capture different types of alignment phenomena is evaluated. In analyzing alignment performance, Chinese-English word alignments are shown to be comparable to those of IBM Model 4 even when models are trained over large parallel texts. In translation performance, phrase-based statistical machine translation systems based on these HMM alignments can equal and exceed systems based on Model 4 alignments, and this is shown in Arabic-English and Chinese-English translation. These alignment models can also be used to generate posterior statistics over collections of parallel text, and this is used to refine and extend phrase translation tables with a resulting improvement in translation quality."	"In the context of word alignment , #AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations ."	"['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment , #AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', 'This encourages more transitions and hence shorter phrases.', 'For the task of unsupervised dependency parsing, Smith and Eisner (2006) add a constraint of the form ""the average length of dependencies should be X"" to capture the locality of syntax (at least half of the dependencies are between adjacent words), using a scheme they call structural annealing.', ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", 'The factor δ changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form ""scale the total length of edges"", which depending on the value of δ will prefer to have more shorter/longer edges.', 'Such statements are not data dependent.', 'Depending on the value of δ, for instance if δ ≤ 0, even if the data is such that the model already uses too many short edges on average, this value of δ will push for more short edges.', 'By contrast the statements we can make in PR are of the form ""there should be more short edges than long edges"".', 'Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes.']"	0
CC658	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	europarl a parallel corpus for statistical machine translation	['Philipp Koehn']		"We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead."	"results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) ."	"['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We consider a parse tree on the source language as a set of dependency edges to be transferred.', 'For each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'These edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']"	5
CC659	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	tailoring word alignments to syntactic machine translation	"['John DeNero', 'Dan Klein']"		"Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model's predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments."	This heuristic is called soft union ( #AUTHOR_TAG ) .	"['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final (Och and Ney 2003).', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.', 'One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.', 'Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.', 'In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union ( #AUTHOR_TAG ) .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.', 'This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.', 'Applying the symmetrization to the model with symmetry constraints does not affect performance.']"	5
CC660	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	bootstrapping parsers via syntactic projection across parallel texts natural language engineering	"['Rebecca Hwa', 'Philip Resnik', 'Amy Weinberg', 'Clara Cabezas', 'Okan Kolak']"	introduction	"Broad coverage, high quality parsers are available for only a handful of languages. A prerequisite for developing broad coverage parsers for more languages is the annotation of text with the desired linguistic representations (also known as ""treebanking""). However, syntactic annotation is a labor intensive and time-consuming process, and it is difficult to find linguistically annotated text in sufficient quantities. In this article, we explore using parallel text to help solving the problem of creating syntactic annotation in more languages. The central idea is to annotate the English side of a parallel corpus, project the analysis to the second language, and then train a stochastic analyzer on the resulting noisy annotations. We discuss our background assumptions, describe an initial study on the ""projectability"" of syntactic relations, and then present two experiments in which stochastic parsers are developed with minimal human intervention via projection from English."	"But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) ."	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"	4
CC661	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	a systematic comparison of various statistical alignment models	"['Franz Josef Och', 'Hermann Ney']"	introduction	"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."	"Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG ) ."	"['A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al. 1993b).', 'There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est allé), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly.', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG ) .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', 'We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns.', 'Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link.', 'Sure links are represented as squares with borders, and possible links']"	0
CC662	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	a systematic comparison of various statistical alignment models	"['Franz Josef Och', 'Hermann Ney']"		"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."	For MT the most commonly used heuristic is called grow diagonal final ( #AUTHOR_TAG ) .	"['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final ( #AUTHOR_TAG ) .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.', 'One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.', 'Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.', 'In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union (DeNero and Klein 2007).', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.', 'This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.', 'Applying the symmetrization to the model with symmetry constraints does not affect performance.']"	1
CC663	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	simple robust scalable semisupervised learning via expectation regularization	"['G Mann', 'A McCallum']"	related work	"Although semi-supervised learning has been an active area of research, its use in deployed applications is still relatively rare because the methods are often difficult to implement, fragile in tuning, or lacking in scalability. This paper presents expectation regularization, a semi-supervised learning method for exponential family parametric models that augments the traditional conditional label-likelihood objective function with an additional term that encourages model predictions on unlabeled data to match certain expectations---such as label priors. The method is extremely easy to implement, scales as well as logistic regression, and can handle non-independent features. We present experiments on five different data sets, showing accuracy improvements over other semi-supervised methods."	"PR is closely related to the work of #AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning ."	"['PR is closely related to the work of #AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', 'In the original GE framework, the posteriors of the model on unlabeled data are regularized directly.', 'They train a discriminative model, using conditional likelihood on labeled data and an ""expectation regularization"" penalty term on the unlabeled data:']"	0
CC664	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora	"['David Yarowsky', 'Grace Ngai']"	introduction	"This paper investigates the potential for projecting linguistic annotations including part-of-speech tags and base noun phrase bracketings from one language to another via automatically word-aligned parallel corpora. First, experiments assess the accuracy of unmodified direct transfer of tags and brackets from the source language English to the target languages French and Chinese, both for noisy machine-aligned sentences and for clean hand-aligned sentences. Performance is then substantially boosted over both of these baselines by using training techniques optimized for very noisy data, yielding 94-96% core French part-of-speech tag accuracy and 90% French bracketing F-measure for stand-alone monolingual tools trained without the need for any human-annotated data in the given language."	"But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) ."	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"	4
CC665	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	what’s in a translation rule	"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']"	introduction	"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data."	"Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006)."	"['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1�5) for statistical machine translation and the concept of �word-by- word� alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']"	0
CC666	J10-3007	Learning Tractable Word Alignment Models with Complex Constraints	annealing structural bias in multilingual weighted grammar induction	"['Noah A Smith', 'Jason Eisner']"	related work	"We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). Next, by annealing the free parameter that controls this bias, we achieve further improvements. We then describe an alternative kind of structural bias, toward ""broken "" hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1-17 % (absolute) over CE (and 8-30% over EM), achieving to our knowledge the best results on this task to date. Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems."	"For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."	"['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations.', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', 'This encourages more transitions and hence shorter phrases.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", 'The factor δ changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form ""scale the total length of edges"", which depending on the value of δ will prefer to have more shorter/longer edges.', 'Such statements are not data dependent.', 'Depending on the value of δ, for instance if δ ≤ 0, even if the data is such that the model already uses too many short edges on average, this value of δ will push for more short edges.', 'By contrast the statements we can make in PR are of the form ""there should be more short edges than long edges"".', 'Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes.']"	0
CC667	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	a corpus for modeling morphosyntactic agreement in arabic gender number and rationality	"['Sarah Alkuhlani', 'Nizar Habash']"	conclusion	"We present an enriched version of the Penn Arabic Treebank (Maamouri et al., 2004), where latent features necessary for modeling morpho-syntactic agreement in Arabic are manually annotated. We describe our process for efficient annotation, and present the first quantitative analysis of Arabic morpho-syntactic phenomena."	"We use the agreement checker code developed by #AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference ."	"['Grammaticality of parse trees.', 'We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns.', 'We use the agreement checker code developed by #AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference .', 'The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not.', 'The accuracy number reflects the percentage of such relations found which meet the agreement criteria.', 'Note that we use the syntax given by the tree, not the gold syntax.', 'For all three trees, however, we used gold morphological features for this evaluation even when those features were not used in the parsing task.', 'This is because we want to see to what extent the predicted morphological features help find the correct syntactic relations, not whether the predicted trees are intrinsically coherent given possibly false predicted morphology.', 'The results can be found in Table 18.', 'We note that the grammaticality of the gold corpus is not 100%; this is approximately equally due to errors in the checking script and to annotation errors in the gold standard.', 'We take the given grammaticality of the gold corpus as a topline for this analysis.', 'Nominal modification has a smaller error band between baseline and gold compared with subject-verb agreement.', 'We assume this is because subject-verb agreement is more complex (it depends on their relative order), and because nominal modification can have multiple structural targets, only one of which is correct, although all, however, are plausible from the point of view of agreement.', 'The error reduction relative to the gold topline is 62% and 76% for nominal agreement and verb agreement, respectively.', 'Thus, we see that our second hypothesis-that the use of morphological features will reduce grammaticality errors in the resulting parse trees with respect to agreement phenomena-is borne out.']"	5
CC668	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	buckwalter arabic morphological analyzer version 20 linguistic data consortium	['Timothy A Buckwalter']	experiments		"In comparison, the tag set of the Buckwalter Morphological Analyzer ( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8"	"['Linguistically, words have associated POS tags, e.g., �verb� or �noun,� which further abstract over morphologically and syntactically similar lexemes.', 'Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles.', 'In comparison, the tag set of the Buckwalter Morphological Analyzer ( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8', 'Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (Rambow et al. 2006; Petrov, Das, and McDonald 2012).', 'We have adapted the list from Rambow et al. (2006) for Arabic, and call it here CORE12.', 'It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX).', 'The CATIB6 tag set can be viewed as a further reduction, with the exception that CATIB6 contains a passive voice tag (a mor- phological feature); this tag constitutes only 0.5% of the tags in the training, however.']"	1
CC669	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	better arabic parsing baselines evaluations and analysis	"['Spence Green', 'Christopher D Manning']"	conclusion	"In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree-banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2--5% F1."	"For better comparison with work of others , we adopt the suggestion made by #AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long ."	"['For better comparison with work of others , we adopt the suggestion made by #AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .', 'We report these filtered results in Table 14.', 'Filtered results are consistently higher (as expected).', 'Results are about 0.9% absolute higher on the development set, and about 0.6% higher on the test set.', 'The contribution of the RAT feature across sets is negligible (or small and unstable), resulting in less than 0.1% absolute loss on the dev set, but about 0.15% gain on the test set.', 'For clarity and conciseness, we only show the best model (with RAT) in Table 14.']"	5
CC670	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	algorithms for deterministic incremental dependency parsing	['Joakim Nivre']	related work	"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework."	"As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) ."	"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"	0
CC671	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	catib the columbia arabic treebank	"['Nizar Habash', 'Ryan Roth']"	experiments	"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed"	We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG ) .	"['We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG ) .', 'Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.', ""CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations."", 'It has a reduced POS tag set consisting of six tags only (henceforth CATIB6).', 'The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX (punctuation).', 'CATiB uses a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, respectively (whether they appear pre-or postverbally); IDF for the idafa (possessive) relation; MOD for most other modifications; and other less common relations that we will not discuss here.', 'For other PATB-based POS tag sets, see Sections 2.6 and 2.7.']"	5
CC672	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	a statistical parser for czech	"['Michael Collins', 'Jan Hajic', 'Lance Ramshaw', 'Christoph Tillmann']"	introduction	"This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results- 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text."	"For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy ."	"['Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;.']"	4
CC673	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	the penn arabic treebank building a largescale annotated arabic corpus	"['Mohamed Maamouri', 'Ann Bies', 'Timothy A Buckwalter', 'Wigdan Mekki']"	related work	"From our three year experience of developing a large-scale corpus of annotated Arabic text, our paper will address the following: (a) review pertinent Arabic language issues as they relate to methodology choices, (b) explain our choice to use the Penn English Treebank style of guidelines, (requiring the Arabic-speaking annotators to deal with a new grammatical system) rather than doing the annotation in a more traditional Arabic grammar style (requiring NLP researchers to deal with a new system); (c) show several ways in which human annotation is important and automatic analysis difficult, including the handling of orthographic ambiguity by both the morphological analyzer and human annotators; (d) give an illustrative example of the Arabic Treebank methodology, focusing on a particular construction in both morphological analysis and tagging and syntactic analysis and following it in detail through the entire annotation process, and finally, (e) conclude with what has been achieved so far and what remains to be done."	"For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 ."	"['For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .', 'We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set.', 'For all experiments, unless specified otherwise, we used the dev set. 10 We kept the test unseen (""blind"") during training and model development.', 'Statistics about this split (after conversion to the CATiB dependency format) are given in Table 1.']"	5
CC674	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	a corpus for modeling morphosyntactic agreement in arabic gender number and rationality	"['Sarah Alkuhlani', 'Nizar Habash']"	related work	"We present an enriched version of the Penn Arabic Treebank (Maamouri et al., 2004), where latent features necessary for modeling morpho-syntactic agreement in Arabic are manually annotated. We describe our process for efficient annotation, and present the first quantitative analysis of Arabic morpho-syntactic phenomena."	"18 In this article , we use a newer version of the corpus by #AUTHOR_TAG than the one we used in Marton , Habash , and Rambow ( 2011 ) ."	"['18 In this article , we use a newer version of the corpus by #AUTHOR_TAG than the one we used in Marton , Habash , and Rambow ( 2011 ) .']"	5
CC675	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	identifying broken plurals irregular gender and rationality in arabic text	"['Sarah Alkuhlani', 'Nizar Habash']"	experiments	"Arabic morphology is complex, partly because of its richness, and partly because of common irregular word forms, such as broken plurals (which resemble singular nouns), and nouns with irregular gender (feminine nouns that look masculine and vice versa). In addition, Arabic morpho-syntactic agreement interacts with the lexical semantic feature of rationality, which has no morphological realization. In this paper, we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number, and rationality in Arabic. We compare two techniques, using simple maximum likelihood (MLE) with back-off and a support vector machine based sequence tagger (Yamcha). We study a number of orthographic, morphological and syntactic learning features. Our results show that the MLE technique is preferred for words seen in the training data, while the Yam-cha technique is optimal for unseen words, which are our real target. Furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more. A combination of the two techniques improves overall performance even further."	"In this article , we use an in-house system which provides functional gender , number , and rationality features ( #AUTHOR_TAG ) ."	"['Some inflectional features, specifically gender and number, are expressed using different morphemes in different words (even within the same POS).', 'There are four sound gender-number suffixes in Arabic: 5 +φ (null morpheme) for masculine singular, + + for feminine singular, + +wn for masculine plural, and + +At for feminine plural.', 'Form-based GENDER and NUMBER feature values are set only according to these four morphemes (and a few others, ignored for simplicity).', 'There are exceptions and alternative ways to express GENDER and NUMBER, however, and functional feature values take them into account: Depending on the lexeme, plurality can be expressed using sound plural suffixes or using a pattern change together with singular suffixes.', ""A sound plural example is the word pair / Hafiyd+a /Hafiyd+At ('granddaughter/granddaughters.)"", ""On the other hand, the plural of the inflectionally and morphemically feminine singular word madras+a ('school') is the word madAris+φ ('schools'), which is feminine and plural inflectionally, but has a masculine singular suffix."", 'This irregular inflection, known as broken plural, is similar to the English mouse/mice, but is much more common in Arabic (over 50% of plurals in our training data).', ""A similar inconsistency appears in feminine nominals that are not inflected using sound gender suffixes, for example, the feminine form of the masculine singular adjective Âzraq+φ ('blue') is zarqA'+φ not * *Âzraq+a ."", 'To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smrž (2007), we distinguish between two types of inflectional features: formbased (a.k.a.', 'surface, or illusory) features and functional features. 6', 'ost available Arabic NLP tools and resources model morphology using formbased (""surface"") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smrž 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article , we use an in-house system which provides functional gender , number , and rationality features ( #AUTHOR_TAG ) .', 'See Section 5.2 for more details.']"	5
CC676	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	catib the columbia arabic treebank	"['Nizar Habash', 'Ryan Roth']"	related work	"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed"	"As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( #AUTHOR_TAG ) ."	"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( #AUTHOR_TAG ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"	0
CC677	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	an efficient algorithm for easyfirst nondirectional dependency parsing	"['Yoav Goldberg', 'Michael Elhadad']"	related work		"Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #AUTHOR_TAG ) ( Section 6 ) ."	"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;).', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #AUTHOR_TAG ) ( Section 6 ) .', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	5
CC678	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	a corpus for modeling morphosyntactic agreement in arabic gender number and rationality	"['Sarah Alkuhlani', 'Nizar Habash']"	related work	"We present an enriched version of the Penn Arabic Treebank (Maamouri et al., 2004), where latent features necessary for modeling morpho-syntactic agreement in Arabic are manually annotated. We describe our process for efficient annotation, and present the first quantitative analysis of Arabic morpho-syntactic phenomena."	"To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #AUTHOR_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) ."	"['The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #AUTHOR_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .', 'We conducted experiments with gold features to assess the potential of these features, and with predicted fea- tures, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012).', '19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).', 'The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.', 'The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.', 'The last part of the table revalidates the gains achieved with the best controlled feature combination, using CATIBEX�the best performing tag set with predicted in- put.', 'Note, however, that the 1% (absolute) advantage of CATIBEX (without additional features) over the morphology-free CORE12 on machine-predicted input (Table 2) has shrunk with these functional feature combinations to 0.3%.', 'We take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form-based morphological information embedded in the CATIBEX POS tags.']"	5
CC679	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	arabic tokenization partofspeech tagging and morphological disambiguation in one fell swoop	"['Nizar Habash', 'Owen Rambow']"	related work	"We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties."	"Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7)."	"['So far we discussed optimal (gold) conditions.', 'But in prac- tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags.14', 'The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear.', 'Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'It turned out that BW, the best gold performer but with lowest POS pre- diction accuracy (81.8%), suffered the biggest drop (11.4%) and was the worst performer with predicted tags.', 'The simplest tag set, CATIB6, and its extension, CATIBEX, benefited from the highest POS prediction accuracy (97.7%), and their performance suffered the least.', 'CATIBEX was the best performer with predicted POS tags.', 'Performance drop and POS prediction accuracy are given in columns 8 and 9.']"	5
CC680	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	algorithms for deterministic incremental dependency parsing	['Joakim Nivre']	related work	"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework."	"For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG ."	"['All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination.', 'Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given.', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG ."", 'We denote p < 0.05 and p < 0.01 with + and ++ , respectively.']"	5
CC681	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	parsing indian languages with maltparser	['Joakim Nivre']	related work	"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data."	"For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering)."	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.', 'All experiments were done using the Nivre ""eager"" algorithm.', '11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on.', 'Kübler, McDonald, and Nivre (2009) describe a ""typical"" MaltParser model configuration of attributes and features.', '13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3-1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration.', 'To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre ""eager"" algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']"	5
CC682	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	identifying broken plurals irregular gender and rationality in arabic text	"['Sarah Alkuhlani', 'Nizar Habash']"	related work	"Arabic morphology is complex, partly because of its richness, and partly because of common irregular word forms, such as broken plurals (which resemble singular nouns), and nouns with irregular gender (feminine nouns that look masculine and vice versa). In addition, Arabic morpho-syntactic agreement interacts with the lexical semantic feature of rationality, which has no morphological realization. In this paper, we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number, and rationality in Arabic. We compare two techniques, using simple maximum likelihood (MLE) with back-off and a support vector machine based sequence tagger (Yamcha). We study a number of orthographic, morphological and syntactic learning features. Our results show that the MLE technique is preferred for words seen in the training data, while the Yam-cha technique is optimal for unseen words, which are our real target. Furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more. A combination of the two techniques improves overall performance even further."	"We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #AUTHOR_TAG ) .19"	"['The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011).18', 'This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #AUTHOR_TAG ) .19', 'The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).', 'The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.', 'The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.', 'The last part of the table revalidates the gains achieved with the best controlled feature combination, using CATIBEX�the best performing tag set with predicted in- put.', 'Note, however, that the 1% (absolute) advantage of CATIBEX (without additional features) over the morphology-free CORE12 on machine-predicted input (Table 2) has shrunk with these functional feature combinations to 0.3%.', 'We take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form-based morphological information embedded in the CATIBEX POS tags.']"	5
CC683	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	the penn arabic treebank building a largescale annotated arabic corpus	"['Mohamed Maamouri', 'Ann Bies', 'Timothy A Buckwalter', 'Wigdan Mekki']"	experiments	"From our three year experience of developing a large-scale corpus of annotated Arabic text, our paper will address the following: (a) review pertinent Arabic language issues as they relate to methodology choices, (b) explain our choice to use the Penn English Treebank style of guidelines, (requiring the Arabic-speaking annotators to deal with a new grammatical system) rather than doing the annotation in a more traditional Arabic grammar style (requiring NLP researchers to deal with a new system); (c) show several ways in which human annotation is important and automatic analysis difficult, including the handling of orthographic ambiguity by both the morphological analyzer and human annotators; (d) give an illustrative example of the Arabic Treebank methodology, focusing on a particular construction in both morphological analysis and tagging and syntactic analysis and following it in detail through the entire annotation process, and finally, (e) conclude with what has been achieved so far and what remains to be done."	"Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;  Habash, Rambow, and Roth 2012)."	"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;  Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012).', 'See Section 5.2 for more details.']"	1
CC684	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	arabic tokenization partofspeech tagging and morphological disambiguation in one fell swoop	"['Nizar Habash', 'Owen Rambow']"	experiments	"We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties."	"Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #AUTHOR_TAG ;  Habash, Rambow, and Roth 2012)."	"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #AUTHOR_TAG ;  Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012).', 'See Section 5.2 for more details.']"	1
CC685	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	catib the columbia arabic treebank	"['Nizar Habash', 'Ryan Roth']"	experiments	"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed"	"The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #AUTHOR_TAG ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( Buckwalter 2004 ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 ."	"[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #AUTHOR_TAG ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( Buckwalter 2004 ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .""]"	5
CC686	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	buckwalter arabic morphological analyzer version 20 linguistic data consortium	['Timothy A Buckwalter']	experiments		"Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer ( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012)."	"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer ( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012).', 'See Section 5.2 for more details.']"	1
CC687	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	conllx shared task on multilingual dependency parsing	"['Sabine Buchholz', 'Erwin Marsi']"	related work	"Each year the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?"	"As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) ."	"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"	0
CC688	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	parsing indian languages with maltparser	['Joakim Nivre']	introduction	"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data."	"It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) ."	"['Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example, modeling CASE in Czech improves Czech parsing (Collins et al. 1999): CASE is relevant, not redundant, and can be predicted with sufficient accuracy.', 'It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']"	4
CC689	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	algorithms for deterministic incremental dependency parsing	['Joakim Nivre']	introduction	"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework."	The result holds for both the MaltParser ( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .	"['results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition.', 'In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.', 'This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .']"	5
CC690	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	buckwalter arabic morphological analyzer version 20 linguistic data consortium	['Timothy A Buckwalter']	experiments		"The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( Habash and Roth 2009 ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #AUTHOR_TAG ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 ."	"[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( Habash and Roth 2009 ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #AUTHOR_TAG ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .""]"	5
CC691	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	a statistical parser for czech	"['Michael Collins', 'Jan Hajic', 'Lance Ramshaw', 'Christoph Tillmann']"	related work	"This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results- 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text."	#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .	"['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', '#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'The results for Czech are the opposite of our results for Arabic, as we will see.', 'This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajič and Vidová-Hladká 1998) compared with Arabic (≈14.0%,', 'see Table 3).', 'Similarly, Cowan and Collins (2005) report that the use of a subset of Spanish morphological features (number for adjectives, determiners, nouns, pronouns, and verbs; and mode for verbs) outperforms other combinations.', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We also find that the number feature helps for Arabic.', ""Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima'an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.""]"	0
CC692	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	getting more from morphology in multilingual dependency parsing	"['Matt Hohensee', 'Emily M Bender']"	related work	"We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser. Compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks. We find increases in accuracy of up to 5.3% absolute. While some of this results from the feature set capturing information unrelated to morphology, there is still significant improvement, up to 4.6% absolute, due to the agreement model."	"9 We do not relate to specific results in their study because it has been brought to our attention that #AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) ."	"['9 We do not relate to specific results in their study because it has been brought to our attention that #AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .']"	1
CC693	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	an efficient algorithm for easyfirst nondirectional dependency parsing	"['Yoav Goldberg', 'Michael Elhadad']"	introduction		The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG ) .	"['results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition.', 'In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.', 'This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG ) .']"	5
CC694	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	introduction to arabic natural language processing	['Nizar Habash']	experiments	"Abstract This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues. The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters. Table of Contents: What is ""Arabic""? / Arabic Script / Arabic Phonology and Orthography / Arabic Mo..."	A more detailed discussion of the various available Arabic tag sets can be found in #AUTHOR_TAG .	"['The notion of ""POS tag set"" in natural language processing usually does not refer to a core set.', 'Instead, the Penn English Treebank (PTB) uses a set of 46 tags, including not only the core POS, but also the complete set of morphological features (this tag set is still fairly small since English is morphologically impoverished).', 'In PATB-tokenized MSA, the corresponding type of tag set (core POS extended with a complete description of morphology) would contain upwards of 2,000 tags, many of which are extremely rare (in our training corpus of about 300,000 words, we encounter only POS tags with complete morphology).', 'Therefore, researchers have proposed tag sets for MSA whose size is similar to that of the English PTB tag set, as this has proven to be a useful size computationally.', 'These tag sets are hybrids in the sense that they are neither simply the core POS, nor the complete morphologically enriched tag set, but instead they selectively enrich the core POS tag set with only certain morphological features.', 'A more detailed discussion of the various available Arabic tag sets can be found in #AUTHOR_TAG .']"	0
CC695	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	catib the columbia arabic treebank	"['Nizar Habash', 'Ryan Roth']"	experiments	"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed"	"For more information on CATiB , see #AUTHOR_TAG and Habash , Faraj , and Roth ( 2009 ) ."	"['The CATiB Treebank uses the word segmentation of the PATB.', ""It splits off several categories of orthographic clitics, but not the definite article + Al+ ('the')."", 'In all of the experiments reported in this article, we use the gold segmentation.', 'Tokenization involves further decisions on the segmented token forms, such as spelling normalization, which we only briefly touch on here (in Section 4.1).', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see #AUTHOR_TAG and Habash , Faraj , and Roth ( 2009 ) .']"	0
CC696	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	an efficient algorithm for easyfirst nondirectional dependency parsing	"['Yoav Goldberg', 'Michael Elhadad']"	experiments		"Some researchers , however , including #AUTHOR_TAG , train on predicted feature values instead ."	"['So far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including #AUTHOR_TAG , train on predicted feature values instead .', 'It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', 'But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature values are reinforced (since they repeat the gold patterns), whereas noisy predicted feature values are still represented in training (in patterns that do not repeat the gold). 21', 'To test our hypothesis, we start this section by comparing three variations:']"	1
CC697	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	identifying broken plurals irregular gender and rationality in arabic text	"['Sarah Alkuhlani', 'Nizar Habash']"	related work	"Arabic morphology is complex, partly because of its richness, and partly because of common irregular word forms, such as broken plurals (which resemble singular nouns), and nouns with irregular gender (feminine nouns that look masculine and vice versa). In addition, Arabic morpho-syntactic agreement interacts with the lexical semantic feature of rationality, which has no morphological realization. In this paper, we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number, and rationality in Arabic. We compare two techniques, using simple maximum likelihood (MLE) with back-off and a support vector machine based sequence tagger (Yamcha). We study a number of orthographic, morphological and syntactic learning features. Our results show that the MLE technique is preferred for words seen in the training data, while the Yam-cha technique is optimal for unseen words, which are our real target. Furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more. A combination of the two techniques improves overall performance even further."	"19 The paper by #AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article ."	"['19 The paper by #AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article .']"	1
CC698	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	towards an optimal pos tag set for modern standard arabic processing	['Mona Diab']	related work		"As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) ."	"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"	0
CC699	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	algorithms for deterministic incremental dependency parsing	['Joakim Nivre']	related work	"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework."	"11 #AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies ."	"['11 #AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .', 'The Nivre �standard� algorithm is also reported there to do better on Arabic, but in a preliminary experimentation, it did slightly worse than the �eager� one, perhaps due to the high percentage of right branching (left headed structures) in our Arabic training set�an observation already noted in Nivre (2008).']"	1
CC700	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	parsing indian languages with maltparser	['Joakim Nivre']	related work	"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data."	"Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #AUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.13"	"['There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense),12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #AUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.13', 'Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3�1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration. To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, 7 POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre �eager� algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']"	5
CC701	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	better arabic parsing baselines evaluations and analysis	"['Spence Green', 'Christopher D Manning']"	related work	"In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree-banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2--5% F1."	"As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) ."	"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"	0
CC702	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	an efficient algorithm for projective dependency parsing	['Joakim Nivre']	related work	"This paper presents a deterministic parsing algorithm for projective dependency grammar. The running time of the algorithm is linear in the length of the input string, and the dependency graph produced is guaranteed to be projective and acyclic. The algorithm has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85% with a very simple grammar."	"For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; KÃ¼bler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering)."	"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; KÃ¼bler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.', 'All experiments were done using the Nivre ""eager"" algorithm.', '11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on.', 'Kübler, McDonald, and Nivre (2009) describe a ""typical"" MaltParser model configuration of attributes and features.', '13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3-1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration.', 'To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre ""eager"" algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']"	5
CC703	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	atanas chanev gulsen eryigit sandra kubler svetoslav marinov and erwin marsi	"['Joakim Nivre', 'Johan Hall', 'Jens Nilsson']"	related work		"Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT ."	"['As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"	0
CC704	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	morphology and reranking for the statistical parsing of spanish	"['Brooke Cowan', 'Michael Collins']"	related work	"We present two methods for incorporating detailed features in a Spanish parser, building on a baseline model that is a lexicalized PCFG. The first method exploits Spanish morphology, and achieves an F1 constituency score of 83.6%. This is an improvement over 81.2 % accuracy for the baseline, which makes little or no use of morphological information. The second model uses a reranking approach to add arbitrary global features of parse trees to the morphological model. The reranking model reaches 85.1 % F1 accuracy on the Spanish parsing task. The resulting model for Spanish parsing combines an approach that specifically targets morphological information with an approach that makes use of general structural features"	"Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations ."	"['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Collins et al. (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size �3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'The results for Czech are the opposite of our results for Arabic, as we will see.', 'This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic� and Vidov�-Hladk� 1998) compared with Arabic (�14.0%, see Table 3).', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We also find that the number feature helps for Arabic.', 'Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima�an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.']"	0
CC705	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	introduction to arabic natural language processing	['Nizar Habash']	experiments	"Abstract This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues. The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters. Table of Contents: What is ""Arabic""? / Arabic Script / Arabic Phonology and Orthography / Arabic Mo..."	"7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) ."	"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"	0
CC706	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	an efficient algorithm for easyfirst nondirectional dependency parsing	"['Yoav Goldberg', 'Michael Elhadad']"	experiments		"In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) ."	"['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).', 'Unlike MaltParser, however, it does not attempt to attach arcs ""eagerly"" as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments).', 'Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, ""easy"" attachment, to low, as estimated by the classifier).', 'Labeling the relation arcs is done in a second pass, with a separate training step, after all attachments have been decided (the code for which was added after the publication of Goldberg and Elhadad (2010), which only included an unlabeled attachment version).']"	5
CC707	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	parsing indian languages with maltparser	['Joakim Nivre']	related work	"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data."	"Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) ."	"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	1
CC708	J13-1008	Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features	getting more from morphology in multilingual dependency parsing	"['Matt Hohensee', 'Emily M Bender']"	related work	"We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser. Compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks. We find increases in accuracy of up to 5.3% absolute. While some of this results from the feature set capturing information unrelated to morphology, there is still significant improvement, up to 4.6% absolute, due to the agreement model."	#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .	"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;).', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']"	0
CC709	J14-2004	Unsupervised Event Coreference Resolution	unsupervised event coreference resolution with rich linguistic features	"['Cosmin Adrian Bejan', 'Sanda Harabagiu']"	related work	"This paper examines how a new class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task. Designed with the purpose of clustering complex linguistic objects, these models consider a potentially infinite number of features and categorical outcomes. The evaluation performed for solving both within- and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task."	This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG ) .	"['This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG ) .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.', 'As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents).', 'In the next section, we provide additional information on how we performed the annotation of this corpus.', 'Another major contribution of this article is an extended description of the unsupervised models for solving event coreference.', 'In particular, we focused on providing further explanations about the implementation of the mIBP framework as well as its integration into the HDP and iHMM models.', 'Finally, in this work, we significantly extended the experimental results section, which also includes a novel set of experiments performed over the OntoNotes English corpus (LDC-ON 2007).']"	2
CC710	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	the harpy speech understanding system in lea	"['B Lowerre', 'R Reddy']"			"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #AUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) ."	"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #AUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']"	1
CC711	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	scripts plans goals and understanding lawrence erlbaum associates	"['R Schank', 'R Abelson']"	experiments		"Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #AUTHOR_TAG ."	"['Though the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #AUTHOR_TAG .""]"	0
CC712	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	algorithmic program debugging	['E Shapiro']		"The thesis lays a theoretical framework for program debugging, with the goal of partly mechanizing this activity. In particular, we formalize and develop algorithmic solutions to the following two questions: (1) How do we identify a bug in a program that behaves incorrectly? (2) How do we fix a bug, once one is identified?  We develop interactive diagnosis algorithms that identify a bug in a program that behaves incorrectly, and implement them in Prolog for the diagnosis of Prolog programs. Their performance suggests that they can be the backbone of debugging aids that go far beyond what is offered by current programming environments.  We develop an inductive inference algorithm that synthesizes logic programs from examples of their behavior. The algorithm incorporates the diagnosis algorithms as a component. It is incremental, and progresses by debugging a program with respect to the examples. The Model Inference System is a Prolog implementation of the algorithm. Its range of applications and efficiency is comparable to existing systems for program synthesis from examples and grammatical inference.  We develop an algorithm that can fix a bug that has been identified, and integrate it with the diagnosis algorithms to form an interactive debugging system. By restricting the class of bugs we attempt to correct, the system can debug programs that are too complex for the Model Inference System to synthesize."	There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of #AUTHOR_TAG .	"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969), assertional statements as in Michalski (1980), or semantic nets as in Winston (1975).', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of #AUTHOR_TAG .']"	1
CC713	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	computer control via limited natural language	"['P Fink', 'A Sigmon', 'A Biermann']"	conclusion	"A natural language processor is described for control of a machine in task-oriented situations. Particular emphasis is given to issues related to flow-of-control statements in dialogue. These include branching constructs, as in `if row 1 contains a positive entry, then . . .' and looping constructs, as in `repeat for all other rows'. Special problems are discussed concerning the processing of deeply nested control structures, pronoun resolution, and the handling of conjunctions. An experiment is described in which the robustness of the conditional feature was tested with a group of computer naive subjects. It was found that subjects could discover and use the feature effectively in solving problems even though the fact of its existence was systematically withheld during the training session."	"â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 ) , â¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns ."	"[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 ) , â\x80¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]"	3
CC714	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	the nomad system expectationbased detection and correction of errors during understanding of syntactically illformed text	['R Granger']			"The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , #AUTHOR_TAG , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) ."	"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , #AUTHOR_TAG , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']"	1
CC715	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	linguistic analysis of natural language communication with computers	['B Thompson']		"Interaction with computers in natural  language requires a language that is flexible  and suited to the task. This study of natural  dialogue was undertaken to reveal those characteristics  which can make computer English more  natural. Experiments were made in three modes  of communication: face-to-face, terminal-to-terminal  and human-to-computer, involving over  80 subjects, over 80,000 words and over 50  hours. They showed some striking similarities,  especially in sentence length and proportion of  words in sentences. The three modes also share  the use of fragments, typical of dialogue.  Detailed statistical analysis and comparisons  are given. The nature and relative frequency of  fragments, which have been classified into  twelve categories, is shown in all modes. Special  characteristics of the face-to-face mode  are due largely to these fragments (which  include phatics employed to keep the channel of  communication open). Special characteristics of  the computational mode include other fragments,  namely definitions, which are absent from other  modes. Inclusion of fragments in computational  grammar is considered a major factor in improving  computer naturalness.    The majority of experiments involved a real  life task of loading Navy cargo ships. The  peculiarities of face-to-face mode were similar  in this task to results of earlier experiments  involving another task. It was found that in  task oriented situations the syntax of interactions  is influenced in all modes by this context  in the direction of simplification, resulting in  short sentences (about 7 words long). Users  seek to maximize efficiency In solving the problem.  When given a chance, in the computational  mode, to utilize special devices facilitating  the solution of the problem, they all resort to  them.    Analyses of the special characteristics of  the computational mode, including the analysis  of the subjects"" errors, provide guidance for  the improvement of the habitability of such systems.  The availability of the REL System, a  high performance natural language system, made  the experiments possible and meaningful. The  indicated improvements in habitability are now  being embodied in the POL (Problem Oriented  Language) System, a successor to REL."	"The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , #AUTHOR_TAG , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) ."	"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , #AUTHOR_TAG , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']"	1
CC716	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	toward natural language computation	"['A Biermann', 'B Ballard']"	experiments	"A computer programming system called the ""Natural Language Computer"" (NLC) is described which allows a user to type English commands while watching them executed on sample data appearing on a display screen. Direct visual feedback enables the user to detect most misinterpretation errors as they are made so that incorrect or ambiguous commands can be retyped or clarified immediately. A sequence of correctly executed commands may be given a name and used as a subroutine, thus extending the set of available operations and allowing larger English-language programs to be constructed hierarchically. In addition to discussing the transition network syntax and procedural semantics of the system, special attention is devoted to the following topics: the nature of imperative sentences in the matrix domain; the processing of non-trivial noun phrases; conjunction; pronominals; and programming constructs such as ""if"", ""repeat"", and procedure definition."	"An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG ) ."	"['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]']"	0
CC717	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	learning structural descriptions from examples in	['P Winston']		Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1970. Ph.D.	"The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #AUTHOR_TAG ."	"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #AUTHOR_TAG .', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982).']"	1
CC718	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	the dialogue designing dialogue system dissertation	['T-P Ho']			Another dialogue acquisition system has been developed by #AUTHOR_TAG .	"['The problem of handling ill-formed input has been studied by Carbonell and Hayes (1983), Granger (1983), Jensen et al. (1983), Kwasny and Sondheimer (1981), Riesbeek and Schank (1976), Thompson (1980), Weischedel and Black (1980), and Weischedel and Sondheimer (1983.', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by #AUTHOR_TAG .', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']"	1
CC719	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	the hwim speech understanding system in lea	"['J Wolf', 'W Woods']"			"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and #AUTHOR_TAG ) ."	"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and #AUTHOR_TAG ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']"	1
CC720	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	relaxation techniques for parsing grammatically illformed input in natural language understanding systems	"['S Kwasny', 'N Sondheimer']"		"This paper investigates several language phenomena either considered deviant by linguistic standards or insufficiently addressed by existing approaches. These include co-occurrence violations, some forms of ellipsis and extraneous forms, and conjunction. Relaxation techniques for their treatment in Natural Language Understanding Systems are discussed. These techniques, developed within the Augmented Transition Network (ATN) model, are shown to be adequate to handle many of these cases."	"The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , #AUTHOR_TAG , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) ."	"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , #AUTHOR_TAG , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']"	1
CC721	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	pattern recognition as ruleguided inductive inference	['R Michalski']		"The determination of pattern recognition rules is viewed as a problem of inductive inference, guided by generalization rules, which control the generalization process, and problem knowledge rules, which represent the underlying semantics relevant to the recognition problem under consideration. The paper formulates the theoretical framework and a method for inferring general and optimal (according to certain criteria) descriptions of object classes from examples of classification or partial descriptions. The language for expressing the class descriptions and the guidance rules is an extension of the first-order predicate calculus, called variable-valued logic calculus VL21. VL21 involves typed variables and contains several new operators especially suited for conducting inductive inference, such as selector, internal disjunction, internal conjunction, exception, and generalization. Important aspects of the theory include: 1) a formulation of several kinds of generalization rules; 2) an ability to uniformly and adequately handle descriptors (i.e., variables, functions, and predicates) of different type (nominal, linear, and structured) and of different arity (i.e., different number of arguments); 3) an ability to generate new descriptors, which are derived from the initial descriptors through a rule-based system (i.e., an ability to conduct the so called constructive induction); 4) an ability to use the semantics underlying the problem under consideration. An experimental computer implementation of the method is briefly described and illustrated by an example."	"The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #AUTHOR_TAG , or semantic nets as in Winston ( 1975 ) ."	"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #AUTHOR_TAG , or semantic nets as in Winston ( 1975 ) .', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982).']"	1
CC722	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	construction of programs from example computations	"['A Biermann', 'R Krishnaswamy']"		"An autoprogrammer is an interactive computer programming system which automatically constructs computer programs from example computations executed by the user. The example calculations are done in a scratch pad fashion at a computer display using a light pen or other graphic input device, and the system stores a detailed history of all of the steps executed in the process. Then the system automatically synthesizes the shortest possible program which is capable of executing the observed examples. The paper describes the computational environment provided by the system, proves that the program synthesis technique is both ""sound"" and ""complete,"" describes the design of the system, and gives some programs it was used to create."	The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .	"['The problem of handling ill-formed input has been studied by Carbonell and Hayes (1983), Granger (1983), Jensen et al. (1983), Kwasny and Sondheimer (1981), Riesbeek and Schank (1976), Thompson (1980), Weischedel and Black (1980), and Weischedel and Sondheimer (1983.', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']"	1
CC723	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	scripts plans goals and understanding lawrence erlbaum associates	"['R Schank', 'R Abelson']"	experiments		"The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #AUTHOR_TAG ) , a deep parse of Si , or some other representation ."	"['We denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #AUTHOR_TAG ) , a deep parse of Si , or some other representation .', 'A user behavior is represented by a network, or directed graph, of such meanings.', 'At the beginning of a task, the state of the interaction is represented by the start state of the graph.']"	0
CC724	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	the sperry univac system for continuous speech recognition in	['M Medress']			"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #AUTHOR_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) ."	"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #AUTHOR_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']"	1
CC725	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	the hearsayii speech understanding system integrating knowledge to resolve uncertainty	"['L Erman', 'F Hayes-Roth', 'V Lesser', 'D Reddy']"		"The Hearsay-II system, developed during the DARPA-sponsored five-year speech-understanding research program, represents both a specific solution to the speech-understanding problem and a general framework for coordinating independent processes to achieve cooperative problem-solving behavior. As a computational problem, speech understanding reflects a large number of intrinsically interesting issues. Spoken sounds are achieved by a long chain of successive transformations, from intentions, through semantic and syntactic structuring, to the eventually resulting audible acoustic waves. As a consequence, interpreting speech means effectively inverting these transformations to recover the speaker's intention from the sound. At each step in the interpretive process, ambiguity and uncertainty arise.    The Hearsay-II problem-solving framework reconstructs an intention from hypothetical interpretations formulated at various levels of abstraction. In addition, it allocates limited processing resources first to the most promising incremental actions. The final configuration of the Hearsay-II system comprises problem-solving components to generate and evaluate speech hypotheses, and a focus-of-control mechanism to identify potential actions of greatest value. Many of these specific procedures reveal novel approaches to speech problems. Most important, the system successfully integrates and coordinates all of these independent activities to resolve uncertainty and control combinatorics. Several adaptations of the Hearsay-II framework have already been undertaken in other problem domains, and it is anticipated that this trend will continue; many future systems necessarily will integrate diverse sources of knowledge to solve complex problems cooperatively.    Discussed in this paper are the characteristics of the speech problem in particular, the special kinds of problem-solving uncertainty in that domain, the structure of the Hearsay-II system developed to cope with that uncertainty, and the relationship between Hearsay-II's structure and those of other speech-understanding systems. The paper is intended for the general computer science audience and presupposes no speech or artificial intelligence background."	"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) ."	"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']"	1
CC726	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	parse fitting and prose fixing getting a hold on illformedness	"['K Jensen', 'G Heidorn', 'L Miller', 'Y Ravin']"		"Processing syntactically ill-formed language is an important mission of the EPISTLE system, Ill-formed input is treated by this system in various ways. Misspellings are highlighted by a standard spelling checker; syntactic errors are detected and corrections are suggested; and stylistic infelicities are called to the user's attention.Central to the EPISTLE processing strategy is its technique of fitted parsing. When the rules of a conventional syntactic grammar are unable to produce a parse for an input string, this technique can be used to produce a reasonable approximate parse that can serve as input to the remaining stages of processing.This paper first describes the fitting process and gives examples of ill-formed language situations where it is called into play. We then show how a fitted parse allows EPISTLE to carry on its text-critiquing mission where conventional grammars would fail either because of input problems or because of limitations in the grammars themselves. Some inherent difficulties of the fitting technique are also discussed. In addition, we explore how style critiquing relates to the handling of ill-formed input, and how a fitted parse can be used in style checking."	"The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , #AUTHOR_TAG , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) ."	"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , #AUTHOR_TAG , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']"	1
CC727	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	natural language with discrete speech as a mode for humantomachine communication	"['A Biermann', 'R Rodman', 'D Rubin', 'J Heidlage']"	experiments	"A voice interactive natural language system, which allows users to solve problems with spoken English commands, has been constructed. The system utilizes a commercially available discrete speech recognizer which requires that each word be followed by approximately a 300 millisecond pause. In a test of the system, subjects were able to learn its use after about two hours of training. The system correctly processed about 77 percent of the over 6000 input sentences spoken in problem-solving sessions. Subjects spoke at the rate of about three sentences per minute and were able to effectively use the system to complete the given tasks. Subjects found the system relatively easy to learn and use, and gave a generally positive report of their experience."	"[ The current system should be distinguished from an earlier voice system ( VNLC , #AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]"	"['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[ The current system should be distinguished from an earlier voice system ( VNLC , #AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]']"	1
CC728	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	transition network grammars for natural language analysis	['W Woods']	experiments	"The use of augmented transition network grammars for the analysis of natural language sentences is described. Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing. The advantages of this model for natural language analysis are discussed in detail and illustrated by examples. An implementation of an experimental parsing system for transition network grammars is briefly described."	The expectation parser uses an ATN-like representation for its grammar ( #AUTHOR_TAG ) .	"['The expectation parser uses an ATN-like representation for its grammar ( #AUTHOR_TAG ) .', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (Ballard 1979).', 'An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.', 'Sentences have the same ""meaning"" if they ""result in identical tasks being performed.', 'The various sentence structures that']"	5
CC729	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	the acquisition and use of dialogue expectation in speech recognition	['P Fink']	experiments		"The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #AUTHOR_TAG ) ."	"['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #AUTHOR_TAG ) .', '[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]']"	0
CC730	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	the sdc speech understanding system in lea	"['J Barnett', 'M Berstein', 'R Gillman', 'I Kameny']"			"A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) ."	"['A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']"	1
CC731	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	the acquisition and use of dialogue expectation in speech recognition	['P Fink']			A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #AUTHOR_TAG .	"['A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980.', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #AUTHOR_TAG .']"	0
CC732	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	construction of programs from example computations	"['A Biermann', 'R Krishnaswamy']"	conclusion	"An autoprogrammer is an interactive computer programming system which automatically constructs computer programs from example computations executed by the user. The example calculations are done in a scratch pad fashion at a computer display using a light pen or other graphic input device, and the system stores a detailed history of all of the steps executed in the process. Then the system automatically synthesizes the shortest possible program which is capable of executing the observed examples. The paper describes the computational environment provided by the system, proves that the program synthesis technique is both ""sound"" and ""complete,"" describes the design of the system, and gives some programs it was used to create."	"â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG , â¢ a `` conditioning '' facility as described by Fink et al. ( 1985 ) , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns ."	"[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described by Fink et al. ( 1985 ) , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]"	3
CC733	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	recovery strategies for parsing extragrammatical language	"['J Carbonell', 'P Hayes']"		"Practical natural language interfaces must exhibit robust behaviour in the presence of extragrammatical user input. This paper classifies different types of grammatical deviations and related phenomena at the lexical, sentential and dialogue levels and presents recovery strategies tailored to specific phenomena in the classification. Such strategies constitute a tool chest of computationally tractable methods for coping with extragrammatieality in restricted domain natural language. Some of the strategies have been tested and proven viable in existing parsers.  </p"	"The problem of handling ill-formed input has been studied by #AUTHOR_TAG , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) ."	"['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']"	1
CC734	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	perceptrons	"['M Minsky', 'S Papert']"		"Ashkin-Teller type perceptron models are introduced. Their maximal capacity per number of couplings is calculated within a first-step replica-symmetry-breaking Gardner approach. The results are compared with extensive numerical simulations using several algorithms.Comment: 8 pages in Latex with 2 eps figures, RSB1 calculations has been adde"	"The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) ."	"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) .', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982).']"	1
CC735	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	semantic processing for a natural language programming system	['B Ballard']	experiments		"An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #AUTHOR_TAG , Biermann and Ballard 1980 ) ."	"['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #AUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]']"	0
CC736	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	speech recognition by machine a review	['D Reddy']		"This paper presents a brief survey on Automatic Speech Recognition and discusses the major themes and advances made in the past 60 years of research, so as to provide a technological perspective and an appreciation of the fundamental progress that has been accomplished in this important area of speech communication. After years of research and development the accuracy of automatic speech recognition remains one of the important research challenges (e.g., variations of the context, speakers, and environment).The design of Speech Recognition system requires careful attentions to the following issues: Definition of various types of speech classes, speech representation, feature extraction techniques, speech classifiers, database and performance evaluation. The problems that are existing in ASR and the various techniques to solve these problems constructed by various research workers have been presented in a chronological order. Hence authors hope that this work shall be a contribution in the area of speech recognition. The objective of this review paper is to summarize and compare some of the well known methods used in various stages of speech recognition system and identify research topic and applications which are at the forefront of this exciting and challenging field."	"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , #AUTHOR_TAG , Walker 1978 , and Wolf and Woods 1980 ) ."	"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , #AUTHOR_TAG , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']"	1
CC737	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	semantic processing for a natural language programming system	['B Ballard']	experiments		"The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #AUTHOR_TAG ) ."	"['The expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #AUTHOR_TAG ) .', 'An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.', 'Sentences have the same ""meaning"" if they ""result in identical tasks being performed.', 'The various sentence structures that']"	0
CC738	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	the acquisition and use of dialogue expectation in speech recognition	['P Fink']	experiments		How it is done is beyond the scope of this paper but is explained in detail in #AUTHOR_TAG .	"[""In such a situation, the user's intentions may be reflected more correctly by the following expected sentence set: double (rARG) 1.0 which signifies that any row may be referred to."", 'However, though this simplified expected sentence set may be a good generalization of the pattern observed, it has ramifications for error correction.', 'Specifically, it will be unable to fill in a row number should that value be missing in the incoming sentence.', 'The first option also has its drawbacks.', 'In this case, should the row number be missing in the sentence, the expectation parser will error correct the sentence to the most probable value, or the first one in the set if the probabilities are equal, here the value one for row 1.', 'Thus, both options are imperfect in terms of the error correction capabilities that they can provide.', 'The comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second.', 'How it is done is beyond the scope of this paper but is explained in detail in #AUTHOR_TAG .']"	0
CC739	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	automatic program construction techniques	"['A Biermann', 'G Guiho', 'Y Kodratoff', 'Eds']"		"The purpose of Avignon'86 is to provide a forum for presentat ion of new implementat ions of expert systems and basic tools and techniques for building expert systems. Aimed at developers and users of expert systems, the conference and exhibit ion will o f fer an assessment of available tools and techniques; will provide practical guidelines for making decisions concerning the application of expert system technology; and will help define, clarify, and make sense of the claims, promises, and realit ies of practical expert system applications."	There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of Shapiro ( 1982 ) .	"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969), assertional statements as in Michalski (1980), or semantic nets as in Winston (1975).', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of Shapiro ( 1982 ) .']"	1
CC740	J86-1002	The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding	organization and operation of a connected speech understanding system at lexical syntactic and semantic levels	"['J Haton', 'J Pierrel']"		"This paper describes a connected speech understanding system being implemented in Nancy, thanks to the work done in automatic speech recognition since 1968. This system is made up of four parts : an acoustic recognizer which gives a string of phoneme-like segments from a spoken sentence, a syntactic parser which controls the recognition process, a word recognizer working on words predicted by the parser and a dialog procedure which takes in account semantic constraints in order to avoid some of the errors and ambiguities. Some original features of the system are pointed out : modularily (e.g. the language used is considered as a parameter), possibility of processing slightly syntactically incorrect sentences, ... The application both in data management and in oral control of a telephone center has given very promising results. Work is in progress for generalizing our model : extension of the vocabulary and of the grammar, multi-speaker operation, etc."	"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) ."	"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']"	1
CC741	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	processing dictionary definitions with phrasal pattern hierarchies in this issue	['Hiyan Alshawi']	introduction	"This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns. An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English. A property of this dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions. The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary. Examples illustrating the output generated are presented, and some qualitative performance results and problems that were encountered are discussed. The analysis process applies successively more specific phrasal analysis rules as determined by a hierarchy of patterns in which less specific patterns dominate more specific ones. This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible, resulting in a relatively robust analysis mechanism. Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems: coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions."	In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .	"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '(Michiels (1982) contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']"	0
CC742	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	exploiting a large dictionary data base	['Archibal Michiels']			#AUTHOR_TAG and Akkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .	"['The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.', '#AUTHOR_TAG and Akkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.']"	0
CC743	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	a parser for generalised phrase structure grammars	"['John Phillips', 'Henry Thompson']"	introduction		"The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English ."	"['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']"	0
CC744	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	machinereadable dictionaries lexical data bases and the lexical system	['Nicoletta Calzolari']			"However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG for further discussion ) ."	"['The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth.', 'However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG for further discussion ) .', 'For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.', 'For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used.', 'These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character.', 'In Figure 1 above the definition of rivet as verb includes the noun definition of ""RIVET 1\'\', as signalled by the font change and the numerical superscript which indicates that it is the first (i.e.', 'noun entry) homograph; additional notation exists for word senses within homographs.', 'On the typesetting tape, font control characters are indicated by hexadecimal numbers within curly brackets.', 'In addition, there is a further complication because this sense is used in the plural and the plural morpheme must be removed before RIVET can be associated with a dictionary entry.', 'However, the restructuring program can achieve this because such morphology is always italicised, so the program knows that, in the context of non-core vocabulary items, the italic font control character signals the Figure 3 occurrence of a morphological variant of a LDOCE head entry.']"	0
CC745	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	complement types in english	['Robert Ingria']		"223 p.Thesis (Ph.D.)--University of Illinois at Urbana-Champaign, 1971.U of I OnlyRestricted to the U of I community idenfinitely during batch ingest of legacy ETD"	#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .	"['The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.', 'Michiels (1982) and Akkerman et al. (1985) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description.', '#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .']"	1
CC746	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	syntactic argumentation and the structure of english	"['D M Perlmutter', 'S Soames']"		"Syntactic Argumentation and the Structure of English (SASE) presents the major theoretical developments in generative syntax and the empirical arguments motivating them. Beautifully and lucidly written, it is an invaluable resource for working linguists as well as a pedagogical tool of unequaled depth and breadth. The chief focus of the book is syntactic argumentation. Beginning with the fundamentals of generative syntax, it proceeds by a series of gradually unfolding arguments to analyses of some of the most sophisticated proposals. It includes a wide variety of problems that guide the reader in constructing arguments deciding between alternative analyses of syntactic constructions and alternative theoretical formulations. Someone who has worked through the problems and arguments in this book will be able to apply the skills in argumentation it develops to novel issues in syntax. While teaching syntactic argumentation, SASE covers the major empirical results of generative syntax. Its contents include: Transformations in single-clause sentences; Complementation and multi-clause transformations; Universal principles governing rule interaction: the cycle and strict cyclicity; Movement rules; Ross' constraints; Pronominal reference and anaphora. SASE is an important book for several different audiences: for students, it is an introduction to syntax that teaches argumentation as well as a wide range of empirical results in the field; for linguists, it is a sourcebook of classical analyses and arguments, with some new arguments bearing on classical issues; and, for scholars, teachers, and students in related fields, it is a comprehensive guide to the major empirical and theoretical developments in generative syntax. SASE contains enough material for a two-semester or three-quarler sequence in syntax. Because it assumes no previous background, it can be used as the main text in an introduction to syntax. Since it covers a wide range of material not available in other texts, it is also suitable for intermediate and advanced syntax courses and as a supplementary source in more specialized courses and courses in other disciplines. A storehouse of classical and original arguments, SASE will prove to be of lasting value to the teacher, the student, and researchers in both linguistics and related fields."	"Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #AUTHOR_TAG:472 ), but these are the only ones which are explicit in the LDOCE coding system."	"['Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #AUTHOR_TAG:472 ), but these are the only ones which are explicit in the LDOCE coding system.']"	0
CC747	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	word formation in natural language processing systems	['Roy Byrd']	introduction		"Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources ."	"['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', 'Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', 'In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']"	1
CC748	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	a dictionary and morphological analyser for english	"['Graham Russell', 'Steve Pulman', 'Graeme Ritchie', 'Alan Black']"		"This paper describes the current state of a three-year project aimed at the development of software for use in handling large quantit ies of dictionary information within natural language processing systems. 1 The project was accepted for funding by SERC/Alvey commencing ill June 1984, and is being carried out by Graeme Ritchie and Alan Black at the Universi ty of Edinburgh and Steve Puhnan and Graham Russell at the Universi ty of Cambridge. It is one of three closely related projects funded under the Alvey IKBS Programme (Natural Language Tlleme); a parser is under development at Edinburgh by Henry Thompson and John Phillips, and a sentence grammar is being devised by Ted Briscoe and Clare Grover at Lancaster and Bran Boguraev and John Carroll at Cambridge. It is intended tha t the software and rules produced by all three projects wil l be directly compatible and capable of functioning in an integrated system. Realistic and useful na tura l language processing systems such as database f ront-ends require large numbers of words, together wi th associated syntactic and semantic Information, to be efficiently stored in machine-readable form. Our system is Intended to provide the necessary facilities, being designed to store a large number (at least 10,000) of words and to perform morphological analysis on them, covering both Inflectional and derlvatlonal morphology. In pursuit of these objectives, the dictionary associates wi th each word information concerning its morphosyntactlc properties. Users are free to modify the system In a number of ways; they may add to the lexical entries Lisp functions tha t perform semantic manipulatlons, and tailor the dictionary to the particular subject mat ter they are interested in (different databases, for example). I t Is also hoped that the system is general enough to be of use to linguists wishing to Investigate the morphology of English and other languages. Contents of the basle data files may be altered or replaced: 1. A 'Word Grammar ' file contains rules assigning internal s t ructure to complex words, 2. A 'Lexicon' file holds the morpheme entries which include syntactic and other Information associated wi th stems and affixes. 3. A 'Spelling Rules' file contains rules governing permissible correspondences between the form of morphemes listed in the lextcon and complex words consisting of sequences of these morphemes. Once these data flies have been prepared, they are compiled using a number of pre-processtng functions tha t operate to produce a set of output files. These constitute a fu l ly expanded and cross-Indexed dictionary which can then be accessed from within LISP. The process of morphological analysis consists of parslng a sequence of Input morphemes wi th respect to the word grammar, It Is Implemented as an active chart parser (Thompson & Rltchle (1984)), and builds a s t ructure in the form of a tree in which each node has two"	"No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #AUTHOR_TAG ) ."	"['The program which transforms the LDOCE grammar codes into lexical entries utilisable by a parser takes as input the decompacted codes and produces a relatively theory neutral representation of the lexical entry for a particular word, in the sense that this representation could be further transformed into a format suitable for most current parsing systems.', 'For example, if the input were the third sense of believe, as in Figure 4, the program would generate the (partial) entry shown in Figure 8  Figure 8 At the time of writing, rules for producing adequate entries to drive a parsing system have only been developed for verb codes.', 'In what follows we will describe the overall transformation strategy and the particular rules we have developed for the verb codes.', 'Extending the system to handle nouns, adjectives and adverbs would present no problems of principle.', 'However, the LDOCE coding of verbs is more comprehensive than elsewhere, so verbs are the obvious place to start in an evaluation of the usefulness of the coding system.', 'No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #AUTHOR_TAG ) .']"	0
CC749	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	towards a lexicon support environment for real time parsing	"['Hiyan Alshawi', 'Branimir Boguraev', 'Ted Briscoe']"	introduction		In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE .	"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '(Michiels (1982) contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE .']"	0
CC750	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	a critical assessment of the ldoce coding system to appear in	['Erik Akkerman']			"One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #AUTHOR_TAG ) ."	"['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment).', 'One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #AUTHOR_TAG ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', 'The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', 'In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing.']"	0
CC751	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	the design of a computer language for linguistic information	['S Shieber']		"A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics. It has been the goal of the PATR group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate.Engineering and Applied Science"	To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #AUTHOR_TAG and references therein ) .	"['The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #AUTHOR_TAG and references therein ) .', 'PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.', 'We The latter employs a grammatical formalism based on GPSG; the comparatively theory neutral lexical entries that we construct from LDOCE should translate straightforwardly into this framework as well.']"	5
CC752	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	designing a computerised lexicon for linguistic purposes	"['Erik Akkerman', 'Pieter Masereeuw', 'Willem Meijs']"			"One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 ) ."	"['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment).', 'One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', 'The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', 'In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing.']"	0
CC753	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	natural language information processing	['N Sager']	introduction	"Our aim is to extract information about literary characters in unstructured texts. We employ natural language processing and reasoning on domain ontologies. The first task is to identify the main characters and the parts of the story where these characters are described or act. We illustrate the system in a scenario in the folktale domain. The system relies on a folktale ontology that we have developed based on Propp's model for folktales morphology.Comment: IEEE 11 International Conference on Intelligent Computer   Communication and Processing (ICCP2015), Cluj-Napoca, Romania, 3-5 September   201"	"Two exceptions to this generalisation are the Linguistic String Project ( #AUTHOR_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources ."	"['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', 'Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project ( #AUTHOR_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', 'In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']"	1
CC754	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	database design for a dictionary of the future	['Frank Tompa']			"Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #AUTHOR_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape ."	"['There is a well recognised problem with providing computational support for machine readable dictionaries, in particular where issues of access are concerned.', ""On the one hand, dictionaries exhibit far too much structure for conventional techniques for managing 'flat' text to apply to them."", 'On the other hand, the equally large amounts of free text in dictionary entries, as well as the implicitly marked relationships commonly used to encode linguistic information, makes a dictionary difficult to represent as a structured database of a standard, eg.', 'relational, type.', 'In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natural language processing systems, we need to provide fast access from Lisp to data held in secondary storage.', 'Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #AUTHOR_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .']"	0
CC755	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	computer exploitation of ldoces grammatical codes paper presented at a conference on survey of english language	"['A Moulin', 'J Jansen', 'A Michiels']"			"In addition , #AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly ."	"['The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require.', 'None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule.', 'In addition , #AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .', 'Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.', '""intend""), however, when it is used in this sense it must be treated as an Object Equi verb.']"	1
CC756	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	and forthcoming machine readable dictionaries and research in computational linguistics	['Branimir Boguraev']	introduction		"In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG for details ) ."	"['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', 'Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG for details ) .', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']"	0
CC757	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	lexicalfunctional grammar a formal system for grammatical representation in jbresnan ed the mental representation of grammatical relations	"['Ronald Kaplan', 'Joan Bresnan']"	introduction		"Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language ."	"['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects (Boguraev, 1987;Russell et al., 1986;Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological and syntactic analyser for English.', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']"	0
CC758	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	a key to the brandeis verb catalog unpublished mimeo under nsf grant ist8420073 quotinformation structure of a natural language lexiconquot	"['Ray Jackendoff', 'Jane Grimshaw']"			This deficiency is rectified in the verb classification system employed by #AUTHOR_TAG in the Brandeis verb catalogue .	"['Prefer is misclassified as Object Raising, rather than as Object Equi, because the relevant code field contains a T5 code, as well as a V3 code.', 'The T5 code is marked as \'rare\', and the occurrence of prefer with a tensed sentential complement, as opposed to with an infinitive, is certainly marginal:  This example also highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as ""would"".', 'This deficiency is rectified in the verb classification system employed by #AUTHOR_TAG in the Brandeis verb catalogue .']"	1
CC759	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	a dictionary and morphological analyser for english	"['Graham Russell', 'Steve Pulman', 'Graeme Ritchie', 'Alan Black']"	introduction	"This paper describes the current state of a three-year project aimed at the development of software for use in handling large quantit ies of dictionary information within natural language processing systems. 1 The project was accepted for funding by SERC/Alvey commencing ill June 1984, and is being carried out by Graeme Ritchie and Alan Black at the Universi ty of Edinburgh and Steve Puhnan and Graham Russell at the Universi ty of Cambridge. It is one of three closely related projects funded under the Alvey IKBS Programme (Natural Language Tlleme); a parser is under development at Edinburgh by Henry Thompson and John Phillips, and a sentence grammar is being devised by Ted Briscoe and Clare Grover at Lancaster and Bran Boguraev and John Carroll at Cambridge. It is intended tha t the software and rules produced by all three projects wil l be directly compatible and capable of functioning in an integrated system. Realistic and useful na tura l language processing systems such as database f ront-ends require large numbers of words, together wi th associated syntactic and semantic Information, to be efficiently stored in machine-readable form. Our system is Intended to provide the necessary facilities, being designed to store a large number (at least 10,000) of words and to perform morphological analysis on them, covering both Inflectional and derlvatlonal morphology. In pursuit of these objectives, the dictionary associates wi th each word information concerning its morphosyntactlc properties. Users are free to modify the system In a number of ways; they may add to the lexical entries Lisp functions tha t perform semantic manipulatlons, and tailor the dictionary to the particular subject mat ter they are interested in (different databases, for example). I t Is also hoped that the system is general enough to be of use to linguists wishing to Investigate the morphology of English and other languages. Contents of the basle data files may be altered or replaced: 1. A 'Word Grammar ' file contains rules assigning internal s t ructure to complex words, 2. A 'Lexicon' file holds the morpheme entries which include syntactic and other Information associated wi th stems and affixes. 3. A 'Spelling Rules' file contains rules governing permissible correspondences between the form of morphemes listed in the lextcon and complex words consisting of sequences of these morphemes. Once these data flies have been prepared, they are compiled using a number of pre-processtng functions tha t operate to produce a set of output files. These constitute a fu l ly expanded and cross-Indexed dictionary which can then be accessed from within LISP. The process of morphological analysis consists of parslng a sequence of Input morphemes wi th respect to the word grammar, It Is Implemented as an active chart parser (Thompson & Rltchle (1984)), and builds a s t ructure in the form of a tree in which each node has two"	"The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English ."	"['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']"	0
CC760	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	exploiting a large dictionary data base	['Archibal Michiels']			"There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #AUTHOR_TAG , for further details ) ."	"['There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #AUTHOR_TAG , for further details ) .', 'However, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries.']"	0
CC761	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	the automatic grammatical tagging of the lob corpus	"['Geoffrey Leech', 'Roger Garside', 'Erik Atwell']"	conclusion	"In collaboration with the English Department, University of Oslo, and the Nowegian Computing Centre for the Humanities, Bergen we have been engaged in the automatic grammatical tagging of the LOB (LancasterOslo/Bergen) Corpus of British English. The computer programs for this task are running at a success rate of approximately 96.7% and a substantial part of the 1,000,000-word corpus has already been tagged. The purpose of this paper is to give an account of the project, with special reference to the methods of tagging we have adopted."	"In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG ) ."	"['In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system.', 'A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis.', 'However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system.', 'This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis.', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG ) .', 'However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.']"	3
CC762	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	diagram a grammar for dialogues	['Jane Robinson']	introduction		"#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand ."	"['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', '#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .', 'Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.', 'In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']"	1
CC763	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	predication linguistic inquiry	['E S Williams']			"Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #AUTHOR_TAG , for further discussion ) ."	"['The solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense.', 'In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.', 'In these situations the parser can use the semantic type of the verb to compute this relationship.', 'Expanding on a suggestion of Michiels (1982), we classify verbs as Subject Equi, Object Equi, Subject Raising or Object Raising for each sense which has a predicate complement code associated with it.', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.', 'For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #AUTHOR_TAG , for further discussion ) .']"	0
CC764	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	exploiting a large dictionary data base	['Archibal Michiels']	introduction		( #AUTHOR_TAG contains further description and discussion of LDOCE . )	"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE . )', 'In this paper we focus on the exploitation of the LDOCE grammar coding system; Alshawi et al. (1985) and Alshawi (1987) describe further research in Cambridge utilising different types of information available in LDOCE.']"	0
CC765	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	exploiting a large dictionary data base	['Archibal Michiels']			"Expanding on a suggestion of #AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it ."	"['The solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense.', 'In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.', 'In these situations the parser can use the semantic type of the verb to compute this relationship.', 'Expanding on a suggestion of #AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.', 'For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.', 'Michiels proposed rules for doing this for infinitive complement codes; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP, AP and PP predication (see Williams (1980), for further discussion).']"	2
CC766	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	an information theoretic analysis of phonetic dictionary access computer speech and language	['David Carter']			"In addition to headwords , dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) ."	"['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) .', 'In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987).', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.']"	5
CC767	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	a grammar of contemporary english longman group limited	"['Randolph Quirk', 'Sidney Greenbaum', 'Geoffrey Leech', 'Jan Svartvik']"			"The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #AUTHOR_TAG , 1985 ) ."	"['Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #AUTHOR_TAG , 1985 ) .', 'The codes are doubly articulated; capital letters represent the grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in.', 'Most of the subcategorisation frames are specified by syntactic category, but some are very ill-specified; for instance, 9 is defined as ""needs a descriptive word or phrase"".', 'In practice many adverbial and predicative complements will satisfy this code, when attached to a verb; for example, put [xg] where the code marks a locative adverbial prepositional phrase vs. make under sense 14 (hereafter written make( 14)) is coded IX9] where it marks a predicative noun phrase or prepositional phrase.']"	2
CC768	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	automatic analysis of texts in informatics 7	['Archibal Michiels']			"Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #AUTHOR_TAG ) ."	"['Given that we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp, and since the natural data structure for Lisp is the s-expression, we adopted the approach of converting the tape source into a set of list structures, one per entry.', 'Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #AUTHOR_TAG ) .']"	0
CC769	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	designing a computerised lexicon for linguistic purposes	"['Erik Akkerman', 'Pieter Masereeuw', 'Willem Meijs']"			Michiels ( 1982 ) and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .	"['The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.', 'Michiels ( 1982 ) and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.']"	0
CC770	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	the design of a computer language for linguistic information	['S Shieber']	introduction	"A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics. It has been the goal of the PATR group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate.Engineering and Applied Science"	"Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language ."	"['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects (Boguraev, 1987;Russell et al., 1986;Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological and syntactic analyser for English.', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']"	0
CC771	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	phonotactic and lexical constraints in speech recognition	"['Daniel Huttenlocher', 'Victor Zue']"		"We demonstrate a method for partitioning a large lexicon into small equivalence classes, based on sequential phonetic and prosodic constraints. The representation is attractive for speech recognition systems because it allows all but a small number of word candidates to be excluded, using only gross phonetic and prosodic information. The approach is a robust one in that the representation is relatively insensitive to phonetic variability and recognition error."	"In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #AUTHOR_TAG ) ."	"['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #AUTHOR_TAG ) .', 'In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987).', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.']"	0
CC772	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	a comprehensive grammar of english longman group limited	"['Randolph Quirk', 'Sidney Greenbaum', 'Geoffrey Leech', 'Jan Svartvik']"			"The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG ) ."	"['In what follows we will discuss the format of the grammar codes in some detail as they are the focus of the current paper, however, the reader should bear in mind that they represent only one comparatively constrained field of an LDOCE entry and therefore, a small proportion of the overall restructuring task.', 'Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.', 'Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG ) .', 'A grammar code describes a particular pattern of behaviour of a word.', 'Patterns are descriptive, and are used to convey a range of information: eg.', 'distinctions between count and mass nouns (dog vs. desire), predicative, postpositive and attributive adjectives (asleep vs. elect vs. jokular), noun complementation (fondness, fact) and, most importantly, verb complementation and valency.']"	0
CC773	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	syntactic argumentation and the structure of english	"['D M Perlmutter', 'S Soames']"		"Syntactic Argumentation and the Structure of English (SASE) presents the major theoretical developments in generative syntax and the empirical arguments motivating them. Beautifully and lucidly written, it is an invaluable resource for working linguists as well as a pedagogical tool of unequaled depth and breadth. The chief focus of the book is syntactic argumentation. Beginning with the fundamentals of generative syntax, it proceeds by a series of gradually unfolding arguments to analyses of some of the most sophisticated proposals. It includes a wide variety of problems that guide the reader in constructing arguments deciding between alternative analyses of syntactic constructions and alternative theoretical formulations. Someone who has worked through the problems and arguments in this book will be able to apply the skills in argumentation it develops to novel issues in syntax. While teaching syntactic argumentation, SASE covers the major empirical results of generative syntax. Its contents include: Transformations in single-clause sentences; Complementation and multi-clause transformations; Universal principles governing rule interaction: the cycle and strict cyclicity; Movement rules; Ross' constraints; Pronominal reference and anaphora. SASE is an important book for several different audiences: for students, it is an introduction to syntax that teaches argumentation as well as a wide range of empirical results in the field; for linguists, it is a sourcebook of classical analyses and arguments, with some new arguments bearing on classical issues; and, for scholars, teachers, and students in related fields, it is a comprehensive guide to the major empirical and theoretical developments in generative syntax. SASE contains enough material for a two-semester or three-quarler sequence in syntax. Because it assumes no previous background, it can be used as the main text in an introduction to syntax. Since it covers a wide range of material not available in other texts, it is also suitable for intermediate and advanced syntax courses and as a supplementary source in more specialized courses and courses in other disciplines. A storehouse of classical and original arguments, SASE will prove to be of lasting value to the teacher, the student, and researchers in both linguistics and related fields."	"Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #AUTHOR_TAG:460 ff . )"	"['This small experiment demonstrates a number of points.', 'Firstly, it seems reasonable to conclude that the assignment of individual codes to verbs is on the whole relatively accurate in LDOCE.', 'Of the 139 verbs tested, we only found code omissions in 10 cases.', 'Secondly though, when we consider the interaction between the assignments of codes and word sense classification, LDOCE appears less reliable.', 'This is the primary source of error in the case of the Object Raising rule.', 'Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system.', 'Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #AUTHOR_TAG:460 ff . )', 'However, only two of these criteria are explicit in the coding system.']"	3
CC774	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	a multipurpose interface to an online dictionary	"['Branimir Boguraev', 'David Carter', 'Ted Briscoe']"			"In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG ) ."	"['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords, dictionary search through the pronunciation field is available; Carter (1987) has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure (Huttenlocher and Zue, 1983).', 'In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG ) .', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.']"	0
CC775	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	the grammar of english predicate complement constructions	['P S Rosenbaum']		"A set of phrase structure rules and a set of transformational rules are proposed for which the claim is made that these rules enumerate the underlying and derived sentential structures which exemplify two productive classes of sentential embedding in English. These are sentential embedding in noun phrases and sentential embedding in verb phrases. First, following a statement of the grammatical rules, the phrase structure rules are analyzed and defended. Second, the transformational rules which map the underlying structures generated by the phrase structure rules onto appropriate derived structures are justified with respect to noun phrase and verb phrase complementation. Finally, a brief treatment is offered for the extension of the proposed descriptive apparatus to noun phrase and verb phrase complementation in predicate adjectival constructions. Thesis Supervisor: Noam Chomsky Title: Professor of Modern Languages"	We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ) .	"['We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ) .', 'Figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system.']"	5
CC776	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	processing dictionary definitions with phrasal pattern hierarchies in this issue	['Hiyan Alshawi']		"This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns. An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English. A property of this dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions. The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary. Examples illustrating the output generated are presented, and some qualitative performance results and problems that were encountered are discussed. The analysis process applies successively more specific phrasal analysis rules as determined by a hierarchy of patterns in which less specific patterns dominate more specific ones. This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible, resulting in a relatively robust analysis mechanism. Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems: coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions."	"As #AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate."	"['A series of systems in Cambridge are implemented in Lisp running under UnixTM.', 'They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams.', 'A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved.', 'As #AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.', 'To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls.']"	0
CC777	J87-3002	Annotation-based finite state processing in a large-scale NLP arhitecture	a natural language toolkit reconciling theory with practice	['Branimir Boguraev']	introduction	"Generalized Phrase Structure Grammar (GPSG) is a highly restrictive theory of natural language syntax, characterised by complex interaction between its various rule types and constraints. Motivated by desire for declarative semantics, the theory defines these as applying simultaneously in the process of licensing local trees. As a result, as far as practical implementations of GPSG are concerned, the theory loses its apparent efficient parsability and becomes computationally intractable. This paper describes one aspect of an UK collaborative effort to produce a general purpose morphological and syntactic analyser for English within the theoretical framework of Generalized Phrase Structure Grammar, namely the development of a tractable grammatical formalism with clear semantics, capable of supporting the task of writing a substantial grammar. The paper outlines the intellectual and pragmatic background of the development effort and traces the incremental evolution of this formalism, following discussions concerning the fundamental issues of rules interpretation, feature system, grammar organisation, parser strategy, environment for grammar writing and support, and the construction of a lexicon linked to the grammar. Particular emphasis is placed on the quesiton of how theoretical standpoints have been reconciled with practical constraints, and how the commitment to deliver a functional morphological and syntactic analyser of wide scope and coverage of English has influenced the current state of the grammatical formalism."	"The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English ."	"['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']"	0
CC778	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	synthesis of speech from unrestricted text	['J Allen']	experiments	"E diatribe ekhei san stokho ten melete ton prosodikon kanonon tes ellenikes glossas. Ta apotelesmata mporoun na ensomatothoun se opoiodepote sustema suntheses omilias anexarteta apo ten epilegmene strategike suntheses. E prosodia parousiazetai san polumetrike sunartese tes themeliodous sukhnotetas tes entases kai tes diarkeias ton phonematon kai parousiazontai montela se epipedo lexes toso gia argo oso kai gia gregoro ruthmo ekphoras. Epises parousiazontai montela kai kanones se epipleon protaseis me tropo pou exantlei ola ta suntaktika phainomena tes ellenikes. Proteinetai oti e prosodia se epipedo protases mporei na suntethei apo montela epipedou lexes upertithemena pano se mia pherousa se epipedo protases e klise tes opoias exartatai apo ten uparxe phainomenon emphases.This thesis aims at the study of the prosodic rules of the Greek language for use in a text to speech synthesis from unrestricted text. Regardless of the underlying synthesis stratregie (diphones, phonemes, etc). Prosody is treated as a polymetric function of fundamental frequency, intensity and duration of the phonemes. Prosodic models are presented first for isolated intonation words for various tempos including slow and fast. Models for large sentences are also presented in a way that all syntactic phenomena of the language are included and respected. It is suggested that sentence level prosodic models can be derived and synthesized from word-level models that are superimposed on a carrier spanning the whole sentence. The trend of the carrier is dependent upon various emphatic phenomena such as local stress or sentence emphasis"	"Many investigators ( e.g. #AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech ."	"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. #AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']"	4
CC779	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	texttospeechan overview	"['S P Olive', 'M Y Liberman']"	introduction		"In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) ."	"['In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']"	0
CC780	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	from sad to glad emotional computer voices	['J Cahn']	experiments	"Synthesized English speech is readily distinguished from human speech on the basis of inappropriate intonation and insu cient expressiveness. This is a drawback for conversational computer systems. Intonation is the carrier of emphasis or de-emphasis, serving to clarify meaning for the spoken word much as variations in typeface and punctuation do for the written word. Expressiveness is not tied to word or phrase meaning but is global in scope. It provides the context in which the intonation occurs, and reveals the speaker's intentions and general mental state. In synthesized speech, intonation makes the message easier to understand; enhanced expressiveness contributes to dramatic e ect, making the message easier to listen to."	"Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech ."	"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']"	4
CC781	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	texttospeechan overview	"['S P Olive', 'M Y Liberman']"	experiments		We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .	"['We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'Two concerns motivated our implementation.', 'First, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences.', 'Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text-to-speech synthesizer.']"	5
CC782	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	now lets talk about now identifying cue phrases intonationally	"['J Hirschberg', 'D Litman']"	introduction	"Cue phrases are words and phrases such as now and by the way which may be used to convey explicit information about the structure of a discourse. However, while cue phrases may convey discourse structure, each may also be used to different effect. The question of how speakers and hearers distinguish between such uses of cue phrases has not been addressed in discourse studies to date. Based on a study of now in natural recorded discourse, we propose that cue and non-cue usage can be distinguished intonationally, on the basis of phrasing and accent."	#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .	"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']"	0
CC783	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	aspects of the theory of syntax	['N Chomsky']	introduction	Abstract : Contents: Methodological preliminaries: Generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammars; formal and substantive grammars; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning; generative capacity and its linguistic relevance Categories and relations in syntactic theory: Scope of the base; aspects of deep structure; illustrative fragment of the base component; types of base rules Deep structures and grammatical transformations Residual problems: Boundaries of syntax and semantics; structure of the lexicon	"Sentences like 12 , from #AUTHOR_TAG , are frequently cited ."	"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12 , from #AUTHOR_TAG , are frequently cited .', '(Square brackets mark off the NP constituents that contain embed- ded sentences.)']"	0
CC784	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	the contribution of parsing to prosodic phrasing in an experimental texttospeech system	"['J Bachenko', 'E Fitzpatrick', 'C E Wright']"	introduction	"While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody, the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody. We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules. The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries. These specifications are converted to annotations for the Bell Labs text-to-speech system that dictate modulations in pitch and duration for the input sentence.We discuss the results of an experiment to determine the performance of our system. We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate."	"In previous work ( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) ."	"['In previous work ( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']"	2
CC785	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	finitestate parsing of phrasestructure languages and the status of readjustment rules in grammar	['D T Langendoen']	introduction		"#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model ."	"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']"	0
CC786	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	performance structures a psycholinguistic and linguistic appraisal	"['J P Gee', 'F Grosjean']"	introduction		"The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and #AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency ."	"['The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and #AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths; this is the case in Chickens were eating I[ the remaining green vegetables, where the subject-predicate boundary finds no prosodic correspondent. 4']"	0
CC787	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	on stress and linguistic rhythm	"['M Y Liberman', 'A Prince']"	introduction	"JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. The MIT Press is collaborating with JSTOR to digitize, preserve and extend access to Linguistic Inquiry."	"3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) ."	"['Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.', '3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .', 'This approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing.']"	1
CC788	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	the sound pattern of english	"['N Chomsky', 'M Halle']"	introduction	"Since this classic work in phonology was published in 1968, there has been no other book that gives as broad a view of the subject, combining generally applicable theoretical contributions with analysis of the details of a single language. The theoretical issues raised in The Sound Pattern of English continue to be critical to current phonology, and in many instances the solutions proposed by Chomsky and Halle have yet to be improved upon.Noam Chomsky and Morris Halle are Institute Professors of Linguistics and Philosophy at MIT."	"In #AUTHOR_TAG , this flattening process is not part of the grammar ."	"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In #AUTHOR_TAG , this flattening process is not part of the grammar .', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']"	0
CC789	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	on stress and linguistic rhythm	"['M Y Liberman', 'A Prince']"	experiments	"JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. The MIT Press is collaborating with JSTOR to digitize, preserve and extend access to Linguistic Inquiry."	"An alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree ."	"['Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .', 'Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing.']"	1
CC790	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	prosodic systems and intonation in english	['D Crystal']	introduction	Preface 1. Some preliminary considerations 2. Past work on prosodic features 3. Voice-quality and sound attributes in prosodic study 4. The prosodic features of English 5. The intonation system of English 6. The grammar of intonation 7. The semantics of intonation Bibliography Index of persons Index of subjects.	"#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct ."	"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']"	0
CC791	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	speech rhythm its relation to performance universals and articulatory timing	['G Allen']	introduction		"The psycholinguistic studies of Martin ( 1970 ) , #AUTHOR_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency ."	"['The psycholinguistic studies of Martin ( 1970 ) , #AUTHOR_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths; this is the case in Chickens were eating I[ the remaining green vegetables, where the subject-predicate boundary finds no prosodic correspondent. 4']"	0
CC792	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	capacity demands in shortterm memory for synthetic and natural speech	"['P A Luce', 'T C Feustel', 'D B Pisoni']"	experiments	"Three experiments were performed that compared recall for synthetic and natural lists of monosyllabic words. In the first experiment, presentation intervals of 1, 2, and 5 s per word were used. Although free recall was consistently poorer overall for the synthetic lists at all presentation rates, the decrement for synthetic stimuli did not increase differentially with faster rates. In a second experiment, zero, three, and six digits were presented visually for retention prior to free recall of each spoken word list in a preload paradigm. Fewer subjects were able to correctly recall all of the digits for the six-digit list than the three-digit list when the following word lists were synthetic. The third experiment required ordered recall of lists of natural and synthetic words. Differences in ordered recall between the synthetic and natural word lists were substantially larger for the primacy portion of the serial position curve than the recency portion. These results indicate that difficulties observed in the perception and comprehension of synthetic speech are due, in part, to increased processing demands in short-term memory."	"Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech ."	"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']"	4
CC793	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	toward treating english nominals correctly	"['R W Sproat', 'M Y Liberman']"	experiments	We describe a program for assigning correct stress contours to nominals in English. It makes use of idiosyncratic knowledge about the stress behavior of various nominal types and general knowledge about English stress rules. We have also investigated the related issue of parsing complex nominals in English. The importance of this work and related research to the problem of text-to-speech is &apos;discussed	"Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing ."	"['Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .']"	1
CC794	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	the contribution of parsing to prosodic phrasing in an experimental texttospeech system	"['J Bachenko', 'E Fitzpatrick', 'C E Wright']"	introduction	"While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody, the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody. We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules. The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries. These specifications are converted to annotations for the Bell Labs text-to-speech system that dictate modulations in pitch and duration for the input sentence.We discuss the results of an experiment to determine the performance of our system. We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate."	"Previous versions of our work , as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure ."	"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work , as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']"	2
CC795	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	syntax and speech	"['W Cooper', 'J Paccia-Cooper']"	introduction	"Interactions between phonology and syntax are inspected in continuous speech samples from 30 speech-delayed children. Two types of interactions are examined: The co-occurrence of speech and language delay and the effects of phonological reduction on the realization of phonetically complex morphophonemes. Four possible patterns of association between the phonological and syntactic systems are outlined, and subjects are assigned to these patterns based on their phonological and syntactic performance. Results indicate that two-thirds of the subjects display evidence of overall syntactic delay, whereas half show some limitation in the use of phonetically complex morphophonemes, their performance in that area being below the level of their syntactic production. Implications of these findings for a theory of speech delay and for management programming are discussed"	"This observation has led some researchers , e.g. , #AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase ."	"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers , e.g. , #AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']"	0
CC796	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	performance structures a psycholinguistic and linguistic appraisal	"['J P Gee', 'F Grosjean']"	introduction		"Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) ."	"['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']"	1
CC797	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	aspects of prosody	['J Bing']	introduction		"The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse ."	"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']"	0
CC798	J90-3003	Prosodic phrasing for speech synthesis of written telecommunications by the deaf	prosodic structure and spoken word recognition	"['F Grosjean', 'J P Gee']"	introduction		"Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) ."	"['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.', 'This is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree.', 'Function words, e.g.', 'auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts.', 'Content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone.']"	5
CC799	J91-2003	On compositional semantics	logic and conversationquot	['H P Grice']	introduction		"Secondly , the cooperative principle of #AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader ."	"['However, there are at least three arguments against iterating PT.', 'First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.', 'Secondly , the cooperative principle of #AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .', 'Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.']"	4
CC800	J91-2003	On compositional semantics	coherence and coreferencequot	['J R Hobbs']	introduction		"We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence ."	"['At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .', 'The quoted works seem to be good representatives for each of the directions; they also point to related literature.']"	1
CC801	J91-2003	On compositional semantics	domain circumscription a reevaluationquot	"['D W Etherington', 'R E Mercer']"	introduction		"Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG), and their kin."	"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG), and their kin.', 'Similarly, the notion of R+ M-abduction is spiritually related to the ""abduc- tive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of Berwick (1986).', 'But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']"	1
CC802	J91-2003	On compositional semantics	the boundaries of words and their meaningsquot	['W Labov']	introduction		W. #AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it .	['W. #AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it .']	0
CC803	J91-2003	On compositional semantics	a grammar of contemporary english	"['R Quirk', 'S Greenbaum', 'G Leech', 'J Svartvik']"	introduction	"The publication of this important volume fills the need for an up-to-date survey of the entire scope of English syntax. Though it falls short of a perfectly balanced treatment of the whole system, it touches upon all the essential topics and treats in depth a number of crucial problems of current interest such as case, ellipsis, and information focus. Even the publishers' claims are vindicated to a surprising degree. The statement that it ""constitutes a standard reference grammar"" is reasonably well justified. Recent investigations, including the authors' own research, are integrated into the ""accumulated grammatical tradition"" quite effectively. But whether it is ""the fullest and most comprehensive synchronic description of English grammar ever written"" is arguable. No one acquainted with Poutsma's work would agree with that. Very advanced foreign students o r native speakers of English who want to learn about basic grammar will find some of thel sections suitable for their needs, such as the lesson about restrictive and nonrestrictive relative clauses, though even here some of the explanations require very intensive study. Most of the chapters are rather like an advanced textbook for teachers or linguists. The organization and viewpoint give the impression of a carefully planned university lecture supplemented by diagrams, charts, and lists. A good example is the lesson on auxiliaries and verb phrases, which starts with a set of sample sentences demonstrating that ""should see"" and ""happen to see"" behave differently under various transformations and expansions. After the essential concepts are explained and exemplified-lexical verb, semi-auxiliary, operator, and the like-lists and paradigms are given as in the usual reference work. A particularly useful feature of this chapter is the outline of modal auxiliaries with examples of their divergent meanings."	"Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) ."	"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .']"	0
CC804	J91-2003	On compositional semantics	cohesion in english	"['M A K Halliday', 'R Hasan']"	introduction	"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good"	"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure ."	"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']"	1
CC805	J91-2003	On compositional semantics	a dictionary of modern english usage	['H W Fowler']	introduction		"This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence ."	"['Still, our definition of coherence may not be restrictive enough: two collections of sentences, one referring to ""black"" (about black pencils, black pullovers, and black poodles), the other one about ""death"" (war, cancer, etc.), connected by a sentence referring to both of these, could be interpreted as one paragraph about the new, broader topic ""black + death.""', ""This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence .""]"	0
CC806	J91-2003	On compositional semantics	mental models	['P N Johnson-Laird']	introduction	"The complexity of conceptualizing mental models has made Virtual Reality an interesting way to enhance communication and understanding between individuals working together on a project or idea. Here, the authors discuss practical applications of using VR for this purpose"	"This Principle of Finitism is also assumed by #AUTHOR_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics ."	"['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by #AUTHOR_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']"	1
CC807	J91-2003	On compositional semantics	a theory of truth and semantic representationquot	['H Kamp']	introduction		"But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."	"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"	1
CC808	J91-2003	On compositional semantics	so what can we talk about nowquot	['B Webber']	introduction	Impure water is made suitable for drinking in an apparatus comprising a pressurizable holding tank attached to a purification cartridge containing an impurities adsorbent and a fine filter. A gas-containing cartridge is pierced to provide a bactericidal gas for killing pathogenic microorganisms and for pressurizing the holding tank to force the water through the purification cartridge.	"Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role , too ."	"['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']"	0
CC809	J91-2003	On compositional semantics	coherence and coreferencequot	['J R Hobbs']	introduction		"According to #AUTHOR_TAG , p. 67 ) , these two sentences are incoherent ."	"['According to #AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .', 'However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.)', 'suddenly (for Hobbs) becomes coherent.', ""It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added."", 'On the other hand, this change is easily explained when we treat the first two sentences as a paragraph: if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'And the paragraph obtained by adding the third sentence is coherent.', 'Moreover, coherence here is clearly the result of the existence of the topic ""John likes spinach.""']"	1
CC810	J91-2003	On compositional semantics	artificial intelligence the very idea	['J Haugeland']	introduction	"The idea that human thinking and machine computing are ""radically the same"" provides the central theme for this marvelously lucid and witty book on what artificial intelligence is all about. Although presented entirely in nontechnical"	"For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap"	"['We adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense reasoning.', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap']"	0
CC811	J91-2003	On compositional semantics	the episode schema in story processingquot	"['K Haberlandt', 'C Berian', 'J Sandson']"	introduction		Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG .	"['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG .']"	0
CC812	J91-2003	On compositional semantics	paragraph structure inference	['E J Crothers']	introduction		"Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph ."	"['However, there are at least three arguments against iterating PT.', 'First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.', 'Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .', 'Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.']"	4
CC813	J91-2003	On compositional semantics	krypton a functional approach to knowledge representationquot	"['R J Brachman', 'R E Fikes', 'H J Levesque']"	introduction		"The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG ) ."	"['The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG ) .', 'Brachman et al. 1985).', ""KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains information expressed in a frame-based language equivalent to a fragment of FOL."", ""However, the distinction between the two parts is purely functional--that is, characterized in terms of the system's behavior."", 'From the logical point of view, the knowledge base is the union of the two boxes, i.e. a theory, and the entailment is standard.', 'In our system, we also distinguish between the ""definitional"" and factual information, but the ""definitional"" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.', 'Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, ""coherence"" and ""dominance,"" which are not variants of the standard first order entailment, but abduction.']"	1
CC814	J91-2003	On compositional semantics	abductive inferencequot	['J A Reggia']	introduction		"Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."	"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."", 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']"	1
CC815	J91-2003	On compositional semantics	the interpretation of tense in discoursequot	['B Webber']	introduction		The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .	"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.).""]"	0
CC816	J91-2003	On compositional semantics	organizational patterns in discoursequot	['J Hinds']	introduction		"According to #AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases ."	"['The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists, classifies, and discusses various types of inference, by which he means, generally, ""the linguistic-logical notions of consequent and presupposition"" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences).', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to #AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .', 'Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).', 'Segments themselves are composed of clauses and regulated by ""switching"" patterns, such as the question-answer pattern and the remark-reply pattern.']"	0
CC817	J91-2003	On compositional semantics	a theory of diagnosis from first principlesquot	['R Reiter']	introduction		"Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of #AUTHOR_TAG , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."	"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of #AUTHOR_TAG , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."", 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']"	1
CC818	J91-2003	On compositional semantics	inference without chainingquot	['A Frisch']	introduction		"Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG ; Patel-Schneider 1985 ) ."	"['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]"	1
CC819	J91-2003	On compositional semantics	the game of language	['J Hintikka']	introduction		"This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG ."	"['Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references.', ""This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG ."", 'Hintikka (1985).']"	1
CC820	J91-2003	On compositional semantics	intended models circumscription and commonsense reasoningquot	['W Zadrozny']	introduction	"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations."	"Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAGb )."	"['Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAGb ).', 'We will have, however, no need for ""strong"" notions in this paper.', 'Also, in a practical system, ""satisfies"" should be probably replaced by ""violates fewest.""']"	1
CC821	J91-2003	On compositional semantics	38 examples of elusive antecedents from published texts	['J R Hobbs']	introduction		"Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation ."	"['Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .', 'Later, Hobbs (1979Hobbs ( , 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ""salience"" in choosing facts from this knowledge base.']"	0
CC822	J91-2003	On compositional semantics	temporal ontology in natural languagequot	"['M Moens', 'M Steedman']"	introduction		The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .	"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.).""]"	0
CC823	J91-2003	On compositional semantics	intended models circumscription and commonsense reasoningquot	['W Zadrozny']	introduction	"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations."	"Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAGa , 1987b ) ."	"['Since it is the ""highest"" path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence.', 'Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAGa , 1987b ) .', 'Zadrozny 1987aZadrozny , 1987b.', 'Another theory, consisting of f~ = {el, sh2, pl, b2~ dl} and S, saying that A space vehicle came into the harbor and caused a disease~illness is less plausible according to that ordering.', 'As it turns out, f~ is never constructed in the process of building an interpretation of a paragraph containing the sentence S, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story.']"	0
CC824	J91-2003	On compositional semantics	the paragraph as a grammatical unit	['R E Longacre']	introduction		"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure ."	"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']"	1
CC825	J91-2003	On compositional semantics	analysis without actual infinityquot	['J Mycielski']	introduction		"As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG ) ."	"['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983, Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics.', 'As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG ) .', 'Mycielski 1981).']"	0
CC826	J91-2003	On compositional semantics	disambiguating prepositional phrase attachments by using online dictionary definitionsquot computational linguistics 1334251260 special issue on the lexicon	"['K Jensen', 'J-L Binot']"	introduction		"We have shown elsewhere ( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment ."	"['We have shown elsewhere ( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']"	2
CC827	J91-2003	On compositional semantics	introduction to artificial intelligence	"['E Charniak', 'D McDermott']"	introduction	"This book is an introduction on artificial intelligence. Topics include reasoning under uncertainty, robot plans, language understanding, and learning. The history of the field as well as intellectual ties to related disciplines are presented."	"The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature ."	"['The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']"	0
CC828	J91-2003	On compositional semantics	universal grammarquot	['R Montague']	introduction		"The logical notation of #AUTHOR_TAG is more sophisticated , and may be considered another possibility ."	"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of #AUTHOR_TAG is more sophisticated , and may be considered another possibility .', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"	1
CC829	J91-2003	On compositional semantics	semantics and cognition	['R Jackendoff']	introduction	"A new perspective on cognition views cortical cell assemblies linking together knowledge about actions and perceptions not only as the vehicles of integrated action and perception processing but, furthermore, as a brain basis for a wide range of higher cortical functions, including attention, meaning and concepts, sequences, goals and intentions, and even communicative social interaction. This article explains mechanisms relevant to mechanistic action perception theory, points to concrete neuronal circuits in brains along with artificial neuronal network simulations, and summarizes recent brain imaging and other experimental data documenting the role of action perception circuits in cognition, language and communication"	"This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #AUTHOR_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics ."	"['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #AUTHOR_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']"	1
CC830	J91-2003	On compositional semantics	paragraph structure inference	['E J Crothers']	introduction		"He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #AUTHOR_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) ."	"['The textualist approach to paragraph analysis is exemplified by E. J. Crothers.', 'His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #AUTHOR_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'Paragraphs therefore give hierarchical structure to sentences.', 'Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), expository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).', 'Segments themselves are composed of clauses and regulated by ""switching"" patterns, such as the question-answer pattern and the remark-reply pattern.']"	0
CC831	J91-2003	On compositional semantics	dont blame the toolquot	['W Woods']	introduction		"However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG ) ."	"['Once the word ""up"" is given its meaning relative to our experience with gravity, it is not free to ""slip"" into its opposite.', '""Up"" means up and not down ....', 'We have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model.', 'Mothers have a different role than fathers in this model, and thus there is a reason why ""Death is the father of beauty"" fails poetically while ""Death is the mother of beauty"" succeeds ....', 'It is precisely this ""grounding"" of logical predicates in other conceptual structures that we would like to capture.', 'We investigate here only the ""grounding"" in logical theories.', 'However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG ) .', 'Woods 1987).']"	0
CC832	J91-2003	On compositional semantics	semantics and cognition	['R Jackendoff']	introduction	"A new perspective on cognition views cortical cell assemblies linking together knowledge about actions and perceptions not only as the vehicles of integrated action and perception processing but, furthermore, as a brain basis for a wide range of higher cortical functions, including attention, meaning and concepts, sequences, goals and intentions, and even communicative social interaction. This article explains mechanisms relevant to mechanistic action perception theory, points to concrete neuronal circuits in brains along with artificial neuronal network simulations, and summarizes recent brain imaging and other experimental data documenting the role of action perception circuits in cognition, language and communication"	"#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"	"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", ""#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"", 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"	1
CC833	J91-2003	On compositional semantics	cues people use to paragraph text	"['S J Bond', 'J R Hayes']"	introduction	"This paper reports the results of three studies on the paragraph. In Study 1, subjects were asked to paragraph a text from which paragraph indentations had been removed. Results indicate that readers can consistently paragraph unparagraphed text, thus supporting Young and Becker's (1966) assertion that the paragraph is a psychologically real unit of discourse. Results also reveal that readers rely heavily on breaks in text cohesion (e.g., topic shift) and on paragraph length as paragraphing cues. In Study 2, four new subjects were asked to paragraph the same text used in Study 1, and to ""think aloud,"" giving their reasons for paragraphing, as they did so. Analysis of these thinking aloud protocols reveal that, in the absence of a strong paragraphing cue, readers will read ahead in a text, sometimes flagging weaker paragraphing cues as they go. If they feel the unparagraphed text is too long, they will go back and paragraph at these weaker cues until all paragraphs in the text are an acceptable length. Based on the results of Studies 1 and 2, a model of how readers paragraph was devised. The model was tested in Study 3 on new subjects who were asked to think aloud as they paragraphed the same text used in Study 1 , and another, longer text. The model predicted the new data quite accurately. Deciding whether paragraph boundaries are psychologically real or arbitrary is very much like deciding whether geographical boundaries are psychologically real or arbitrary. Typically, state boundaries are not psychologically real because travelers cannot find them without the help of signs. Coast lines, on the other hand, are very real. People who miss them fall into the ocean. In the same way, we would consider paragraph boundaries artificial if people could find them only with the help of paragraphing marks, and real if people could consistently find them in texts from which paragraphing marks had been removed. Some linguists such as Hodges (1941) have viewed the paragraph as an arbitrary device used by the writer to ""give the reader a breathing spell"" (p. 311). Similarly, Rodgers (1967) has suggested that a section of text ""becomes a paragraph not by virtue of its structure, but because the writer elects to indent"" (p. 182). However, Young and Becker (1966) and Koen, Becker, and Young (1969) have provided strong evidence that paragraphs are indeed psychologically real. They asked readers to paragraph text from which all paragraphing markers had been removed and found that their readers Research in the Teaching of English, Vol. 18, No. 2, May 1984"	An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG .	"['An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG .', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980).']"	0
CC834	J91-2003	On compositional semantics	paragraph structure inference	['E J Crothers']	introduction		"#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."	"['This demonstrates that information needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '(Other reference works could be treated as additional sources of world knowledge.)', 'This type of consultation uses existing natural language texts as a referential level for processing purposes.', 'It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', ""#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."", 'With respect to that independent source of knowledge, our main contributions are two.', 'First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation.', 'In other words, we recognize it as a separate logical level--the referential level.', 'Second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill the role of the referential level.']"	0
CC835	J91-2003	On compositional semantics	coherence and coreferencequot	['J R Hobbs']	introduction		"Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG ) , or the metarules of Section 5.2?"	"['6.1.1 Was the Use of a Gricean Maxim Necessary?', 'Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG ) , or the metarules of Section 5.2?', 'It seems to us that the answer is no.']"	0
CC836	J91-2003	On compositional semantics	intended models circumscription and commonsense reasoningquot	['W Zadrozny']	introduction	"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations."	"This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning ."	"['We adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance, relating ""they"" to ""apples"" in the sentence (cf.', 'Haugeland 1985 p. 195;Zadrozny 1987a):']"	5
CC837	J91-2003	On compositional semantics	learning from positiveonly examples the subset principle and three case studiesquot	['R C Berwick']	introduction		"Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of #AUTHOR_TAG ."	"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', 'Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of #AUTHOR_TAG .', 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']"	1
CC838	J91-2003	On compositional semantics	the episode schema in story processingquot	"['K Haberlandt', 'C Berian', 'J Sandson']"	introduction		"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure ."	"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']"	1
CC839	J91-2003	On compositional semantics	focusing in the comprehension of definite anaphoraquot	['C Sidner']	introduction		"Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too ."	"['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']"	0
CC840	J91-2003	On compositional semantics	death is the mother of beauty	['M Turner']	introduction	"Let's read! We will often find out this sentence everywhere. When still being a kid, mom used to order us to always read, so did the teacher. Some books are fully read in a week and we need the obligation to support reading. What about now? Do you still love reading? Is reading only for you who have obligation? Absolutely not! We here offer you a new book enPDFd death is the mother of beauty to read."	"Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds ."	"['The referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .', 'We have models of up and down that are based by the way our bodies actually function.']"	0
CC841	J91-2003	On compositional semantics	passing markers a theory of contextual influence in language comprehensionquot	['E Charniak']	introduction		"`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text ."	"['The idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]"	1
CC842	J91-2003	On compositional semantics	a logic of implicit and explicit beliefsquot	['H J Levesque']	introduction		"Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) ."	"['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]"	1
CC843	J91-2003	On compositional semantics	the representation and use of focus in a system for understanding dialogsquot	['B J Grosz']	introduction	"As a dialog progresses the objects and actions that are most relevant to the conversation, and hence in the focus of attention of the dialog participants, change. This paper describes a representation of focus for language understanding systems, emphasizing its use in understanding task-oriented dialogs. The representation highlights that part of the knowledge base relevant at a given point in a dialog. A model of the task is used both to structure the focus representation and to provide an index into potentially relevant concepts in the knowledge base The use of the focus representation to make retrieval of items from the knowledge base more efficient is described."	"Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too ."	"['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']"	0
CC844	J91-2003	On compositional semantics	cohesion in english	"['M A K Halliday', 'R Hasan']"	introduction	"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good"	"This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG , p. 329 ) ."	"['Note: In our translation from English to logic we are assuming that ""it"" is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around).', 'This means that the ""it"" that brought the disease in P1 will not be considered to refer to the infection ""i"" or the death ""d"" in P3.', 'This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG , p. 329 ) .']"	4
CC845	J91-2003	On compositional semantics	parsing strategies in a broadcoverage grammar of english research report rc 12147 ibm tj watson research center	['K Jensen']	introduction		" #AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language ."	"['We can also hope for some fine-tuning of the notion of topic, which would prevent many offensive examples.', 'This approach is taken in computational syntactic grammars (e.g.', ' #AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .']"	0
CC846	J91-2003	On compositional semantics	semantic interpretation and the resolution of ambiguity	['G Hirst']	introduction	"Preface 1. Introduction 2. Semantic interpretation 3. The Absity semantic interpreter 4. Lexical disambiguation 5. Polaroid words 6. Structural disambiguation 7. The semantic enquiry desk 8. Conclusion 9. Speculations, partially baked ideas, and exercises for the reader References Index of names Index of subjects."	"`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text ."	"['The idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]"	1
CC847	J91-2003	On compositional semantics	coherence and coreferencequot	['J R Hobbs']	introduction		"We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG ."	"['We do not claim that Gla is the best or unique way of expressing the rule ""assume that the writer did not say too much.""', 'Rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG .']"	2
CC848	J91-2003	On compositional semantics	cohesion in english	"['M A K Halliday', 'R Hasan']"	introduction	"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good"	"Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) ."	"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"	0
CC849	J91-2003	On compositional semantics	the flow of thought and the flow of languagequot	['W L Chafe']	introduction		"#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure ."	"['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']"	1
CC850	J91-2003	On compositional semantics	a theory of truth and semantic representationquot	['H Kamp']	introduction		"This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics ."	"['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']"	1
CC851	J91-2003	On compositional semantics	episodes as chunks in narrative memoryquot	"['J B Black', 'G H Bower']"	introduction		Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .	"['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']"	0
CC852	J91-2003	On compositional semantics	intended models circumscription and commonsense reasoningquot	['W Zadrozny']	introduction	"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations."	"We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment ."	"['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']"	2
CC853	J91-2003	On compositional semantics	intended models circumscription and commonsense reasoningquot	['W Zadrozny']	introduction	"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations."	It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .	"['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions.', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']"	1
CC854	J91-2003	On compositional semantics	coherence and coreferencequot	['J R Hobbs']	introduction		"Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base ."	"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"	0
CC855	J91-2003	On compositional semantics	resolving pronoun referencesquot	['J R Hobbs']	introduction		"The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature ."	"['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']"	0
CC856	J91-2003	On compositional semantics	languages with self reference i foundationsquot	['D Perlis']	introduction		"Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator ."	"['5.1.1', 'Translation to Logic.', 'The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject (e.g.', 'Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like.', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .', 'Extending and revising Jackendoff\'s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (""that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon""---ibid.).', 'However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.', 'After these remarks we can begin constructing the model of the example paragraph.', 'We assume that constants are introduced by NPs.', 'We have then (i) Constants s, m, d, i, b, 1347 satisfying: ship(s), Messina(m), disease(d), infection(i), death(b), year(1347).']"	0
CC857	J91-2003	On compositional semantics	lectures on contemporary syntactic theories csli lecture notes	['P Sells']	introduction		"For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"	"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.', 'We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.', ""That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end."", 'We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.']"	4
CC858	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	the interaction of word recognition and linguistic processing in speech understandingquot	['H Niemann']	introduction	"This contribution describes an approach to integrate a speech understanding and dialog system into a homogeneous architecture based on semantic networks. The definition of the network as well as its use in speech understanding is described briefly. A scoring function for word hypotheses meeting the requirements of a graph search algorithm is presented. The main steps of the linguistic analysis, i.e. syntax, semantics, and pragmatics, are described and their realization in the semantic network is shown. The processing steps alternating between data- and model-driven phases are outlined using an example sentence which demonstrates a tight interaction between word recognition and linguistic processing."	"Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #AUTHOR_TAG , and Young ( 1989 ) ."	"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #AUTHOR_TAG , and Young ( 1989 ) .']"	0
CC859	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	automatic speech recognition the development of the sphinx system appendix i	['K F Lee']			A formula for the test set perplexity ( #AUTHOR_TAG ) is :13	['A formula for the test set perplexity ( #AUTHOR_TAG ) is :13']	0
CC860	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	the voyager speech understanding system preliminary development and evaluationquot	"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']"		"Early experience with the development of the MIT VOYAGER spoken language system is described, and its current performance is documented. The three components of VOYAGER, the speech recognition component, the natural language component, and the application back-end, are described.>"	The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG ) .	"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986).', 'The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years.', 'The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG ) .', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']"	5
CC861	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	the collection and preliminary analysis of a spontaneous speech databasequot darpa speech and natural language workshop	"['V Zue', 'N Daly', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff', 'M Soclof']"			Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG ) .	"['To obtain training and test data for this task, we had a number of naive subjects use the system as if they were trying to obtain actual information.', 'Their speech was recorded in a simulation mode in which the speech recognition component was excluded.', 'Instead, an experimenter in a separate room typed in the utterances as spoken by the subject.', 'Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG ) .']"	5
CC862	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	on whmovementquot in formal syntax edited by	['Noam Chomsky']			( #AUTHOR_TAG ) .	['( #AUTHOR_TAG ) .']	0
CC863	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	grammaticallybased automatic word class formationquot	"['L Hirschman', 'R Grishman', 'N Sager']"			This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .	"['Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies.', 'This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'There is obviously a great deal more work to be done in this important area.']"	1
CC864	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	error bounds for convolutional codes and an asymptotically optimal decoding algorithmquot	['A Viterbi']		"The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."	"The search algorithm is the standard Viterbi search ( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence ."	"['At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'In both of these systems, TINA provides the interface between the recognizer and the application back-end.', 'In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area.', 'In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The recognizer for these systems is the SUMMIT system (Zue et al. 1989), which uses a segmental-based framework and includes an auditory model in the front-end processing.', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search ( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .']"	5
CC865	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	the case for casequot in universals in linguistic theory	['C J Fillmore']			Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements .	"['Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements .', 'For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as ""leave.""', 'Thus a flight can ""leave for Chicago from Boston at nine,"" or, equivalently, ""leave at nine for Chicago from Boston.""', 'If these complements are each allowed to follow the other, then in TINA an infinite sequence of [from-place]s, [to-place]s and [at-time]s is possible.', 'This is of course unacceptable, but it is straightforward to have each node, as it occurs, or in a semantic bit specifying its case frame, and, in turn, fail if that bit has already been set.', 'We have found that this strategy, in conjunction with the capability of erasing all semantic bits whenever a new clause is entered (through the meta level ""detach"" operation mentioned previously) serves the desired goal of eliminating the unwanted redundancies.']"	5
CC866	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	the bbn spoken language systemquot	"['S Boisen', 'Y-L Chow', 'A Haas', 'R Ingria', 'S Roukos', 'D Stallard']"	introduction		"Representative systems are described in #AUTHOR_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) ."	"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in #AUTHOR_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .']"	0
CC867	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	full integration of speech and language understanding in the mit spoken language systemquot	"['D Goodine', 'S Seneff', 'L Hirschman', 'M Phillips']"			"We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG ) ."	"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions (Zue et al. 1991), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG ) ."", ""Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]"	5
CC868	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	speech database development design and analysis of the acousticphonetic corpusquot	"['L Lamel', 'R H Kassel', 'S Seneff']"			The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG ) .	"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG ) .', 'The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years.', 'The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']"	5
CC869	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	development and preliminary evaluation of the mit atis systemquot	"['S Seneff', 'J Glass', 'D Goddeau', 'D Goodine', 'L Hirschman', 'H Leung', 'M Phillips', 'J Polifroni', 'V Zue']"			"However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem ."	"['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).', 'For example, a [subject] is a noun-phrase node with the label ""topic.""', 'During the top-down cycle, it creates a blank frame and inserts it into a ""topic"" slot in the frame that was handed to it.', 'It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.', 'It then passes along to the right sibling the same frame that was handed to it from above, with the completed topic slot filled with the information delivered by the children.']"	3
CC870	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	development and preliminary evaluation of the mit atis systemquot	"['S Seneff', 'J Glass', 'D Goddeau', 'D Goodine', 'L Hirschman', 'H Leung', 'M Phillips', 'J Polifroni', 'V Zue']"			"The second one, ATIS ( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights."	"['We currently have two application domains that can carry on a spoken dialog with a user.', 'One, the VOYAGER domain (Zue et al. 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University.', 'The second one, ATIS ( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues.']"	5
CC871	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	discovery procedures for sublanguage selectional patterns initial experimentsquot	"['R Grishman', 'L Hirschman', 'N T Nhan']"		"Selectional constraints specify, for a particular domain, the combinations of semantic classes acceptable in subject-verb-object relationships and other syntactic structures. These constraints are important in blocking incorrect analyses in natural language processing systems. However, these constraints are domain-specific and hence must be developed anew when a system is ported to a new domain. A discovery procedure for selectional constraints is therefore essential in enhancing the portability of such systems.This paper describes a semi-automated procedure for collecting the co-occurrence patterns from a sample of texts in a domain, and then using these patterns as the basis for selectional constraints in analyzing further texts. We discuss some of the difficulties in automating the collection process, and describe two experiments that measure the completeness of these patterns and their effectiveness compared with manually-prepared patterns. We then describe and evaluate a procedure for selectional constraint relaxation, intended to compensate for gaps in the set of patterns. Finally, we suggest how these procedures could be combined with a system that queries a domain expert, in order to produce a more efficient discovery procedure."	This approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions .	"['Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies.', 'This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'This approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'There is obviously a great deal more work to be done in this important area.']"	1
CC872	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	estimation of probabilities from sparse data for the language model component of a speech recognizerquot assp35	['S M Katz']		"The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."	"Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG ) ."	"['This appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in ""three hundred and sixteen.""', 'Included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'Since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'This is a problem to be aware of in building grammars from example sentences.', 'In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG ) .']"	0
CC873	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	on whmovementquot in formal syntax edited by	['Noam Chomsky']			"To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator ."	"['2.5.1 Gaps.', 'The mechanism to deal with gaps resembles in certain respects the Hold register idea of ATNs, but with an important difference, reflecting the design philoso-phy that no node can have access to information outside of its immediate domain.', 'The mechanism involves two slots that are available in the feature vector of each parse node.', 'These are called the CURRENT-FOCUS and the FLOAT-OBJECT, respectively.', 'The CURRENT-FOCUS slot contains, at any given time, a pointer to the most recently mentioned content phrase in the sentence.', 'If the FLOAT-OBJECT slot is occupied, it means that there is a gap somewhere in the future that will ultimately be filled by the partial parse contained in the FLOAT-OBJECT.', 'The process of getting into the FLOAT-OBJECT slot (which is analogous to the Hold register) requires two steps, executed independently by two different nodes.', 'The first node, the generator, fills the CURRENT-FOCUS slot with the subparse returned to it by its children.', 'The second node, the activator, moves the CURRENT-FOCUS into the FLOAT-OBJECT position, for its children, during the top-down cycle.', 'It also requires that the FLOAT-OBJECT be absorbed somewhere among its descendants by a designated absorber node, a condition that is checked during the bottom-up cycle.', 'The CURRENT-FOCUS only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree.', 'That is to say, the CURRENT-FOCUS is a feature, like verb-mode, that is blocked when an [end] node is encountered.', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .', 'Finally, certain blocker nodes block the transfer of the FLOAT-OBJECT to their children.']"	0
CC874	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	the voyager speech understanding system preliminary development and evaluationquot	"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']"	conclusion	"Early experience with the development of the MIT VOYAGER spoken language system is described, and its current performance is documented. The three components of VOYAGER, the speech recognition component, the natural language component, and the application back-end, are described.>"	"One , the VOYAGER domain ( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University ."	"['We_currently have two application domains that can carry on a spoken dialog with a user.', 'One , the VOYAGER domain ( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .', 'The second one, ATIS (Seneff et al. 1991), is a system for accessing data in the Official Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains.']"	5
CC875	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	semantics and quantification in natural language question answeringquot	['W A Woods']			"The example used to illustrate the power of ATNs ( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator ."	"[""The example used to illustrate the power of ATNs ( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator ."", 'Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator.', 'The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)?', 'The CURRENT-FOCUS slot is not restricted to nodes that represent nouns.', 'Some of the generators are adverbial or adjectival parts of speech (pos).', 'An absorber checks for agreement in POS before it can accept the FLOAT-OBJECT as its subparse.', 'As an example, the question, ""(How oily)/do you like your salad dressing (ti)?"" contains a [q-subject] ""how oily"" that is an adjective.', 'The absorber [pred-adjective] accepts the available float-object as its subparse, but only after confirming that POS is ADJECTIVE.']"	1
CC876	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	the use of a semantic network in speech dialoguequot	['G Th Niedermair']	introduction		"Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #AUTHOR_TAG , Niemann ( 1990 ) , and Young ( 1989 ) ."	"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #AUTHOR_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .']"	0
CC877	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	benchmark tests for darpa resource management database performance evaluationsquot	['D Pallett']		"A nominally 1000-word resource management database for continuous speech recognition was developed for use in the DARPA Speech Research Program. This database has now been used at several sites for benchmark tests, and the database is expected to be made available to a wider community in the near future. The author documents the structure of the benchmark tests, including the selection of test material and details of studies of scoring algorithms.>"	The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .	"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986).', 'The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .', 'The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']"	5
CC878	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	a formal basis for the heuristic determination of minimum cost pathsquot	"['P Hart', 'N J Nilsson', 'B Raphael']"		"Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies."	"For the A * algorithm ( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion ."	"['Some modification of this scheme is necessary when the input stream is not deterministic.', 'For the A * algorithm ( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .', 'Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found.']"	5
CC879	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	the collection and preliminary analysis of a spontaneous speech databasequot darpa speech and natural language workshop	"['V Zue', 'N Daly', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff', 'M Soclof']"			"The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing ."	"['At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'In both of these systems, TINA provides the interface between the recognizer and the application back-end.', 'In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area.', 'In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence.']"	5
CC880	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	transition network grammars for natural language analysisquot	['W A Woods']		"The use of augmented transition network grammars for the analysis of natural language sentences is described. Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing. The advantages of this model for natural language analysis are discussed in detail and illustrated by examples. An implementation of an experimental parsing system for transition network grammars is briefly described."	"The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes."	"['This section describes how TINA handles several issues that are often considered to be part of the task of a parser.', 'These include agreement constraints, semantic restrictions, subject-tagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in ""(which article)/do you think I should read (ti)?"") (Chomsky 1977).', 'The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.']"	1
CC881	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	the minds system using context and dialog to enhance speech recognitionquot	['S R Young']	introduction		"Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG ."	"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']"	0
CC882	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	integration of speech recognition and natural language processing in the mit voyager systemquot	"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']"		"The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>"	"We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) ."	"['When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', 'A simple word-pair grammar constrained the search space.', 'If the parse failed, then the sentence was rejected.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) ."", 'Both the A* and the Viterbi search are left-to-right search algorithms.', 'However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.', 'That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold.']"	5
CC883	J92-1004	TINA: a probabilistic syntactic parser for speech understanding systems	integration of speech recognition and natural language processing in the mit voyager systemquot	"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']"		"The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>"	"Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing ."	"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]"	5
CC884	J97-4003	On Expressing Lexical Generalizations in HPSG	towards a semantics for lexical rules as used in hpsg	['Detmar Meurers']	introduction		The formalization of DLRs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .	"['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .', 'This description can then be given the standard set-theoretical interpretation of King (1989, 1994).']"	0
CC885	J97-4003	On Expressing Lexical Generalizations in HPSG	offline constraint propagation for efficient hpsg processing	"['Detmar Meurers', 'Guido Minnen']"	introduction	We investigate the use of a technique developed in the constraint programming community called constraint propagation to automatically make a HPSG theory more specific at those places where linguistically motivated underspecification would lead to inefficient processing. We discuss two concrete HPSG examples showing how off-line constraint propagation helps improve processing efficiency.	The reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation.	"['The most specific generalization does not necessarily provide additional constrain- ing information.', 'However, usually it is the case that lexical entries resulting from lexical rule application differ in very few specifications compared to the number of specifica- tions in a base lexical entry.', 'Most of the specifications of a lexical entry are assumed to be passed unchanged via the automatically generated frame specification.', 'Therefore, after lifting the common information into the extended lexical entry, the out-argument in many cases contains enough information to permit a postponed execution of the interaction predicate.', 'When C is the common information, and D1, ..., Dk are the definitions of the interaction predicate called, we use distributivity to factor out C in (C A D1) V -.. V (C A Dk): We compute C A (D1 V ... V Dk), where the r) are assumed to contain no further common factors.', 'Once we have computed c, we use it to make the extended lexical entry more specific.', 'This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'The reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation.']"	0
CC886	J97-4003	On Expressing Lexical Generalizations in HPSG	the compleat lkb	['Ann Copestake']	related work		"This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG , 31 ) ."	"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"	1
CC887	J97-4003	On Expressing Lexical Generalizations in HPSG	controlling the application of lexical rules	"['Ted Briscoe', 'Ann Copestake']"	introduction	"In this paper, we describe an item-familiarity account of the semi-productivity of morphological and lexical rules, and illustrate how it can be applied to practical issues which arise when building large scale lexical knowledge bases which utilize lexical rules. Our approach assumes that attested uses of derived words and senses are explicitly recorded, but that productive use of lexical rules is also possible, though controlled by probabilities associated with rule application. We discuss how the necessary probabilities and estimates of lexical rule productivity may be acquired from corpora."	"27 #AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry ."	"['The way these predicates interconnect is represented in Figure 19.', '27 #AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .', '28 In order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.', 'Since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given.']"	0
CC888	J97-4003	On Expressing Lexical Generalizations in HPSG	headdriven phrase structure grammar	"['Carl Pollard', 'Ivan Sag']"	introduction		"/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements."	"['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"	0
CC889	J97-4003	On Expressing Lexical Generalizations in HPSG	the representation of lexical semantic information cognitive science research paper csrp 280	['Ann Copestake']	introduction		"This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG ) ."	"['This disjunction thus constitutes the base lexicon.', 'The disjuncts in the constraint on derived-word, on the other hand, encode the lexical rules.', 'The in-specification of a lexical rule specifies the IN feature, the out-specification, the derived word itself.', 'Note that the value of the IN feature is of type word and thus also has to satisfy either a base lexical entry or an out-specification of a lexical rule.', 'While this introduces the recursion necessary to permit successive lexical rule application, it also grounds the recursion in a word described by a base lexical entry.', 'Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory.', 'Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.', 'This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG ) .', 'Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.', 'As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.', 'The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules.', '9']"	0
CC890	J97-4003	On Expressing Lexical Generalizations in HPSG	transformations of logic programs foundations and techniques	"['Alberto Pettorossi', 'Maurizio Proietti']"	introduction		"As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ) ."	"['The automata resulting from word class specialization group the lexical entries into natural classes.', 'In case the automata corresponding to two lexical entries are identical, the entries belong to the same natural class.', 'However, each lexical rule application, i.e., each transition in an automaton, calls a frame predicate that can have a large number of defining clauses.', 'Intuitively understood, each defining clause of a frame predicate corresponds to a subclass of the class of lexical entries to which a lexical rule can be applied.', 'During word class specialization, though, when the finite-state automaton representing global lexical rule application is pruned with respect to a particular base lexical entry, we know which subclass we are dealing with.', 'For each interaction definition we can therefore check which of the flame clauses are applicable and discard the non-applicable ones.', 'We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.', 'The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (Tamaki and Sato 1984).', '29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ) .', 'Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.', 'To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.', '3° The successive unfolding steps are schematically represented in Figure 20.']"	1
CC891	J97-4003	On Expressing Lexical Generalizations in HPSG	modularizing contexted constraints	['John Griffith']	introduction	"This paper describes a method for compiling a constraint-based grammar into a potentially more efficient form for processing. This method takes dependent disjunctions within a constraint formula and factors them into non-interacting groups whenever possible by determining their independence. When a group of dependent disjunctions is split into smaller groups, an exponential amount of redundant information is reduced. At runtime, this means that an exponential amount of processing can be saved as well. Since the performance of an algorithm for processing constraints with dependent disjunctions is highly determined by its input, the transformation presented in this paper should prove beneficial for all such algorithms. 1 Introduction  There are two facts that conspire to make the treatment of disjunction an important consideration when building a natural language processing (NLP) system. The first fact is that natural languages are full of ambiguities, and in a grammar many of these ambi.."	32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation .	"['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation .', 'Encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'For analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing.']"	0
CC892	J97-4003	On Expressing Lexical Generalizations in HPSG	towards a semantics for lexical rules as used in hpsg	['Detmar Meurers']	introduction		"12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG ."	"['The translation of the lexical rule into a predicate is trivial.', 'The result is displayed description language.', '12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG .', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A more detailed presentation can be found in Minnen (in preparation).', '14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 Hinrichs and Nakazawa (1996) show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler.']"	0
CC893	J97-4003	On Expressing Lexical Generalizations in HPSG	partialvp and splitnp topicalization in german an hpsg analysis	"['Erhard Hinrichs', 'Tsuneko Nakazawa']"	introduction		"6 The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG , 10 ) is a linguistic example ."	"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world (Gerdemann and King 1994;Gerdemann 1995).', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG , 10 ) is a linguistic example .', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']"	0
CC894	J97-4003	On Expressing Lexical Generalizations in HPSG	open and closed world types in nlp systems	['Dale Gerdemann']	introduction		4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG ) .	"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']"	0
CC895	J97-4003	On Expressing Lexical Generalizations in HPSG	on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars	['Detmar Meurers']	introduction	"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix."	"Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG ) ."	"['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', '(Pollard and Sag [1994, 314], following Flickinger [1987]).', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AI (McCarthy and Hayes 1969), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG ) .']"	0
CC896	J97-4003	On Expressing Lexical Generalizations in HPSG	on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars	['Detmar Meurers']	introduction	"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix."	As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .	"['A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.', 'Most current HPSG analyses of Dutch, German, Italian, and French fall into that category.', '1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time.', 'Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned.', 'As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .']"	4
CC897	J97-4003	On Expressing Lexical Generalizations in HPSG	unfoldfold transformation of logic programs	"['Hisao Tamaki', 'Taisuke Sato']"	introduction		The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG ) .	"['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG ) .', '29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of the clause.', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']"	5
CC898	J97-4003	On Expressing Lexical Generalizations in HPSG	statische programmtransformationen zur effizienten verarbeitung constraintbasierter grammatiken diplomarbeit	['Annette Opalka']	related work		"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG ; Sanfilippo 1995)."	"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG ; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"	1
CC899	J97-4003	On Expressing Lexical Generalizations in HPSG	the formalism and implementation of patr ii	"['Stuart Shieber', 'Hans Uszkoreit', 'Fernando Pereira', 'Jane Robinson', 'Mabry Tyson']"	related work		A similar method is included in PATR-II ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .	"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"	1
CC900	J97-4003	On Expressing Lexical Generalizations in HPSG	the typed feature structure representation formalism	['Martin Emele']	related work		A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG ) .	"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"	1
CC901	J97-4003	On Expressing Lexical Generalizations in HPSG	lexical rules in hpsg what are they	"['Mike Calcagno', 'Carl Pollard']"	introduction		"Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the ."	"['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).', '5']"	0
CC902	J97-4003	On Expressing Lexical Generalizations in HPSG	an expanded logical formalism for headdriven phrase structure grammar	['Paul King']		"Though Pollard and Sag 1994] assumes that an unspeciied variant of the formal logic of Carpenter 1992] will provide a formalism for HPSG, a precise formulation of the envisaged formalism is not immediately obvious, primarily because a principal tenet of Carpenter 1992], that feature structures represent partial information, seems to connict with a principal tenet of Pollard and Sag 1994], that feature structures represent abstract linguistic entities. This has caused many HPSGians to be mistakenly concerned with partial-information speciic notions, such as subsumption, that are appropriate for the Carpenter 1992] logic but inappropriate for the formalism Pollard and Sag 1994] envisages. This paper hopes to allay this concern and the confusion it engenders by substituting King 1989] for Carpenter 1992] as the basis of the envisaged formalism. It demonstrates that the formal logic of King 1989] provides a formalism for HPSG that meets all Pollard and Sag 1994] asks of the envisaged formalism. It further shows that the most credible variant of the Carpenter 1992] logic consistent with the aims of Pollard and Sag 1994] is not only incompatible with the tenet that feature structures represent partial information, but also an instance of the King 1989] logic."	The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .	"['The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .', 'We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and Nakazawa 1994).', 'This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'Our compiler distinguished seven word classes.', 'Some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations.']"	5
CC903	J97-4003	On Expressing Lexical Generalizations in HPSG	interpreting lexical rules	['Mike Calcagno']	introduction		"Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the ."	"['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .', 'lexical rules (DLRs; Meurers 1995).', '5']"	0
CC904	J97-4003	On Expressing Lexical Generalizations in HPSG	flipped out aux in german	"['Erhard Hinrichs', 'Tsuneko Nakazawa']"	introduction		"/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements."	"['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"	0
CC905	J97-4003	On Expressing Lexical Generalizations in HPSG	ale—the attribute logic engine users guide version 201	"['Bob Carpenter', 'Gerald Penn']"	related work	"ale 3.0 is completely compatible with ale 2.0 grammars, and adds the following new features: * A semantic-head-driven generator, based on the algorithm presented in Shieber et al. (1990). The generator was adapted to the logic of typed feature structures by Octav Popescu in his Carnegie Mellon Master's Thesis, Popescu (1996). Octav also wrote most of the generation code for this release. Grammars can be compiled for parsing only, generation only, or both. Some glue-code is also available from the ale homepage, to parse and generate with different grammars through a unix pipe. * A source-level debugger with a graphical XEmacs interface. This debugger works only with SICStus Prolog 3.0.6 and higher. A debugger with reduced functionality will be made available to SWI Prolog users in a later release. This debugger builds on, and incorporates the functionality of the code for the SICStus source-level debugger, written by Per Mildner at Uppsala University. * a /1 atoms. There are now an infinite number of atoms (types with no appropriate features), implicitly declared in every signature. These atoms can be arbitrary Prolog terms, including unbound variables, and can be used wherever normal ale types can, e.g., f:(a p(3.7)). a /1 atoms are extensional as Prolog terms, i.e., are taken to be identical according to the Prolog predicate , ==/2. In particular, this means that ground atoms behave exactly as ale extensional types. * Optional edge subsumption checking. For completeness of parsing, one only needs to ensure that, for every pair of nodes in the chart, the most general feature structure spanning those nodes is stored in the chart. This can reduce the number of edges in many domains. * An autonomous intro/2 operator. Features can now be declared on their own in a separate part of the grammar. * Default specifications for types. These are NOT default types. If a type appears on the right-hand side of a sub/2 or intro/2 specification, but not on iii iv CONTENTS the left-hand side of one, ale will assume this type is maximal, i.e., assume the specification, T ype sub []. Similarly, if it occurs on a left-hand side, but not on a right-hand side, ale will assume the type is immediately subsumed by bot, the most general type. In both cases, ale will announce these assumptions during compilation. * Several bug corrections and more compile-time warning and error messages. * An SWI Prolog 2.9.7 ..."	"A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time ."	"['A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'In the ALE system, for example, a depth bound can be specified for this purpose.', 'Finally, as shown in Section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding.']"	1
CC906	J97-4003	On Expressing Lexical Generalizations in HPSG	a logical formalism for headdriven phrase structure grammar	['Paul King']	introduction		"A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG , 1994 ) ."	"['A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG , 1994 ) .', 'The formal language of King allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the (token) identity of objects.', 'These atomic expressions can be combined using conjunction, disjunction, and negation.', 'The expressions are interpreted by a set-theoretical semantics.']"	0
CC907	J97-4003	On Expressing Lexical Generalizations in HPSG	prolog and natural language analysis csli lecture notes center for the study of language and information	"['Fernando Pereira', 'Stuart Shieber']"	introduction		"The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG ."	"['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29', 'The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG .', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of the clause.', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']"	0
CC908	J97-4003	On Expressing Lexical Generalizations in HPSG	typed unification grammars	"['Martin Emele', 'Remi Zajac']"	related work	"This paper defines unification based ID/LP grammars based on typed feature structures as nonterminals and proposes a variant of Earley's algorithm to decide whether a given input sentence is a member of the language generated by a particular typed unification ID/LP grammar. A solution to the problem of the nonlocal flow of information in unification ID/LP grammars as discussed in Seiffert (1991) is incorporated into the algorithm. At the same time, it tries to connect this technical work with linguistics by presenting an example of the problem resulting from HPSG approaches to linguistics (Hinrichs and Nakasawa 1994, Richter and Sailer 1995) and with computational linguistics by drawing connections from this approach to systems implementing HPSG, especially the TROLL system, Gerdemann et al. (forthcoming).Comment: paper (81 pages), appendix (17 pages, Prolog code), format: .ps   compressed and uuencode"	A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG ; Emele 1994 ) .	"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"	1
CC909	J97-4003	On Expressing Lexical Generalizations in HPSG	a logical formalism for headdriven phrase structure grammar	['Paul King']	introduction		"This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG , 1994 ) . '"	"['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1.', ""This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG , 1994 ) . '"", '11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism.', 'We do not make the linguistic claim that passives should be analyzed using such a lexical rule.', 'For space reasons, the SYNSEM feature is abbreviated by its first letter.', 'The traditional (First I Rest) list notation is used, and the operator • stands for the append relation in the usual way.', '1l Manandhar (1995) proposes to unify these two steps by including an update operator in the The computational treatment we discuss in the rest of the paper follows this setup in that it automatically computes, for each lexical rule specification, the frames necessary to preserve the properties not changed by it.', '12 We will show that the detection and specification of frames and the use of program transformation to advance their integration into the lexicon encoding is one of the key ingredients of the covariation approach to HPSG lexical rules.']"	0
CC910	J97-4003	On Expressing Lexical Generalizations in HPSG	hpsg lexicon without lexical rules	['Karel Oliva']	related work	"this paper, I shall try  (i) to show that ueglecting standtu:d insights of tile orgauization of lexicon is detrimental both to the linguistic adequacy and to the practical useful- hess of the lexicon,  (ii) to make a proposal of an alternative reconcil ing the needs of HPSG with the usual lexicographic practic"	"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) ."	"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"	1
CC911	J97-4003	On Expressing Lexical Generalizations in HPSG	the update operation in feature logic	['Suresh Manandhar']	introduction		11 #AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language.	['11 #AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language.']	0
CC912	J97-4003	On Expressing Lexical Generalizations in HPSG	an overview of disjunctive constraint satisfaction	"['John Maxwell', 'Ronald Kaplan']"	introduction	This paper presents a new algorithm for solving disjunctive systems of constraints. The algorithm determines whether a system is satisfiable and produces the models if the system is satisfiable. There are three main steps for determining whether or not the system is satisfiable: 1 ) turn the disjunctive system into an equi-satisfiable conjunctive system in polynomial time 2) convert the conjunctive system into canonical form using extensions of standard techniques #3) extract and solve a propositional 'disjunctive residue'	32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .	"['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .', 'Encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'For analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing.']"	0
CC913	J97-4003	On Expressing Lexical Generalizations in HPSG	the generative power of categorial grammars and headdriven phrase structure grammars with lexical rules	['Bob Carpenter']	related work	"In this paper, it is shown that the addition of simple and linguistically motivated forms of lexical rules to grammatical theories based on subcategorization lists, such as categorial grammars (CG) or head-driven phrase structure grammars (HPSG), results in a system that can generate all and only the recursively enumerable languages. The proof of this result is carried out by means of a reduction of generalized rewriting systems. Two restrictions are considered, each of which constrains the generative power of the resulting system to context-free languages."	The powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems .	"['The powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems .', 'In this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper.']"	0
CC914	J97-4003	On Expressing Lexical Generalizations in HPSG	some philosophical problems from the standpoint of artificial intelligence	"['John McCarthy', 'Patrick Hayes']"	introduction		"This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule ."	"['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', '(Pollard and Sag [1994, 314], following Flickinger [1987]).', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up (Meurers 1994).']"	1
CC915	J97-4003	On Expressing Lexical Generalizations in HPSG	the craft of prolog	"[""Richard O'Keefe""]"	introduction	"Hacking your program is no substitute for understanding your problem. Prolog is different, but not that different. Elegance is not optional. These are the themes that unify Richard O'Keefe's very personal statement on how Prolog programs should be written. The emphasis in ""The Craft of Prolog"" is on using Prolog effectively. It presents a loose collection of topics that build on and elaborate concepts learning in a first course. These may be read in any order following the first chapter, ""Basic Topics in Prolog, "" which provides a basis for the rest of the material in the book."	"Using an accumulator passing technique ( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules ."	"['Encoding a finite-state automaton as definite relations is rather straightforward.', 'In fact, one can view the representations as notational variants of one another.', 'Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'Using an accumulator passing technique ( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .', 'Because of the word class specialization step discussed in Section 3.3, the execution avoids trying out many lexical rule applications that are guaranteed to fail.']"	5
CC916	J97-4003	On Expressing Lexical Generalizations in HPSG	applying lexical rules under subsumption	"['Erhard Hinrichs', 'Tsuneko Nakazawa']"	introduction	"Lexical rules are used in constraint based grammar formalisms such as Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag 1994) to express generalizations among lexical entries. This paper discusses a number of lexical rules from recent HPSG analyses of German (Hinrichs and Nakazawa 1994) and shows that the grammar in some cases vastly overgenerates and in other cases introduces massive spurious structural ambiguity, if lexical rules apply under unification. Such problems of overgeneration or spurious ambiguity do not arise, if a lexical rule applies to a given lexical entry iff the lexical entry is subsumed by the left-hand side of the lexical rule. Finally, the paper discusses computational consequences of applying lexical rules under subsumption."	15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .	"['The translation of the lexical rule into a predicate is trivial.', 'The result is displayed description language.', '12 In order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in Meurers (1995).', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A more detailed presentation can be found in Minnen (in preparation).', '14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler.']"	0
CC917	J97-4003	On Expressing Lexical Generalizations in HPSG	word formation in lexical type hierarchies a case study of baradjectives in german masters thesis	['Susanne Riehemann']	related work		"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) ."	"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"	1
CC918	J97-4003	On Expressing Lexical Generalizations in HPSG	verb second by underspecification	['Annette Frank']	related work		"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) ."	"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"	1
CC919	J97-4003	On Expressing Lexical Generalizations in HPSG	an expanded logical formalism for headdriven phrase structure grammar	['Paul King']	introduction	"Though Pollard and Sag 1994] assumes that an unspeciied variant of the formal logic of Carpenter 1992] will provide a formalism for HPSG, a precise formulation of the envisaged formalism is not immediately obvious, primarily because a principal tenet of Carpenter 1992], that feature structures represent partial information, seems to connict with a principal tenet of Pollard and Sag 1994], that feature structures represent abstract linguistic entities. This has caused many HPSGians to be mistakenly concerned with partial-information speciic notions, such as subsumption, that are appropriate for the Carpenter 1992] logic but inappropriate for the formalism Pollard and Sag 1994] envisages. This paper hopes to allay this concern and the confusion it engenders by substituting King 1989] for Carpenter 1992] as the basis of the envisaged formalism. It demonstrates that the formal logic of King 1989] provides a formalism for HPSG that meets all Pollard and Sag 1994] asks of the envisaged formalism. It further shows that the most credible variant of the Carpenter 1992] logic consistent with the aims of Pollard and Sag 1994] is not only incompatible with the tenet that feature structures represent partial information, but also an instance of the King 1989] logic."	4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG ; Gerdemann 1995 ) .	"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG ; Gerdemann 1995 ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']"	0
CC920	J97-4003	On Expressing Lexical Generalizations in HPSG	french clitic climbing without clitics or climbing	"['Philip Miller', 'Ivan Sag']"	introduction		"de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements ."	"['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', 'de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements .', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"	0
CC921	J97-4003	On Expressing Lexical Generalizations in HPSG	headdriven phrase structure grammar	"['Carl Pollard', 'Ivan Sag']"	introduction		"16 A linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20)."	"['16 A linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20).', 'In such a Predicative Lexical Rule (which we only note as an example and not as a linguistic proposal) the subtype of the head object undergoing the rule as well as the value of the features only appropriate for the subtypes of substantive either is lost or must be specified by a separate rule for each of the subtypes.']"	0
CC922	J97-4003	On Expressing Lexical Generalizations in HPSG	towards a semantics for lexical rules as used in hpsg	['Detmar Meurers']	introduction		"The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program ."	"['Definite relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding: We show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency.', 'The resulting encoding allows the execution of lexical rules on-the-fly, i.e., coroutined with other constraints at some time after lexical lookup.', 'The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .']"	2
CC923	J97-4003	On Expressing Lexical Generalizations in HPSG	passive without lexical rules in	['Andreas Kathol']	related work		"In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) ."	"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"	1
CC924	J97-4003	On Expressing Lexical Generalizations in HPSG	lexical polymorphism and word disambiguation	['Antonio Sanfilippo']	related work	We present an approach to lexical ambiguity where regularities about sense/u~ge extensibillty are represented by underepecifying word entries through lexic~d polymorphism. Word diumbiguation is carried out using contextual information gathered during language processing to ground polymorphic lexical entries.	"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) ."	"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"	1
CC925	J97-4003	On Expressing Lexical Generalizations in HPSG	headdriven phrase structure grammar	"['Carl Pollard', 'Ivan Sag']"	introduction		"/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements."	"['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"	0
CC926	J97-4003	On Expressing Lexical Generalizations in HPSG	a computational treatment of hpsg lexical rules as covariation in lexical entries	"['Detmar Meurers', 'Guido Minnen']"	introduction	We describe a compiler which translates a set of HPSG lexical rules and their interaction into definite relations used to constrain lexical entries. The compiler ensures automatic transfer of properties unchanged by a lexical rule. Thus an operational semantics for the full lexical rule mechanism as used in HPSG linguistics is provided. Program transformation techniques are used to advance the resulting encoding. The final output constitutes a computational counterpart of the linguistic generalizations captured by lexical rules and allows ``on the fly'' application.	"Based on the research results reported in #AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG ."	"['Based on the research results reported in #AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .', 'We developed a compiler that takes as its input a set of lexical rules, deduces the nec- essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'Each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'The definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries.']"	4
CC927	J97-4003	On Expressing Lexical Generalizations in HPSG	the representation of lexical semantic information cognitive science research paper csrp 280	['Ann Copestake']	related work		"This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) ."	"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"	1
CC928	J97-4003	On Expressing Lexical Generalizations in HPSG	on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars	['Detmar Meurers']	introduction	"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix."	"However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided ."	"['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'In the latter case, we can also take care of transferring the value of z.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .', 'Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.', 'So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17']"	4
CC929	J97-4003	On Expressing Lexical Generalizations in HPSG	headdriven phrase structure grammar	"['Carl Pollard', 'Ivan Sag']"	introduction		"Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch ."	"['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']"	0
CC930	J97-4003	On Expressing Lexical Generalizations in HPSG	featurebased inheritance networks for computational lexicons	"['Hans-Ulrich Krieger', 'John Nerbonne']"	related work	"The virtues of viewing the lexicon as an inheritance network are its succinctness and its tendency to highlight significant clusters of linguistic properties. From its succinctness follow two practical advantages, namely its ease of maintenance and modification. In this paper we present a feature-based foundation for lexical inheritance. We argue that the feature-based foundation is both more economical and expressively more powerful than non-feature-based systems. It is more economical because it employs only mechanisms already assumed to be present elsewhere in the grammar (viz., in the feature system), and it is more expressive because feature systems are more expressive than other mechanisms used in expressing lexical inheritance (cf. DATR). The lexicon furthermore allows the use of default unification, based on the ideas of default unification, defined by Bouma. These claims are buttressed in sections sketching the opportunities for lexical description in feature-based lexicons in two central lexical topics, inflection and derivation. Briefly, we argue that the central notion of paradigm may be defined in feature structures, and that it may be more satisfactorily (in fact, immediately) linked to the syntactic information in this fashion. Our discussion of derivation is more programmatic; but here, too, we argue that feature structures of a suitably rich sort provide a foundation for the definition of lexical rules. We illustrate theoretical claims in application to German lexis. This work is currently under implementation in a natural language understanding effort (DISCO) at the German Artiffical Intelligence Center (Deutsches Forschungszentrum fur Kunstliche Intelligenz)."	"In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995)."	"['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"	1
CC931	J97-4003	On Expressing Lexical Generalizations in HPSG	of csli lecture notes center for the study of language and information	"['Carl Pollard', 'Ivan Sag']"	introduction		"Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch ."	"['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']"	1
CC932	J97-4003	On Expressing Lexical Generalizations in HPSG	towards a semantics for lexical rules as used in hpsg	['Detmar Meurers']	introduction		"Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )"	"['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']"	0
CC933	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	design challenges and misconceptions in named entity recognition	"['L Ratinov', 'D Roth']"	related work	"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset."	"#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge ."	"['The task of mention detection is closely related to Named Entity Recognition (NER).', 'Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches.', 'They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'However, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .', 'NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g.', 'Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004).', 'The typical BIO representation was introduced in Ramshaw and Marcus (1995); OC representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities.']"	0
CC934	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	understanding the value of features for coreference resolution	"['E Bengtson', 'D Roth']"		"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features."	"For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in #AUTHOR_TAG ."	"['Here, y u,v = 1 iff mentions u, v are directly linked.', 'Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are co-referred.', 'For this mention-pair coreference model Ï\x86 ( u , v ) , we use the same set of features used in #AUTHOR_TAG .']"	5
CC935	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	understanding the value of features for coreference resolution	"['E Bengtson', 'D Roth']"	introduction	"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features."	"In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG ) ."	"['Here, phrases in the brackets are mentions and the underlined simple phrases are mention heads.', 'Moreover, mention boundaries can be nested (the boundary of a mention is inside the boundary of another mention), but mention heads never overlap.', 'This property also simplifies the problem of mention head candidate generation.', 'In the example above, the first ""they"" refers to ""Multinational companies investing in China"" and the second ""They"" refers to ""Domestic manufacturers, who are also suffering"".', 'In both cases, the mention heads are sufficient to support the decisions: ""they"" refers to ""companies"", and ""They"" refers to ""manufacturers"".', 'In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG ) .']"	0
CC936	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	a joint model for entity analysis coreference typing and linking	"['G Durrett', 'D Klein']"	experiments	"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines."	"For Berkeley system , we use the reported results from #AUTHOR_TAG ."	"['The latest scorer is version v8.01, but MUC, B , CEAF e and CoNLL average scores are not changed.', 'For evaluation on ACE-2004, we convert the system output and gold annotations into CoNLL format. 12', 'We do not provide results from Berkeley and HOTCoref on ACE-2004 dataset as they do not directly support ACE input.', 'Results for HOTCoref are slightly different from the results reported in Björkelund and Kuhn (2014).', 'For Berkeley system , we use the reported results from #AUTHOR_TAG .']"	1
CC937	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	a joint model for entity analysis coreference typing and linking	"['G Durrett', 'D Klein']"	experiments	"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines."	"Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) ."	"['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .', 'Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']"	1
CC938	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	understanding the value of features for coreference resolution	"['E Bengtson', 'D Roth']"	experiments	"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features."	"We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) ."	"['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']"	5
CC939	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	headdriven statistical models for natural language parsing	['M Collins']		"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."	"We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG ) to identify their heads ."	"['Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples.', 'We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples.', 'Specifically, after mention head candidate generation (described in Sec.', '3), we train on a set of candidates with precision larger than 50%.', 'We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG ) to identify their heads .', 'When these extracted heads do not overlap with gold mention heads, we treat them as negative examples.']"	5
CC940	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	a constrained latent variable model for coreference resolution	"['K-W Chang', 'R Samdani', 'D Roth']"	introduction	"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature."	"Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) ."	"['Mention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception).', 'Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .', 'However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', 'Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1.', 'Current state-of-the-art systems show a very significant drop in performance when running on system generated mentions.', 'These performance gaps are worrisome, since the real goal of NLP systems is to process raw data.', '1: Performance gaps between using gold mentions and predicted mentions for three state-of-the-art coreference resolution systems.', 'Performance gaps are always larger than 10%.', ""Illinois's system (Chang et al., 2013) is evaluated on CoNLL (2012CoNLL ( , 2011) Shared Task and ACE-2004 datasets."", 'It reports an average F1 score of MUC, B and CEAF e metrics using CoNLL v7.0 scorer.', ""Berkeley's system (Durrett and Klein, 2013) reports the same average score on the CoNLL-2011 Shared Task dataset."", ""Results of Stanford's system (Lee et al., 2011) are for B 3 metric on ACE-2004 dataset."", 'This paper focuses on improving end-to-end coreference performance.', 'We do this by: 1) Developing a new ILP-based joint learning and inference formulation for coreference and mention head detection.', '2) Developing a better mention head candidate generation algorithm.', 'Importantly, we focus on heads rather than mention boundaries since those can be identified more robustly and used effectively in an end-to-end system.', 'As we show, this results in a dramatic improvement in the quality of the MD component and, consequently, a significant reduction in the performance gap between coreference on gold mentions and coreference on raw data.']"	0
CC941	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	design challenges and misconceptions in named entity recognition	"['L Ratinov', 'D Roth']"		"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset."	"Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG ."	"['Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG .', 'The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks.', 'The problem is then transformed into a simple, but constrained, 5-class classification problem.']"	4
CC942	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	conll2012 shared task modeling multilingual unrestricted coreference in ontonotes	"['S Pradhan', 'A Moschitti', 'N Xue', 'O Uryupina', 'Y Zhang']"	experiments	"The CoNLL-2012 shared task involved predicting coreference in three languages -- English, Chinese and Arabic -- using OntoNotes data. It was a follow-on to the English-only task organized in 2011. Until the creation of the OntoNotes corpus, resources in this subfield of language processing have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ACE entities. OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types and covering multiple languages. OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, evaluation criteria, and presents and discusses the results achieved by the participating systems. Being a task that has a complex evaluation history, and multiple evalation conditions, it has, in the past, been difficult to judge the improvement in new algorithms over previously reported results. Having a standard test set and evaluation parameters, all based on a resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference."	"The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG ) , contains 3,145 annotated documents ."	"['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008).', 'The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG ) , contains 3,145 annotated documents .', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']"	5
CC943	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	a constrained latent variable model for coreference resolution	"['K-W Chang', 'R Samdani', 'D Roth']"	experiments	"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature."	"Developed Systems Our developed system is built on the work by #AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 ."	"['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Björkelund and Kuhn, 2014).', 'Developed Systems Our developed system is built on the work by #AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']"	5
CC944	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	a constrained latent variable model for coreference resolution	"['K-W Chang', 'R Samdani', 'D Roth']"	related work	"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature."	"In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG in our experiments ."	"['Coreference resolution has been extensively studied, with several state-of-the-art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013;Björkelund and Kuhn, 2014;Song et al., 2012).', 'Many of the early rule-based systems like Hobbs (1978) and Lappin and Leass (1994) gained considerable popularity.', 'The early designs were easy to understand and the rules were designed manually.', 'Machine learning approaches were introduced in many works (Connolly et al., 1997;Ng and Cardie, 2002;Bengtson and Roth, 2008;Soon et al., 2001).', 'The introduction of ILP methods has influenced the coreference area too Denis and Baldridge, 2007).', 'In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG in our experiments .']"	5
CC945	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	firstorder probabilistic models for coreference resolution	"['A Culotta', 'M Wick', 'A McCallum']"	experiments	"Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases. In this paper, we propose a machine learning method that enables features over sets of noun phrases, resulting in a first-order probabilistic model for coreference. We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases. This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently."	"We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) ."	"['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']"	5
CC946	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	a constrained latent variable model for coreference resolution	"['K-W Chang', 'R Samdani', 'D Roth']"		"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature."	More details can be found in #AUTHOR_TAG et al. (2013).	"['More details can be found in #AUTHOR_TAG et al. (2013).', 'The difference here is that we also consider the validity of mention heads using �(u),�(m)']"	0
CC947	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	a joint model for entity analysis coreference typing and linking	"['G Durrett', 'D Klein']"	related work	"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines."	Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity-linking .	"['Several recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity-linking .', 'The work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different.']"	0
CC948	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	headdriven statistical models for natural language parsing	['M Collins']	experiments	"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."	"Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information ."	"['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Björkelund and Kuhn, 2014).', 'Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']"	5
CC949	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	a constrained latent variable model for coreference resolution	"['K-W Chang', 'R Samdani', 'D Roth']"		"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature."	Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .	"['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.', 'This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.', 'The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.', 'By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from Recasens et al. (2013).', 'This joint framework aims to improve performance on both mention head detection and on coreference.']"	5
CC950	K15-1002	A Joint Framework for Coreference Resolution and Mention Head Detection	ontonotes the 90 solution	"['E Hovy', 'M Marcus', 'M Palmer', 'L Ramshaw', 'R Weischedel']"	experiments	"We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007."	"We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) ."	"['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).', 'Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets.']"	5
CC951	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	weaklysupervised bayesian learning of a ccg supertagger	"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']"	method		We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG ) .	"['For the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross-linguistically-plausible categories.', 'To formalize the notion of what it means for a category to be more ""plausible"", we extend the category generator of our previous work, which we will call P CAT .', 'We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG ) .', 'The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in §5) with probability p del , or a standard CCG category C:']"	0
CC952	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	a generative constituentcontext model for improved grammar induction	"['Dan Klein', 'Christopher D Manning']"	related work		#AUTHOR_TAG 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) .	"[""#AUTHOR_TAG 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."", 'They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', 'While CCM is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels.']"	0
CC953	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	bayesian inference for pcfgs via markov chain monte carlo	"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']"	introduction	"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar."	"In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities ."	"['In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .', 'However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior.', 'Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios.']"	5
CC954	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	building a large annotated corpus of english the penn treebank	"['Mitchell P Marcus', 'Beatrice Santorini', 'Mary Ann Marcinkiewicz']"	experiments	"Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."	"We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #AUTHOR_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) ."	"['In our evaluation we compared our supertagcontext approach to (our reimplementation of) the best-performing model of our previous work (Garrette et al., 2015), which SCM extends.', 'We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #AUTHOR_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .']"	5
CC955	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	weaklysupervised bayesian learning of a ccg supertagger	"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']"	method		The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG .	"[""The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG ."", ""Thus, the right-side context prior mean θ RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label.""]"	1
CC956	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	weaklysupervised bayesian learning of a ccg supertagger	"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']"	experiments		We use the same splits as #AUTHOR_TAG .	"['Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set.', 'We use the same splits as #AUTHOR_TAG .', 'Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.', 'In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (Parker et al., 2011) and supplemented Italian with the out-of-domain WaCky corpus (Baroni et al., 1999).', 'For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k).', 'For Chinese and Italian, for training efficiency, we used only raw sentences that were 50 words or fewer (note that we did not drop tag dictionary set or test set sentences).']"	5
CC957	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	online learning of relaxed ccg grammars for parsing to logical form	"['Luke S Zettlemoyer', 'Michael Collins']"	experiments	"We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar--for example allowing flexible word order, or insertion of lexical items-- with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006)."	"This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar ."	"['When evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence: every dependency would be ""wrong"".', 'Thus, it is important that we make a best effort to find a parse.', 'To accomplish this, we implemented a parsing backoff strategy.', 'The parser first tries to find a valid parse that has either s dcl or np at its root.', 'If that fails, then it searches for a parse with any root.', 'If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without).', 'This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .', 'We add unary rules of the form D →u for every potential supertag u in the tree.', 'Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t→ D , v and t→ v, D .', 'Recall that in §3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.', 'Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents.']"	1
CC958	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	weaklysupervised bayesian learning of a ccg supertagger	"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']"	introduction		We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG ) .	"['Baldridge observed is that, cross-linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures.', 'In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015).', 'Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable.', 'We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG ) .']"	2
CC959	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	typesupervised hidden markov models for partofspeech tagging with incomplete tag dictionaries	"['Dan Garrette', 'Jason Baldridge']"	method	"Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets. Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries. Taking the min-greedy algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics. We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. Altogether, our augmentations produce improvements to performance over the original min-greedy algorithm for both English and Italian data."	"We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4"	"['We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4', 'This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'These counts are then combined with estimates of the �openness� of each tag in order to assess its likelihood of appearing with new words.']"	5
CC960	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	a generative constituentcontext model for improved grammar induction	"['Dan Klein', 'Christopher D Manning']"	method		"Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG ) ."	"['Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG ) .']"	4
CC961	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	widecoverage efficient statistical parsing with ccg and loglinear models	"['Stephen Clark', 'James R Curran']"			We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of #AUTHOR_TAG .	"['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of #AUTHOR_TAG .']"	5
CC962	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	bayesian inference for pcfgs via markov chain monte carlo	"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']"		"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar."	"To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees ."	"['To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees .', 'For a sentence w, the strategy is to use the Inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non-terminal position spanning words w i through w j−1 and category t, going ""up"" the tree, the probability of generating w i , . . .', ', w j−1 via any arrangement of productions that is rooted by y ij = t.']"	5
CC963	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	bayesian inference for pcfgs via markov chain monte carlo	"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']"		"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar."	Our strategy is based on the approach presented by #AUTHOR_TAG .	"['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.', 'By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities.']"	5
CC964	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	a generative constituentcontext model for improved grammar induction	"['Dan Klein', 'Christopher D Manning']"	introduction		"One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear ."	"['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB.', 'This DET-VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN.', 'From this, we might deduce that DET-VERB is a likely context for a noun phrase.', 'CCM is able to learn which POS contexts are likely, and does so via a probabilistic generative model, providing a statistical, data-driven take on substitutability.', 'However, since there is nothing intrinsic about the POS pair DET-VERB that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data.', 'Baldridge (2008) observed that unlike opaque, atomic POS labels, the rich structures of Combinatory Categorial Grammar (CCG) (Steedman, 2000;Steedman and Baldridge, 2011) categories reflect universal grammatical properties.', 'CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents.', 'For example, a category might encode that ""this constituent can combine with a noun phrase to the right (an object) and then a noun phrase to the left (a subject) to produce a sentence"" instead of simply VERB.', 'CCG has proven useful as a framework for grammar induction due to its ability to incorporate linguistic knowledge to guide parser learning by, for example, specifying rules in lexical-expansion algorithms (Bisk and Hockenmaier, 2012;2013) or encoding that information as priors within a Bayesian framework (Garrette et al., 2015).']"	0
CC965	K15-1003	A Supertag-Context Model for Weakly-Supervised CCG Parser Learning	a ccg parsing with a supertagfactored model	"['Mike Lewis', 'Mark Steedman']"			"We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules ."	"['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007).']"	5
CC966	N01-1003	SPoT	discriminative reranking for natural language parsing	['Michael Collins']		"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation"	"Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG ."	"['Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones.', 'In total, we used 3,291 features in training the SPR.', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .', 'The motivation for the features was to capture declaratively decisions made by the randomized SPG.', 'We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times.']"	1
CC967	N01-1003	SPoT	clause aggregation using linguistic knowledge	['James Shaw']	related work	"By combining multiple clauses into one single sentence, a text generation system can express the same amount of information in fewer words and at the same time, produce a great variety of complex constructions. In this paper, we describe hypotactic and paratactic operators for generating complex sentences from clause-sized semantic representations. These two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar."	Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ) .	"['Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ) .', 'This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.', 'Presumably, this is the reason why dialog systems to date have not used this kind of sentence planning.']"	0
CC968	N01-1003	SPoT	discriminative reranking for natural language parsing	['Michael Collins']	introduction	"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation"	"The SPR uses rules automatically learned from training data , using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 ) ."	"['Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into sentences. 1', 'For example, consider the required capabilities of a sentence planner for a mixed-initiative spoken dialog system for travel planning: (D1) System1: Welcome.... What airport would you like to fly out of?', 'User2: I need to go to Dallas.', 'System3: Flying to Dallas.', 'What departure airport was that?', 'User4: from Newark on September the 1st.', 'System5: What time would you like to travel on September the 1st to Dallas from Newark?', ""Utterance System1 requests information about the caller's departure airport, but in User2, the caller takes the initiative to provide information about her destination."", ""In System3, the system's goal is to implicitly confirm the destination (because of the possibility of error in the speech recognition component), and request information (for the second time) of the caller's departure airport."", 'In User4, the caller provides this information but also provides the month and day of travel.', ""Given the system's dialog strategy, the communicative goals for its next turn are to implicitly confirm all the information that the user has provided so far, i.e. the departure and destination cities and the month and day information, as well as to request information about the time of travel."", ""The system's representation of its communicative goals for utterance System5 is in Figure 1."", 'The job of the sentence planner is to decide among the large number of potential realizations of these communicative goals.', 'Some example alternative realizations are in Figure 2. 2 implicit-confirm(orig-city:NEWARK) implicit-confirm(dest-city:DALLAS) implicit-confirm(month:9) implicit-confirm(day-number:1) request(depart-time) In this paper, we present SPoT, for ""Sentence Planner, Trainable"".', 'We also present a new methodology for automatically training SPoT on the basis of feedback provided by human judges.', 'In order to train SPoT, we reconceptualize its task as consisting of two distinct phases.', 'In the first phase, the sentence-plan-generator (SPG) generates a potentially large sample of possible sentence plans for a given text-plan input.', 'In the second phase, the sentence-plan-ranker (SPR) ranks the sample sentence plans, and then selects the top-ranked output to input to the surface realizer.', 'Our primary contribution is a method for training the SPR.', 'The SPR uses rules automatically learned from training data , using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 ) .']"	1
CC969	N01-1003	SPoT	clause aggregation using linguistic knowledge	['James Shaw']		"By combining multiple clauses into one single sentence, a text generation system can express the same amount of information in fewer words and at the same time, produce a great variety of complex constructions. In this paper, we describe hypotactic and paratactic operators for generating complex sentences from clause-sized semantic representations. These two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar."	"These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form ."	"['As already mentioned, we divide the sentence planning task into two phases.', 'In the first phase, the sentenceplan-generator (SPG) generates 12-20 possible sentence plans for a given input text plan.', ""Each speech act is assigned a canonical lexico-structural representation (called a DSyntS -Deep Syntactic Structure (Mel'čuk, 1988))."", 'The sentence plan is a tree recording how these elementary DSyntS are combined into larger DSyntSs; the DSyntS for the entire input text plan is associated with the root node of the tree.', 'In the second phase, the sentence plan ranker (SPR) ranks sentence plans generated by the SPG, and then selects the top-ranked output as input to the surface realizer, RealPro (Lavoie and Rambow, 1997)  The research presented here is primarily concerned with creating a trainable SPR.', 'A strength of our approach is the ability to use a very simple SPG, as we explain below.', 'The basis of our SPG is a set of clausecombining operations that incrementally transform a list of elementary predicate-argument representations (the DSyntSs corresponding to elementary speech acts, in our case) into a single lexico-structural representation, by combining these representations using the following combining operations.', 'Examples can be found in Figure  ADJECTIVE.', 'This transforms a predicative use of an adjective into an adnominal construction.', 'PERIOD.', 'Joins two complete clauses with a period.', 'These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form .']"	0
CC970	N01-1003	SPoT	sentence planning as description using tree adjoining grammar	"['Matthew Stone', 'Christine Doran']"		"We present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG). This approach captures naturally and elegantly the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential interactions between multiple descriptions in a sentence. At the same time, it exploits linguistically motivated, declarative specifications of the discourse functions of syntactic constructions to make contextually appropriate syntactic choices."	"The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes ."	"['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'The interior nodes are associated with DSyntSs by executing their clausecombing operation on their two daughter nodes.', '(A PE-RIOD node results in a DSyntS headed by a period and whose daughters are the two daughter DSyntSs.)', 'If a clause combination fails, the sp-tree is discarded (for example, if we try to create a relative clause of a structure which already contains a period).', 'As a result, the DSyntS for the entire turn is associated with the root node.', 'This DSyntS can be sent to RealPro, which returns a sentence (or several sentences, if the DSyntS contains period nodes).', 'The SPG is designed in such a way that if a DSyntS is associated with the root node, it is a valid structure which can be realized.', 'Figure 2 shows some of the realizations of alternative sentence plans generated by our SPG for utterance Sys- 3 The sp-tree is inspired by (Lavoie and Rambow, 1998).', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', '4 shows the result of applying the soft-merge operation when Args 1 and 2 are implicit confirmations of the origin and destination cities. Figure 8 illustrates the relationship between the sp-tree and the DSyntS for alternative 8.', 'The labels and arrows show the DSyntSs associated with each node in the sp-tree (in Figure 7), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'The complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.', 'In our approach, we do not need to encode such constraints.', 'Rather, we generate a random sample of possible sentence plans for each text plan, up to a pre-specified maximum number of sentence plans, by randomly selecting among the operations according to some probability distribution. 4']"	0
CC971	N01-1006	Transformation Based Learning in the Fast Lane	independence and commitment assumptions for rapid training and execution of rulebased pos taggers	['M Hepple']	method	"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method."	"2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG ."	"['2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG .']"	5
CC972	N01-1006	Transformation Based Learning in the Fast Lane	independence and commitment assumptions for rapid training and execution of rulebased pos taggers	['M Hepple']		"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method."	"The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG ) ."	"['The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG ) .']"	1
CC973	N01-1006	Transformation Based Learning in the Fast Lane	classifier combination for improved lexical disambiguation	"['E Brill', 'J Wu']"	experiments	"One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees.."	"The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG ."	"['The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG .']"	5
CC974	N01-1006	Transformation Based Learning in the Fast Lane	independence and commitment assumptions for rapid training and execution of rulebased pos taggers	['M Hepple']	experiments	"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method."	"â¢ The regular TBL , as described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( #AUTHOR_TAG ) ."	"[""â\x80¢ The regular TBL , as described in section 2 ; â\x80¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â\x80¢ The FastTBL algorithm ; â\x80¢ The ICA algorithm ( #AUTHOR_TAG ) .""]"	1
CC975	N01-1006	Transformation Based Learning in the Fast Lane	independence and commitment assumptions for rapid training and execution of rulebased pos taggers	['M Hepple']		"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method."	The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .	['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']	0
CC976	N01-1010	Tree-cut and a lexicon based on systematic polysemy	automatic extraction of systematic polysemy using treecut	['N Tomuro']	experiments	"This paper describes an automatic method for extracting systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Systematic polysemy is a set of word senses that are related in systematic and predictable ways. Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. We compare the systematic relations extracted by our automatic method to manually extracted WordNet cousins."	"Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from WordNet ."	"['Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .', 'In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method.']"	2
CC977	N01-1010	Tree-cut and a lexicon based on systematic polysemy	automatic extraction of systematic polysemy using treecut	['N Tomuro']	introduction	"This paper describes an automatic method for extracting systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Systematic polysemy is a set of word senses that are related in systematic and predictable ways. Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. We compare the systematic relations extracted by our automatic method to manually extracted WordNet cousins."	"In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability ."	"['In this paper, we describes a lexicon organized around systematic polysemy.', 'The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998).', 'In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', 'In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996).', 'The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data.']"	2
CC978	N01-1024	Knowledge-free induction of inflectional morphologies	knowledgefree induction of morphology using latent semantic analysis	"['P Schone', 'D Jurafsky']"	method	"Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction. Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Relying on stem-and-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes (""ally"" stemming to ""all""). We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plus-affix are sufficiently similar semantically. We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system."	"In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG ) ."	"['In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG ) .', 'Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix.', 'The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1.', ""The matrix is structured such that for a given word w's row, the first N columns denote words that -NCS (µ,1)""]"	2
CC979	N12-1010	A user modeling-based performance analysis of a wizarded uncertainty-adaptive dialogue system corpus	examining the impacts of dialogue content and system automation on affect models in a spoken tutorial dialogue system	"['J Drummond', 'D Litman']"		"Many dialogue system developers use data gathered from previous versions of the dialogue system to build models which enable the system to detect and respond to users ' affect. Previous work in the dialogue systems community for domain adaptation has shown that large differences between versions of dialogue systems affect performance of ported models. Thus, we wish to investigate how more minor differences, like small dialogue content changes and switching from a wizarded system to a fully automated system, influence the performance of our affect detection models. We perform a post-hoc experiment where we use various data sets to train multiple models, and compare against a test set from the most recent version of our dialogue system. Analyzing these results strongly suggests that these differences do impact these models ' performance"	"Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) ."	"['To train our DISE model, we first extracted the set of speech and dialogue features shown in Figure 2 from the user turns in our corpus.', 'As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue.', 'The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\'s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (""incorrect runs"").', 'We also included two user-based features, gender and pretest score.', 'Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .', 'To date, however, these features have only decreased the crossvalidation performance of our models.', '8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of ""response appropriateness"" in other domains, while pretest score corresponds to the general notion of domain expertise).', 'Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed.', 'To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users.']"	2
CC980	P00-1004	Translation with Cascaded Finite State Transducers	a polynomialtime algorithm for statistical machine translation	['D Wu']	introduction		"Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( #AUTHOR_TAG ) ."	"['Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( #AUTHOR_TAG ) .']"	0
CC981	P00-1004	Translation with Cascaded Finite State Transducers	a polynomialtime algorithm for statistical machine translation	['D Wu']			It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG ) .	['It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG ) .']	5
CC982	P00-1006	A Maximum Entropy/Minimum Divergence Translation Model	a maximum entropy approach to natural language processing	"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']"	method	"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."	It can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .	['It can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .']	4
CC983	P00-1006	A Maximum Entropy/Minimum Divergence Translation Model	a maximum entropy approach to natural language processing	"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']"		"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."	#AUTHOR_TAG describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .	['#AUTHOR_TAG describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .']	0
CC984	P00-1006	A Maximum Entropy/Minimum Divergence Translation Model	a maximum entropy approach to natural language processing	"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']"	introduction	"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."	A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .	['A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .']	5
CC985	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	memorybased shallow parsing	"['Walter Daelemans', 'Sabine Buchholz', 'Jorn Veenstra']"	conclusion		"As ( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well ."	"['As ( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well .']"	3
CC986	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	statistical decisiontree models for parsing	['David M Magerman']			"The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed ."	"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .']"	1
CC987	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	three generative lexicalised models for statistical parsing	['M Collins']	introduction	"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97"	"A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , #AUTHOR_TAG , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) ."	"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , #AUTHOR_TAG , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']"	0
CC988	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	structural ambiguity and lexical relations	"['D Hindle', 'M Rooth']"	conclusion	"We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus. This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning."	"It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG , for PP attachment ) ."	"['It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG , for PP attachment ) .']"	1
CC989	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	a maximumentropy partial parser for unrestricted text	"['W Skut', 'T Brants']"	introduction		Another approach for partial parsing was presented by #AUTHOR_TAG .	['Another approach for partial parsing was presented by #AUTHOR_TAG .']	0
CC990	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	a linear observed time statistical parser based on maximum entropy models	['A Ratnaparkhi']	introduction	"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall."	"A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , #AUTHOR_TAG , and Sekine ( 1998 ) ) ."	"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , #AUTHOR_TAG , and Sekine ( 1998 ) ) .']"	0
CC991	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	three generative lexicalised models for statistical parsing	['M Collins']		"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97"	"Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used ."	"['Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used .']"	1
CC992	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	cascaded grammatical relation assignment	"['S Buchholz', 'J Veenstra', 'W Daelemans']"	conclusion	"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder."	"In a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures ."	"['In a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures .']"	3
CC993	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	statistical decisiontree models for parsing	['David M Magerman']	introduction		"A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , #AUTHOR_TAG , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) ."	"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , #AUTHOR_TAG , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']"	0
CC994	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	a linear observed time statistical parser based on maximum entropy models	['A Ratnaparkhi']		"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall."	"The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed ."	"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .']"	1
CC995	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	three generative lexicalised models for statistical parsing	['M Collins']		"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97"	"The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , #AUTHOR_TAG , and Ratnaparkhi ( 1997 ) , and became a common testbed ."	"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , #AUTHOR_TAG , and Ratnaparkhi ( 1997 ) , and became a common testbed .']"	1
CC996	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	cascaded grammatical relation assignment	"['S Buchholz', 'J Veenstra', 'W Daelemans']"	introduction	"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder."	"One approach to partial parsing was presented by #AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing ."	"['One approach to partial parsing was presented by #AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing .']"	0
CC997	P00-1007	Incorporating Compositional Evidence in Memory-Based Partial Parsing	a maximumentropy partial parser for unrestricted text	"['W Skut', 'T Brants']"	conclusion		"In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures ."	"['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']"	3
CC998	P00-1012	The order of prenominal adjectives in natural language generation	ordering among premodifiers	"['James Shaw', 'Vasileios Hatzivassiloglou']"	experiments	"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus."	"To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG propose to assign a weight to each link ."	"['Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the second.', 'The first ordered pairs are more frequent, as are the individual adjectives involved.', 'To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG propose to assign a weight to each link .', 'Say the order a, b occurs m times and the pair {a, b} occurs n times in total.', 'Then the weight of the pair a → b is:']"	0
CC999	P00-1012	The order of prenominal adjectives in natural language generation	distributional clustering of english words	"['Fernando Pereira', 'Naftali Tishby', 'Lilian Lee']"	conclusion	"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."	"More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself ."	"['While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.', 'Future work will pursue at least two directions for improving the results.', 'First, while semantic information is not available for all adjectives, it is clearly available for some.', 'Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .', 'Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results.']"	3
CC1000	P00-1012	The order of prenominal adjectives in natural language generation	ordering among premodifiers	"['James Shaw', 'Vasileios Hatzivassiloglou']"	experiments	"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus."	The simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method .	"['The simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method .', 'To order the pair {a, b}, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often.']"	0
CC1001	P00-1012	The order of prenominal adjectives in natural language generation	boosting applied to tagging and pp attachment	"['Steven Abney', 'Robert E Schapire', 'Yoram Singer']"	conclusion	Boosting is a machine learning algorithm that is not well known in computational linguistics. We apply it to part-of-speech tagging and prepositional phrase attachment. Performance is very encouraging. We also show how to improve data quality by using boosting to identify annotation errors.	"In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly ."	"['The second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data.', 'It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .']"	3
CC1002	P00-1012	The order of prenominal adjectives in natural language generation	ordering among premodifiers	"['James Shaw', 'Vasileios Hatzivassiloglou']"	experiments	"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus."	#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .	"['One way to think of the direct evidence method is to see that it defines a relation ≺ on the set of English adjectives.', 'Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a ≺ b.', 'If the re-verse is true, and b, a is found more often than a, b , then b ≺ a.', 'If neither order appears in the training data, then neither a ≺ b nor b ≺ a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .', 'That is, if a ≺ c and c ≺ b, we can conclude that a ≺ b.', 'To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'However, the pairs large, new and new, green occur fairly frequently.', 'Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order.']"	0
CC1003	P00-1012	The order of prenominal adjectives in natural language generation	generation that exploits corpusbased statistical knowledge	"['Irene Langkilde', 'Kevin Knight']"	method	"We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities."	"One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."	"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.', 'It also should be straightforwardly applicable to the more specific problem we are addressing here.', 'To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.', 'This has the advantage of reducing the problem of adjective ordering to the problem of estimating n-gram probabilities, something which is relatively well understood.']"	5
CC1004	P02-1001	Parameter estimation for probabilistic finite-state transducers	translation with finitestate devices	"['Kevin Knight', 'Yaser Al-Onaizan']"	introduction		"Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG ) ."	"['The availability of toolkits for this weighted case (Mohri et al., 1998;van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"	0
CC1005	P02-1001	Parameter estimation for probabilistic finite-state transducers	a rational design for a weighted finitestate transducer library	"['M Mohri', 'F Pereira', 'M Riley']"			"fÎ¸ on demand ( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently . )"	"['10 Traditionally log(strength) values are called weights, but this paper uses ""weight"" to mean something else.', 'fÎ¸ on demand ( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently . )']"	0
CC1006	P02-1001	Parameter estimation for probabilistic finite-state transducers	rational series and their languages	"['Jean Berstel', 'Christophe Reutenauer']"		"I. Rational Series.- 1 Semirings.- 2 Formal Series.- 3 The Topology of K""X"".- 4 Rational Series.- 5 Recognizable Series.- 6 The Fundamental Theorem.- Exercises for Chapter I.- Notes to Chapter I.- II. Minimization.- 1 Syntactic Ideals.- 2 Reduced Linear Representations.- 3 The Reduction Algorithm.- Exercises for Chapter II.- Notes to Chapter II.- III. Series and Languages.- 1 The Theorem of Kleene.- 2 Series and Rational Languages.- 3 Supports.- 4 Iteration.- 5 Complementation.- Exercises for Chapter III.- Notes to Chapter III.- IV. Rational Series in One Variable.- 1 Rational Functions.- 2 The Exponential Polynomial.- 3 A Theorem of Polya.- 4 A Theorem of Skolem, Mahler and Lech.- Notes to Chapter IV.- V. Changing the Semiring.- 1 Rational Series over a Principal Ring.- 2 Positive Rational Series.- 3 Fatou Extensions.- Exercises for Chapter V.- Notes to Chapter V.- VI. Decidability.- 1 Problems of Supports.- 2 Growth.- Exercises for Chapter VI.- Notes to Chapter VI.- VII. Noncommutative Polynomials.- 1 The Weak Algorithm.- 2 Continuant Polynomials.- 3 Inertia.- 4 Gauss's Lemma.- Exercises for Chapter VII.- Notes to Chapter VII.- VIII. Codes and Formal Series.- 1 Codes.- 2 Completeness.- 3 The Degree of a Code.- 4 Factorization.- Exercises for Chapter VIII.- Notes to Chapter VIII.- References."	#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .	"['The denominator of equation ( 1) is the total probability of all accepting paths in x i • f • y i .', 'But while computing this, we will also compute the numerator.', 'The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'We will enforce an invariant: the weight of any pathset Π must be ( π∈Π P (π), π∈Π P (π) val(π)) ∈ R ≥0 × V , from which (1) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .', 'Multiplication and addition are replaced by binary operations ⊗ and ⊕ on K. Thus ⊗ is used to combine arc weights into a path weight and ⊕ is used to combine the weights of alternative paths.', 'To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = ∞ i=0 k i .', 'The usual finite-state algorithms work if (K, ⊕, ⊗, * ) has the structure of a closed semiring. 15', 'rdinary probabilities fall in the semiring (R ≥0 , +, ×, * ). 16', 'Our novel weights fall in a novel 14 Formal derivation of (1):']"	5
CC1007	P02-1001	Parameter estimation for probabilistic finite-state transducers	an inequality and associated maximization technique in statistical estimation of probabilistic functions of a markov process	['L E Baum']			"For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ) ."	"['• In many cases of interest, T i is an acyclic graph. 20', ""hen Tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of ⊕ and ⊗ operations."", 'For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ) .', 'But notice that it has no backward pass.', 'In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.', 'This is slower because our ⊕ and ⊗ are vector operations, and the vectors rapidly lose sparsity as they are added together.']"	0
CC1008	P02-1001	Parameter estimation for probabilistic finite-state transducers	a rational design for a weighted finitestate transducer library	"['M Mohri', 'F Pereira', 'M Riley']"	introduction		"The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP ."	"['The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', 'Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998).', 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"	0
CC1009	P02-1001	Parameter estimation for probabilistic finite-state transducers	learning string edit distance	"['E Ristad', 'P Yianilos']"	introduction	"In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other. In this report, we provide a stochastic model for string-edit distance. Our stochastic model allows us to learn a string-edit distance function from a corpus of examples. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech. In this application, we learn a string-edit distance with nearly one-fifth the error rate of the untrained Levenshtein distance. Our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototypes."	"For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #AUTHOR_TAG ) trains only stochastic edit distance ."	"['Unfortunately, there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #AUTHOR_TAG ) trains only stochastic edit distance .']"	0
CC1010	P02-1001	Parameter estimation for probabilistic finite-state transducers	iterative methods for solving linear systems	['Anne Greenbaum']		"bthkhady`  withiithamchamsamhrabhaaphlechlyrabbechingesnmii`yuudwykan 2 aebbaihy + khuue` withiithamcham`yaangningkabwithiipriphuumiy`yaikhrl`f bthkhwaamwichaakaarnii`phipraayaenwkhidthawaipaelaethkhnikhphuuenthaankh`ngwithiithamcham`yaangning aidaek withiicchaaokhbii withiiekaas-aichedl aelawithiiph`nprnekinsuuebenuue`ng n`kcchaaknanyangphicchaarnaawithiithamchamthiiphathnaat`y`dcchaakwithiidangklaaw aidaek withiiph`nprnekinaebberng withiithamchamthiimiithaancchaakekrediiynt eaelawithiithamchamkamlangs`ngn`ysud samhrabwithiipriphuumiy`yaikhrl`fnanmiitnaebbmaacchaakwithiikh`ncchuuektekrediiynt withiidangklaawcchasraangthaanhlakechingtangchaakkh`ngpriphuumiaebbyukhlidcchaakemthrikchsamprasiththiodyphicchaarnaacchaakekrediiyntkh`ngfangkchankamlangs`ngthiis`dkhl`ng thaanhlakdangklaawprak`bdwyewket`rthiimiithisthaangthiithamaihphlechlykhaapramaanekhaaaiklphlechlycchringaiderwthiisud klaawodysrupaidwaa withiithamcham`yaangning 4 withiiaerkthiiklaawmaanancchakaarantiikaarluuekhaakh`nglamdabkh`ngphlechlyodypramaansuuphlechlycchringemuue`aichkabrabbthiimiiemthrikchsamprasiththi`yuuainruupaebbechphaaa echn emthrikchaenwthaeyngmumkhmaeth emthrikchldth`naimaid aelaemthrikchaebbae`l odyt`ngkamhndtawaepresrimthiiehmaaasm swnwithiithamchamthiimiithaancchaakekrediiyntaelawithiithamchamkamlangs`ngn`ysudaichaidkabrabbthiimiiemthrikchsamprasiththimiikhaalamdabchanetm samhrabwithiikh`ncchuuektekrediiyntaichaidkabrabbthiiemthrikchsamprasiththiepnemthrikchsmmaatrthiiepnbwkaenn`n   khamsamkhay: rabbechingesn withiiph`nprnekinsuuebenuue`ng  withiiph`nprnekinaebberng withiithamchamthiimiithaancchaakekrediiynt  withiikh`ncchuuektekrediiynt     ABSTRACT There are two major types of iterative methods for solving linear systems, namely, stationary iterative methods and Krylov subspace methods. This survey article discusses general ideas and elementary techniques for stationary iterative methods such as Jacobi method, Gauss-Seidel method, and the successive over-relaxation method. Moreover, we investigate further developed methods, namely, the accelerated over-relaxation method, the gradient based iterative method, and the least squares iterative method. On the other hand, Krylov subspace methods have prototypes from the conjugate gradient method. The latter method constructs an orthogonal basis for the Euclidean space from the gradient of the associated quadratic function. Such basis consists of vectors in directions so that the approximated solutions fastest approach to the exact solution. In conclusions, all 1st-4th mentioned stationary iterative methods guarantee the convergence of the sequence of approximated solutions to the exact solution when applying to the system with specific coefficient matrices such as strictly diagonally dominant matrices, irreducible matrices, and L-matrices. Here, the parameters in the methods must be appropriate. The gradient based iterative method and the least squares iterative method can be applied to systems with full-column rank coefficient matrices. The conjugate gradient method is applicable for the system whose coefficient matrix is a positive definite symmetric matrix.Keywords: linear system, successive over-relaxation method, accelerated over-relaxation method, gradient based iterative method, conjugate gradient metho"	"The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG ) ."	"['We therefore reintroduce a backward pass that lets us avoid ⊕ and ⊗ when computing t i (so they are needed only to construct T i ).', 'This speedup also works for cyclic graphs and for any V .', 'Write w jk as (p jk , v jk ), and let w 1 jk = (p 1 jk , v 1 jk ) denote the weight of the edge from j to k. 19 Then it can be shown that w 0n = (p 0n , j,k p 0j v 1 jk p kn ).', 'The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â\x88\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG ) .']"	0
CC1011	P02-1001	Parameter estimation for probabilistic finite-state transducers	rational series and their languages	"['Jean Berstel', 'Christophe Reutenauer']"	introduction	"I. Rational Series.- 1 Semirings.- 2 Formal Series.- 3 The Topology of K""X"".- 4 Rational Series.- 5 Recognizable Series.- 6 The Fundamental Theorem.- Exercises for Chapter I.- Notes to Chapter I.- II. Minimization.- 1 Syntactic Ideals.- 2 Reduced Linear Representations.- 3 The Reduction Algorithm.- Exercises for Chapter II.- Notes to Chapter II.- III. Series and Languages.- 1 The Theorem of Kleene.- 2 Series and Rational Languages.- 3 Supports.- 4 Iteration.- 5 Complementation.- Exercises for Chapter III.- Notes to Chapter III.- IV. Rational Series in One Variable.- 1 Rational Functions.- 2 The Exponential Polynomial.- 3 A Theorem of Polya.- 4 A Theorem of Skolem, Mahler and Lech.- Notes to Chapter IV.- V. Changing the Semiring.- 1 Rational Series over a Principal Ring.- 2 Positive Rational Series.- 3 Fatou Extensions.- Exercises for Chapter V.- Notes to Chapter V.- VI. Decidability.- 1 Problems of Supports.- 2 Growth.- Exercises for Chapter VI.- Notes to Chapter VI.- VII. Noncommutative Polynomials.- 1 The Weak Algorithm.- 2 Continuant Polynomials.- 3 Inertia.- 4 Gauss's Lemma.- Exercises for Chapter VII.- Notes to Chapter VII.- VIII. Codes and Formal Series.- 1 Codes.- 2 Completeness.- 3 The Degree of a Code.- 4 Factorization.- Exercises for Chapter VIII.- Notes to Chapter VIII.- References."	"4To prove ( 1 ) â ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp ."	"['4To prove ( 1 ) â\x87\x92 ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .', 'A full proof is straightforward, as are proofs of (3)_(2), (2)_(1).']"	5
CC1012	P02-1001	Parameter estimation for probabilistic finite-state transducers	a systolic array algorithm for the algebraic path problem shortest paths matrix inversion	['G¨unter Rote']		"It is shown how the Gauss-Jordan Elimination algorithm for the Algebraic Path Problem can be implemented on a hexagonal systolic array of a quadratic number of simple processors in linear time. Special instances of this general algorithm include parallelizations of the Warshall-Floyd Algorithm, which computes the shortest distances in a graph or the transitive closure of a relation, and of the Gauss-Jordan Elimination algorithm for computing the inverse of a real matrix.ZusammenfassungEs wird dargestellt, wie man den gaus-Jordanschen Eliminationsalgorithmus fur das algebraische Wegproblem auf einem hexagonalen systolischen Feld (systolic array) mit einer quadratischen Anzahl einfacher Prozessoren in linearer Zeit ausfuhren kann. Zu den Anwendungsbeispielen dieses allgemeinen Algorithmus gehort der Warshall-Floyd-Algorithmus zur Berechnung der kurzesten Wegen in einem Graphen oder zur Bestimmung der transitiven Hulle einer Relation sowie der Gauss-Jordansche Eliminationsalgorithmus zur Inversion reeller Matrizen."	Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG ) .	"['Division and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ).', 'Division is commonly used in defining f θ (for normalization). 19', 'Multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG ) .']"	3
CC1013	P02-1001	Parameter estimation for probabilistic finite-state transducers	expectation semirings flexible em for finitestate transducers	['Jason Eisner']			"Per-state joint normalization ( #AUTHOR_TAGb , Â§ 8.2 ) is similar but drops the dependence on a ."	"['13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a ∈ Σ; their weights are normalized to sum to 1.', 'Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a.', 'Then the predicted vector given θ is j,a dj,a • (expected feature counts on a randomly chosen arc in Dj,a).', 'Per-state joint normalization ( #AUTHOR_TAGb , Â§ 8.2 ) is similar but drops the dependence on a .', 'The difficult case is global conditional normalization.', 'It arises, for example, when training a joint model of the form']"	1
CC1014	P02-1001	Parameter estimation for probabilistic finite-state transducers	maximum likelihood from incomplete data via the em algorithm	"['A P Dempster', 'N M Laird', 'D B Rubin']"			The EM algorithm ( #AUTHOR_TAG ) can maximize these functions .	"['The EM algorithm ( #AUTHOR_TAG ) can maximize these functions .', 'Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f θ , which FST paths stand a chance of having been the path used?', '(Guessing the path also guesses the exact input and output.)', 'The M step updates θ to make those paths more likely.', 'EM alternates these steps and converges to a local optimum.', ""The M step's form depends on the parameterization and the E step serves the M step's needs.""]"	5
CC1015	P02-1001	Parameter estimation for probabilistic finite-state transducers	compilation of weighted finitestate transducers from decision trees	"['Richard Sproat', 'Michael Riley']"	introduction	"We report on a method for compiling decision trees into weighted finite-state transducers. The key assumptions are that the tree predictions specify how to rewrite symbols from an input string, and the decision at each tree node is stateable in terms of regular expressions on the input string. Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996)."	"For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below ."	"['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']"	0
CC1016	P02-1001	Parameter estimation for probabilistic finite-state transducers	algebraic structures for transitive closure	['D J Lehmann']		"AbstractClosed semi-rings and the closure of matrices over closed semi-rings are defined and studied. Closed semi-rings are structures weaker than the structures studied by Conway [3] and Aho, Hopcroft and Ullman [1]. Examples of closed semi-rings and closure operations are given, including the case of semi-rings on which the closure of an element is not always defined. Two algorithms are proved to compute the closure of a matrix over any closed semi-ring; the first one based on Gauss-Jordan elimination is a generalization of algorithms by Warshall, Floyd and Kleene; the second one based on Gauss elimination has been studied by Tarjan [11, 12], from the complexity point of view in a slightly different framework. Simple semi-rings, where the closure operation for elements is trivial, are defined and it is shown that the closure of an n x n-matrix over a simple semi-ring is the sum of its powers of degree less than n. Dijkstra semi-rings are defined and it is shown that the rows of the closure of a matrix over a Dijkstra semi-ring, can be computed by a generalized version of Dijkstra's algorithm"	"Now for some important remarks on efficiency : â¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted)."	"['Now for some important remarks on efficiency : â\x80¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).', 'It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (Mohri, 2002) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version.', 'If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph Ti, Tarjan (1981b) shows how to partition into �hard� subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.', 'The overhead of partitioning and recombining is essentially only O(m).']"	0
CC1017	P02-1001	Parameter estimation for probabilistic finite-state transducers	conditional random fields probabilistic models for segmenting and labeling sequence data	"['J Lafferty', 'A McCallum', 'F Pereira']"	introduction	"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."	"Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; #AUTHOR_TAG ) ."	"['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; #AUTHOR_TAG ) .']"	0
CC1018	P02-1001	Parameter estimation for probabilistic finite-state transducers	a gaussian prior for smoothing maximum entropy models	"['Stanley F Chen', 'Ronald Rosenfeld']"		"Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance."	"In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG ) ."	"['Maximum-posterior estimation tries to maximize P (θ) • i f θ (x i , y i ) where P (θ) is a prior probability.', 'In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG ) .']"	5
CC1019	P02-1001	Parameter estimation for probabilistic finite-state transducers	hidden markov models with finite state supervision in	['E Ristad']		"In this chapter we provide a supervised training paradigm for hidden Markov models (HMMs). Unlike popular ad-hoc approaches, our paradigm is completely general, need not make any simplifying assumptions about independence, and can take better advantage of the information contained in the training corpus."	"For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11"	"['As training data we are given a set of observed (input, output) pairs, (xi, yi).', 'These are assumed to be independent random samples from a joint dis- tribution of the form f_�(x, y); the goal is to recover the true _�.', 'Samples need not be fully observed (partly supervised training): thus xi _ __, yi _ �_ may be given as regular sets in which input and output were observed to fall.', 'For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11']"	0
CC1020	P02-1001	Parameter estimation for probabilistic finite-state transducers	speech recognition by composition of weighted finite automata	"['Fernando C N Pereira', 'Michael Riley']"	introduction	"We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition."	"Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."	"['The availability of toolkits for this weighted case (Mohri et al., 1998;van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"	0
CC1021	P02-1001	Parameter estimation for probabilistic finite-state transducers	generic epsilonremoval and input epsilonnormalization algorithms for weighted transducers	['M Mohri']			"It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version ."	"['• Computing t i is an instance of the well-known algebraic path problem (Lehmann, 1977;Tarjan, 1981a).', 'Let T i = x i •f •y i .', 'Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted).', 'It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .', 'If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph T i , Tarjan (1981b) shows how to partition into ""hard"" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.', 'The overhead of partitioning and recombining is essentially only O(m).']"	0
CC1022	P02-1001	Parameter estimation for probabilistic finite-state transducers	conditional random fields probabilistic models for segmenting and labeling sequence data	"['J Lafferty', 'A McCallum', 'F Pereira']"	introduction	"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."	"Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG ) ."	"['• An easy approach is to normalize the options at each state to make the FST Markovian.', 'Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.', 'Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG ) .', 'Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since ""dead ends"" leak probability mass). 8', ' A better-founded approach is global normalization, which simply divides each f (x, y) by']"	0
CC1023	P02-1001	Parameter estimation for probabilistic finite-state transducers	expectation semirings flexible em for finitestate transducers	['Jason Eisner']	introduction		"A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAGa ) ."	"['A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAGa ) .', 'A leisurely journal-length version with more details has been prepared and is available.']"	2
CC1024	P02-1001	Parameter estimation for probabilistic finite-state transducers	regular approximation of contextfree grammars through transformation	"['M Mohri', 'M-J Nederhof']"	introduction	We present an algorithm for approximating context-free languages with regular languages. The algorithm is based on a simple transformation that applies to any context-free grammar and guarantees that the result can be compiled into a finite automaton. The resulting grammar contains at most one new nonterminal for any nonterminal symbol of the input grammar. The result thus remains readable and if necessary modifiable. We extend the approximation algorithm to the case of weighted context-free grammars. We also report experiments with several grammars showing that the size of the minimal deterministic automata accepting the resulting approximations is of practical use for applications such as speech recognition.	"A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation ."	"['w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. 6,7', 'here are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988).', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution.""]"	0
CC1025	P02-1001	Parameter estimation for probabilistic finite-state transducers	an efficient compiler for weighted rewrite rules	"['Mehryar Mohri', 'Richard Sproat']"	introduction	"Context-dependent rewrite rules are used in many areas of natural language and speech processing. Work in computational phonology has demonstrated that, given certain conditions, such rewrite rules can be represented as finite-state transducers (FSTs). We describe a new algorithm for compiling rewrite rules into FSTs. We show the algorithm to be simpler and more efficient than existing algorithms. Further, many of our applications demand the ability to compile weighted rules into weighted FSTs, transducers generalized by providing transitions with weights. We have extended the algorithm to allow for this."	"For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below ."	"['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']"	0
CC1026	P02-1001	Parameter estimation for probabilistic finite-state transducers	maximum entropy markov models for information extraction and segmentation	"['A McCallum', 'D Freitag', 'F Pereira']"	introduction	"Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ's."	"Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) ."	"['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) .']"	0
CC1027	P02-1001	Parameter estimation for probabilistic finite-state transducers	practical experiments with regular approximation of contextfree languages	['Mark-Jan Nederhof']	introduction		"A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation ."	"['w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. 6,7', 'here are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988).', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution.""]"	0
CC1028	P02-1001	Parameter estimation for probabilistic finite-state transducers	an inequality and associated maximization technique in statistical estimation of probabilistic functions of a markov process	['L E Baum']	introduction		"For example , the forward-backward algorithm ( #AUTHOR_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance ."	"['Unfortunately, there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm ( #AUTHOR_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .']"	0
CC1029	P02-1001	Parameter estimation for probabilistic finite-state transducers	expectation semirings flexible em for finitestate transducers	['Jason Eisner']	introduction		"Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) ."	"['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"	0
CC1030	P02-1001	Parameter estimation for probabilistic finite-state transducers	inducing features of random fields	"['S Della Pietra', 'V Della Pietra', 'J Lafferty']"		"We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."	"The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and"	"['• If arc probabilities (or even λ, ν, µ, ρ) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f θ whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (Σ * , ∆ * ).', 'If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13']"	5
CC1031	P02-1001	Parameter estimation for probabilistic finite-state transducers	speech recognition by composition of weighted finite automata	"['Fernando C N Pereira', 'Michael Riley']"	introduction	"We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition."	"A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) ."	"['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (ρ). 4 To prove (1)⇒(3), express f as an FST and apply the well-known Kleene-Schützenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'A full proof is straightforward, as are proofs of (3)⇒( 2), ( 2)⇒(1).']"	0
CC1032	P05-3005	Dynamically generating a protein entity dictionary using online resources	dr introducing refseq and locuslink curated human genome resources at the ncbi trends genet	"['Pruitt KD', 'Katz KS', 'H Sicotte', 'Maglott']"			"The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and"	"['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and', 'Additionally, several model organism databases or nomenclature databases were used.', 'Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14].', 'The following provides a brief description of each of the database.']"	5
CC1033	P05-3005	Dynamically generating a protein entity dictionary using online resources	sgd saccharomyces genome database nucleic acids res	"['Cherry JM', 'C Adler', 'C Ball', 'Chervitz SA', 'Dwight SS', 'Hester ET', 'Y Jia', 'G Juvik', 'T Roe', 'M Schroeder']"			"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."	"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"	5
CC1034	P05-3005	Dynamically generating a protein entity dictionary using online resources	boddy wj et al the mouse genome database mgd integrating biology with the genome nucleic acids res	"['Bult CJ', 'Blake JA', 'Richardson JE', 'Kadin JA', 'Eppig JT', 'Baldarelli RM', 'K Barsanti', 'M Baya', 'Beal JS']"			"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."	"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"	5
CC1035	P05-3005	Dynamically generating a protein entity dictionary using online resources	al wormbase a multispecies resource for nematode biology and genomics nucleic acids res	"['Harris TW', 'N Chen', 'F Cunningham', 'M TelloRuiz', 'I Antoshechkin', 'C Bastiani', 'T Bieri', 'D Blasiar', 'K Bradnam', 'Chan J et']"			"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #AUTHOR_TAG , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."	"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #AUTHOR_TAG , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"	5
CC1036	P05-3005	Dynamically generating a protein entity dictionary using online resources	the unified medical language system umls integrating biomedical terminology	['O Bodenreider']		"The Unified Medical Language System (http://umlsks.nlm.nih.gov) is a repository of biomedical vocabularies developed by the US National Library of Medicine. The UMLS integrates over 2 million names for some 900,000 concepts from more than 60 families of biomedical vocabularies, as well as 12 million relations among these concepts. Vocabularies integrated in the UMLS Metathesaurus include the NCBI taxonomy, Gene Ontology, the Medical Subject Headings (MeSH), OMIM and the Digital Anatomist Symbolic Knowledge Base. UMLS concepts are not only inter-related, but may also be linked to external resources such as GenBank. In addition to data, the UMLS includes tools for customizing the Metathesaurus (MetamorphoSys), for generating lexical variants of concept names (lvg) and for extracting UMLS concepts from text (MetaMap). The UMLS knowledge sources are updated quarterly. All vocabularies are available at no fee for research purposes within an institution, but UMLS users are required to sign a license agreement. The UMLS knowledge sources are distributed on CD-ROM and by FTP."	The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG .	"['The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG .', 'It contains three knowledge sources: the Metathesaurus (META), the SPECIALIST lexicon, and the Semantic Network.', 'The META provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'The SPECIALIST lexicon contains syntactic information for many terms, component words, and English words, including verbs, which do not appear in the META.', 'The Semantic Network contains information about the types or categories (e.g., ""Disease or Syndrome"", ""Virus"") to which all META concepts have been assigned.']"	0
CC1037	P05-3005	Dynamically generating a protein entity dictionary using online resources	mining the biomedical literature in the genomic era an overview	"['H Shatkay', 'R Feldman']"	introduction	"The past decade has seen a tremendous growth in the amount of experimental and computational biomedical data, specifically in the areas of genomics and proteomics. This growth is accompanied by an accelerated increase in the number of biomedical publications discussing the findings. In the last few years, there has been a lot of interest within the scientific community in literature-mining tools to help sort through this abundance of literature and find the nuggets of information most relevant and useful for specific analysis tasks. This paper provides a road map to the various literature-mining methods, both in general and within bioinformatics. It surveys the disciplines involved in unstructured-text analysis, categorizes current work in biomedical literature mining with respect to these disciplines, and provides examples of text analysis methods applied towards meeting some of the current challenges in bioinformatics."	"With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG ."	"['With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining.', 'Such task is called biological entity tagging.', 'Biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely: synonymy (i.e., different terms refer to the same entity), ambiguity (i.e., one term is associated with different entities), and coverage (i.e., entity terms or entities are not present in databases or knowledge bases).']"	0
CC1038	P05-3005	Dynamically generating a protein entity dictionary using online resources	online mendelian inheritance in man omim a knowledgebase of human genes and genetic disorders nucleic acids res	"['A Hamosh', 'Scott AF', 'Amberger JS', 'Bocchini CA', 'McKusick VA']"		"Online Mendelian Inheritance in Man (OMIM) is a comprehensive, authoritative and timely knowledgebase of human genes and genetic disorders compiled to support human genetics research and education and the practice of clinical genetics. Started by Dr Victor A. McKusick as the definitive reference Mendelian Inheritance in Man, OMIM (http://www.ncbi.nlm.nih.gov/omim/) is now distributed electronically by the National Center for Biotechnology Information, where it is integrated with the Entrez suite of databases. Derived from the biomedical literature, OMIM is written and edited at Johns Hopkins University with input from scientists and physicians around the world. Each OMIM entry has a full-text summary of a genetically determined phenotype and/or gene and has numerous links to other genetic databases such as DNA and protein sequence, PubMed references, general and locus-specific mutation databases, HUGO nomenclature, MapViewer, GeneTests, patient support groups and many others. OMIM is an easy and straightforward portal to the burgeoning information in human genetics."	"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."	"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"	5
CC1039	P05-3005	Dynamically generating a protein entity dictionary using online resources	godzik a clustering of highly homologous sequences to reduce the size of large protein databases bioinformatics	"['W Li', 'L Jaroszewski']"		"We present a fast and flexible program for clustering large protein databases at different sequence identity levels. It takes less than 2 h for the all-against-all sequence comparison and clustering of the non-redundant protein database of over 560,000 sequences on a high-end PC. The output database, including only the representative sequences, can be used for more efficient and sensitive database searches."	"Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence ."	"['PIR Resources -There are three databases in PIR: the Protein Sequence Database (PSD), iProClass, and PIR-NREF.', 'PSD database includes functionally annotated protein sequences.', 'The iProClass database is a central point for exploration of protein information, which provides summary descriptions of protein family, function and structure for all protein sequences from PIR, Swiss-Prot, and TrEMBL (now UniProt).', 'Additionally, it links to over 70 biological databases in the world.', 'The PIR-NREF database is a comprehensive database for sequence searching and protein identification.', 'It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB.', 'Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .', 'NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE.', 'GenPept entries are those translated from the GenBanknucleotide sequence database.', 'RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms.', ""Entrez GENE provides a unified query environment for genes defined by sequence and/or in NCBI's Map Viewer."", 'It records gene names, symbols, and many other attributes associated with genes and the products they encode.']"	5
CC1040	P05-3005	Dynamically generating a protein entity dictionary using online resources	kwitek a et al rat genome database rgd mapping disease onto the genome nucleic acids res	"['S Twigger', 'J Lu', 'M Shimoyama', 'D Chen', 'D Pasko', 'H Long', 'J Ginster', 'Chen CF', 'R Nigam']"			"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."	"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"	5
CC1041	P05-3005	Dynamically generating a protein entity dictionary using online resources	suzek be et al the protein information resource nucleic acids res	"['Wu CH', 'Yeh LS', 'H Huang', 'L Arminski', 'J Castro-Alvear', 'Y Chen', 'Z Hu', 'P Kourtesis', 'Ledley RS']"			"The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG , and"	"['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG , and', 'Additionally, several model organism databases or nomenclature databases were used.', 'Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14].', 'The following provides a brief description of each of the database.']"	5
CC1042	P05-3005	Dynamically generating a protein entity dictionary using online resources	the flybase database of the drosophila genome projects and community literature nucleic acids res	['F Consortium']		"FlyBase (http://flybase.bio.indiana.edu/) provides an integrated view of the fundamental genomic and genetic data on the major genetic model Drosophila melanogaster and related species. FlyBase has primary responsibility for the continual reannotation of the D. melanogaster genome. The ultimate goal of the reannotation effort is to decorate the euchromatic sequence of the genome with as much biological information as is available from the community and from the major genome project centers. A complete revision of the annotations of the now-finished euchromatic genomic sequence has been completed. There are many points of entry to the genome within FlyBase, most notably through maps, gene products and ontologies, structured phenotypic and gene expression data, and anatomy."	"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."	"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"	5
CC1043	P05-3005	Dynamically generating a protein entity dictionary using online resources	enzyme nomenclature functional or structural rna	['P Gegenheimer']		"Altman and colleagues (this issue) call attention to the inability of current standardized enzyme nomenclature to distinguish between enzymatic activities that reside in nonhomologous macromolecules+ This issue is highlighted by the fact that the pre-tRNA 59-maturation activities of bacteria and plant chloroplasts present the first instance (of which I am aware) of two naturally occurring enzymes that cannot be evolutionarily related, but which catalyze an identical reaction+ (In the classic example of convergent evolution between the trypsin family and subtilisin, the enzymes do not have an identical substrate specificity+) Altman and colleagues propose that a single trivial name be used only for members of a family of homologous macromolecules; in other words, that different trivial names be given to enzymes that catalyze the same precursor-product conversion but do so with different catalytic mechanisms, or which are not members of a single family of homologous macromolecules+ I am not convinced that there is a problem needing solution+ The current proposal seems to run counter to the rationale behind current EC nomenclature, and could create more confusion than it would alleviate+ One can distinguish between a function-based nomenclature based on the biochemical reaction catalyzed--the substrate-product conversion--and a structure-based nomenclature based on the physical nature of the catalyst+ For a classical enzymologist, the reaction type being catalyzed is paramount: It is the reaction that one uses to purify the enzyme+ One identifies the enzyme based on its activity,whereas its physical structure may initially be of secondary importance+ The value of function-based nomenclature is precisely that it allows the biochemical reaction (the substrate- product conversion) to be described, specified, and studied concomitant with continuing purification and analysis of the corresponding enzyme+ Further, as more is learned about the enzyme's structure and catalytic mechanism, it is not necessary to rename it+ Indeed, the utility of function-based nomenclature is exemplified by the history of bacterial RNase P purification and characterization+"	"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG ."	"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG .']"	5
CC1044	P07-1007	Estimating class priors in domain adaptation for word sense disambiguation	an empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation	"['Y K Lee', 'H T Ng']"		"In this paper, we evaluate a variety  of knowledge sources and supervised  learning algorithms for word sense  disambiguation on SENSEVAL-2 and  SENSEVAL-1 data. Our knowledge  sources include the part-of-speech of  neighboring words, single words in the  surrounding context, local collocations,  and syntactic relations. The learning algorithms  evaluated include Support Vector  Machines (SVM), Naive Bayes, AdaBoost,  and decision tree algorithms. We  present empirical results showing the relative  contribution of the component knowledge  sources and the different learning  algorithms. In particular, using all of  these knowledge sources and SVM (i.e.,  a single learning algorithm) achieves accuracy  higher than the best official scores  on both SENSEVAL-2 and SENSEVAL-1  test data"	These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG ) .	"['For our experiments, we use naive Bayes as the learning algorithm.', 'The knowledge sources we use include parts-of-speech, local collocations, and surrounding words.', 'These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG ) .']"	2
CC1045	P07-1068	Advanced Machine Learning Models for Coreference Resolution	the nonutility of predicateargument frequencies for pronoun interpretation	"['A Kehler', 'D Appelt', 'L Taylor', 'A Simma']"	introduction		"While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance ."	"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']"	0
CC1046	P07-1068	Advanced Machine Learning Models for Coreference Resolution	a mentionsynchronous coreference resolution algorithm based on the bell tree	"['X Luo', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'S Roukos']"	introduction	This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.	"More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms."	"['Many ACE participants have also adopted a corpus-based approach to SC deter- mination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006)).', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Un- like them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowl- edge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC clas- sifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and an- notated with their SCs.', 'This provides us with a train- ing set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']"	1
CC1047	P07-1068	Advanced Machine Learning Models for Coreference Resolution	a corpusbased evaluation of centering and pronoun resolution	['J Tetreault']	introduction		"In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #AUTHOR_TAG ) ."	"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #AUTHOR_TAG ) .', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']"	0
CC1048	P07-1068	Advanced Machine Learning Models for Coreference Resolution	bbn pronoun coreference and entity type corpus linguistica data consortium	"['R Weischedel', 'A Brunstein']"	introduction		"Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs."	"['Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006)).', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.', 'This provides us with a training set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004)); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']"	5
CC1049	P07-1068	Advanced Machine Learning Models for Coreference Resolution	anaphora resolution	['R Mitkov']	introduction	"In anaphora resolution for English, animacy identification can play an integral role in the application of agreement restrictions between pronouns and candidates, and as a result, can improve the accuracy of anaphora resolution systems. In this paper, two methods for animacy identification are proposed and evaluated using intrinsic and extrinsic measures. The first method is a rule-based one which uses information about the unique beginners in WordNet to classify NPs on the basis of their animacy. The second method relies on a machine learning algorithm which exploits a WordNet enriched with animacy information for each sense. The effect of word sense disambiguation on the two methods is also assessed. The intrinsic evaluation reveals that the machine learning method reaches human levels of performance. The extrinsic evaluation demonstrates that animacy identification can be beneficial in anaphora resolution, especially in the cases where animate entities are identified with high precision"	"While these approaches have been reasonably successful ( see #AUTHOR_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance ."	"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful ( see #AUTHOR_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']"	0
CC1050	P07-1068	Advanced Machine Learning Models for Coreference Resolution	libsvm a library for support vector machines software available at httpwwwcsientuedutw∼cjlinlibsvm	"['C-C Chang', 'C-J Lin']"	introduction		"Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features."	"['In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.�s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner.', 'Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.', 'More specifically, let L = i li\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2).', 'To represent i, we generate one feature from each non-empty subset of Li.']"	5
CC1051	P07-1068	Advanced Machine Learning Models for Coreference Resolution	an algorithm that learns what’s in a name	"['D M Bikel', 'R Schwartz', 'R M Weischedel']"	introduction	"In this paper, we present IdentiFinderTM, a hidden Markov model that learns to recognize and classify names, dates, times, and numerical quantities. We have evaluated the model in English (based on data from the Sixth and Seventh Message Understanding Conferences [MUC-6, MUC-7] and broadcast news) and in Spanish (based on data distributed through the First Multilingual Entity Task [MET-1]), and on speech input (based on broadcast news). We report results here on standard materials only to quantify performance on data available to the community, namely, MUC-6 and MET-1. Results have been consistently better than reported by any other learning algorithm. IdentiFinder's performance is competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available. We also present a controlled experiment showing the effect of training set size on performance, demonstrating that as little as 100,000 words of training data is adequate to get performance around 90% on newswire. Although we present our understanding of why this algorithm performs so well on this class of problems, we believe that significant improvement in performance may still be possible."	"( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."	"[""( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."", 'If NPi is determined to be a PERSON or ORGANIZATION, we create an NE feature whose value is simply its MUC NE type.', 'However, if NPi is determined to be a LOCATION, we create a feature with value GPE (because most of the MUC LOCA- TION NEs are ACE GPE NEs).', 'Otherwise, no NE feature will be created (because we are not interested in the other MUC NE types).']"	5
CC1052	P07-1068	Advanced Machine Learning Models for Coreference Resolution	comparing knowledge sources for nominal anaphora resolution	"['K Markert', 'M Nissim']"	introduction	"We compare two ways of obtaining lexical knowledge for antecedent selection in other-anaphora  and definite noun phrase coreference. Specifically, we compare an algorithm that relies on links  encoded in the manually created lexical hierarchy WordNet and an algorithm that mines corpora  by means of shallow lexico-semantic patterns. As corpora we use the British National  Corpus (BNC), as well as the Web, which has not been previously used for this task. Our  results show that (a) the knowledge encoded in WordNet is often insufficient, especially for  anaphor-antecedent relations that exploit subjective or context-dependent knowledge; (b) for  other-anaphora, the Web-based method outperforms the WordNet-based method; (c) for definite  NP coreference, the Web-based method yields results comparable to those obtained using  WordNet over the whole dataset and outperforms the WordNet-based method on subsets of the  dataset; (d) in both case studies, the BNC-based method is worse than the other methods because  of data sparseness. Thus, in our studies, the Web-based method alleviated the lexical knowledge  gap often encountered in anaphora resolution, and handled examples with context-dependent relations  between anaphor and antecedent. Because it is inexpensive and needs no hand-modelling  of lexical knowledge, it is a promising knowledge source to integrate in anaphora resolution systems"	"However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #AUTHOR_TAG ) ."	"['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #AUTHOR_TAG ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]"	0
CC1053	P07-1068	Advanced Machine Learning Models for Coreference Resolution	unsupervised models for named entity classification	"['M Collins', 'Y Singer']"	introduction	"This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of la-beled examples should be required to train a classi-fier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple ""seed "" rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98)."	"We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) ."	"['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']"	5
CC1054	P07-1068	Advanced Machine Learning Models for Coreference Resolution	a modeltheoretic coreference scoring scheme	"['M Vilain', 'J Burger', 'J Aberdeen', 'D Connolly', 'L Hirschman']"			"We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved ."	"['As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, NP i , correctly resolved if NP i and its closest antecedent are in the same coreference chain in the resulting partition.', 'In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.']"	5
CC1055	P07-1068	Advanced Machine Learning Models for Coreference Resolution	a machine learning approach to coreference resolution of noun phrases	"['W M Soon', 'H T Ng', 'D Lim']"		"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set"	"Following #AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj."	"['After training, the decision tree classifier is used to select an antecedent for each NP in a test text.', 'Following #AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.', 'If no such NP exists, no antecedent is selected for NPj.']"	4
CC1056	P07-1068	Advanced Machine Learning Models for Coreference Resolution	factorizing complex models a case study in mention detection	"['R Florian', 'H Jing', 'N Kambhatla', 'I Zitouni']"	introduction	"As natural language understanding research advances towards deeper knowledge modeling, the tasks become more and more complex: we are interested in more nuanced word characteristics, more linguistic properties, deeper semantic and syntactic features. One such example, explored in this article, is the mention detection and recognition task in the Automatic Content Extraction project, with the goal of identifying named, nominal or pronominal references to real-world entities---mentions---and labeling them with three types of information: entity type, entity subtype and mention type. In this article, we investigate three methods of assigning these related tags and compare them on several data sets. A system based on the methods presented in this article participated and ranked very competitively in the ACE'04 evaluation."	"Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG ) ."	"['Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG ) .', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.', 'This provides us with a training set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004)); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']"	1
CC1057	P07-1068	Advanced Machine Learning Models for Coreference Resolution	coreference resolution using competitive learning approach	"['X Yang', 'G Zhou', 'J Su', 'C L Tan']"		"In this paper we propose a competition learning approach to coreference resolution. Traditionally, supervised machine learning approaches adopt the single-candidate model. Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model. By contrast, our approach adopts a twin-candidate learning model. Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected. Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution. The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the single-candidate model."	"Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG , as described below ."	"['Our baseline coreference system uses the C4.5 deci- sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG , as described below .']"	1
CC1058	P07-1068	Advanced Machine Learning Models for Coreference Resolution	c45 programs for machine learning	['J R Quinlan']			Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .	"['Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NP j , and its closest antecedent, NP i ; and a negative instance is created for NP j paired with each of the intervening NPs, NP i+1 , NP i+2 , . .', '., NP j−1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']"	5
CC1059	P07-1068	Advanced Machine Learning Models for Coreference Resolution	improving machine learning approaches to coreference resolution	"['V Ng', 'C Cardie']"		"We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge."	"Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below."	"['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj.', 'Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below.']"	1
CC1060	P07-1068	Advanced Machine Learning Models for Coreference Resolution	exploiting semantic role labeling wordnet and wikipedia for coreference resolution	"['S P Ponzetto', 'M Strube']"		"In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns."	"Following #AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition ."	"['As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (Vilain et al., 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following #AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .', 'In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.']"	5
CC1061	P07-1068	Advanced Machine Learning Models for Coreference Resolution	unsupervised learning of contextual role knowledge for coreference resolution	"['D Bean', 'E Riloff']"	introduction	"We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a Dempster-Shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns."	"As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #AUTHOR_TAG ) ."	"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #AUTHOR_TAG ) .']"	0
CC1062	P07-1068	Advanced Machine Learning Models for Coreference Resolution	a machine learning approach to coreference resolution of noun phrases	"['W M Soon', 'H T Ng', 'D Lim']"	introduction	"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set"	"However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) ."	"['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]"	0
CC1063	P07-1068	Advanced Machine Learning Models for Coreference Resolution	a machine learning approach to coreference resolution of noun phrases	"['W M Soon', 'H T Ng', 'D Lim']"		"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set"	"Following previous work ( e.g. , #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 ."	"['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work ( e.g. , #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .', '., NP j−1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']"	5
CC1064	P07-1068	Advanced Machine Learning Models for Coreference Resolution	unsupervised word sense disambiguation rivaling supervised methods	['D Yarowsky']	introduction	"This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%"	"We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) ."	"['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']"	4
CC1065	P07-1068	Advanced Machine Learning Models for Coreference Resolution	exploiting semantic role labeling wordnet and wikipedia for coreference resolution	"['S P Ponzetto', 'M Strube']"	introduction	"In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns."	"As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) ."	"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']"	0
CC1066	P07-1068	Advanced Machine Learning Models for Coreference Resolution	using semantic relations to refine coreference decisions	"['H Ji', 'D Westbrook', 'R Grishman']"	introduction	We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses. Experiments show that this new framework can improve performance on two quite different languages - English and Chinese.	"As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) ."	"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']"	0
CC1067	P07-1068	Advanced Machine Learning Models for Coreference Resolution	automatic retrieval and clustering of similar words	['D Lin']	introduction	Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.	( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1, while others appear to be correlated with these ACE SCs. 2 (6) INDUCED CLASS: Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992)).', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"	4
CC1068	P07-1068	Advanced Machine Learning Models for Coreference Resolution	automatic acquisition of hyponyms from large text corpora	['M Hearst']	introduction	"We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. 1 Introduction  Currently there is much interest in the automatic acquisition of lexical syntax and semantics, with the goal of building up large lexicons for natural language processing. Projects.."	"These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) ."	"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '(7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"	4
CC1069	P10-2059	Classification of Feedback Expressions in Multimodal Data	contextual recognition of head gestures	"['Louis-Philippe Morency', 'Candace Sidner', 'Christopher Lee', 'Trevor Darrell']"	introduction	"Head pose and gesture offer several key conversational grounding cues and are used extensively in face-to-face interaction among people. We investigate how dialog context from an embodied conversational agent (ECA) can improve visual recognition of user gestures. We present a recogntion framework which (1) extracts contextual features from an ECA's dialog manager, (2) computes a predicition of head nod and head shakes, and (3) integrates the contextual predictions with the visual observation of a vision-based head gesture recognizer. We found a subset of lexical, punctuation and timing features that are easily available in most ECA architectures and can be used to learn how to predict user feedback. Using a discriminative approach to contextual prediction and multi-modal integration, we were able to improve the performancae of head gesture detection even when the topic of the test set was significantly different than the training set"	"Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) ."	"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"	0
CC1070	P10-2059	Classification of Feedback Expressions in Multimodal Data	the mumin coding scheme for the annotation of feedback turn management and sequencing multimodal corpora for modelling human multimodal behaviour	"['Jens Allwood', 'Loredana Cerrato', 'Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']"			All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG ) .	"['All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG ) .', 'The MU-MIN scheme is a general framework for the study of gestures in interpersonal communication.', 'In this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'Therefore, only a subset of the MUMIN attributes has been used, i.e.', 'Smile, Laughter, Scowl, FaceOther for facial expressions, and Nod, Jerk, Tilt, SideTurn, Shake, Waggle, Other for head movements.']"	5
CC1071	P10-2059	Classification of Feedback Expressions in Multimodal Data	turnyielding cues in taskoriented dialogue	"['Agustin Gravano', 'Julia Hirschberg']"	introduction		"Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues ."	"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"	0
CC1072	P10-2059	Classification of Feedback Expressions in Multimodal Data	a conversation robot using head gesture recognition as paralinguistic information	"['Shinya Fujie', 'Y Ejiri', 'K Nakajima', 'Y Matsusaka', 'Tetsunor Kobayashi']"	introduction	"A conversation robot that recognizes user's head gestures and uses its results as para-linguistic information is developed. In the conversation, humans exchange linguistic information, which can be obtained by transcription of the utterance, and para-linguistic information, which helps the transmission of linguistic information. Para-linguistic information brings a nuance that cannot be transmitted by linguistic information, and the natural and effective conversation is realized. We recognize user's head gestures as the para-linguistic information in the visual channel. We use the optical flow over the head region as the feature and model them using HMM for the recognition. In actual conversation, while the user performs a gesture, the robot may perform a gesture, too. In this situation, the image sequence captured by the camera mounted on the eyes of the robot includes sways caused by the movement of the camera. To solve this problem, we introduced two artifices. One is for the feature extraction: the optical flow of the body area is used to compensate the swayed images. The other is for the probability models: mode-dependent models are prepared by the MLLR model adaptation technique, and the models are switched according to the motion mode of the robot. Experimental results show the effectiveness of these techniques."	"Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) ."	"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"	0
CC1073	P10-2059	Classification of Feedback Expressions in Multimodal Data	linguistic functions of head movements in the context of speech	['Evelyn McClave']	introduction		Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .	"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.']"	0
CC1074	P10-2059	Classification of Feedback Expressions in Multimodal Data	coefficient kappa some uses misuses and alternatives	"['Robert L Brennan', 'Dale J Prediger']"	introduction	"This paper considers some appropriate and inappropriate uses of coefficient kappa and alternative kappa-like statistics. Discussion is restricted to the descriptive characteristics of these statistics for measuring agreement with categorical data in studies of reliability and validity. Special consideration is given to assumptions about whether marginals are fixed a priori, or free to vary. In reliability studies, when marginals are fixed, coefficient kappa is found to be appropriate. When either or both of the marginals are free to vary, however, it is suggested that the ""chance"" term in kappa be replaced by 1/n, where n is the number of categories. In validity studies, we suggest considering whether one wants an index of improvement beyond ""chance"" or beyond the best a priori strategy employing base rates. In the former case, considerations are similar to those in reliability studies with the marginals for the criterion measure considered as fixed. In the latter case, it is suggested that the largest marginal proportion for the criterion measure be used in place of the ""chance"" term in kappa. Similarities and differences among these statistics are discussed and illustrated with synthetic data."	"Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 ."	"['In general, dialogue act, agreement and turn anno- tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement.', 'A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 ."", 'Anvil divides the annotations in slices and compares each slice.', 'We used slices of 0.04 seconds.', 'The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.']"	5
CC1075	P10-2059	Classification of Feedback Expressions in Multimodal Data	the hcrc map task corpus language and speech	"['Anne H Anderson', 'Miles Bader', 'Ellen Gurman Bard', 'Elizabeth Boyle', 'Gwyneth Doherty', 'Simon Garrod', 'Stephen Isard', 'Jacqueline Kowtko', 'Jan McAllister', 'Jim Miller', 'Catherine Sotillo', 'Henry S Thompson', 'Regina Weinert']"	introduction		"Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers ."	"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"	0
CC1076	P10-2059	Classification of Feedback Expressions in Multimodal Data	data mining practical machine learning tools and techniques	"['Ian H Witten', 'Eibe Frank']"		"As with any burgeoning technology that enjoys commercial attention, the use of data mining is surrounded by a great deal of hype. Exaggerated reports tell of secrets that can be uncovered by setting algorithms loose on oceans of data. But there is no magic in machine learning, no hidden power, no alchemy. Instead there is an identifiable body of practical techniques that can extract useful information from raw data. This book describes these techniques and shows how they work. The book is a major revision of the first edition that appeared in 1999. While the basic core remains the sam"	"These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG ) ."	"['The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG ) .', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005).', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']"	5
CC1077	P10-2059	Classification of Feedback Expressions in Multimodal Data	combining lexical syntactic and prosodic cues for improved online dialog act tagging	"['Vivek Kumar Rangarajan Sridhar', 'Srinivas Bangaloreb', 'Shrikanth Narayanan']"	introduction		"#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues ."	"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"	0
CC1078	P10-2059	Classification of Feedback Expressions in Multimodal Data	a coefficient of agreement for nominal scales	['Jacob Cohen']	introduction	"CONSIDER Table 1. It represents in its formal characteristics a situation which arises in the clinical-social-personality areas of psychology, where it frequently occurs that the only useful level of measurement obtainable is nominal scaling (Stevens, 1951, pp. 2526), i.e. placement in a set of k unordered categories. Because the categorizing of the units is a consequence of some complex judgment process performed by a &dquo;two-legged meter&dquo; (Stevens, 1958), it becomes important to determine the extent to which these judgments are reproducible, i.e., reliable. The procedure which suggests itself is that of having two (or more) judges independently categorize a sample of units and determine the degree, significance, and"	"Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 ."	"['In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement.', 'A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 ."", 'Anvil divides the annotations in slices and compares each slice.', 'We used slices of 0.04 seconds.', 'The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.']"	5
CC1079	P10-2059	Classification of Feedback Expressions in Multimodal Data	head gestures for perceptual interfaces the role of context in improving recognition	"['Louis-Philippe Morency', 'Candace Sidner', 'Christopher Lee', 'Trevor Darrell']"	introduction	"AbstractHead pose and gesture offer several conversational grounding cues and are used extensively in face-to-face interaction among people. To accurately recognize visual feedback, humans often use contextual knowledge from previous and current events to anticipate when feedback is most likely to occur. In this paper we describe how contextual information can be used to predict visual feedback and improve recognition of head gestures in human-computer interfaces. Lexical, prosodic, timing, and gesture features can be used to predict a user's visual feedback during conversational dialog with a robotic or virtual agent. In non-conversational interfaces, context features based on user-interface system events can improve detection of head gestures for dialog box confirmation or document browsing. Our user study with prototype gesture-based components indicate quantitative and qualitative benefits of gesture-based confirmation over conventional alternatives. Using a discriminative approach to contextual prediction and multi-modal integration, performance of head gesture detection was improved with context features even when the topic of the test set was significantly different than the training set"	"Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) ."	"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"	0
CC1080	P10-2059	Classification of Feedback Expressions in Multimodal Data	intercoder agreement for computational linguistics	"['Ron Artstein', 'Massimo Poesio']"	introduction		"Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."	"['It should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'For example, a yes can be an Answer to a question, an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained.', ""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element.']"	0
CC1081	P10-2059	Classification of Feedback Expressions in Multimodal Data	detecting action meetings in meetings	"['Gabriel Murray', 'Steve Renals']"	introduction		Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .	"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .']"	0
CC1082	P10-2059	Classification of Feedback Expressions in Multimodal Data	clustering experiments on the communicative prop erties of gaze and gestures	"['Kristiina Jokinen', 'Anton Ragni']"	introduction		"For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels ."	"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.']"	0
CC1083	P10-2059	Classification of Feedback Expressions in Multimodal Data	distinguishing the communicative functions of gestures	"['Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']"	introduction	"This paper deals with the results of a machine learning experiment conducted on annotated gesture data from two case studies (Danish and Estonian). The data concern mainly facial displays, that are annotated with attributes relating to shape and dynamics, as well as communicative function. The results of the experiments show that the granularity of the attributes used seems appropriate for the task of distinguishing the desired communicative functions. This is a promising result in view of a future automation of the annotation task."	"For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels ."	"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi- modal corpus.']"	0
CC1084	P10-2059	Classification of Feedback Expressions in Multimodal Data	measuring nominal scale agreement among many raters	['Joseph L Fleiss']	introduction		"Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG ) ."	"['It should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'For example, a yes can be an Answer to a question, an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained.', ""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG ) ."", 'Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element.']"	0
CC1085	P10-2059	Classification of Feedback Expressions in Multimodal Data	distinguishing the communicative functions of gestures	"['Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']"		"This paper deals with the results of a machine learning experiment conducted on annotated gesture data from two case studies (Danish and Estonian). The data concern mainly facial displays, that are annotated with attributes relating to shape and dynamics, as well as communicative function. The results of the experiments show that the granularity of the attributes used seems appropriate for the task of distinguishing the desired communicative functions. This is a promising result in view of a future automation of the annotation task."	"These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme."	"['The head gestures in the DanPASS data have been coded by non expert annotators (one annotator per video) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions.', 'In the case of gestures we also measured agreement on gesture segmentation.', 'The figures obtained are given in Table 3.', 'These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.']"	1
CC1086	P10-2059	Classification of Feedback Expressions in Multimodal Data	feedback in head gesture and speech to appear in	"['Patrizia Paggio', 'Costanza Navarretta']"	introduction		"The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions ."	"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"	1
CC1087	P10-2059	Classification of Feedback Expressions in Multimodal Data	hidden naive bayes	"['Harry Zhang', 'Liangxiao Jiang', 'Jiang Su']"			The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .	"['The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (Witten and Frank, 2005).', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']"	5
CC1088	P10-2059	Classification of Feedback Expressions in Multimodal Data	gesture generation by imitation  from human behavior to computer character animation	['Michael Kipp']	introduction	"In an e ort to extend traditional human-computer interfaces research has introduced embodied agents to utilize the modalities of everyday human-human communication, like facial expression, gestures and body postures. However, giving computer agents a human-like body introduces new challenges. Since human users are very sensitive and critical concerning bodily behavior the agents must act naturally and individually in order to be believable. This dissertation focuses on conversational gestures. It shows how to generate conversational gestures for an animated embodied agent based on annotated text input. The central idea is to imitate the gestural behavior of a human individual. Using TV show recordings as empirical data, gestural key parameters are extracted for the generation of natural and individual gestures. The gesture generation task is solved in three stages: observation, modeling and generation. For each stage, a software module was developed. For observation, the video annotation research tool ANVIL was created. It allows the eAEcient transcription of gesture, speech and other modalities on multiple layers. ANVIL is application-independent by allowing users to de ne their own annotation schemes, it provides various import/export facilities and it is extensible via its plugin interface. Therefore, the tool is suitable for a wide variety of research elds. For this work, selected clips of the TV talk show  Das Literarische Quartett"" were transcribed and analyzed, arriving at a total of 1,056 gestures. For the modeling stage, the NOVALIS module was created to compute individual gesture pro les from these transcriptions with statistical methods. A gesture pro le models the aspects handedness, timing and function of gestures for a single human individual using estimated conditional probabilities. The pro les are based on a shared lexicon of 68 gestures, assembled from the data. Finally, for generation, the NOVA generator was devised to create gestures based on gesture pro les in an overgenerate-andlter approach. Annotated text input is processed in a graph-based representation in multiple stages where semantic data is added, the location of potential gestures is determined by heuristic rules, and gestures are added and ltered based on a gesture pro le. NOVA outputs a linear, player-independent action script in XML."	Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .	"['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .', 'To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.', 'This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.', 'Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.', 'Finally, the two turn management categories Turn-Take and TurnElicit were also coded.']"	5
CC1089	P10-2059	Classification of Feedback Expressions in Multimodal Data	praat doing phonetics by computer retrieved	"['Paul Boersma', 'David Weenink']"	introduction		The Praat tool was used ( #AUTHOR_TAG ) .	"['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Gr�nnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']"	5
CC1090	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	linking semantic and knowledge representations in a multidomain dialogue system	"['Myroslava O Dzikovska', 'James F Allen', 'Mary D Swift']"	experiments	"We describe a two-layer architecture for supporting semantic interpretation and domain reasoning in dialogue systems. Building system that supports both semantic interpretation and domain reasoning in a transparent and well-integrated manner is an unresolved problem because of the diverging requirements of the semantic representations used in contextual interpretation versus the knowledge representations used in domain reasoning. We propose an architecture that provides both portability and efficiency in natural language interpretation by maintaining separate semantic and domain knowledge representations, and integrating them via an ontology mapping procedure. The ontology mapping is used to obtain representations of utterances in a form most suitable for domain reasoners and to automatically specialize the lexicon. The use of a linguistically motivated parser for producing semantic representations for complex natural language sentences facilitates building portable semantic interpretation components as well as connections with domain reasoners. Two evaluations demonstrate the effectiveness of our approach: we show that a small number of mapping rules are sufficient for customizing the generic semantic representation to a new domain, and that our automatic lexicon specialization technique improves parser speed and accuracy."	"The diagnoser , based on #AUTHOR_TAGb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer ."	"['The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser , based on #AUTHOR_TAGb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer ."", 'At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008).']"	2
CC1091	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	developing pedagogically effective tutorial dialogue tactics experiments and a testbed	"['Kurt VanLehn', 'Pamela Jordan', 'Diane Litman']"	introduction	"Although effective tutorial dialogue strategies are well understood, tutorial tactics that govern brief episodes of tutoring, such as a single step, are not. Because better tactics seem to be crucial for further improving pedagogical effectiveness, we have begun investigating the effects of varying tutorial tactics. In this paper we describe two planned experiments and the testbed we have created to support this experimentation."	"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) ."	"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']"	0
CC1092	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	controlling content realization with functional unification grammars in	"['Michael Elhadad', 'Jacques Robin']"	experiments	"Standard Functional Unification Grammars (FUGs) provide a structurally guided top-down control regime for sentence generation. When using FUGs to perform content realization as a whole, including lexical choice, this regime is no longer appropriate for two reasons: (1) the unification of non-lexicalized semantic input with an integrated lexico-grammar requires mapping ""floating"" semantic elements which can trigger extensive backtracking and (2) lexical choice requires accessing external constraint sources on demand to preserve the modularity between conceptual and linguistic knowledge."	"The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #AUTHOR_TAG ) generation system to produce the appropriate text ."	"[""The strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer (e.g., part of the answer to confirm), is passed to content planning and generation."", 'The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #AUTHOR_TAG ) generation system to produce the appropriate text .', 'Templates are used to generate some stock phrases such as ""When you are ready, go on to the next slide.""']"	5
CC1093	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	cohesion and learning in a tutorial spoken dialog system	"['Arthur Ward', 'Diane Litman']"	conclusion	"Two measures of lexical cohesion were developed and applied to a corpus of human-computer tutoring dialogs. For both measures, the amount of cohesion in the tutoring dialog was found to be significantly correlated to learning for students with below-mean pretest scores, but not for those with above-mean pre-test scores, even though both groups had similar amounts of cohesion. We also find that only cohesion between tutor and student is significant: the cohesiveness of tutor, or of student, utterances is not. These results are discussed in light of previous work in textual cohesion and recall."	Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ) .	"['In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ) .', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.']"	3
CC1094	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	a natural language tutorial dialogue system for physics	"['Pamela Jordan', 'Maxim Makatchev', 'Umarani Pappuswamy', 'Kurt VanLehn', 'Patricia Albacete']"	introduction	Abstract : We describe the WHY2-ATLAS intelligent tutoring system for qualitative physics that interacts with students via natural language dialogue. We focus on the issue of analyzing and responding to multi-sentential explanations. We explore approaches for achieving a deeper understanding of these explanations and dialogue management approaches and strategies for providing appropriate feedback on them.	"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) ."	"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']"	0
CC1095	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	information state and dialogue management in the trindi dialogue move engine toolkit	"['Staffan Larsson', 'David Traum']"	experiments	"We introduce an architecture and toolkit for building dialogue managers currently being developed in the TRINDI project, based on the notions of information state and dialogue move engine. The aim is to provide a framework for experimenting with implementations of different theories of information state, information state update and dialogue control. A number of dialogue managers are currently being built using the toolkit, and we present overviews of two of them. We believe that this framework will make implementation of dialogue processing theories easier, also facilitating comparison of different types of dialogue systems, thus helping to achieve a prerequisite for arriving at a best practice for the development of the dialogue management component of a spoken dialogue system."	Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ) .	"['Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ) .', 'The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer.', 'Once the complete answer has been accumulated, the system accepts it and moves on.', 'Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student.']"	5
CC1096	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	contentlearning correlations in spoken tutoring dialogs at word turn and discourse levels	"['Amruta Purandare', 'Diane Litman']"	introduction	"We study correlations between dialog content and learning in a corpus of human-computer tutoring dialogs. Using an online encyclopedia, we first extract domainspecific concepts discussed in our dialogs. We then extend previously studied shallow dialog metrics by incorporating content at three levels of granularity (word, turn and discourse) and also by distinguishing between students' spoken and written contributions. In all experiments, our content metrics show strong correlations with learning, and outperform the corresponding shallow baselines. Our word-level results show that although verbosity in student writings is highly associated with learning, verbosity in their spoken turns is not. On the other hand, we notice that content along with conciseness in spoken dialogs is strongly correlated with learning. At the turn-level, we find that effective tutoring dialogs have more content-rich turns, but not necessarily more or longer turns. Our discourse-level analysis computes the distribution of content across larger dialog units and shows high correlations when student contributions are rich but unevenly distributed across dialog segments. Copyright (c) 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."	"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) ."	"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']"	0
CC1097	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	the impact of interpretation problems on tutorial dialogue	"['Myroslava O Dzikovska', 'Johanna D Moore', 'Natalie Steinhauser', 'Gwendolyn Campbell']"	conclusion	"Supporting natural language input may improve learning in intelligent tutoring systems. However, interpretation errors are unavoidable and require an effective recovery policy. We describe an evaluation of an error recovery policy in the BEETLE II tutorial dialogue system and discuss how different types of interpretation problems affect learning gain and user satisfaction. In particular, the problems arising from student use of non-standard terminology appear to have negative consequences. We argue that existing strategies for dealing with terminology problems are insufficient and that improving such strategies is important in future ITS research."	"The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG ) ."	"['In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG ) ."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.']"	3
CC1098	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	learning to assess lowlevel conceptual understanding	"['Rodney D Nielsen', 'Wayne Ward', 'James H Martin']"	introduction		"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) ."	"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']"	0
CC1099	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	generalizing tutorial dialogue results	"['Diane Litman', 'Johanna Moore', 'Myroslava Dzikovska', 'Elaine Farrow']"	introduction		"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) ."	"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']"	0
CC1100	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	dealing with interpretation errors in tutorial dialogue	"['Myroslava O Dzikovska', 'Charles B Callaway', 'Elaine Farrow', 'Johanna D Moore', 'Natalie B Steinhauser', 'Gwendolyn C Campbell']"	experiments	"We describe an approach to dealing with interpretation errors in a tutorial dialogue system. Allowing students to provide explanations and generate contentful talk can be helpful for learning, but the language that can be understood by a computer system is limited by the current technology. Techniques for dealing with understanding problems have been developed primarily for spoken dialogue systems in informationseeking domains, and are not always appropriate for tutorial dialogue. We present a classification of interpretation errors and our approach for dealing with them within an implemented tutorial dialogue system."	"At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ) ."	"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ) .', 'Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue.', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']"	5
CC1101	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	resolving pronominal reference to abstract entities	['Donna K Byron']	experiments	"This paper describes PHORA, a technique for resolving pronominal reference to either individual or abstract entities. It defines processes for evoking abstract referents from discourse and for resolving both demonstrative and personal pronouns. It successfully interprets 72% of test pronouns, compared to 37% for a leading technique without these features."	"The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output ."	"['We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output ."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']"	1
CC1102	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	the beetle and beediff tutoring systems	"['Charles B Callaway', 'Myroslava Dzikovska', 'Elaine Farrow', 'Manuel Marques-Pita', 'Colin Matheson', 'Johanna D Moore']"	introduction	"We describe two tutorial dialogue systems that adapt techniques from task-oriented dialogue systems to tutorial dialogue. Both systems employ the same reusable deep natural language understanding and generation components to interpret students' written utterances and to automatically generate adaptive tutorial responses, with separate domain reasoners to provide the necessary knowledge about the correctness of student answers and hinting strategies. We focus on integrating the domain-independent language processing components with domain-specific reasoning and tutorial components in order to improve the dialogue interaction, and present a preliminary analysis of BeeDiff's evaluation. Index Terms: tutoring systems, dialogue, deep processing"	The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG ) .	"['The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG ) .', 'It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'This allows the system to consistently apply the same tutorial policy across a range of questions.', 'To some extent, this comes at the expense of being able to address individual student misconceptions.', ""However, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning.""]"	0
CC1103	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	learning to assess lowlevel conceptual understanding	"['Rodney D Nielsen', 'Wayne Ward', 'James H Martin']"	experiments		"At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG ."	"['The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer."", 'At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG .']"	3
CC1104	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	simulated tutors in immersive learning environments empiricallyderived design principles	"['N B Steinhauser', 'L A Butler', 'G E Campbell']"	introduction		"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) ."	"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']"	0
CC1105	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	towards tutorial dialog to support selfexplanation adding natural language understanding to a cognitive tutor	"['V Aleven', 'O Popescu', 'K R Koedinger']"	introduction	"Self-explanation is an effective metacognitive strategy, as a number of cognitive science studies have shown. In a previous study we showed that self-explanation can be supported effectively in a cognitive tutor for geometry problem solving. In that study, students explained their own problem-solving steps by selecting from a menu the name of a problem-solving principle that justifies the step. They learned with greater understanding, as compared to students who did not explain their reasoning. Currently, we are working toward testing the hypothesis that students will learn even better when they provide explanations in their own words rather than selecting them from a menu. We have implemented a prototype of a cognitive tutor that understands students' explanations and provides feedback. The tutor uses a knowledge-based approach to natural language understanding. We are entering a phase of pilot testing, both for the purpose of assessing the coverage of the natural language understanding component and for gaining insight into the kinds of dialog strategies that are needed."	"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) ."	"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']"	0
CC1106	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	linking semantic and knowledge representations in a multidomain dialogue system	"['Myroslava O Dzikovska', 'James F Allen', 'Mary D Swift']"	experiments	"We describe a two-layer architecture for supporting semantic interpretation and domain reasoning in dialogue systems. Building system that supports both semantic interpretation and domain reasoning in a transparent and well-integrated manner is an unresolved problem because of the diverging requirements of the semantic representations used in contextual interpretation versus the knowledge representations used in domain reasoning. We propose an architecture that provides both portability and efficiency in natural language interpretation by maintaining separate semantic and domain knowledge representations, and integrating them via an ontology mapping procedure. The ontology mapping is used to obtain representations of utterances in a form most suitable for domain reasoners and to automatically specialize the lexicon. The use of a linguistically motivated parser for producing semantic representations for complex natural language sentences facilitates building portable semantic interpretation components as well as connections with domain reasoners. Two evaluations demonstrate the effectiveness of our approach: we show that a small number of mapping rules are sufficient for customizing the generic semantic representation to a new domain, and that our automatic lexicon specialization technique improves parser speed and accuracy."	"The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #AUTHOR_TAGa ) to produce a domain-specific semantic representation of the student 's output ."	"['We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #AUTHOR_TAGa ) to produce a domain-specific semantic representation of the student 's output ."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']"	5
CC1107	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	the beetle and beediff tutoring systems	"['Charles B Callaway', 'Myroslava Dzikovska', 'Elaine Farrow', 'Manuel Marques-Pita', 'Colin Matheson', 'Johanna D Moore']"	experiments	"We describe two tutorial dialogue systems that adapt techniques from task-oriented dialogue systems to tutorial dialogue. Both systems employ the same reusable deep natural language understanding and generation components to interpret students' written utterances and to automatically generate adaptive tutorial responses, with separate domain reasoners to provide the necessary knowledge about the correctness of student answers and hinting strategies. We focus on integrating the domain-independent language processing components with domain-specific reasoning and tutorial components in order to improve the dialogue interaction, and present a preliminary analysis of BeeDiff's evaluation. Index Terms: tutoring systems, dialogue, deep processing"	Other factors such as student confidence could be considered as well ( #AUTHOR_TAG ) .	['Other factors such as student confidence could be considered as well ( #AUTHOR_TAG ) .']	3
CC1108	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	autotutor a simulation of a human tutor cognitive systems research	"['A C Graesser', 'P Wiemer-Hastings', 'P WiemerHastings', 'R Kreuz']"	introduction		"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) ."	"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']"	0
CC1109	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	towards modelling and using common ground in tutorial dialogue	"['Mark Buckley', 'Magdalena Wolska']"	introduction	"In order to avoid miscommunication par-ticipants in dialogue continuously attempt to align their mutual knowledge (the ""common ground""). A setting that is per-haps most prone to misalignment is tu-toring. We propose a model of common ground in tutoring dialogues which explic-itly models the truth and falsity of do-main level contributions and show how it can be used to detect and repair stu-dents ' false conjectures and facilitate stu-dent modelling."	"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) ."	"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']"	0
CC1110	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	targeted help for spoken dialogue systems intelligent feedback improves naive users’ performance	"['Beth Ann Hockey', 'Oliver Lemon', 'Ellen Campana', 'Laura Hiatt', 'Gregory Aist', 'James Hieronymus', 'Alexander Gruenstein', 'John Dowding']"	experiments	"We present experimental evidence that providing naive users of a spoken dialogue system with immediate help messages related to their out-of-coverage utterances improves their success in using the system. A grammar-based recognizer and a Statistical Language Model (SLM) recognizer are run simultaneously. If the grammar-based recognizer suceeds, the less accurate SLM recognizer hypothesis is not used. When the grammar-based recognizer fails and the SLM recognizer produces a recognition hypothesis, this result is used by the Targeted Help agent to give the user feedback on what was recognized, a diagnosis of what was problematic about the utterance, and a related in-coverage example. The in-coverage example is intended to encourage alignment between user inputs and the language model of the system. We report on controlled experiments on a spoken dialogue system for command and control of a simulated robotic helicopter."	Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .	"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']"	2
CC1111	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	deep linguistic processing for spoken dialogue systems	"['James Allen', 'Myroslava Dzikovska', 'Mehdi Manshadi', 'Mary Swift']"	experiments	"We describe a framework for deep linguistic processing for natural language understanding in task-oriented spoken dialogue systems. The goal is to create domaingeneral processing techniques that can be shared across all domains and dialogue tasks, combined with domain-specific optimization based on an ontology mapping from the generic LF to the application ontology. This framework has been tested in six domains that involve tasks such as interactive planning, coordination operations, tutoring, and learning."	We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .	"['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']"	5
CC1112	P10-4003	BEETLE II: a system for tutoring and computational linguistics experimentation	interpretation and generation in a knowledgebased tutorial system	"['Myroslava O Dzikovska', 'Charles B Callaway', 'Elaine Farrow']"	experiments		"The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world ."	"['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']"	5
CC1113	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	tree edit distance for textual entailment	"['Milen Kouleykov', 'Bernardo Magnini']"	introduction		"All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference ."	"['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']"	0
CC1114	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	statistical phrasebased translation	"['Philipp Koehn', 'Franz Josef Och', 'Daniel Marcu']"		"This work summarizes a comparison between two ap-proaches to Statistical Machine Translation (SMT), namely Ngram-based and Phrase-based SMT. In both approaches, the translation process is based on bilingual units related by word-to-word alignments (pairs of source and target words), while the main differences are based on the extraction process of these units and the sta-tistical modeling of the translation context. The study has been carried out on two different translation tasks (in terms of translation difculty and amount of available training data), and allowing for distortion (reordering) in the decoding pro-cess. Thus it extends a previous work were both approaches were compared under monotone conditions. We nally report comparative results in terms of trans-lation accuracy, computation time and memory size. Re-sults show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1"	They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ) .	"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ) .', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']"	0
CC1115	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	dirt  discovery of inference rules from text	"['Dekang Lin', 'Patrick Pantel']"	introduction	"In this paper, we propose an unsupervised method for discovering inference rules from text, such as ""X is author of Y  X wrote Y"", ""X solved Y  X found a solution to Y"", and ""X caused Y  Y is triggered by X"". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus."	"ones , DIRT ( #AUTHOR_TAG ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) ."	"['ones , DIRT ( #AUTHOR_TAG ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are fre- quently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']"	0
CC1116	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	a semantic approach to recognizing textual entailment	['Marta Tatu andDan Moldovan']	introduction	"Exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art NLP systems. In this paper, we take a step closer to this objective. We combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system. 1 Recognizing Textual Entailment While communicating, humans use different expressions to convey the same meaning. Therefore, numerous NLP applications, such as, Question Answering, Information Extraction, or Summarization require computational models of language that recognize if two texts semantically overlap. Trying to capture the major inferences needed to understand equivalent semantic expressions, the PASCAL Network proposed the Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2005). Given two text fragments, the task is to determine if the meaning of one text (the entailed hypothesis, H) can be inferred from the meaning of the other text (the entailing text, T). Given the wide applicability of this task, there is an increased interest in creating systems which detect the semantic entailment between two texts. The systems that participated in the Pascal RTE challenge competition exploit various inference elements which, later, they combine within statistical models, scoring methods, or machine learning frameworks. Several systems (Bos and Markert, 2005; Herrera et al., 2005; Jijkoun and de Rijke, 2005; Kouylekov and Magnini, 2005; Newman et al., 2005) measured the word overlap between the two text strings. Using either statistical or Word-Net's relations, almost all systems considered lexical relationships that indicate entailment. The degree of similarity between the syntactic parse trees of the two texts was also used as a clue for entailment by several systems (Herrera et al., 2005; Kouylekov and Magnini, 2005; de Salvo Braz et al., 2005; Raina et al., 2005). Several groups used logic provers to show the entailment between T and H (Bayer e"	"All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #AUTHOR_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference ."	"['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #AUTHOR_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']"	0
CC1117	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	creating a bilingual entailment corpus through translations with mechanical turk 100 for a 10day rush	"['Matteo Negri', 'Yashar Mehdad']"	experiments	"This paper reports on experiments in the creation of a bi-lingual Textual Entailment corpus, using non-experts' workforce under strict cost and time limitations ($100, 10 days). To this aim workers have been hired for translation and validation tasks, through the Crowd-Flower channel to Amazon Mechanical Turk. As a result, an accurate and reliable corpus of 426 English/Spanish entailment pairs has been produced in a more cost-effective way compared to other methods for the acquisition of translations based on crowdsourcing. Focusing on two orthogonal dimensions (i.e. reliability of annotations made by non experts, and overall corpus creation costs), we summarize the methodology we adopted, the achieved results, the main problems encountered, and the lessons learned."	"Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG ) ."	"['The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.', 'It consists of 1600 pairs derived from the RTE3 development and test sets (800+800).', 'Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG ) .', ""The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce."", 'Translation jobs return one Spanish version for each hypothesis.', 'Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.', 'At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job.', 'Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus.', 'The validation, carried out by a Spanish native speaker on 100 randomly selected pairs after two translation-validation cycles, showed the good quality of the collected material, with only 3 minor ""errors"" consisting in controversial but substantially acceptable translations reflecting regional Spanish variations.']"	5
CC1118	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	syntacticsemantic structures for textual entailment recognition	"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']"	introduction	"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning."	"Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T ."	"['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'For instance, the reliance of cur- rent monolingual TE systems on lexical resources (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal ex- pressions recognizers and normalizers) has to con- front, at the cross-lingual level, with the limited availability of lexical/semantic resources covering multiple languages, the limited coverage of the ex- isting ones, and the burden of integrating language- specific components into the same cross-lingual ar- chitecture.']"	0
CC1119	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	syntacticsemantic structures for textual entailment recognition	"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']"	introduction	"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning."	"Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :"	"['This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques.', 'Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system�s behaviour.', 'Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system.', 'Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :']"	1
CC1120	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	a systematic comparison of various statistical alignment models	"['Franz Josef Och', 'Hermann Ney']"		"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."	"We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level ."	"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level .', 'Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']"	5
CC1121	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	tracking and summarizing news on a daily basis with columbias newsblaster	"['Kathleen R McKeown', 'Regina Barzilay', 'David Evans', 'Vasileios Hatzivassiloglou', 'Judith L Klavans', 'Ani Nenkova', 'Carl Sable', 'Barry Schiffman', 'Sergey Sigelman']"		"Recently, there have been significant advances in several areas of language technology, including clustering, text categorization, and summarization. However, efforts to combine technology from these areas in a practical system for information access have been limited. In this paper, we present Columbia's Newsblaster system for online news summarization. Many of the tools developed at Columbia over the years are combined together to produce a system that crawls the web for news articles, clusters them on specific topics and produces multidocument summaries for each cluster."	"They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #AUTHOR_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) ."	"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #AUTHOR_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']"	4
CC1122	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	paraphrasing with bilingual parallel corpora	"['Colin Bannard', 'Chris Callison-Burch']"	introduction	"Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments."	"Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words ."	"['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']"	0
CC1123	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	making largescale support vector machine learning practical	['Thorsten Joachims']		"Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, oo-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."	"To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG ) , using each score as a feature ."	"['To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG ) , using each score as a feature .']"	5
CC1124	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	lexical selection and paraphrase in a meaning text generation model	"['Lidija Iordanskaja', 'Richard Kittredge', 'Alain Polg re']"		"We introduce a computationally tractable model for language generation based on the Meaning-Text Theory of Mel'cuk et al., in which the lexicon plays a central role. To illustrate the descriptive scope and paraphrase capabilities of the model, we show how the lexicon influences the set of choices at four different points during the multi-stage realization process: (1) semantic net simplification, (2) determination of root lexical node for the deep syntactic dependency tree, (3) possible application of deep paraphrase rules using lexical functions, and (4) surface syntactic realization. We also show some of the ways in which the theme/rheme specifications within the semantic net influence lexical and syntactic choices during realization. Examples are taken primarily from an implemented system which generates paragraph-length reports about the usage of operating systems."	"They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) ."	"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']"	4
CC1125	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	verbocean mining the web for finegrained semantic verb relations	"['Timothy Chklovski', 'Patrick Pantel']"	introduction	"Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/."	"These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) ."	"['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']"	0
CC1126	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	multiwordnet developing and aligned multilingual database	"['Emanuele Pianta', 'Luisa Bentivogli', 'Christian Girardi']"	introduction		"Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage ."	"['Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources.', 'As regards the first issue, it�s worth noting that in the monolingual scenario simple �bag of words� (or �bag of n- grams�) approaches are per se sufficient to achieve results above baseline.', 'In contrast, their application in the cross-lingual setting is not a viable solution due to the impossibility to perform direct lex- ical matches between texts and hypotheses in different languages.', 'This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.', 'However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English.', 'Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .', 'As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet�s synsets, thus making the coverage issue even more problematic than for TE.', 'As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE.', 'However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage.', 'In addition, featuring a bias towards named entities, the information acquired through cross-lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources (e.g bilingual dictionaries).']"	0
CC1127	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	inference rules and their application to recognizing textual entailment	"['Georgiana Dinu', 'Rui Wang']"		"In this paper, we explore ways of improv-ing an inference rule collection and its ap-plication to the task of recognizing textual entailment. For this purpose, we start with an automatically acquired collection and we propose methods to refine it and ob-tain more rules using a hand-crafted lex-ical resource. Following this, we derive a dependency-based structure representa-tion from texts, which aims to provide a proper base for the inference rule appli-cation. The evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible im-provements."	"They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #AUTHOR_TAG ) ."	"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #AUTHOR_TAG ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']"	4
CC1128	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	paraphrasing with bilingual parallel corpora	"['Colin Bannard', 'Chris Callison-Burch']"		"Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments."	One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ) .	"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ) .', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']"	0
CC1129	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	syntacticsemantic structures for textual entailment recognition	"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']"	introduction	"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning."	"These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) ."	"['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']"	0
CC1130	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	probabilistic textual entailment generic applied modeling of language variability	"['Ido Dagan', 'Oren Glickman']"	introduction		"Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T ."	"['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'For instance, the reliance of cur- rent monolingual TE systems on lexical resources (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal expressions recognizers and normalizers) has to confront, at the cross-lingual level, with the limited availability of lexical/semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating language- specific components into the same cross-lingual architecture.']"	0
CC1131	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	the berkeley framenet project	"['Collin F Baker', 'Charles J Fillmore', 'John B Lowe']"	introduction	"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work"	"These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) ."	"['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']"	0
CC1132	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	fluency adequacy or hter exploring different human judgments with a tunable mt metric	"['Matthew Snover', 'Nitin Madnani', 'Bonnie Dorr', 'Richard Schwartz']"		"Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, mea-sure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT met-ric: TER-Plus, which extends the Transla-tion Edit Rate evaluation metric with tun-able parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest aver-age rank in terms of Pearson and Spear-man correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating sig-nificant differences between the types of human judgments."	"After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases ."	"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']"	0
CC1133	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	extending the meteor machine translation evaluation metric to the phrase level	"['Michael Denkowski', 'Alon Lavie']"		"This paper presents METEOR-NEXT, an ex-tended version of the METEOR metric de-signed to have high correlation with post-editing measures of machine translation qual-ity. We describe changes made to the met-ric's sentence aligner and scoring scheme as well as a method for tuning the metric's pa-rameters to optimize correlation with human-targeted Translation Edit Rate (HTER). We then show that METEOR-NEXT improves cor-relation with HTER over baseline metrics, in-cluding earlier versions of METEOR, and ap-proaches the correlation level of a state-of-the-art metric, TER-plus (TERp)."	"They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) ."	"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']"	4
CC1134	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	extracting paraphrase patterns from bilingual parallel corpora	"['Shiqi Zhao', 'Haifeng Wang', 'Ting Liu', 'Sheng Li']"	introduction	"Paraphrase patterns are semantically equivalent patterns, which are useful in both paraphrase recognition and generation. This paper presents a pivot approach for extracting paraphrase patterns from bilingual parallel corpora, whereby the paraphrase patterns in English are extracted using the patterns in another language as pivots. We make use of log-linear models for computing the paraphrase likelihood between pattern pairs and exploit feature functions based on maximum likelihood estimation (MLE), lexical weighting (LW), and monolingual word alignment (MWA). Using the presented method, we extract more than 1 million pairs of paraphrase patterns from about 2 million pairs of bilingual parallel sentences. The precision of the extracted paraphrase patterns is above 78%. Experimental results show that the presented method significantly outperforms a well-known method called discovery of inference rules from text (DIRT). Additionally, the log-linear model with the proposed feature functions are effective. The extracted paraphrase patterns are fully analyzed. Especially, we found that the extracted paraphrase patterns can be classified into five types, which are useful in multiple natural language processing (NLP) applications."	"Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words ."	"['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']"	0
CC1135	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	syntacticsemantic structures for textual entailment recognition	"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']"	experiments	"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning."	"For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."	"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"	1
CC1136	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	a syntaxbased statistical translation model	"['Kenji Yamada', 'Kevin Knight']"	conclusion		"One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) ."	"['Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .', 'Another interesting direction is to investigate the potential of paraphrase patterns (i.e.', 'patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).', 'On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.', 'As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.', '1343']"	3
CC1137	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	jlsi a tool for latent semantic indexing software available at httptccitcitresearchtextectoolsresourcesjlsihtml	['Claudio Giuliano']	experiments		We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .	"['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).', 'In this way we obtained 13760 word pairs.']"	5
CC1138	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	syntacticsemantic structures for textual entailment recognition	"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']"	experiments	"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning."	"Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) ."	"['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.', 'This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', '63.50%).', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', 'In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.', ""Finally, it's worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%)."", 'This demonstrates that phrase tables can successfully replace MT systems in the CLTE task.']"	1
CC1139	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	moses open source toolkit for statistical machine translation	"['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Burch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen']"		"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks."	"Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) ."	"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']"	5
CC1140	P11-1134	Optimizing textual entailment recognition using particle swarm optimization	recognizing textual entailment rational evaluation and approaches	"['Ido Dagan', 'Bill Dolan', 'Bernardo Magnini', 'Dan Roth']"	introduction	"The goal of identifying textual entailment - whether one piece of text can be plausibly inferred  from another - has emerged in recent years as a generic core problem in natural language  understanding. Work in this area has been largely driven by the PASCAL Recognizing  Textual Entailment (RTE) challenges, which are a series of annual competitive meetings.  The current work exhibits strong ties to some earlier lines of research, particularly automatic  acquisition of paraphrases and lexical semantic relationships and unsupervised inference in  applications such as question answering, information extraction and summarization. It has  also opened the way to newer lines of research on more involved inference methods, on  knowledge representations needed to support this natural language understanding challenge  and on the use of learning methods in this context. RTE has fostered an active and growing  community of researchers focused on the problem of applied entailment. This special issue  of the JNLE provides an opportunity to showcase some of the most important work in this  emerging area"	"Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) ."	"['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']"	0
CC1141	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	morphology and meaning in the english mental lexicon	"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']"	introduction	"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect"	Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks .	"['Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks .', 'The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '(See Sec. 2 for models of morphological organization and access and related experiments).']"	0
CC1142	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	the measurement of interrater agreement statistical methods for rates and proportions2212–236	"['Joseph L Fleiss', 'Bruce Levin', 'Myunghee Cho Paik']"	method		We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .	"['Organization and Processing of Compound Verbs in the Mental Lexicon Compound verbs, as discussed above, are special type of verb sequences consisting of two or more verbs acting as a single verb and express a single expression of meaning.', 'The verb V1 is known as pole and V2 is called as vector.', 'For example, ""ওঠে পড়া "" (getting up) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'However, not all V1+V2 combinations are CVs.', 'For example, expressions like, ""নিঠে য়াও ""(take and then go) and "" নিঠে আঠ ়া"" (return back) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as CV.', 'The key question linguists are trying to identify for a long time and debating a lot is whether to consider CVs as a single lexical units or consider them as two separate units.', 'Since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'A clear understanding about these phenomena may help us to classify or extract actual CVs from other verb sequences.', 'In order to do so, presently we have applied three different techniques to collect user data.', 'In the first technique, we annotated 4500 V1+V2 sequences, along with their example sentences, using a group of three linguists (the expert subjects).', 'We asked the experts to classify the verb sequences into three classes namely, CV, not a CV and not sure.', 'Each linguist has received 2000 verb pairs along with their respective example sentences.', 'Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .', 'Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers.', 'We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional.', 'We found an agreement of κ=0.69 among the subjects.', 'We also observe a continuum of compositionality score among the verb sequences.', 'This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV.', ""We then, compare the compositionality score with that of the expert user's annotation."", 'We found a significant correlation between the expert annotation and the compositionality score.', 'We observe verb sequences that are annotated as CVs (like, খেঠে খিল , ওঠে পড , কঠে খি ) have got low compositionality score (average score ranges between 1-4) on the other hand high compositional values are in general tagged as not a cv (নিঠে য়া (come and get), নিঠে আে (return back), তু ঠল খেঠেনি (kept), গনিঠে পিল (roll on floor)).', 'This reflects that verb sequences which are not CV shows high degree of compositionality.', 'In other words non CV verbs can directly interpret from their constituent verbs.', 'This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning.', 'In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences.', 'We followed the same experimental procedure as discussed in (Taft, 2004) for English polymorphemic words.', 'However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.', 'The reaction time (RT) of each subject is recorded.', 'Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.', 'This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed.', 'However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results.']"	5
CC1143	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	serial verb construction in marathiquot	['R Pandharipande']	related work		"Similar findings have been proposed by #AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints ."	"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by #AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"	0
CC1144	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	morphological decomposition and the reverse base frequency effect	['M Taft']	method	"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition."	We followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for English polymorphemic words .	"['Organization and Processing of Compound Verbs in the Mental Lexicon Compound verbs, as discussed above, are special type of verb sequences consisting of two or more verbs acting as a single verb and express a single expression of meaning.', 'The verb V1 is known as pole and V2 is called as vector.', 'For example, ""ওঠে পড়া "" (getting up) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'However, not all V1+V2 combinations are CVs.', 'For example, expressions like, ""নিঠে য়াও ""(take and then go) and "" নিঠে আঠ ়া"" (return back) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as CV.', 'The key question linguists are trying to identify for a long time and debating a lot is whether to consider CVs as a single lexical units or consider them as two separate units.', 'Since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'A clear understanding about these phenomena may help us to classify or extract actual CVs from other verb sequences.', 'In order to do so, presently we have applied three different techniques to collect user data.', 'In the first technique, we annotated 4500 V1+V2 sequences, along with their example sentences, using a group of three linguists (the expert subjects).', 'We asked the experts to classify the verb sequences into three classes namely, CV, not a CV and not sure.', 'Each linguist has received 2000 verb pairs along with their respective example sentences.', 'Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'We measure the inter annotator agreement using the Fleiss Kappa (Fleiss et al., 1981) measure (κ) where the agreement lies around 0.79.', 'Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers.', 'We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional.', 'We found an agreement of κ=0.69 among the subjects.', 'We also observe a continuum of compositionality score among the verb sequences.', 'This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV.', ""We then, compare the compositionality score with that of the expert user's annotation."", 'We found a significant correlation between the expert annotation and the compositionality score.', 'We observe verb sequences that are annotated as CVs (like, খেঠে খিল , ওঠে পড , কঠে খি ) have got low compositionality score (average score ranges between 1-4) on the other hand high compositional values are in general tagged as not a cv (নিঠে য়া (come and get), নিঠে আে (return back), তু ঠল খেঠেনি (kept), গনিঠে পিল (roll on floor)).', 'This reflects that verb sequences which are not CV shows high degree of compositionality.', 'In other words non CV verbs can directly interpret from their constituent verbs.', 'This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning.', 'In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences.', 'We followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for English polymorphemic words .', 'However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.', 'The reaction time (RT) of each subject is recorded.', 'Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.', 'This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed.', 'However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results.']"	5
CC1145	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	causal chains and compound verbsquot	['E Bashir']	related work		"#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind""."	"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"	0
CC1146	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	morphological decomposition and the reverse base frequency effect	['M Taft']	related work	"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition."	"Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose ."	"['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']"	0
CC1147	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	lexical representation of derivational relation	['D Bradley']	related work		"The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ) ."	"['Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ) .', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988).']"	0
CC1148	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	masked morphological priming in visual word recognition	"['J Grainger', 'P Cole', 'J Segui']"	introduction	"Masked priming studies with adult readers have provided evidence for a form-based morpho-orthographic segmentation mechanism that ""blindly"" decomposes any word with the appearance of morphological complexity. The present studies investigated whether evidence for structural morphological decomposition can be obtained with developing readers. We used a masked primed lexical decision design first adopted by Rastle, Davis, and New (2004), comparing truly suffixed (golden-GOLD) and pseudosuffixed (mother-MOTH) prime-target pairs with nonsuffixed controls (spinach-SPIN). Experiment 1 tested adult readers, showing that priming from both pseudo- and truly suffixed primes could be obtained using our own set of high-frequency word materials. Experiment 2 assessed a group of Year 3 and Year 5 children, but priming only occurred when prime and target shared a true morphological relationship, and not when the relationship was pseudomorphological. This pattern of results indicates that morpho-orthographic decomposition mechanisms do not become automatized until a relatively late stage in reading development.21 page(s"	"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) ."	"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']"	0
CC1149	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	conscious choice and some light verbs in urduquot	['M Butt']	related work		#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .	"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"	0
CC1150	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	morphology and meaning in the english mental lexicon	"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']"	introduction	"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect"	"With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla ."	"['With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .', 'Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated.', 'These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes.', 'Further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of Bangla polymorphemic words.', 'Our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix.']"	5
CC1151	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	repetition priming and frequency attenuation in lexical access	"['K I Forster', 'C Davis']"	method		"We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words ."	"['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', 'The subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'The same target word is again probed but with a different audio or visual probe called the control word.', 'The control shows no relationship with the target.', 'For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age).']"	5
CC1152	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	what can we learn from the morphology of hebrew a maskedpriming investigation of morphological representation	"['R Frost', 'K I Forster', 'A Deutsch']"	introduction	"All Hebrew words are composed of 2 interwoven morphemes: a triconsonantal root and a phonological word pattern. the lexical representations of these morphemic units were examined using masked priming. When primes and targets shared an identical word pattern, neither lexical decision nor naming of targets was facilitated. In contrast root primes facilitated both lexical decisions and naming of target words that were derived from these roots. This priming effect proved to be independent of meaning similarity because no priming effects were found when primes and targets were semantically but not morphologically related. These results suggest that Hebrew roots are lexical units whereas word patterns are not. A working model of lexical organization in Hebrew is offered on the basis of these results."	"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) ."	"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']"	0
CC1153	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	morphology and meaning in the english mental lexicon	"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']"	method	"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect"	"We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words ."	"['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', 'The subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'The same target word is again probed but with a different audio or visual probe called the control word.', 'The control shows no relationship with the target.', 'For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age).']"	5
CC1154	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	wordnet an electronic lexical database	['C Fellbaum']	introduction	"Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet."	"Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency ."	"['A clear understanding of the structure and the processing mechanism of the mental lexicon will further our knowledge of how the human brain processes language.', 'Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications.', 'Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .']"	0
CC1155	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	morphological decomposition and the reverse base frequency effect	['M Taft']	introduction	"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition."	"On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ) ."	"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English, Hebrew, Italian, French, Dutch, and few other languages (Marslen-Wilson et al., 2008;Frost et al., 1997;Grainger, et al., 1991;Drews and Zwitserlood, 1995).', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ) .', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']"	0
CC1156	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	lexical access and inflectional morphology	"['A Caramazza', 'A Laudanna', 'C Romani']"	related work		"For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG ) ."	"['Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon (Bradley, 1980;Butterworth, 1983).', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG ) .']"	0
CC1157	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	hindi structures intermediate levelquot michigan papers on south and	['P E Hook']	related work		#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .	"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"	0
CC1158	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	lexical representation of derivational relation	['D Bradley']	related work		"Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose ."	"['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']"	0
CC1159	P13-3018	"DNA, Words and Models, Statistics of Exceptional Words"	and orthographic similarity in visual word recognition	"['E Drews', 'P Zwitserlood']"	introduction		"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) ."	"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']"	0
CC1160	P97-1063	A word-to-word model of translational equivalence	a geometric approach to mapping bitext correspondencequot	['I D Melamed']	method		"Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) ."	"['2We could just as easily use other symmetric ""association"" measures, such as ¢2  or the Dice coefficient (Smadja, 1992).', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']"	0
CC1161	P97-1063	A word-to-word model of translational equivalence	a statistical approach to machine translationquot	"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']"	introduction		"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993a ) ."	"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"	0
CC1162	P97-1063	A word-to-word model of translational equivalence	robust word alignment for machine aided translationquot	"['I Dagan', 'K Church', 'SZ W Gale']"	method		Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #AUTHOR_TAG ) .	"['2We could just as easily use other symmetric ""association"" measures, such as ¢2  or the Dice coefficient (Smadja, 1992).', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', ""Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #AUTHOR_TAG ) ."", 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']"	0
CC1163	P97-1063	A word-to-word model of translational equivalence	using bitextual alignment for translation validation the transcheck systemquot	['E Macklovitch']	introduction		"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)."	"['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"	0
CC1164	P97-1063	A word-to-word model of translational equivalence	a program for aligning sentences in bilingual corporaquot	"['W Gale', 'K W Church']"			"This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) ."	"['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']"	0
CC1165	P97-1063	A word-to-word model of translational equivalence	a program for aligning sentences in bilingual corporaquot	"['W Gale', 'K W Church']"	introduction		"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)."	"['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"	0
CC1166	P97-1063	A word-to-word model of translational equivalence	a geometric approach to mapping bitext correspondencequot	['I D Melamed']	introduction		"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)."	"['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"	0
CC1167	P97-1063	A word-to-word model of translational equivalence	how to compile a bilingual collocational lexicon automaticallyquot	['F Smadja']	method		"2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) ."	"['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']"	1
CC1168	P97-1063	A word-to-word model of translational equivalence	a statistical approach to language translationquot	"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']"	introduction		"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) ."	"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"	0
CC1169	P97-1063	A word-to-word model of translational equivalence	a geometric approach to mapping bitext correspondencequot	['I D Melamed']	introduction		"The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) ."	"['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']"	0
CC1170	P97-1063	A word-to-word model of translational equivalence	a program for aligning sentences in bilingual corporaquot	"['W Gale', 'K W Church']"			"We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) ."	"['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']"	5
CC1171	P97-1063	A word-to-word model of translational equivalence	robust word alignment for machine aided translationquot	"['I Dagan', 'K Church', 'SZ W Gale']"	method		"It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) ."	"['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']"	0
CC1172	P97-1063	A word-to-word model of translational equivalence	semiautomatic acquisition of domainspecific translation lexiconsquot	['personal communication Nasr']			#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .	"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al.'s Model 2 on 74 million words of the Canadian Hansards."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.', 'Therefore, we ignored English words that were linked to nothing.']"	0
CC1173	P97-1063	A word-to-word model of translational equivalence	a program for aligning sentences in bilingual corporaquot	"['W Gale', 'K W Church']"	introduction		"Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995)."	"['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other Melamed, 1996a).', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']"	0
CC1174	P97-1063	A word-to-word model of translational equivalence	deriving translation data from bilingual textsquot	"['R Catizone', 'G Russell', 'S Warwick']"	introduction		"Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996)."	"['Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"	0
CC1175	P97-1063	A word-to-word model of translational equivalence	how to compile a bilingual collocational lexicon automaticallyquot	['F Smadja']	conclusion		Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) .	"['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']"	3
CC1176	P97-1063	A word-to-word model of translational equivalence	accurate methods for the statistics of surprise and coincidencequot	['T Dunning']	method		"For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 ."	"['Our translation model consists of the hidden parameters A+ and A-, and likelihood ratios L(u, v).', 'The two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'L(u,v) represents the likelihood that u and v can be mutual translations.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play.""]"	5
CC1177	P97-1063	A word-to-word model of translational equivalence	automatic evaluation and uniform filter cascades for inducing nbest translation lexiconsquot	['I D Melamed']	introduction	"This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora."	"With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) ."	"['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a).', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997).']"	0
CC1178	P97-1063	A word-to-word model of translational equivalence	building probabilistic models for natural language	['S Chen']	method	"Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation. In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.Engineering and Applied Science"	"It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) ."	"['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']"	1
CC1179	P97-1063	A word-to-word model of translational equivalence	maximum likelihood from incomplete data via the em algorithmquot	"['A P Dempster', 'N M Laird', 'D B Rubin']"			"By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set ."	"[""One advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', 'In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.', 'We have adopted the simple heuristic that the model ""has converged"" when this probability stops increasing.']"	0
CC1180	P97-1063	A word-to-word model of translational equivalence	measuring semantic entropyquot	['I D Melamed']	conclusion		"Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) ."	"['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).', 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']"	3
CC1181	P97-1063	A word-to-word model of translational equivalence	measuring semantic entropyquot	['I D Melamed']	method		"For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) ."	"['In the basic word-to-word model, the hidden parameters A + and A-depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']"	0
CC1182	P97-1063	A word-to-word model of translational equivalence	a statistical approach to machine translationquot	"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']"	method		"It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) ."	"['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']"	1
CC1183	P97-1063	A word-to-word model of translational equivalence	a program for aligning sentences in bilingual corporaquot	"['W Gale', 'K W Church']"	method		"2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."	"[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']"	1
CC1184	P97-1063	A word-to-word model of translational equivalence	the mathematics of statistical machine translation parameter estimationquot	"['P F Brown', 'V J Della Pietra', 'S A Della Pietra', 'R L Mercer']"	introduction		"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) ."	"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"	0
CC1185	P97-1063	A word-to-word model of translational equivalence	building probabilistic models for natural language	['S Chen']		"Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation. In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.Engineering and Applied Science"	"This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) ."	"['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']"	0
CC1186	P97-1063	A word-to-word model of translational equivalence	the mathematics of statistical machine translation parameter estimationquot	"['P F Brown', 'V J Della Pietra', 'S A Della Pietra', 'R L Mercer']"	method		This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .	"['To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']"	1
CC1187	P97-1063	A word-to-word model of translational equivalence	line em up advances in alignment technology and their impact on translation support toolsquot	"['E Macklovitch', 'M-L Hannan']"		"We present a quantitative evaluation of one well-known word-alignment algorithm, as well as an analysis of frequent errors in terms of this model's underlying assumptions. Despite error rates that range from 22% to 32%, we argue that this technology can be put to good use in certain automated aids for human translators. We support our contention by pointing to several successful applications and outline ways in which text alignments below the sentence level would allow us to improve the performance of other translation support tools."	"The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."	"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.', 'Therefore, we ignored English words that were linked to nothing.']"	1
CC1188	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	a wordclass approach to labeling pscfg rules for machine translation	"['Andreas Zollmann', 'Stephan Vogel']"	experiments	"In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features.    Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chinese-to-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available."	"11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags ."	"['For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.', 'The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine.', 'For the hyperparameters, we set Į to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.', 'We built an s2t translation system with the achieved U-trees after the 1000th iteration.', 'We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']"	4
CC1189	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	a bayesian model of syntaxdirected tree to string grammar induction	"['Trevor Cohn', 'Phil Blunsom']"	method	"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step."	"Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment ."	"['Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules 7 in terms of frontier nodes (Galley et al., 2004).', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics.']"	4
CC1190	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	hierarchical phrasebased translation	['David Chiang']	related work	"Left-to-right (LR) decoding (Watanabe et al., 2006) is promising decoding algorithm for hierarchical phrase-based translation (Hiero) that visits input spans in arbitrary order producing the output translation in left to right order. This leads to far fewer language model calls, but while LR decod-ing is more efficient than CKY decoding, it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower transla-tion quality as a result. This paper in-troduces two improvements to LR decod-ing that make it comparable in translation quality to CKY-based Hiero."	The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .	"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"	1
CC1191	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	bayesian synchronous grammar induction	"['Phil Blunsom', 'Trevor Cohn', 'Miles Osborne']"	related work	"We present a novel method for inducing synchronous context free grammars (SCFGs) from a corpus of parallel string pairs. SCFGs can model equivalence between strings in terms of substitutions, insertions and deletions, and the reordering of sub-strings. We develop a non-parametric Bayesian model and apply it to a machine translation task, using priors to replace the various heuristics commonly used in this field. Using a variational Bayes training procedure, we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations, showing improvements in translation performance over maximum likelihood models."	"#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus ."	"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"	1
CC1192	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	spmt statistical machine translation with syntactified target language phrases	"['Daniel Marcu', 'Wei Wang', 'Abdessamad Echihabi', 'Kevin Knight']"	experiments	"We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5."	"The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) ."	"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']"	5
CC1193	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	treebased translation without using parse trees	"['Feifei Zhai', 'Jiajun Zhang', 'Yu Zhou', 'Chengqing Zong']"	related work	"Parse trees are indispensable to the existing tree- based translation models. However, there exist two major challenges in utilizing parse trees: 1) Fo r most language pairs, it is hard to get parse trees due to the lack of syntactic resources for training. 2) Numero us parse trees are not compatible with word alignment which is generally learned by GIZA++. Therefore, a number of useful translation rules are often excluded. To overcome these two problems, in this paper we make a great effort to bypass the parse trees and induce effective unsupervised trees for treebased translation models. Our unsupervised trees depend only on the word alignment without utilizing any syntactic resource or linguistic pars er. Hence, they are very beneficial for the translation between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms th e stringto-tree system using parse trees."	Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .	"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"	1
CC1194	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	binarizing syntax trees to improve syntaxbased machine translation accuracy	"['Wei Wang', 'Kevin Knight', 'Daniel Marcu']"	experiments	We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.	"Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system ."	"['To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006).', 'Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']"	5
CC1195	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	spmt statistical machine translation with syntactified target language phrases	"['Daniel Marcu', 'Wei Wang', 'Abdessamad Echihabi', 'Kevin Knight']"	introduction	"We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5."	"Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) ."	"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"	0
CC1196	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	a new stringtodependency machine translation algorithm with a target dependency language model	"['Libin Shen', 'Jinxi Xu', 'Ralph Weischedel']"	introduction	"In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set."	"Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) ."	"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']"	0
CC1197	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	a bayesian model of syntaxdirected tree to string grammar induction	"['Trevor Cohn', 'Phil Blunsom']"	related work	"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step."	#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .	"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"	1
CC1198	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	a bayesian model of syntaxdirected tree to string grammar induction	"['Trevor Cohn', 'Phil Blunsom']"	method	"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step."	"Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :"	"['The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules.', 'Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']"	5
CC1199	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	a wordclass approach to labeling pscfg rules for machine translation	"['Andreas Zollmann', 'Stephan Vogel']"	related work	"In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features.    Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chinese-to-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available."	#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .	"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"	1
CC1200	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	what’s in a translation rule	"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']"	method	"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data."	"Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) ."	"['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently,  designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment.', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics.']"	5
CC1201	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	retraining monolingual parser bilingually for syntactic smt	"['Shujie Liu', 'Chi-Ho Li', 'Mu Li', 'Ming Zhou']"	related work	"The training of most syntactic SMT approaches involves two essential components, word alignment and monolingual parser. In the current state of the art these two components are mutually independent, thus causing problems like lack of rule generalization, and violation of syntactic correspondence in translation rules. In this paper, we propose two ways of re-training monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices. One is targeted self-training with a simple evaluation function; the other is based on training data selection from forced alignment of bilingual data. We also propose an auxiliary method for boosting alignment quality, by symmetrizing alignment matrices with respect to parse trees. The best combination of these novel methods achieves 3 Bleu point gain in an IWSLT task and more than 1 Bleu point gain in NIST tasks."	#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .	"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"	1
CC1202	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	statistical significance tests for machine translation evaluation	['Philipp Koehn']	experiments	"If two translation systems differ differ in perfor-mance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling meth-ods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real."	The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .	"['The experiments are conducted on Chinese-to-English translation.', 'The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.', 'We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment.', 'We train a 5gram language model on the Xinhua portion of the English Gigaword corpus and the English part of the training data.', 'For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set.', 'We use MERT (Och, 2004) to tune parameters.', 'Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.', 'The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']"	5
CC1203	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	scalable inference and training of contextrich syntactic translation models	"['Michel Galley', 'Jonathan Graehl', 'Kevin Knight', 'Daniel Marcu', 'Steve DeNeefe', 'Wei Wang', 'Ignacio Thayer']"	experiments	"Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language fluency. Syntactic approaches seek to remedy these problems. In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words. Second, we propose probability estimates and a training procedure for weighting these rules. We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules."	The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .	"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']"	5
CC1204	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	a bayesian model for learning scfgs with discontiguous rules	"['Abby Levenberg', 'Chris Dyer', 'Phil Blunsom']"	related work	"We describe a nonparametric model and corresponding inference algorithm for learning Synchronous Context Free Grammar derivations for parallel text. The model employs a Pitman-Yor Process prior which uses a novel base distribution over synchronous grammar rules. Through both synthetic grammar induction and statistical machine translation experiments, we show that our model learns complex translational correspondences--- including discontiguous, many-to-many alignments---and produces competitive translation results. Further, inference is efficient and we present results on significantly larger corpora than prior work."	#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .	"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"	1
CC1205	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	two languages are better than one for syntactic parsing	"['David Burkett', 'Dan Klein']"	related work	"We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation."	#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .	"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"	1
CC1206	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	what’s in a translation rule	"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']"	method	"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data."	9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .	"['The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM).', 'Actually, the frequent AEs also greatly impair the conventional TM.', 'Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs.', 'Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs.', 'Our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']"	5
CC1207	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	what’s in a translation rule	"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']"	experiments	"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data."	"In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side ."	"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on (Galley et al., 2006) and ).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']"	5
CC1208	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	what’s in a translation rule	"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']"	method	"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data."	"Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment ."	"['Our first Gibbs operator, Rotate, just works by sampling value of the Ȍparameters, one at a time, and changing the U-tree accordingly.', 'For example, in Figure 3(a), the s-node is currently in the left VWDWHȌ :HVDPSOHWKHȌRIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRIȌLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state.', 'Otherwise, we change its state to the right state Ȍ , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of Ȍ would define two different U-trees.', 'Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .', 'Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.', ""In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node's lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node."", 'For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (Ȍ ) defines only one rule: Using these STSG rules, the two derivations are evaluated as follows (We use the value of Ȍ to denote the corresponding STSG derivation):']"	5
CC1209	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	what’s in a translation rule	"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']"	introduction	"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data."	"Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) ."	"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"	0
CC1210	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	dependency treelet translation syntactically informed phrasal smt	"['Chris Quirk', 'Arul Menezes', 'Colin Cherry']"	introduction	"We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser. 1"	"Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) ."	"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"	0
CC1211	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	syntax augmented machine translation via chart parsing	"['Andreas Zollmann', 'Ashish Venugopal']"	experiments	"We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop's baseline system. Our translation system is available open-source under the GNU General Public License."	"To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively ."	"['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']"	5
CC1212	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	learning accurate compact and interpretable tree annotation	"['Slav Petrov', 'Leon Barrett', 'Romain Thibaux', 'Dan Klein']"	experiments	"We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple Xbar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2 % on the Penn Treebank, higher than fully lexicalized systems."	"To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) ."	"['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']"	5
CC1213	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	binarizing syntax trees to improve syntaxbased machine translation accuracy	"['Wei Wang', 'Kevin Knight', 'Daniel Marcu']"	method	We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.	"This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) ."	"['is the probability of producing the target tree fragment frag.', 'To generate frag,  used a geometric prior to decide how many child nodes to assign each node.', 'Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']"	4
CC1214	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	treetostring alignment template for statistical machine translation	"['Yang Liu', 'Qun Liu', 'Shouxun Lin']"	introduction	"We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models."	"Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) ."	"['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"	0
CC1215	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	restructuring relabeling and realigning for syntaxbased machine translation	"['Wei Wang', 'Jonathan May', 'Kevin Knight', 'Daniel Marcu']"	introduction		This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .	"['However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training.', '2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']"	0
CC1216	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	joint parsing and alignment with weakly synchronized grammars	"['David Burkett', 'John Blitzer', 'Dan Klein']"	related work	"Syntactic machine translation systems extract rules from bilingual, word-aligned, syntacti-cally parsed text, but current systems for pars-ing and word alignment are at best cascaded and at worst totally independent of one an-other. This work presents a unified joint model for simultaneous parsing and word alignment. To flexibly model syntactic divergence, we de-velop a discriminative log-linear model over two parse trees and an ITG derivation which is encouraged but not forced to synchronize with the parses. Our model gives absolute improvements of 3.3 F1 for English pars-ing, 2.1 F1 for Chinese parsing, and 5.5 F1 for word alignment over each task's indepen-dent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chi-nese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments."	Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .	"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"	1
CC1217	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	transforming trees to improve syntactic convergence	"['David Burkett', 'Dan Klein']"	related work	"We describe a transformation-based learning method for learning a sequence of mono-lingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Transla-tion Treebank, we show how our method au-tomatically discovers transformations that ac-commodate differences in English and Chi-nese syntax. Furthermore, when transforma-tions are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU im-provement over baseline trees."	#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .	"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"	1
CC1218	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	a bayesian model of syntaxdirected tree to string grammar induction	"['Trevor Cohn', 'Phil Blunsom']"	method	"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step."	"Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."	"['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']"	4
CC1219	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	a gibbs sampler for phrasal synchronous grammar induction	"['Phil Blunsom', 'Trevor Cohn', 'Chris Dyer', 'Miles Osborne']"	method	"We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches."	"Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."	"['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']"	4
CC1220	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	a gibbs sampler for phrasal synchronous grammar induction	"['Phil Blunsom', 'Trevor Cohn', 'Chris Dyer', 'Miles Osborne']"		"We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches."	"In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 ."	"['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']"	1
CC1221	Q13-1020	Unsupervised Tree Induction for Tree-based Translation	syntax augmented machine translation via chart parsing	"['Andreas Zollmann', 'Ashish Venugopal']"	related work	"We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop's baseline system. Our translation system is available open-source under the GNU General Public License."	#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .	"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"	1
CC1222	W00-1017	WIT	implementation of coordinative nodding behavior on spoken dialogue systems	"['Jun-ichi Hirasawa', 'Noboru Miyazaki', 'Mikio Nakano', 'Takeshi Kawabata']"		"This paper proposes a mechanism that contributes to the implementation of a spoken dialogue system with which a user can communicate e ortlessly. In a dialogue, exchanges between participants promote the establishment of shared information and this leads to e ortless communication. This is called  dialogue coordination"". In particular, revealing the respondent's internal state, such as through nodding and back-channel feedback, promotes the establishment of shared information. This is called  manifestation"", which is one aspect of coordinative behavior, and a mechanism for handling manifestation is introduced. In a human-human dialogue, the listener's manifestative behavior often occurs during a speaker's utterance. However, systems using conventional speech recognition technologies cannot respond during the speaker's utterance. In order to solve this problem, the proposed mechanism, ISTAR protocol transmission, utilizes the intermediate speech recognition results without waiting for the end of the speaker's utterance. This realizes a system with exible manifestative behavior."	"This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses ."	"['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992).', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'A phrase is a sequence of words, which is to be defined in a domain-dependent way.', 'Sentences can be decomposed into a couple of phrases.', 'The reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self-repairs and repetition.', 'In Japanese, bunsetsu is appropriate for defining phrases.', 'A bunsetsu consists of one content word and a number (possibly zero) of function words.', 'In the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation.']"	0
CC1223	W00-1017	WIT	the philips automatic train timetable information system	"['Harald Aust', 'Martin Oerder', 'Frank Seide', 'Volker Steinbiss']"	introduction		"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) ."	"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']"	0
CC1224	W00-1017	WIT	gus a frame driven dialog system	"['Daniel G Bobrow', 'Ronald M Kaplan', 'Martin Kay', 'Donald A Norman', 'Henry Thompson', 'Teny Winograd']"	conclusion		"There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) ."	"['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', 'Incorporating such techniques would deo crease the system developer workload.', 'However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well.', 'Therefore incorporating those techniques remains as a future work.']"	3
CC1225	W00-1017	WIT	mimic an adaptive mixed initiative spoken dialogue system for information queries	['Junnifer Chu-Carroll']		"This paper describes MIMIC, an adaptive mixed initiative spoken dialogue system that provides movie showtime information. MIMIC improves upon previous dialogue systems in two respects. First, it employs initiative-oriented strategy adaptation to automatically adapt response generation strategies based on the cumulative effect of information dynamically extracted from user utterances during the dialogue. Second, MIMIC's dialogue management architecture decouples its initiative module from the goal and response strategy selection processes, providing a general framework for developing spoken dialogue systems with different adaptation behavior."	They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .	"['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']"	1
CC1226	W00-1017	WIT	asj continuous speech corpus for research	"['Tetsunori Kobayashi', 'Shuichi Itahashi', 'Satoru Hayamizu', 'Toshiyuld Takezawa']"			Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .	"['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses.', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'A phrase is a sequence of words, which is to be defined in a domain-dependent way.', 'Sentences can be decomposed into a couple of phrases.', 'The reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self-repairs and repetition.', 'In Japanese, bunsetsu is appropriate for defining phrases.', 'A bunsetsu consists of one content word and a number (possibly zero) of function words.', 'In the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation.']"	5
CC1227	W00-1017	WIT	learning to predict problematic situations in a spoken dialogue system experiments with how may i help you	"['Marilyn Walker', 'Irene Langkilde', 'Jerry Wright Allen Gorin', 'Diane Litman']"	introduction	"Current spoken dialogue systems are deficient in their strategies for preventing, identifying and repairing problems that arise in the conversation. This paper reports results on learning to automatically identify and predict problematic human-computer dialogues in a corpus of 4774 dialogues collected with the How May I Help You spoken dialogue system. Our expectation is that the ability to predict problematic dialogues will allow the system's dialogue manager to modify its behavior to repair problems, and even perhaps, to prevent them. We train a problematic dialogue classifier using automatically-obtainable features that can identify problematic dialogues significantly better (23%) than the baseline. A classifier trained with only automatic features from the first exchange in the dialogue can predict problematic dialogues 7% more accurately than the baseline, and one trained with automatic features from the first two exchanges can perform 14% better than the baseline."	"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) ."	"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']"	0
CC1228	W00-1017	WIT	jupiter a telephonebased conversational interface for weather information	"['Victor Zue', 'Stephanie Seneff', 'James Glass', 'Joseph Polifroni', 'Christine Pao', 'Timothy J Hazen', 'Lee Hetherington']"	introduction		"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) ."	"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']"	0
CC1229	W00-1017	WIT	understanding unsegmented user utterances in realtime spoken dialogue systems	"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']"	experiments	"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries."	"WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) ."	"['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', ""The weather information system can answer the user's questions about weather forecasts in Japan."", 'The vocabulary size is around 500, and the number of phrase structure rules is 31.', 'The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.']"	2
CC1230	W00-1017	WIT	a robust system for natural spoken dialogue	"['James F Allen', 'Bradford W Miller', 'Eric K Ringger', 'Teresa Sikorsld']"	introduction	"This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains. It specifically addresses the issue of robust interpretation of speech in the presence of recognition errors. Robustness is achieved by a combination of statistical error post-correction, syntactically- and semantically-driven robust parsing, and extensive use of the dialogue context. We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training.Comment: uuencoded, gzipped PostScript. Includes extra Appendi"	"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) ."	"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']"	0
CC1231	W00-1017	WIT	a grammar and a parser for spontaneous speech	"['Mikio Nakano', 'Akira Shimazu', 'Kiyoshi Kogure']"	conclusion	"This paper classifies distinctive phenomena occurring in Japanese spontaneous speech, and proposes a grammar and processing techniques for handling them. Parsers using a grammar for written sentences cannot deal with spontaneous speech because in spontaneous speech there are phenomena that do not occur in written sentences. A grammar based on analysis of transcripts of dialogues was therefore developed. It has two distinctive features: it uses short units as input units instead of using sentences in grammars for written sentences, and it covers utterances including phrases peculiar to spontaneous speech. Since the grammar is an augmentation of a grammar for written sentences, it can also be used to analyze complex utterances. Incorporating the grammar into the distributed natural language processing model described elsewhere enables the handling of utterances including variety of phenomena peculiar to spontaneous speech."	"For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) ."	"['Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems.', 'Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information.', 'For example, it is possible to represent a discourse stack whose depth is limited.', 'Recording some dialogue history is also possible.', 'Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .', 'The language generation module features Common Lisp functions, so there is no limitation on the description.', 'Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).', 'It is also possible to build a simple finite-state-model-based dialogue system using WIT.', 'States can be represented by dialogue phases in WIT.']"	3
CC1232	W00-1017	WIT	understanding unsegmented user utterances in realtime spoken dialogue systems	"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']"		"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries."	"Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) ."	"['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', 'When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997).']"	0
CC1233	W00-1017	WIT	an efficient dialogue control method under systems limited knowledge	"['Kohji Dohsaka', 'Norihito Yasuda', 'Noboru Miyazaki', 'Mikio Nakano', 'Kiyoald Aikawa']"	experiments	"This paper presents a novel method that controls a dialogue between a spoken dialogue system and a user efficiently so that the system responds as helpfully as possible within the limits of its knowledge. Due to speech recognition errors, a system and user must engage in a ""confirmation dialogue"" to clarify a user's request. Although a confirmation dialogue is unavoidable, it should be as concise as possible. Previous methods do not sufficiently allow for the effect of the limits of the system's knowledge on the efficiency of dialogue. The result is unnecessarily long dialogues to confirm a user's request minutely even if the request is beyond the system's knowledge. This paper describes a method that controls a dialogue efficiently so as to avoid an unnecessary confirmation dialogue and presents a computational efficiency criterion for dialogue control within the limits of the system's knowledge."	"WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) ."	"['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', ""The weather information system can answer the user's questions about weather forecasts in Japan."", 'The vocabulary size is around 500, and the number of phrase structure rules is 31.', 'The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.']"	2
CC1234	W00-1017	WIT	understanding unsegmented user utterances in realtime spoken dialogue systems	"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']"	introduction	"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries."	WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .	"['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .', 'In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier.', 'Although WIT requires more domaindependent specifications than finite-state-modelbased toolkits, WIT-based systems are capable of taking full advantage of language processing technology.', 'WIT has been implemented and used to build several spoken dialogue systems.']"	5
CC1235	W00-1017	WIT	constraint projection an efficient treatment of disjunctive feature descriptions	['Mikio Nakano']		"Unification of disjunctive feature descriptions is important for efficient unification-based parsing. This paper presents constraint projection, a new method for unification of disjunctive feature structures represented by logical constraints. Constraint projection is a generalization of constraint unification, and is more efficient because constraint projection has a mechanism for abandoning information irrelevant to a goal specified by a list of variables."	Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .	"['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'Considering the meeting room reservation system, examples of domain-related sentences are ""I need to book Room 2 on Wednesday"", ""I need to book Room 2"", and ""Room 2"" and dialogue-related ones are ""yes"", ""no"", and ""Okay"".']"	5
CC1236	W00-1017	WIT	understanding unsegmented user utterances in realtime spoken dialogue systems	"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']"	experiments	"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries."	The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .	"['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']"	5
CC1237	W00-1017	WIT	europa a generic framework for developing spoken dialogue systems	"['Munehiko Sasajima', 'Yakehide Yano', 'Yasuyuld Kono']"	introduction	"Voice interfaces are not popular since they are neither useful nor user-friendly for non-specialist users. In this paper, EUROPA, a new framework for developing spoken dialogue systems, is introduced. In developing EUROPA, the authors focused on three points : (1) acceptance of spoken language, (2) portability in terms of domain and task, and (3) practical performance of the applied system. The framework is applied to prototyping a car navigation system called MINOS. MINOS is built on a portable PC, can process over 700 words of recognition vocabulary, and is able to respond to a user's question within a few seconds."	"To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) ."	"['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.', 'However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'Another is GALAXY-II (Seneffet al., 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a dialogue system to communicate with each other.', 'It consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'Although it requires more specifications than finitestate-model-based toolkits, it places less limitations on system functions.']"	0
CC1238	W00-1017	WIT	understanding unsegmented user utterances in realtime spoken dialogue systems	"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']"		"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries."	"The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method ."	"['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']"	5
CC1239	W00-1312	Cross-lingual information retrieval using hidden Markov models	phrasal translation and query expansion techniques for crosslanguage information retrievalquot	"['L Ballesteros', 'W B Croft']"	related work	"Dictionary methods for cross-language information retrieval give performance below that for mono-lingual retrieval. Failure to translate multi-term phrases has been shown to be one of the factors responsible for the errors associated with dictionary methods. First, we study the importance of phrasal translation for this approach. Second, we explore the role of phrases in query expansion via local context analysis and local feedback and show how they can be used to significantly reduce the error associated with automatic dictionary translation."	"Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996)."	"['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']"	1
CC1240	W00-1312	Cross-lingual information retrieval using hidden Markov models	using structured queries for disambiguation in crosslanguage information retrievalquot	['D A Hull']	method		"There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) ."	"['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']"	1
CC1241	W00-1312	Cross-lingual information retrieval using hidden Markov models	using statistical testing in the evaluation of retrieval experimentsquot	['D Hull']			The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .	"['The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.', 'Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']"	5
CC1242	W00-1312	Cross-lingual information retrieval using hidden Markov models	translingual information retrieval a comparative evaluationquot	"['J Carbonell', 'Y Yang', 'R Frederlcing', 'R Brown', 'Y Geng', 'D Lee']"	related work	Translingual information retrieval TIR con sists of providing a query in one language and searching document collections in one or more di erent languages This paper introduces new TIR methods and reports on comparative TIR experiments with these new methods and with previously reported ones in a realistic setting Methods fall into two categories query trans lation based and statistical IR approaches es tablishing translingual associations The re sults show that using bilingual corpora for au tomated extraction of term equivalences in con text outperforms other methods Translin gual versions of the Generalized Vector Space Model GVSM and Latent Semantic Indexing LSI perform relatively well as does translin gual pseudo relevance feedback PRF All showed relatively small performance loss be tween monolingual and translingual versions Query translation based on a general machine readable bilingual dictionary heretofore the most popular method did not match the per formance of other more sophisticated methods Also the previous very high LSI results in the literature were discon rmed by more realistic relevance based evaluations	"The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) ."	"['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999).']"	1
CC1243	W00-1312	Cross-lingual information retrieval using hidden Markov models	a tutorial on hidden markov models and selected applications in speech recognitionquot	['L Rabiner']			â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .	['â\x80¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .']	5
CC1244	W00-1312	Cross-lingual information retrieval using hidden Markov models	word sense disambiguation and information retrievalquot	['M Sanderson']	related work		#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.	"['Another common approach is term translation, e.g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']"	0
CC1245	W00-1312	Cross-lingual information retrieval using hidden Markov models	using structured queries for disambiguation in crosslanguage information retrievalquot	['D A Hull']	related work		"Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG ."	"['Another common approach is term translation, e.g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']"	0
CC1246	W00-1312	Cross-lingual information retrieval using hidden Markov models	a comparative study of query and document translation for crosslanguage information retrievalquot	['D W Oard']	related work	"Cross-language retrieval systems use queries in one natural language to guide retrieval of documents that might be written in another. Acquisition and representation of translation knowledge plays a central role in this process. This paper explores the utility of two sources of translation knowledge for cross-language retrieval. We have implemented six query translation techniques that use bilingual term lists and one based on direct use of the translation output from an existing machine translation system; these are compared with a document translation technique that uses output from the same machine translation system. Average precision measures on a TREC collection suggest that arbitrarily selecting a single dictionary translation is typically no less effective than using every translation in the dictionary, that query translation using a machine translation system can achieve somewhat better effectiveness than simpler techniques, and that document translation may result in further improvements in retrieval effectiveness under some conditions."	"One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) ."	"['Many approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', 'Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.']"	1
CC1247	W00-1312	Cross-lingual information retrieval using hidden Markov models	the effects of query structure and dictionary setups in dictionarybased crosslanguage information retrievalquot	['An Pirkola']	method		"There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) ."	"['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']"	1
CC1248	W00-1312	Cross-lingual information retrieval using hidden Markov models	resolving ambiguity for crosslanguage retrievalquot	"['L Ballesteros', 'W B Croft']"	method		" However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) ."	"['In this section we compare our approach with two other approaches.', 'One approach is ""simple substitution"", i.e., replacing a query term with all its translations and treating the translated query as a bag of words in mono-lingual retrieval.', 'Suppose we have a simple query Q=(a, b), the translations for a are al, a2, a3, and the translations for b are bl, b2. The translated query would be (at, a2, a3, b~, b2).', 'Since all terms are treated as equal in the translated query, this gives terms with more translations (potentially the more common terms) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'Also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']"	0
CC1249	W00-1312	Cross-lingual information retrieval using hidden Markov models	corpusbased stemming using cooccurrence of word variantsquot	"['J Xu', 'W B Croft']"	experiments		A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .	"['For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es)', 'containing around 22,000 English words (16,000 English stems) and processed it similarly.', 'Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .', 'One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.', 'This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.']"	5
CC1250	W00-1312	Cross-lingual information retrieval using hidden Markov models	a language modeling approach to information retrievalquot	"['J Ponte', 'W B Croft']"	related work		"Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 ."	"['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']"	1
CC1251	W00-1312	Cross-lingual information retrieval using hidden Markov models	on relevance probabilistic indexing and information retrievalquot	"['M E Maron', 'K L Kuhns']"	related work		"Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 ."	"['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']"	1
CC1252	W00-1312	Cross-lingual information retrieval using hidden Markov models	finding terminology translations from nonparallel corporaquot	"['P Fung', 'K Mckeown']"		"We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance."	Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .	"['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']"	0
CC1253	W00-1312	Cross-lingual information retrieval using hidden Markov models	a hidden markov model information retrieval systemquot	"['D Miller', 'T Leek', 'R Schwartz']"		"We present a new method for information retrieval using hidden Markov models (HMMs). We develop a general framework for incorporating multiple word generation mechanisms within the same model. We then demonstrate that an extremely simple realization of this model substantially outperforms standard tf :idf ranking on both the TREC-6 and TREC-7 ad hoc retrieval tasks. We go on to present a novel method for performing blind feedback in the HMM framework, a more complex HMM that models bigram production, and several other algorithmic re nements. Together, these methods form a state-of-the-art retrieval system that ranked among the best on the TREC-7 ad hoc retrieval task."	"Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) ."	"['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.', '(A glossary of symbols used appears below.)']"	5
CC1254	W01-0706	Exploring evidence for shallow parsing	three generative lexicalised models for statistical parsing	['M Collins']	introduction	"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97"	"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0
CC1255	W01-0706	Exploring evidence for shallow parsing	introduction to the conll2000 shared task chunking	"['E F Tjong Kim Sang', 'S Buchholz']"	experiments		The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .	"['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'The chunk types are based on the syntactic category part of the bracket label in the Treebank.', 'Roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'The phrases are: adjective phrase (ADJP), adverb phrase (ADVP), conjunction phrase (CONJP), interjection phrase (INTJ), list marker (LST), noun phrase (NP), preposition phrase (PP), particle (PRT), subordinated clause (SBAR), unlike coordinated phrase (UCP), verb phrase (VP).', '(See details in (Tjong Kim Sang and Buchholz, 2000).)']"	5
CC1256	W01-0706	Exploring evidence for shallow parsing	cooccurrence and transformation in linguistic structure	['Z S Harris']	introduction		"Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']"	0
CC1257	W01-0706	Exploring evidence for shallow parsing	statistical parsing with a contextfree grammar and word statistics	['E Charniak']	introduction		"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0
CC1258	W01-0706	Exploring evidence for shallow parsing	three generative lexicalised models for statistical parsing	['M Collins']	introduction	"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97"	"However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) ."	"['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']"	0
CC1259	W01-0706	Exploring evidence for shallow parsing	three generative lexicalised models for statistical parsing	['M Collins']	experiments	"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97"	"For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around ."	"['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.', 'The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank.', 'The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997).']"	5
CC1260	W01-0706	Exploring evidence for shallow parsing	statistical parsing with a contextfree grammar and word statistics	['E Charniak']	introduction		"However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) ."	"['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']"	0
CC1261	W01-0706	Exploring evidence for shallow parsing	the snow learning architecture	"['A Carleson', 'C Cumby', 'J Rosen', 'D Roth']"	experiments		"SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example ."	"['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]"	5
CC1262	W01-0706	Exploring evidence for shallow parsing	a stochastic parts program and noun phrase parser for unrestricted text	['Kenneth W Church']	introduction	A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>	"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0
CC1263	W01-0706	Exploring evidence for shallow parsing	the use of classifiers in sequential inference	"['V Punyakanok', 'D Roth']"	introduction	"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing."	"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0
CC1264	W01-0706	Exploring evidence for shallow parsing	parsing by chunks	['S P Abney']	introduction	"I begin with an intuition: when I read a sentence, I read it a chunk at a time. For example, the previous sentence breaks up something like this:    (1)    [I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time]              These chunks correspond in some way to prosodic patterns. It appears, for instance, that the strongest stresses in the sentence fall one to a chunk, and pauses are most likely to fall between chunks. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. A simple context-free grammar is quite adequate to describe the structure of chunks. By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. Co-occurrence of chunks is determined not just by their syntactic categories, but is sensitive to the precise words that head them; and the order in which chunks occur is much more flexible than the order of words within chunks."	"Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']"	0
CC1265	W01-0706	Exploring evidence for shallow parsing	evaluation techniques for automatic semantic extraction comparing semantic and window based approaches	['G Greffenstette']	introduction	"As large on-line corpora become more prevalent, a number of attempts have been made to automatically extract thesaurus-like relations directly from text using knowledge poor methods. In the absence of any specific application, comparing the results of these attempts is difficult. Here we propose an evaluation method using gold standards, i.e., pre-existing hand-compiled resources, as a means of comparing extraction techniques. Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words. The two techniques are very similar except that in one case selective natural language processing, a partial syntactic analysis, is performed. On a 4 megabyte corpus, syntactic contexts produce significantly better results against the gold standards for the most characteristk: words in the corpus, while windows produce better results for rare words."	"Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']"	0
CC1266	W01-0706	Exploring evidence for shallow parsing	fastus a finitestate processor for information extraction from realworld text	"['D Appelt', 'J Hobbs', 'J Bear', 'D Israel', 'M Tyson']"	introduction		"First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) ."	"['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']"	0
CC1267	W01-0706	Exploring evidence for shallow parsing	introduction to the conll2000 shared task chunking	"['E F Tjong Kim Sang', 'S Buchholz']"	experiments		"Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers ."	"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"	5
CC1268	W01-0706	Exploring evidence for shallow parsing	a learning approach to shallow parsing	"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']"	introduction	"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems."	"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0
CC1269	W01-0706	Exploring evidence for shallow parsing	a linear observed time statistical parser based on maximum entropy models	['A Ratnaparkhi']	introduction	"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall."	"However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) ."	"['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']"	0
CC1270	W01-0706	Exploring evidence for shallow parsing	the use of classifiers in sequential inference	"['V Punyakanok', 'D Roth']"	experiments	"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing."	"The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) ."	"['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]"	5
CC1271	W01-0706	Exploring evidence for shallow parsing	introduction to the conll2000 shared task chunking	"['E F Tjong Kim Sang', 'S Buchholz']"	experiments		Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .	"['We start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 .', 'Here, again, the shallow parser exhibits significantly better performance.', 'Table 3 shows the results of extracting atomic phrases.']"	5
CC1272	W01-0706	Exploring evidence for shallow parsing	text chunking using transformationbased learning	"['L A Ramshaw', 'M P Marcus']"	introduction	"Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ""baseNP"" chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach."	"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0
CC1273	W01-0706	Exploring evidence for shallow parsing	a learning approach to shallow parsing	"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']"	experiments	"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems."	"Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers ."	"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"	2
CC1274	W01-0706	Exploring evidence for shallow parsing	performance structuresa psycholinguistic and linguistic appraisal cognitive psychology	"['J P Gee', 'F Grosjean']"	introduction		"Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint ."	"['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']"	0
CC1275	W01-0706	Exploring evidence for shallow parsing	a new statistical parser based on bigram lexical dependencies	['M Collins']	experiments	"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil.."	"For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around ."	"['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.', 'The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank.', 'The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997).']"	5
CC1276	W01-0706	Exploring evidence for shallow parsing	errordriven pruning of treebanks grammars for base noun phrase identification	"['C Cardie', 'D Pierce']"	introduction	"Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a ""treebank"" corpus; then the grammar is improved by selecting rules with high ""benefit"" scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal."	"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0
CC1277	W01-0706	Exploring evidence for shallow parsing	a learning approach to shallow parsing	"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']"	experiments	"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems."	"The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) ."	"['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]"	5
CC1278	W01-0706	Exploring evidence for shallow parsing	learning to resolve natural language ambiguities a unified approach	['D Roth']	experiments	"We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best."	"SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example ."	"['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]"	5
CC1279	W01-0706	Exploring evidence for shallow parsing	introduction to the conll2000 shared task chunking	"['E F Tjong Kim Sang', 'S Buchholz']"	introduction		would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney�s work (Abney, 1991), who has suggested to �chunk� sentences to base level phrases.', 'For example, the sentence �He reckons the current account deficit will narrow to only $ 1.8 billion in September .�', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']"	0
CC1280	W01-0706	Exploring evidence for shallow parsing	the nyu system for muc6 or where’s syntax in	['R Grishman']	introduction		"First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) ."	"['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']"	0
CC1281	W01-0706	Exploring evidence for shallow parsing	a memorybased approach to learning shallow natural language patterns	"['S Argamon', 'I Dagan', 'Y Krymolowski']"	introduction	"Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction."	"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0
CC1282	W01-0706	Exploring evidence for shallow parsing	cascaded grammatical relation assignment	"['S Buchholz', 'J Veenstra', 'W Daelemans']"	introduction	"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder."	"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0
CC1283	W01-0706	Exploring evidence for shallow parsing	a linear observed time statistical parser based on maximum entropy models	['A Ratnaparkhi']	introduction	"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall."	"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) ."	"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"	0
CC1284	W01-0706	Exploring evidence for shallow parsing	the use of classifiers in sequential inference	"['V Punyakanok', 'D Roth']"	experiments	"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing."	"Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers ."	"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"	2
CC1285	W01-0706	Exploring evidence for shallow parsing	building a large annotated corpus of english the penn treebank	"['M P Marcus', 'B Santorini', 'M Marcinkiewicz']"	experiments	"Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."	"Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 ."	"['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']"	5
CC1286	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	translating the xtag english grammar to hpsg	"['Yuka Tateisi', 'Kentaro Torisawa', 'Yusuke Miyao', 'Jun’ichi Tsujii']"	introduction		Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .	"['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'However, their method depended on translator�s intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars.', 'Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', 'However, given the greater ex- pressive power of HPSG, it is impossible to con- vert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capac- ity.', 'Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad- vantages.']"	1
CC1287	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	hybrid japanese parser with handcrafted grammar and statistics	"['Hiroshi Kanayama', 'Kentaro Torisawa', 'Yutaka Mitsuisi', 'Jun’ichi Tsujii']"	introduction		"There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998)."	"['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']"	0
CC1288	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	efficient ltag parsing using hpsg parsers	"['Naoki Yoshinaga', 'Yusuke Miyao', 'Kentaro Torisawa', 'Jun’ichi Tsujii']"	introduction		A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"	1
CC1289	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	an hpsg parser with cfg filtering	"['Kentaro Torisawa', 'Kenji Nishida', 'Yusuke Miyao', 'Jun’ichi Tsujii']"	experiments		"TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) ."	"['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper  describes the detailed analysis on the factor of the difference of parsing performance.']"	1
CC1290	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	headdriven phrase structure grammar	"['Carl Pollard', 'Ivan A Sag']"	introduction		"This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion ."	"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']"	0
CC1291	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	feature structures based tree adjoining grammars	"['K Vijay-Shanker', 'Aravind K Joshi']"	introduction	"We have embedded Tree Adjoining Grammars (TAG) in a  feature structure based unification system. The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG u27s. We show that FTAG has an enhanced descriptive capacity compared to TAG formalism. We consider some restricted versions of this system and some possible linguistic stipulations that can be made. We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986)involving the logical formulation of feature structures"	"FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism ."	"['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']"	0
CC1292	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	typing as a means for validating feature structures	"['Anoop Sarkar', 'Shuly Wintner']"	introduction	"We present a method for validating the consistency of feature structure speci cations by imposing a type discipline. A typed system facilitates a great number of compile-time checks: many possible errors can be detected before the grammar is used for parsing. We have constructed a type signature for an existing broad-coverage grammar of English, and implemented a type inference algorithm that operates on the feature structure speci cations in the grammar. The algorithm reports occurrences of incompatibility with the type signature. We have detected a large number of errors in the grammar; four types of errors are described in the paper."	"ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) ."	"['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.', 'Investi- gating the relation will be apparently valuable for both communities.']"	0
CC1293	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	feature structures based tree adjoining grammars	"['K Vijay-Shanker', 'Aravind K Joshi']"	introduction	"We have embedded Tree Adjoining Grammars (TAG) in a  feature structure based unification system. The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG u27s. We show that FTAG has an enhanced descriptive capacity compared to TAG formalism. We consider some restricted versions of this system and some possible linguistic stipulations that can be made. We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986)involving the logical formulation of feature structures"	"This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion ."	"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']"	0
CC1294	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	parsing strategies with ‘lexicalized’ grammars application to tree adjoining grammars	"['Yves Schabes', 'Anne Abeille', 'Aravind K Joshi']"	introduction	"In this paper we present a general parsing strategy that arose from the development of an Earley-type parsing algorithm for TAGs (Schabes and Joshi 1988) and from recent linguistic work in TAGs (Abeille 1988).In our approach elementary structures are associated with their lexical heads. These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated. These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure.We state the conditions under which context-free based grammars can be 'lexicalized' without changing the linguistic structures originally produced. We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not give the freedom to choose the head of each structure. We show how adjunction allows us to 'lexicalize' a CFG freely.We then show how a 'lexicalized' grammar naturally follows from the extended domain of locality of TAGs and present some of the linguistic advantages of our approach.A novel general parsing strategy for 'lexicalized' grammars is discussed. In a first stage, the parser builds a set structures corresponding to the input sentence and in a second stage, the sentence is parsed with respect to this set. The strategy is independent of the linguistic theory adopted and of the underlying grammar formalism. However, we focus our attention on TAGs. Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy. Thus, in particular, a top-down strategy can be used since problems due to recursive structures are eliminated. The parser is also able to use non-local information to guide the search.We then explain how the Earley-type parser for TAGs can be modified to take advantage of this approach."	LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar	['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar']	0
CC1295	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	verbmobil a translation system for facetoface dialog	"['M Kay', 'J Gawron', 'P Norvig']"	introduction		"In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) ."	"['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']"	0
CC1296	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	hpsgstyle underspecified japanese grammar with wide coverage	"['Yutaka Mitsuishi', 'Kentaro Torisawa', 'Jun’ichi Tsujii']"	introduction	"This paper describes a wide-coverage Japanese grammar based on HPSG. The aim of this work is to see the coverage and accuracy attainable using an underspecified grammar. Underspecification, allowed in a typed feature structure formalism, enables us to write down a wide-coverage grammar concisely. The grammar we have implemented consists of only 6 ID schemata, 68 lexical entries (assigned to functional words), and 63 lexical entry templates (assigned to parts of speech (POSs) ). Furthermore, word-specific constraints such as subcategorization of verbs are not fixed in the grammar. However, this grammar can generate parse trees for 87% of the 10000 sentences in the Japanese EDR corpus. The dependency accuracy is 78% when a parser uses the heuristic that every bunsetsu is attached to the nearest possible one."	"Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) ."	"['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']"	0
CC1297	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	adapting hpsgtotag compilation to widecoverage grammars	"['Tilman Becker', 'Patrice Lopez']"	introduction		"Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars ."	"['Figure 1 depicts a brief sketch of the RenTAL system.', 'The system consists of the following four modules: Tree converter, Type hierarchy extractor, Lexicon converter and Derivation translator.', 'The tree converter module is a core module of the system, which is an implementation of the grammar conversion algorithm given in Section 3. The type hierarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them.', 'The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries.', 'The derivation translator module takes HPSG parse  (Tateisi et al., 1998).', ""However, their method depended on translator's intuitive analysis of the original grammar."", 'Thus the translation was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .', 'However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity.', 'Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages.']"	1
CC1298	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	hybrid japanese parser with handcrafted grammar and statistics	"['Hiroshi Kanayama', 'Kentaro Torisawa', 'Yutaka Mitsuisi', 'Jun’ichi Tsujii']"	introduction		"Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) ."	"['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']"	0
CC1299	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	the logic of typed feature structures	['Bob Carpenter']	introduction	"For those of us who belonged to the ""Bay Area (Computational) Linguistics Community,"" the early eighties were a heady time. Local researchers working on linguistics, computational linguistics, and logic programming were investigating notions of category, type, feature, term, and partial specification that appeared to converge to a powerful new approach for describing (linguistic) objects and their relationships by monotonic accumulation of constraints between their features. The seed notions had almost independently arisen in generalized phrase structure grammar (GPSG) (Gazdar et al. 1985), lexical-functional grammar (LFG) (Bresnan and Kaplan 1982), functionalunification grammar (FUG) (Kay 1985), logic programming (Colmerauer 1978, Pereira and Warren 1980), and terminological reasoning systems (Ait-Kaci 1984). It took, however, a lot of experimental and theoretical work to identify precisely what the core notions were, how particular systems related to the core notions, and what were the most illuminating mathematical accounts of that core. The development of the unificationbased formalism PATR-II (Shieber 1984) was an early step toward the definition of the core, but its mathematical analysis, and the clarification of the connections between the various systems, are only now coming to a reasonable closure. The Logic of Typed Feature Structures is the first monograph that brings all the main theoretical ideas into one place where they can be related and compared in a unified setting. Carpenter's book touches most of the crucial questions of the developments during the decade, provides proofs for central results, and reaches right up to the edge of current research in the field. These contributions alone make it an indispensable compendium for the researcher or graduate student working on constraint-based grammatical formalisms, and they also make it a very useful reference work for researchers in object-oriented databases and logic programming. Having discharged the main obligation of the reviewer of saying who should read the book under review and why, I will now survey each of the book's four parts while raising some more general questions impinging on the whole book as they arise from the discussion of each part."	"An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) ."	"['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'An ID grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.', 'Figure 6 illustrates an example of bottom-up parsing with an HPSG grammar.', 'First, lexical entries for ""can"" and ""run"" are unified respectively with the daughter feature structures of']"	0
CC1300	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	a lexicalized tree adjoining grammar for english	['The XTAG Research Group']	introduction	"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389"	"We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar ."	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"	5
CC1301	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	evolution of the xtag system	"['Christy Doran', 'Beth Ann Hockey', 'Anoop Sarkar', 'B Srinivas', 'Fei Xia']"	introduction		"The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars ."	"['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']"	0
CC1302	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	grammar conversion from fbltag to hpsg	"['Naoki Yoshinaga', 'Yusuke Miyao']"	introduction		The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .	"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoin-ing Grammar (FB-LTAG 1 ) (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion.', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']"	0
CC1303	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	some experiments on indicators of parsing complexity for lexicalized grammars	"['Anoop Sarkar', 'Fei Xia', 'Aravind Joshi']"	experiments	"In this paper, we identify syntactic lexical ambiguity and sentence complexity as factors that contribute to parsing complexity in fully lexicalized grammar formalisms such as Lexicalized Tree Adjoining Grammars. We also report on experiments that explore the effects of these factors on parsing complexity. We discuss how these constraints can be exploited in improving efficiency of parsers for such grammar formalisms."	"In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) ."	"['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper  describes the detailed analysis on the factor of the difference of parsing performance.']"	1
CC1304	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	a study of tree adjoining grammars	['K Vijay-Shanker']	introduction	"Constrained grammatical system have been the object of study in computational linguistics over the last few years, both with respect to their linguistic adequacy and their computational properties. A Tree Adjoining Grammar (TAG) is a tree rewriting system whose linguistic relevance has been extensively studied. A key property of these systems is that a TAG factors recursion from the co-occurrence restrictions. In this thesis, we study some mathematical properties of TAG u27s. We show that TAG u27s have several interesting properties and are a natural generalization of Context Free Grammars. We show the equivalence of the classes of languages generated by TAG u27s with those generated by Head Grammars and a linear version of Indexed Grammars, which have been studied for their linguistic applicability. We define the embedded pushdown automaton, an extention of the pushdown automaton, and prove that they are equivalent to TAG u27s. We show that the class of Tree Adjoining Languages form a substitution closed abstract family of languages, and that each Tree Adjoining Language is a semilinear language. We show that a TAG can be parsed in polynomial time by adapting the Cocke-Kasami-Younger algorithm for CFL u27s. Feature structures, essentially a set of attribute value pairs, have been used in computational linguistics to make statements of equality to capture some linguistic phenomena such as subcategorization and agreement. We embed TAG u27s in a feature structure based framework. We show that the resulting system has several advantages over TAG u27s. We give a mathematical model of this system based on the logical calculus developed by Rounds, Kasper, and Manaster-Ramer. Finally, we propose a restriction of this system and show how parsing of such a system can be done efficiently"	"FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism ."	"['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']"	0
CC1305	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	on building a more efficient grammar by exploiting types	['Dan Flickinger']	introduction	"Modern grammar development platforms often support multiple devices for representing properties of a natural language, giving the grammar writer some freedom in implementing analyses of linguistic phenomena. These design alternatives can have dramatic consequences for efficiency both in processing and in grammar building. In this paper I report on three experiments in making systematic modifications to a broad-coverage grammar of English in order to gain efficiency without loss of linguistic elegance. While the experiments are to some degree both platform-dependant and theory-bound, the kinds of modifications reported should be applicable to any unification-based grammar which makes use of types. The results make a strong case for a more visible role for the linguist in the collaborative effort to achieve greater processing efficiency."	"Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) ."	"['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']"	0
CC1306	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	evolution of the xtag system	"['Christy Doran', 'Beth Ann Hockey', 'Anoop Sarkar', 'B Srinivas', 'Fei Xia']"	introduction		"There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) ."	"['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']"	0
CC1307	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	an hpsg parser with cfg filtering	"['Kentaro Torisawa', 'Kenji Nishida', 'Yusuke Miyao', 'Jun’ichi Tsujii']"	experiments		"LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) ."	"['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']"	0
CC1308	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	efficient ltag parsing using hpsg parsers	"['Naoki Yoshinaga', 'Yusuke Miyao', 'Kentaro Torisawa', 'Jun’ichi Tsujii']"	experiments		Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .	"['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']"	0
CC1309	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	building a large annotated corpus of english the penn treebank computational linguistics	"['Mitchell Marcus', 'Beatrice Santorini', 'Mary Ann Marcinkiewicz']"	experiments		The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .	"['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm.']"	5
CC1310	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	lilfes — towards a practical hpsg parsers	"['Takaki Makino', 'Minoru Yoshida', 'Kentaro Torisawa', 'Jun’ichi Tsujii']"	experiments		The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .	"['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']"	5
CC1311	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	a study of tree adjoining grammars	['K Vijay-Shanker']	introduction	"Constrained grammatical system have been the object of study in computational linguistics over the last few years, both with respect to their linguistic adequacy and their computational properties. A Tree Adjoining Grammar (TAG) is a tree rewriting system whose linguistic relevance has been extensively studied. A key property of these systems is that a TAG factors recursion from the co-occurrence restrictions. In this thesis, we study some mathematical properties of TAG u27s. We show that TAG u27s have several interesting properties and are a natural generalization of Context Free Grammars. We show the equivalence of the classes of languages generated by TAG u27s with those generated by Head Grammars and a linear version of Indexed Grammars, which have been studied for their linguistic applicability. We define the embedded pushdown automaton, an extention of the pushdown automaton, and prove that they are equivalent to TAG u27s. We show that the class of Tree Adjoining Languages form a substitution closed abstract family of languages, and that each Tree Adjoining Language is a semilinear language. We show that a TAG can be parsed in polynomial time by adapting the Cocke-Kasami-Younger algorithm for CFL u27s. Feature structures, essentially a set of attribute value pairs, have been used in computational linguistics to make statements of equality to capture some linguistic phenomena such as subcategorization and agreement. We embed TAG u27s in a feature structure based framework. We show that the resulting system has several advantages over TAG u27s. We give a mathematical model of this system based on the logical calculus developed by Rounds, Kasper, and Manaster-Ramer. Finally, we propose a restriction of this system and show how parsing of such a system can be done efficiently"	"This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion ."	"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']"	0
CC1312	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	lilfes — towards a practical hpsg parsers	"['Takaki Makino', 'Minoru Yoshida', 'Kentaro Torisawa', 'Jun’ichi Tsujii']"	introduction		"There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) ."	"['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']"	0
CC1313	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	twostep tag parsing revisited	"['Peter Poller', 'Tilman Becker']"	introduction	"Based on the work in (Poller, 1994) and a minor assumption about a normal form for TAGs, we present a highly simplified version of the twostep parsing approach for TAGs which allows for a much easier analysis of run-time and space complexity. It also snggests how restrictions on the grammars might result in improvements in run-time complexity. The main advantage of a two-step parsing system shows in practical applications like Verbmobil (Bub et al., 1997) where the parser must look at multiple hypotheses supplied by a speech recognizer (encoded in a word hypotheses lattice) and filter out illicit hypotheses as early as possible. The first (context-free) step of our parser filters out some illicit hypotheses fast (O(n3 )); the constructed parsing matrix is then reused for the second step, the complete (O(n6 )) TAG parse."	"There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998)."	"['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']"	0
CC1314	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	grammar conversion from fbltag to hpsg	"['Naoki Yoshinaga', 'Yusuke Miyao']"			The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .	['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']	0
CC1315	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	a lexicalized tree adjoining grammar for english	['The XTAG Research Group']	introduction	"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389"	"Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) ."	"['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']"	0
CC1316	W01-1510	Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG	a lexicalized tree adjoining grammar for english	['The XTAG Research Group']	experiments	"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389"	"We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English ."	"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"	5
CC1317	W02-0309	Biomedical text retrieval in languages with a complex morphology	how effective is suffixing	['D Harman']	conclusion	"s and titles from the Cranfield collection (with 225 queries and 1400 documents), comprised the major test collection for this study. The Medlars collection (30 queries and 1033 documents), and the CACM collection (64 queries and 3204 documents) were used to provide information about the variation of stemming performance across different subject areas and test collections. In addition to the standard recall/precision measures, with SMART system averaging (Salton, 1971), several methods more suited to an interactive retrieval environment were adopted. The interactive environment returns lists of the top ranked documents, and allows the users to scan titles of a group of documents a screenful at a time, so that the ranking of individual documents within the screenful is not as important as the total number of relevant titles within a screen. Furthermore, the number of relevant documents in the first few screens is far more important for the user than the number of relevant in the last screenfuls. Three measures were selected which evaluate performance at given rank cutoff points, such as those corresponding to a screenful of document titles. The first measure, the E measure (Van Rijsbergen, 1979), is a weighted combination of recall and precision that evaluates a set of retrieved documents at a given cutoff, ignoring the ranking within that set. The measure may have weights of 0.5, 1.0, and 2.0 which correspond, respectively, to attaching half the importance to recall as to precision, equal importance to both, and double importance to recall. A lower E value indicates a more effective performance. A second measure, the total number of relevant documents retrieved by a given cutoff, was also calculated. Cutoffs of 10 and 30 documents were used, with ten reflecting a minimum number a user might be expected to TABLE 2. Retrieval performance for Cranfteld 225. scan, and 30 being an assumed upper limit of what a user would scan before query modification. The third measure applicable to the interactive environment is the number of queries that retrieve no relevant documents by the given cutoff. This measure is important because many types of query modification techniques, such as relevance feedback, require relevant documents to be in the retrieved set to work well. These measures were all used in Croft (1983) as complementary measures to the standard recall/precision evaluation."	"There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) ."	"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']"	0
CC1318	W02-0309	Biomedical text retrieval in languages with a complex morphology	morphosemantic parsing of medical expressions	"['R Baud', 'C Lovis', 'A-M Rassinoux', 'J-R Scherrer']"	introduction		"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) ."	"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']"	0
CC1319	W02-0309	Biomedical text retrieval in languages with a complex morphology	development of a stemming algorithm	['J Lovins']	introduction	"A stemming algorithm, a procedure to reduce all words with the same stem to a common form, is useful in many areas of computational linguistics and information-retrieval work. While the form of the algorithm varies with its application, certain linguistic problems are common to any stemming procedure. As a basis for evaluation of previous attempts to deal with these problems, this paper first discusses the theoretical and practical attributes of stemming algorithms. Then a new version of a context-sensitive, longest-match stemming algorithm for English is proposed; though developed for use in a library information transfer system, it is of general application. A major linguistic problem in stemming, variation in spelling of stems, is discussed in some detail and several feasible programmed solutions are outlined, along with sample results of one of these methods."	"mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance ."	"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']"	0
CC1320	W02-0309	Biomedical text retrieval in languages with a complex morphology	stemming algorithms a case study for detailed evaluation	['D A Hull']	conclusion	"The majority of information retrieval experiments are evaluated by measures such as average precision and average recall. Fundamental decisions about the superiority of one retrieval technique over another are made solely on the basis of these measures. We claim that average performance figures need to be validated with a careful statistical analysis and that there is a great deal of additional information that can be uncovered by looking closely at the results of individual queries. This article is a case study of stemming algorithms which describes a number of novel approaches to evaluation and demonstrates their value. (c) 1996 John Wiley & Sons, Inc."	"There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) ."	"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']"	0
CC1321	W02-0309	Biomedical text retrieval in languages with a complex morphology	an algorithm for suffix stripping	['M Porter']	introduction	"Purpose - The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal. Design/methodology/approach - An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Findings - Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length. Originality/value - The piece provides a useful historical document on information retrieval."	"mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance ."	"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']"	0
CC1322	W02-0309	Biomedical text retrieval in languages with a complex morphology	viewing stemming as recall enhancement	"['W Kraaij', 'R Pohlmann']"	introduction	"Previous research on stemming has shown both positive and negative effects on retrieval performance. This paper describes an experiment in which several linguistic and non-linguistic stemmers are evaluated on a Dutch test collection. Experiments especially focus on the measurement of Recall. Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linquistic stemming, both in average Precision and R-Recall. Best results are obtained with a linguistic stemmer which is enhanced with compound analysis. This version has a significantly better Recall than a system without stemming, without a significant deterioration of Precision."	"Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved ."	"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]"	0
CC1323	W02-0309	Biomedical text retrieval in languages with a complex morphology	the use of morphosemantic regularities in the medical vocabulary for automatic lexical coding methods ofinformation in	['S Wolff']	introduction	The use of morphosemantic regularities in the medical vocabulary for automatic lexical coding. -	"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) ."	"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"	0
CC1324	W02-0309	Biomedical text retrieval in languages with a complex morphology	an algorithm for suffix stripping	['M Porter']	conclusion	"Purpose - The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal. Design/methodology/approach - An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Findings - Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length. Originality/value - The piece provides a useful historical document on information retrieval."	"There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) ."	"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']"	0
CC1325	W02-0309	Biomedical text retrieval in languages with a complex morphology	automated coding of diagnoses three methods compared	"['P Franz', 'A Zaiss', 'S Schulz', 'U Hahn', 'R Klar']"	experiments	"In Germany, new legal requirements have raised the importance of the accurate encoding of admission and discharge diseases for in- and outpatients. In response to emerging needs for computer-supported tools we examined three methods for automated coding of German-language free-text diagnosis phrases. We compared a language-independent lexicon-free n-gram approach with one which uses a dictionary of medical morphemes and refines the query by a mapping to SNOMED codes. Both techniques produced a ranked output of possible diagnoses within a vector space framework for retrieval. The results did not reveal any significant difference: The correct diagnosis was found in approximately 40% for three-digit codes, and 30% for four-digit codes. The lexicon-based method was then modified by substituting the vector space ranking by a heuristic approach that capitalizes on the semantic structure of SNOMED, thus raising the number of correct diagnoses significantly (approximately 50% for three-digit codes, and 40% for four-digit codes). As a result, we claim that lexicon-based retrieval methods do not perform better than the lexicon-free ones, unless conceptual knowledge is added."	Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .	"['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']"	1
CC1326	W02-0309	Biomedical text retrieval in languages with a complex morphology	morphologic analysis of compound words	['F Wingert']	introduction		"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) ."	"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"	0
CC1327	W02-0309	Biomedical text retrieval in languages with a complex morphology	viewing stemming as recall enhancement	"['W Kraaij', 'R Pohlmann']"	conclusion	"Previous research on stemming has shown both positive and negative effects on retrieval performance. This paper describes an experiment in which several linguistic and non-linguistic stemmers are evaluated on a Dutch test collection. Experiments especially focus on the measurement of Recall. Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linquistic stemming, both in average Precision and R-Recall. Best results are obtained with a linguistic stemmer which is enhanced with compound analysis. This version has a significantly better Recall than a system without stemming, without a significant deterioration of Precision."	"Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) ."	"['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']"	0
CC1328	W02-0309	Biomedical text retrieval in languages with a complex morphology	automatic text processing the transformation analysis and retrieval ofinformation by computer	['Gerard Salton']	experiments		"The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document ."	"['For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language).', 'It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']"	5
CC1329	W02-0309	Biomedical text retrieval in languages with a complex morphology	development of a stemming algorithm	['J Lovins']	conclusion	"A stemming algorithm, a procedure to reduce all words with the same stem to a common form, is useful in many areas of computational linguistics and information-retrieval work. While the form of the algorithm varies with its application, certain linguistic problems are common to any stemming procedure. As a basis for evaluation of previous attempts to deal with these problems, this paper first discusses the theoretical and practical attributes of stemming algorithms. Then a new version of a context-sensitive, longest-match stemming algorithm for English is proposed; though developed for use in a library information transfer system, it is of general application. A major linguistic problem in stemming, variation in spelling of stems, is discussed in some detail and several feasible programmed solutions are outlined, along with sample results of one of these methods."	"There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) ."	"['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']"	0
CC1330	W02-0309	Biomedical text retrieval in languages with a complex morphology	the use of morphosemantic regularities in the medical vocabulary for automatic lexical coding methods ofinformation in	['S Wolff']	introduction	The use of morphosemantic regularities in the medical vocabulary for automatic lexical coding. -	"While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) ."	"['Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988).', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']"	0
CC1331	W02-0309	Biomedical text retrieval in languages with a complex morphology	medical subject headings	['NLM']	experiments	"Automatically assigning MeSH (Medical Subject Headings) to articles is an active research topic. Recent work demonstrated the feasibility of improving the existing automated Medical Text Indexer (MTI) system, developed at the National Library of Medicine (NLM). Encouraged by this work, we propose a novel data-driven approach that uses semantic distances in the MeSH ontology for automated MeSH assignment. Specifically, we developed a graphical model to propagate belief through a citation network to provide robust MeSH main heading (MH) recommendation. Our preliminary results indicate that this approach can reach high Mean Average Precision (MAP) in some scenarios"	"This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system ."	"['Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'The gain is still not overwhelming.', ""With regard to orthographic normalization, we expected a higher performance benefit because of the well-known spelling problems for German medical terms of Latin or Greek origin (such as in 'Zäkum ', 'Cäkum', 'Zaekum', 'Caekum', 'Zaecum', 'Caecum')."", 'For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'The same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided (cf.', 'Section 3).', 'In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'However, synonym mapping is still incomplete in the current state of our subword dictionary.', 'A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .', 'Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).']"	3
CC1332	W02-0309	Biomedical text retrieval in languages with a complex morphology	viewing morphology as an inference process	['R Krovetz']	conclusion	"AbstractMorphology is the area of linguistics concerned with the internal structure of words. Information retrieval has generally not paid much attention to word structure, other than to account for some of the variability in word forms via the use of stemmers. We report on our experiments to determine the importance of morphology, and the effect that it has on performance. We found that grouping morphological variants makes a significant improvement in retrieval performance. Improvements are seen by grouping inflectional as well as derivational variants. We also found that performance was enhanced by recognizing lexical phrases. We describe the interaction between morphology and lexical ambiguity, and how resolving that ambiguity will lead to further improvements in performance"	"There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) ."	"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']"	0
CC1333	W02-0309	Biomedical text retrieval in languages with a complex morphology	the effectiveness of stemming for natural language access to slovene textual data	"['M Popovic', 'P Willett']"	introduction	"There have been several studies of the use of stemming algorithms for conflating morphological variants in freetext retrieval systems. Comparison of stemmed and nonconflated searches suggests that there are no significant increases in the effectiveness of retrieval when stemming is applied to English-language documents and queries. This article reports the use of stemming on Slovene-language documents and queries, and demonstrates that the use of an appropriate stemming algorithm results in a large, and statistically significant, increase in retrieval effectiveness when compared with nonconflated processing; similar comments apply to the use of manual, right-hand truncation. A comparison is made with stemming of English versions of the same documents and queries and it is concluded that the effectiveness of a stemming algorithm is determined by the morphological complexity of the language that it is designed to process."	"This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) ."	"['The efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980; Norton and Pacak, 1983; Wolff, 1984; Wingert, 1985; Dujols et al., 1991; Baud et al., 1998).']"	0
CC1334	W02-0309	Biomedical text retrieval in languages with a complex morphology	morphosemantic analysis of compound word forms denoting surgical procedures methods ofinformation in medicine	"['L Norton', 'M Pacak']"	introduction		"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) ."	"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"	0
CC1335	W02-0309	Biomedical text retrieval in languages with a complex morphology	morphosemantic analysis of itis forms in medical language	"['M Pacak', 'L Norton', 'G Dunham']"	introduction		"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) ."	"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"	0
CC1336	W02-0309	Biomedical text retrieval in languages with a complex morphology	towards new measures of information retrieval evaluation	"['W Hersh', 'D Elliot', 'D Hickam', 'S Wolf', 'A Molnar', 'C Leichtenstien']"	experiments	"All of the methods currently used to assess information retrieval (IR) systems have limitations in their ability to measure how well users are able to acquire information. We utilized a new approach to assessing information obtained, based on a short-answer test given to senior medical students. Students took the ten-question test and then searched one of two IR systems on the five questions for which they were least certain of their answer Our results showed that pre-searching scores on the test were low but that searching yielded a high proportion of answers with both systems. These methods are able to measure information obtained, and will be used in subsequent studies to assess differences among IR systems."	"Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) ."	"['Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'The gain is still not overwhelming.', ""With regard to orthographic normalization, we expected a higher performance benefit because of the well-known spelling problems for German medical terms of Latin or Greek origin (such as in 'Zäkum ', 'Cäkum', 'Zaekum', 'Caekum', 'Zaecum', 'Caecum')."", 'For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'The same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided (cf.', 'Section 3).', 'In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'However, synonym mapping is still incomplete in the current state of our subword dictionary.', 'A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']"	3
CC1337	W02-0309	Biomedical text retrieval in languages with a complex morphology	effective use of natural language processing techniques for automatic conflation of multiword terms the role of derivational morphology part of speech tagging and shallow parsing	"['E Tzoukermann', 'J Klavans', 'C Jacquemin']"	conclusion		"Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) ."	"['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']"	0
CC1338	W02-0309	Biomedical text retrieval in languages with a complex morphology	the contribution of morphological knowledge to french mesh mapping for information retrieval	"['P Zweigenbaum', 'S Darmoni', 'N Grabar']"	introduction	"MeSH-indexed Internet health directories must provide a mapping from natural language queries to MeSH terms so that both health professionals and the general public can query their contents. We describe here the design of lexical knowledge bases for mapping French expressions to MeSH terms, and the initial evaluation of their contribution to Doc'CISMeF, the search tool of a MeSH-indexed directory of French-language medical Internet resources. The observed trend is in favor of the use of morphological knowledge as a moderate (approximately 5%) but effective factor for improving query to term mapping capabilities."	"Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages ."	"['While one may argue that single-word compounds are quite rare in English (which is not the case in the medical domain either), this is certainly not true for German and other basically agglutinative languages known for excessive single-word nominal compounding.', 'This problem becomes even more pressing for technical sublanguages, such as medical German (e.g., �Blut druck mess gera__t� translates to �device for measuring blood pressure�).', 'The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']"	0
CC1339	W02-0309	Biomedical text retrieval in languages with a complex morphology	morphological typology of languages for ir	['A Pirkola']	introduction	"This paper presents a morphological classification of languages from the IR perspective. Linguistic typology research has shown that the morphological complexity of every language in the world can be described by two variables, index of synthesis and index of fusion. These variables provide a theoretical basis for IR research handling morphological issues. A common theoretical framework is needed in particular because of the increasing significance of cross-language retrieval research and CLIR systems processing different languages. The paper elaborates the linguistic morphological typology for the purposes of IR research. It studies how the indexes of synthesis and fusion could be used as practical tools in mono- and cross-lingual IR research. The need for semantic and syntactic typologies is discussed. The paper also reviews studies made in different languages on the effects of morphology and stemming in IR."	"This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) ."	"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']"	0
CC1340	W02-0309	Biomedical text retrieval in languages with a complex morphology	responsa an operational fulltext retrieval system with linguistic components for large corpora	['Y Choueka']	introduction		"Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved ."	"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]"	0
CC1341	W02-0309	Biomedical text retrieval in languages with a complex morphology	the semantic structure of neoclassical compounds	"['A McCray', 'A Browne', 'D Moore']"	introduction		"Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) ."	"['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']"	0
CC1342	W02-1601	A synchronization structure of SSTC and its applications in machine translation	finding structural correspondences from bilingual parsed corpus for corpusbased translation	"['H Watanabe', 'S Kurohashi', 'E Aramaki']"		"In this paper, we describe a system and methods for finding structural correspondences from the paired dependency structures of a source sentence and its translation in a target language. The system we have developed finds word correspondences first, then finds phrasal correspondences based on word correspondences. We have also developed a GUI system with which a user can check and correct the correspondences retrieved by the system. These structural correspondences will be used as raw translation patterns in a corpus-based translation system."	"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) ."	"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"	0
CC1343	W02-1601	A synchronization structure of SSTC and its applications in machine translation	examplebased machine translation based on the synchronous sstc annotation schema	"['M H Al-Adhaileh', 'E K Tang']"		"In this paper, we describe an Example-Based Machine Translation (EBMT) system for English-Malay translation. Our approach is an example-based approach which relies sorely on example translations kept in a Bilingual Knowledge Bank (BKB). In our approach, a flexible annotation schema called Structured String-Tree Correspondence (SSTC) is used to annotate both the source and target sentences of a translation pair. Each SSTC describes a sentence, a representation tree as well as the correspondences between substrings in the sentence and subtrees in the representation tree. With both the source and target SSTCs established, a translation example in the BKB can then be represented effectively in terms of a pair of synchronous SSTCs. In the process of translation, we first try to build the representation tree for the source sentence (English) based on the example-based parsing algorithm as presented in [1]. By referring to the resultant source parse tree, we then proceed to synthesis the target sentence (Malay) based on the target SSTCs as pointed to by the synchronous SSTCs which encode the relationship between source and target SSTCs."	"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) ."	"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']"	0
CC1344	W02-1601	A synchronization structure of SSTC and its applications in machine translation	representation trees and stringtree correspondences	"['C Boitet', 'Y Zaharin']"	introduction		"In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two ."	"['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.', 'Such correspondence is defined in a way that is able to handle some non-standard cases (e.g.', 'non-projective correspondence).']"	0
CC1345	W02-1601	A synchronization structure of SSTC and its applications in machine translation	handling crossed dependencies with the stcg	"['E K Tang', 'Y Zaharin']"			"These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) ."	"['The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']"	0
CC1346	W02-1601	A synchronization structure of SSTC and its applications in machine translation	representation trees and stringtree correspondences	"['C Boitet', 'Y Zaharin']"			"Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC ."	"['In this section, we stress on the fact that in order to describe Natural Language (NL) in a natural manner, three distinct components need to be expressed by the linguistic formalisms; namely, the text, its corresponding abstract linguistic representation and the mapping (correspondence) between these two.', 'Actually, NL is not only a correspondence between different representation levels, as stressed by MTT postulates, but also a sub-correspondence between them.', 'For instance, between the string in a language and its representation tree structure, it is important to specify the sub-correspondences between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation in NLP.', 'It is well known that many linguistic constructions are not projective (e.g.', 'scrambling, cross serial dependencies, etc.).', 'Hence, it is very much desired to define the correspondence in a way to be able to handle the non-standard cases (e.g.', 'non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']"	0
CC1347	W02-1601	A synchronization structure of SSTC and its applications in machine translation	converting a bilingual dictionary into a bilingual knowledge bank based on the synchronous sstc annotation schema	"['M H Al-Adhaileh', 'E K Tang']"		"In this paper, we would like to present an approach to construct a huge Bilingual Knowledge Bank (BKB) from an English Malay bilingual dictionary based on the idea of synchronous Structured String-Tree Correspondence (SSTC). The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be non-projective. With this structure, we are able to match linguistic units at different inter levels of the structure (i.e. define the correspondence between substrings in the sentence, nodes in the tree, subtrees in the tree and sub-correspondences in the SSTC). This flexibility makes synchronous SSTC very well suited for the construction of a Bilingual Knowledge Bank we need for the English-Malay MT application."	#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .	"['However, what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""The proposed S-SSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages' structures."", 'S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications.', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']"	0
CC1348	W02-1601	A synchronization structure of SSTC and its applications in machine translation	chartbased transfer rule application in machine translation	"['A Meyers', 'M Kosaka', 'R Grishman']"		"Transfer-based Machine Translation systems require a procedure for choosing the set of transfer rules for generating a target language translation from a given source language sentence. In an MT system with many competing transfer rules, choosing the best set of transfer rules for translation may involve the evaluation of an explosive number of competing sets. We propose a solution to this problem based on current best-first chart parsing algorithms."	"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) ."	"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"	0
CC1349	W02-1601	A synchronization structure of SSTC and its applications in machine translation	nonisomorphic synchronous tags	"['K Harbusch', 'P Poller']"		"Synchronous tree{adjoining grammars (S{TAGs) combine two standard tree{adjoining grammars (TAGs), e.g., for language transduction in Machine Translation (MT). Recent advances show that the restriction to isomorphic derivation trees (IS{TAGs) ensures eecient transduction because only tree{adjoining languages can be formed in each component. As a result IS{TAGs only allow for  triv-ial"" transfer rules, due to the fact that only isomorphic derivations can be synchronized. This means that only very similar constructions in the two languages can be translated into each other. To overcome these limitations and provide a way of realizing more complex translation phenomena, this paper introduces a new formalism, the dynamic link synchronous tree{adjoining grammars or DLS{TAGs. This formalism allows for the synchronization of non{isomorphic derivation trees by introducing the new concept of dynamic links. DLS{TAGs are more powerful than IS-TAGs. More precisely speaking, DLS{TAGs allow for the formulation of a non{tree{adjoining language in one of the two components. This makes the translation problem more diicult but not untractable as outlined in this paper. However , there remain non{isomorphic translation phenomena which cannot be handled by DLS-TAGs as we also show in this paper."	"It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) ."	"['The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'In this case only TAL can be formed in each component.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']"	0
CC1350	W02-1601	A synchronization structure of SSTC and its applications in machine translation	what is a natural language and how to describe it meaningtext approaches in contrast with generative approaches	['S Kahane']		"The paper expounds the general conceptions of the Meaning- Text theory about what a natural language is and how it must be de- scribed. In a second part, a formalization of these conceptions - the transductive grammars - is proposed and compared with generative ap- proaches."	"From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) ."	"['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.']"	0
CC1351	W02-1601	A synchronization structure of SSTC and its applications in machine translation	towards memorybased translation	"['S Sato', 'M Nagao']"			"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) ."	"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"	0
CC1352	W02-1601	A synchronization structure of SSTC and its applications in machine translation	restricting the weak generative capacity of synchronous tree adjoining grammar	['S Shieber']			"In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) ."	"['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .', 'Shieber (1994) cases).', 'Due to lack of space we will only brief on some of these non-standard cases without going into the details.']"	0
CC1353	W02-1601	A synchronization structure of SSTC and its applications in machine translation	structural matching of parallel texts	"['Y Matsumoto', 'H Ishimoto', 'T Utsuro']"		"This paper describes a nethod for finding structural matching between parallel sentences of two lauguages, (such as Japanese and English). Par- allel sentences are analyzed based on unification grammars, and structural matching is performed by making use of a similarity measure of word pairs in the two languages. Syntactic ambiguities are resolved simultaneously in the matching process. The results serve as a useful source for extracting linguistic and lexical knowledge"	"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) ."	"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"	0
CC1354	W02-1601	A synchronization structure of SSTC and its applications in machine translation	synchronous models of language	"['O Rambow', 'G Satta']"		"In synchronous rewriting, the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings. We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems. We also prove some interesting formal/computational properties of our system.Comment: 8 pages uuencoded gzipped ps fil"	"Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic ."	"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']"	0
CC1355	W02-1601	A synchronization structure of SSTC and its applications in machine translation	pilot implementation of a bilingual knowledge bank	"['V Sadler', 'R Vendelmans']"		"A Bilingual Knowledge Bank is a syntactically and referentially structured pair of corpora, one being a  translation of the other, in which translation units are  cross-codexl between the corpora. A pilot implementation  is described for a corpus of some 20,000 words  each in English, French and Esperanto which has been cross-coded between English and Esperanto and &apos;between Esperanto and French. The aim is to develop a corpus-based general-purpose knowledge sontee for applicatious in machine translation and computer-  aided translation"	"For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus"	"[') are governed by the following constraints:  .', 'This means allowing one-to-one, one-to-many and many-to-many, but the mappings do not overlap.', 'Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]"	0
CC1356	W02-1601	A synchronization structure of SSTC and its applications in machine translation	a bestfirst algorithm for automatic extraction of transfer mappings from bilingual corpora	"['A Menezes', 'S Richardson']"		"Translation systems that automatically extract transfer mappings (rules or examples) from bilingual corpora have been hampered by the difficulty of achieving accurate alignment and acquiring high quality mappings. We describe an algorithm that uses a best-first strategy and a small alignment grammar to significantly improve the quality of the transfer mappings extracted. For each mapping, frequencies are computed and sufficient context is retained to distinguish competing mappings during translation. Variants of the algorithm are run against a corpus containing 200K sentence pairs and evaluated based on the quality of resulting translations."	"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) ."	"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e.', 'Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"	0
CC1357	W02-1601	A synchronization structure of SSTC and its applications in machine translation	synchronous models of language	"['O Rambow', 'G Satta']"	introduction	"In synchronous rewriting, the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings. We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems. We also prove some interesting formal/computational properties of our system.Comment: 8 pages uuencoded gzipped ps fil"	Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .	"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']"	0
CC1358	W02-1601	A synchronization structure of SSTC and its applications in machine translation	representation trees and stringtree correspondences	"['C Boitet', 'Y Zaharin']"			"For more details on the proprieties of SSTC , see #AUTHOR_TAG ."	"['The case depicted in Figure 2, describes how the SSTC structure treats some non-standard linguistic phenomena.', 'The particle ""up"" is featurised into the verb ""pick"" and in discontinuous manner (e.g.', '""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']"	0
CC1359	W02-1601	A synchronization structure of SSTC and its applications in machine translation	natural language analysis in machine translation mt based on the stringtree correspondence grammar stcg	['E K Tang']		"The formalism is argued to be a totally declarative grammar formalism that can associate, to strings in a language, arbitrary tree structures as desired by the grammar writer to be the linguistic representation structures of the strings. More importantly is the facility to specify the correspondence between the string and the associated tree in a very natural manner. These features are very much desired in grammar writing, in particular for the treatment of certain linguistic phenomena which are 'non-standard', namely featurisation, lexicalisation and crossed dependencies [2,3]. Furthermore, a grammar written in this way naturally inherits the desired property of bi-directionality (in fact non-directionality [4]) such that the same grammar can be interpreted for both analysis and generation."	A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .	"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"	5
CC1360	W02-1601	A synchronization structure of SSTC and its applications in machine translation	achieving commercialquality translation with examplebased methods	"['S Richardson', 'W Dolan', 'A Menezes', 'J Pinkham']"			"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) ."	"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']"	0
CC1361	W02-1601	A synchronization structure of SSTC and its applications in machine translation	restricting the weak generative capacity of synchronous tree adjoining grammar	['S Shieber']			"It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) ."	"['The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'In this case only TAL can be formed in each component.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']"	0
CC1362	W02-1601	A synchronization structure of SSTC and its applications in machine translation	examplebased machine translation	['S Sato']		"Example-based machine translation (EBMT) systems, so far, rely on heuristic measures in re-trieving translation examples. Such a heuristic measure costs time to adjust, and might make its algorithm unclear. This paper presents a probabilistic model for EBMT. Under the pro-posed model, the system searches the transla-tion example combination which has the high-est probability. The proposed model clearly for-malizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental re-sults demonstrate that the proposed model has a slightly better translation quality than state-of-the-art EBMT systems."	"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) ."	"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"	0
CC1363	W02-1601	A synchronization structure of SSTC and its applications in machine translation	representation trees and stringtree correspondences	"['C Boitet', 'Y Zaharin']"			"A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG ."	"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"	5
CC1364	W03-0806	Blueprint for a high performance NLP infrastructure	english gigaword corpus catalogue number ldc2003t05	['Linguistic Data Consortium']	introduction		"However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) ."	"['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .', 'Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']"	0
CC1365	W03-0806	Blueprint for a high performance NLP infrastructure	dialogue interaction with the darpa communicator infrastructure the development of useful software	"['Samuel Bayer', 'Christine Doran', 'Bryan George']"		"To support engaging human users in robust, mixed-initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems, the DARPA Communicator program [1] is funding the development of a distributed message-passing infrastructure for dialogue systems which all Communicator participants are using. In this presentation, we describe the features of and requirements for a genuinely useful software infrastructure for this purpose."	"There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) ."	"['The final interface we intend to implement is a collection of web services for NLP.', 'A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']"	0
CC1366	W03-0806	Blueprint for a high performance NLP infrastructure	combining labeled and unlabeled data with cotraining	"['Avrim Blum', 'Tom Mitchell']"			"Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) ."	"['As discussed earlier, there are two main requirements of the system that are covered by ""high performance"": speed and state of the art accuracy.', 'Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.']"	0
CC1367	W03-0806	Blueprint for a high performance NLP infrastructure	gate – a general architecture for text engineering	"['Hamish Cunningham', 'Yorick Wilks', 'Robert J Gaizauskas']"	experiments	"This paper presents the design, implementation and evaluation of GATE, a General Architecture for Text Engineering.GATE lies at the intersection of human language computation and software engineering, and constitutes aninfrastructural system supporting research and development of languageprocessing software."	"Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI ."	"['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']"	0
CC1368	W03-0806	Blueprint for a high performance NLP infrastructure	a distributed architecture for robust automatic speech recognition	"['Kadri Hacioglu', 'Bryan Pellom']"		"In this paper, we attempt to decompose a state-of-the-art speech recognition system into its components and define an infrastructure that allows a flexible, efficient and effective interaction among the components. Motivated by the success of DARPA Communicator program, we select the open source Galaxy architecture as our development test bed. It consists of a hub that allows communication among servers connected to it by message passing and supports the plug-and-play paradigm. In addition to message passing it supports high bandwidth data (binary or audio) transfer between servers via a brokering scheme. For several reasons, we believe that it is the right time to start developing a distributed framework for speech recognition along with data and protocol standards supporting interoperability. We present our work towards that goal using the Colorado University (CU) Sonic recognizer. We divide Sonic into a number of components and structure it around the Hub. We describe the system in some detail and report on its present status with some possibilities for future development. 1"	"There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) ."	"['The final interface we intend to implement is a collection of web services for NLP.', 'A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']"	0
CC1369	W03-0806	Blueprint for a high performance NLP infrastructure	a maximum entropy partofspeech tagger	['Adwait Ratnaparkhi']	conclusion		"For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component ."	"['The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']"	3
CC1370	W03-0806	Blueprint for a high performance NLP infrastructure	bootstrapping postaggers using unlabelled data	"['Stephen Clark', 'James R Curran', 'Miles Osborne']"	experiments	"This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost."	"The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) ."	"['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']"	4
CC1371	W03-0806	Blueprint for a high performance NLP infrastructure	scaling context space	"['James R Curran', 'Marc Moens']"	experiments	"This paper proposes a computationally feasible method for measuring the context-sensitive semantic distance between words. The distance is computed by adaptive scaling of a semantic space. In the semantic space, each word in the vocabulary V is represented by a multidimensional vector which is extracted from an English dictionary through principal component analysis. Given a word set C which specifies a context, each dimension of the semantic space is scaled up or down according to the distribution of C in the semantic space. In the space thus transformed, the distance between words in V  becomes dependent on the context C. An evaluation through a word prediction task shows that the proposed measurement successfully extracts the context of a text. 1 Introduction  Semantic distance (or similarity) between words is one of the basic measurements used in many fields of natural language processing, information retrieval, etc. Word distance provides bottom-up information for text understandi.."	"The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) ."	"['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger.', 'These tools use a highly optimised G I S imple- mentation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster train- ing times when we move to conjugate gradient methods.']"	4
CC1372	W03-0806	Blueprint for a high performance NLP infrastructure	maximum entropy models for natural language ambiguity resolution	['Adwait Ratnaparkhi']	experiments	"This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy.  We discuss the problems of sentence boundary detection, part-of-speech tagging, prepositional phrase attachment, natural language parsing, and text categorization under the maximum entropy framework. In practice, we have found that maximum entropy models offer the following advantages:  State-of-the-art accuracy. The probability models for all of the tasks discussed perform at or near state-of-the-art accuracies, or outperform competing learning algorithms when trained and tested under similar conditions. Methods which outperform those presented here require much more supervision in the form of additional human involvement or additional supporting resources.  Knowledge-poor features. The facts used to model the data, or features, are linguistically very simple, or ""knowledge-poor"", but yet succeed in approximating complex linguistic relationships.  Reusable software technology. The mathematics of the maximum entropy framework are essentially independent of any particular task, and a single software implementation can be used for all of the probability models in this thesis.  The experiments in this thesis suggest that experimenters can obtain state-of-the-art accuracies on a wide range of natural language tasks, with little task-specific effort, by using maximum entropy probability models."	"An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) ."	"['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']"	0
CC1373	W03-0806	Blueprint for a high performance NLP infrastructure	a rational design for a weighted finitestate transducer library	"['Mehryar Mohri', 'Fernando C N Pereira', 'Michael Riley']"	experiments		"Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) ."	"['A number of stand-alone tools have also been developed.', 'For example, the suite of LT tools (Mikheev et al., 1999;Grover et al., 2000) perform tokenization, tagging and chunking on XML marked-up text directly.', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended.']"	0
CC1374	W03-0806	Blueprint for a high performance NLP infrastructure	scaling context space	"['James R Curran', 'Marc Moens']"	introduction	"This paper proposes a computationally feasible method for measuring the context-sensitive semantic distance between words. The distance is computed by adaptive scaling of a semantic space. In the semantic space, each word in the vocabulary V is represented by a multidimensional vector which is extracted from an English dictionary through principal component analysis. Given a word set C which specifies a context, each dimension of the semantic space is scaled up or down according to the distribution of C in the semantic space. In the space thus transformed, the distance between words in V  becomes dependent on the context C. An evaluation through a word prediction task shows that the proposed measurement successfully extracts the context of a text. 1 Introduction  Semantic distance (or similarity) between words is one of the basic measurements used in many fields of natural language processing, information retrieval, etc. Word distance provides bottom-up information for text understandi.."	"Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data ."	"['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']"	0
CC1375	W03-0806	Blueprint for a high performance NLP infrastructure	building a large annotated corpus of english the penn treebank computational linguistics	"['Mitchell Marcus', 'Beatrice Santorini', 'Mary Marcinkiewicz']"	introduction		"For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers ."	"['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']"	0
CC1376	W03-0806	Blueprint for a high performance NLP infrastructure	nltk the natural language toolkit	"['Edward Loper', 'Steven Bird']"			It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .	"['Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language.', 'Python has a number of advantages over other options, such as Java and Perl.', 'Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']"	2
CC1377	W03-0806	Blueprint for a high performance NLP infrastructure	mixedinitiative development of language processing systems	"['David Day', 'John Aberdeen', 'Lynette Hirschman', 'Robyn Kozierok', 'Patricia Robinson', 'Marc Vilain']"	experiments	"Historically, tailoring language processing systems to specific domains and languages for which they were not originally built has required a great deal of effort. Recent advances in corpus-based manual and automatic training methods have shown promise in reducing the time and cost of this porting process. These developments have focused even greater attention on the bottleneck of acquiring reliable, manually tagged training data. This paper describes a new set of integrated tools, collectively called the Alembic Workbench, that uses a mixed-initiative approach to ""bootstrapping"" the manual tagging process, with the goal of reducing the overhead associated with corpus development. Initial empirical studies using the Alembic Workbench to annotate ""named entities"" demonstrates that this approach can approximately double the production rate. As an added benefit, the combined efforts of machine and user produce domain specific annotation rules that can be used to annotate similar texts automatically through the Alembic-NLP system. The ultimate goal of this project is to enable end users to generate a practical domain-specific information extraction system within a single session."	"Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI ."	"['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']"	0
CC1378	W03-0806	Blueprint for a high performance NLP infrastructure	developing language processing components with gate	"['Hamish Cunningham', 'Diana Maynard', 'C Ursu K Bontcheva', 'V Tablan', 'M Dimitrov']"	experiments	"Fluid leakage through soil in a region thereof is controlled by sequentially passing over the region to dig a plurality of parallel, laterally displaced grooves in the surface. Soil dug from each groove is temporarily stored, and a strip of sheet material is laid over a groove as it is created during each pass, the width of the strip being greater than the width of the groove. Thereafter, the temporarily stored soil is deposited on the strip such that it is covered with soil except along one edge, the other edge of the strip overlying the uncovered edge of an adjacent strip laid down during a previous pass over the region. As a consequence, a first layer of overlapping strips of sheet material covered with soil is installed over the region."	"For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) ."	"['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']"	0
CC1379	W03-0806	Blueprint for a high performance NLP infrastructure	generative programming methods tools and applications	"['Krzysztof Czarnecki', 'Ulrich W Eisenecker']"	introduction	"1. What Is This Book About? From Handcrafting to Automated Assembly Lines. Generative Programming. Benefits and Applicability. I. ANALYSIS AND DESIGN METHODS AND TECHNIQUES. 2. Domain Engineering. Why Is This Chapter Worth Reading? What Is Domain Engineering? Domain Analysis. Domain Design and Domain Implementation. Application Engineering. Product-Line Practices. Key Domain Engineering Concepts. Domain. Domain Scope and Scoping. Relationships between Domains. Features and Feature Models. Method Tailoring and Specialization. Survey of Domain Analysis and Domain Engineering Methods. Feature-Oriented Domain Analysis (FODA). Organization Domain Modeling (ODM). Draco. Capture. Domain Analysis and Reuse Environment (DARE). Domain-Specific Software Architecture (DSSA) Approach. Algebraic Approach. Other Approaches. Domain Engineering and Related Approaches. Historical Notes. Summary. 3. Domain Engineering and Object-Oriented Analysis and Design. Why Is This Chapter Worth Reading? OO Technology and Reuse. Solution Space. Problem Space. Relationship between Domain Engineering and Object-Oriented Analysis and Design (OOA/D) Methods. Aspects of Integrating Domain Engineering and OOA/D Methods. Horizontal versus Vertical Methods. Selected Methods. Rational Unified Process. 00ram. Reuse-Driven Software Engineering Business (RSEB). FeatuRSEB. Domain Engineering Method for Reusable Algorithmic Libraries (DEMRAL). 4. Feature Modeling. Why Is This Chapter Worth Reading? Features Revisited. Feature Modeling. Feature Models. Feature Diagrams. Other Infon-Nation Associated with Feature Diagrams in a Feature Model. Assigning Priorities to Variable Features. Availability Sites, Binding Sites, and Binding Modes. Relationship between Feature Diagrams and Other Modeling Notations and Implementation Techniques. Single Inheritance. Multiple Inheritance. Parameterized Inheritance. Static Parameterization. Dynamic Parameterization. Implementing Constraints. Tool Support for Feature Models. Frequently Asked Questions about Feature Diagrams. Feature Modeling Process. How to Find Features. Role of Variability in Modeling. 5. The Process of Generative Programming. Why Is This Chapter Worth Reading? Generative Domain Models. Main Development Steps in Generative Programming. Adapting Domain Engineering for Generative Programming. Domain-Specific Languages. DEMRAL: Example of a Domain Engineering Method for Generative Programming. Outline of DEMRAL. Domain Analysis. Domain Definition. Domain Modeling. Domain Design. Scope Domain Model for Implementation. Identify Packages. Develop Target Architectures and Identify the Implementation Components. Identify User DSLs. Identify Interactions between DSLs. Specify DSLs and Their Translation. Configuration DSLs. Expression DSLs. Domain Implementation. II. IMPLEMENTATION TECHNOLOGIES. 6. Generic Programming. Why Is This Chapter Worth Reading? What Is Generic Programming? Generic versus Generative Programming. Generic Parameters. Parametric versus Subtype Polymorphism. Genericity in Java. Bounded versus Unbounded Polymorphism. A Fresh Look at Polymorphism. Parameterized Components. Parameterized Programming. Types, Interfaces, and Specifications. Adapters. Vertical and Horizontal Parameters. Module Expressions. C++ Standard Template Library. Iterators. Freestanding Functions versus Member Functions. Generic Methodology. Historical Notes. 7. Component-Oriented Template-Based C++ Programming Techniques. Why Is This Chapter Worth Reading? Types of System Configuration. C++ Support for Dynamic Configuration. C++ Support for Static Configuration. Static Typing. Static Binding. Inlining. Templates. Parameterized Inheritance. typedefs. Member Types. Nested Classes. Prohibiting Certain Template Instantiations. Static versus Dynamic Parameterization. Wrappers Based on Parameterized Inheritance. Template Method Based on Parameterized Inheritance. Parameterizing Binding Mode. Consistent Parameterization of Multiple Components. Static Interactions between Components. Components with Influence. Components under Influence. Structured Configurations. Recursive Components. Intelligent Configuration. 8. Aspect-Oriented Decomposition and Composition. Why Is This Chapter Worth Reading? What Is Aspect-Oriented Programming? Aspect-Oriented Decomposition Approaches. Subject-Oriented Programming. Composition Filters. Demeter / Adaptive Programming. Aspect-Oriented Decomposition and Domain Engineering. How Aspects Arise. Composition Mechanisms. Requirements on Composition Mechanisms. Example: Synchronizing a Bounded Buffer. ""Tangled"" Synchronized Stack. Separating Synchronization Using Design Patterns. Separating Synchronization Using SOP. Some Problems with Design Patterns and Some Solutions. Implementing Noninvasive, Dynamic Composition in Smalltalk. Kinds of Crosscutting. How to Express Aspects in Programming Languages. Separating Synchronization Using AspectJ Cool. Implementing Dynamic Cool in Smalltalk. Implementation Technologies for Aspect-Oriented Programming. Technologies for Implementing Aspect-Specific Abstractions. Technologies for Implementing Weaving. AOP and Specialized Language Extensions. AOP and Active Libraries. Final Remarks. 9. Generators. Why Is This Chapter Worth Reading? What Are Generators? Transformational Model of Software Development. Technologies for Building Generators. Compositional versus Transformational Generators. Kinds of Transformations. Compiler Transformations. Source-to-Source Transformations. Transformation Systems. Scheduling Transformations. Existing Transformation Systems and Their Applications. Selected Approaches to Generation. Draco. GenVoca. Approaches Based on Algebraic Specifications. 10. Static Metaprogramming in C++. Why Is This Chapter Worth Reading? What Is Metaprogramming? A Quick Tour of Metaprogramming. Static Metaprogramming. C++ as a Two-Level Language. Functional Flavor of the Static Level. Class Templates as Functions. Integers and Types as Data. Symbolic Names Instead of Variables. Constant Initialization and typedef-Statements Instead of Assignment. Template Recursion Instead of Loops. Conditional Operator and Template Specialization as Conditional Constructs. Template Metaprogramming. Template Metafunctions. Metafinctions as Arguments and Return Values of Other Metafinctions. Representing Metainformation. Member Traits. Traits Classes. Traits Templates. Example: Using Template Metafunctions and Traits Templates to Implement Type Promotions. Compile-Time Lists and Trees as Nested Templates. Compile-Time Control Structures. Explicit Selection Constructs. Template Recursion as a Looping Construct. Explicit Looping Constructs. Code Generation. Simple Code Selection. Composing Templates. Generators Based on Expression Templates. Recursive Code Expansion. Explicit Loops for Generating Code. Example: Using Static Execute Loops to Test Metafunctions. Partial Evaluation in C++. Workarounds for Partial Template Specialization. Problems of Template Metaprogramming. Historical Notes. 11. Intentional Programming. Why Is This Chapter Worth Reading? What Is Intentional Programming? Technology behind IP. System Architecture. Representing Programs in IP: The Source Graph. Source Graph + Methods = Active Source. Working with the IP Programming Environment. Editing. Further Capabilities of the IP Editor. Extending the IP System with New Intentions. Advanced Topics. Questions, Methods, and a Frameworklike Organization. Source-Pattem-Based Polymorphism. Methods as Visitors. Asking Questions Synchronously and Asynchronously. Reduction. The Philosophy behind IP. Why Do We Need Extendible Programming Environments? or What Is the Problem with Fixed Programming Languages? Moving Focus from Fixed Languages to Language Features and the Emergence of an Intention Market. Intentional Programming and Component-Based Development. Frequently Asked Questions. Summary. III. APPLICATION EXAMPLES. 12. List Container. Why Is This Chapter Worth Reading? Overview. Domain Analysis. Domain Design. Implementation Components. Manual Assembly. Specifying Lists. The Generator. Extensions. 13. Bank Account. Why Is This Chapter Worth Reading? The Successful Programming Shop. Design Pattems, Frameworks, and Components. Domain Engineering and Generative Programming. Feature Modeling. Architecture Design. Implementation Components. Configurable Class Hierarchies. Designing a Domain-Specific Language. Bank Account Generator. Testing Generators and Their Products. 14. Generative Matrix Computation Library (GMCL). Why Is This Chapter Worth Reading? Why Matrix Computations? Domain Analysis. Domain Definition. Domain Modeling. Domain Design and Implementation. Matrix Type Generation. Generating Code for Matrix Expressions. Implementing the Matrix Component in IP. APPENDICES. Appendix A: Conceptual Modeling. What Are Concepts? Theories of Concepts. Basic Terminology. The Classical View. The Probabilistic View. The Exemplar View. Summary of the Three Views. Important Issues Concerning Concepts. Stability of Concepts. Concept Core. Informational Contents of Features. Feature Composition and Relationships between Features. Quality of Features. Abstraction and Generalization. Conceptual Modeling, Object-Orientation, and Software Reuse. Appendix B: Instance-Specific Extension Protocol for Smalltalk. Appendix C: Protocol for Attaching Listener Objects in Smalltalk. Appendix D: Glossary of Matrix Computation Terms. Appendix E: Metafunction for Evaluating Dependency Tables. Glossary of Generative Programming Terms. References. Index. 020130977T04062001"	Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .	"['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']"	0
CC1380	W03-0806	Blueprint for a high performance NLP infrastructure	the american national corpus more than the web can provide	"['Nancy Ide', 'Randi Reppen', 'Keith Suderman']"	introduction	"The American National Corpus (ANC) project is developing a corpus comparable to the British National Corpus (BNC), covering American English. Recent interest in the web as a source of corpus materials has caused some in the language processing community to suggest that the development of a corpus of American English is unnecessary. However, we argue that far from being rendered superfluous by the availability of web materials, the ANC is likely to provide a resource for developing web acquisition techniques to support tasks such as genre and language detection and automatic annotation. This paper presents a comparison of the ANC in terms of both content and format with a test corpus compiled from web data, and a discussion of points of intersection and divergence."	"For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers ."	"['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']"	0
CC1381	W03-0806	Blueprint for a high performance NLP infrastructure	lt ttt  a flexible tokenisation tool	"['Claire Grover', 'Colin Matheson', 'Andrei Mikheev', 'Marc Moens']"	experiments		"For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly ."	"['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'However, the source code for these tools is not freely available, so they cannot be extended.']"	0
CC1382	W03-0806	Blueprint for a high performance NLP infrastructure	modern c design generic programming and design patterns applied c indepth series	['Andrei Alexandrescu']	experiments	"Modern C++ Designis an important book. Fundamentally, it demonstrates 'generic patterns' or 'pattern templates' as a powerful new way of creating extensible designs in C++i??a new way to combine templates and patterns that you may never have dreamt was possible, but is. If your work involves C++ design and coding, you should read this book. Highly recommended. i??Herb SutterWhat's left to say about C++ that hasn't already been said? Plenty, it turns out. i??From the Foreword by John VlissidesIn Modern C++ Design, Andrei Alexandrescu opens new vistas for C++ programmers. Displaying extraordinary creativity and programming virtuosity, Alexandrescu offers a cutting-edge approach to design that unites design patterns, generic programming, and C++, enabling programmers to achieve expressive, flexible, and highly reusable code.This book introduces the concept of generic componentsi??reusable design templates that produce boilerplate code for compiler consumptioni??all within C++. Generic components enable an easier and more seamless transition from design to application code, generate code that better expresses the original design intention, and support the reuse of design structures with minimal recoding.The author describes the specific C++ techniques and features that are used in building generic components and goes on to implement industrial strength generic components for real-world applications. Recurring issues that C++ developers face in their day-to-day activity are discussed in depth and implemented in a generic way. These include: Policy-based design for flexibility Partial template specialization Typelistsi??powerful type manipulation structures Patterns such as Visitor, Singleton, Command, and Factories Multi-method enginesFor each generic component, the book presents the fundamental problems and design options, and finally implements a generic solution.In addition, an accompanying Web site, http://www.awl.com/cseng/titles/0-201-70431-5, makes the code implementations available for the generic components in the book and provides a free, downloadable C++ library, called Loki, created by the author. Loki provides out-of-the-box functionality for virtually any C++ project.Get a value-added service! Try out all the examples from this book at www.codesaw.com. CodeSaw is a free online learning tool that allows you to experiment with live code from your book right in your browser. 0201704315B11102003"	"To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes ."	"['The infrastructure will be implemented in C/C++.', 'Templates will be used heavily to provide generality without significantly impacting on efficiency.', 'However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']"	5
CC1383	W03-0806	Blueprint for a high performance NLP infrastructure	scaling to very very large corpora for natural language disambiguation	"['Michele Banko', 'Eric Brill']"	introduction	"The amount of readily available online  text has reached hundreds of  billions of words and continues to  grow. Yet for most core natural  language tasks, algorithms continue  to be optimized, tested and compared  after training on corpora consisting  of only one million words or less. In  this paper, we evaluate the  performance of different learning  methods on a prototypical natural  language disambiguation task,  confusion set disambiguation, when  trained on orders of magnitude more  labeled data than has previously been  used. We are fortunate that for this  particular application, correctly  labeled training data is free. Since  this will often not be the case, we  examine methods for effectively  exploiting very large corpora when  labeled data comes at a cost"	"Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data ."	"['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']"	0
CC1384	W03-0806	Blueprint for a high performance NLP infrastructure	xml tools and architecture for named entity recognition	"['Andrei Mikheev', 'Claire Grover', 'Marc Moens']"	experiments	This paper reports on the development of a Named Entity recognition system developed fully within the xml paradigm	"For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly ."	"['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'However, the source code for these tools is not freely available, so they cannot be extended.']"	0
CC1385	W03-0806	Blueprint for a high performance NLP infrastructure	a corpusbased appreach to language learning	['Eric Brill']			"Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method ."	"['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']"	4
CC1386	W03-0806	Blueprint for a high performance NLP infrastructure	transformationbased learning in the fast lane	"['Grace Ngai', 'Radu Florian']"	experiments		"Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) ."	"['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']"	0
CC1387	W03-0806	Blueprint for a high performance NLP infrastructure	bootstrapping postaggers using unlabelled data	"['Stephen Clark', 'James R Curran', 'Miles Osborne']"		"This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost."	"The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) ."	"['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example of using the Python tagger interface is shown in Figure 1.']"	0
CC1388	W03-0806	Blueprint for a high performance NLP infrastructure	a gaussian prior for smoothing maximum entropy models	"['Stanley Chen', 'Ronald Rosenfeld']"	experiments	"Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance."	These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .	"['The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .', 'We expect even faster training times when we move to conjugate gradient methods.']"	5
CC1389	W03-0806	Blueprint for a high performance NLP infrastructure	deterministic partofspeech tagging with finitestate transducers	"['Emmanuel Roche', 'Yves Schabes']"	experiments		"Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) ."	"['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']"	0
CC1390	W03-0806	Blueprint for a high performance NLP infrastructure	nltk the natural language toolkit	"['Edward Loper', 'Steven Bird']"	experiments		"Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) ."	"['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple.']"	0
CC1391	W03-0806	Blueprint for a high performance NLP infrastructure	investigating gis and smoothing for maximum entropy taggers	"['James R Curran', 'Stephen Clark']"	experiments	"This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar."	"The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) ."	"['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']"	4
CC1392	W03-0806	Blueprint for a high performance NLP infrastructure	a comparison of algorithms for maximum entropy parameter estimation	['Robert Malouf']	experiments	"Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Sur-prisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices."	"An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) ."	"['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']"	0
CC1393	W03-0806	Blueprint for a high performance NLP infrastructure	investigating gis and smoothing for maximum entropy taggers	"['James R Curran', 'Stephen Clark']"		"This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar."	"The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) ."	"['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']"	0
CC1394	W03-0806	Blueprint for a high performance NLP infrastructure	tnt  a statistical partofspeech tagger	['Thorsten Brants']	experiments		"The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second ."	"['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']"	0
CC1395	W03-0806	Blueprint for a high performance NLP infrastructure	software architecture for language engineering	['Hamish Cunningham']	experiments	"Every building, and every computer program, has an architecture: structural and organisational principles that underpin its design and construction. The garden shed  once built by one of the authors had an ad hoc architecture, extracted (somewhat painfully) from the imagination during a slow and non-deterministic process that, luckily, resulted in a structure which keeps the rain on the outside and the mower on the inside (at least for the time being). As well as being ad hoc (i.e. not informed by analysis of similar practice or relevant science or engineering) this architecture is implicit: no explicit design was made, and no records or documentation kept of the construction process.  The pyramid in the courtyard of the Louvre, by contrast, was constructed in a process involving explicit design performed by qualified engineers with a wealth of theoretical and practical knowledge of the properties of materials, the relative merits and strengths of different construction techniques, et cetera.  So it is with software: sometimes it is thrown together by  enthusiastic amateurs; sometimes it is architected, built to last, and intended to be 'not something you finish, but something you start' (to paraphrase Brand (1994). A number of researchers argued in the early and middle 1990s that the field of computational infrastructure or architecture for human language computation merited an increase in attention. The reasoning was that the increasingly large-scale and technologically significant nature of language processing science was placing increasing burdens of an engineering nature on research and development workers seeking robust and practical methods (as was the increasingly collaborative nature of research in this field, which puts a large premium on software integration and interoperation). Over the intervening period a number of significant systems and practices have been developed in what we may call Software Architecture for Language Engineering (SALE).  This special issue represented an opportunity for practitioners in this area to report their work in a coordinated setting, and to present a snapshot of the state-ofthe-art in infrastructural work, which may indicate where further development and further take-up of these systems can be of benefit"	GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .	"['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']"	0
CC1396	W04-0910	Paraphrastic grammars	an open source grammar development environment and broadcoverage english grammar using hpsg	"['Ann Copestake', 'Dan Flickinger']"		"The LinGO (Linguistic Grammars Online) project's English Resource Grammar and the LKB grammar development environment are language resources which are freely available for download for any purpose, including commercial use (see http://lingo.stanford.edu). Executable programs and source code are both included. In this paper, we give an outline of the LinGO English grammar and LKB system, and discuss the ways in which they are currently being used. The grammar and processing system can be used independently or combined to give a central component which can be exploited in a variety of ways. Our intention in writing this paper is to encourage more people to use the technology, which supports collaborative development on many levels."	"Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics ."	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"	0
CC1397	W04-0910	Paraphrastic grammars	modlisation et traitement informatique de la synonymi linguisticae investigationes	['S Ploux']			Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .	"['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998).', 'techniques.']"	0
CC1398	W04-0910	Paraphrastic grammars	automatic paraphrase acquisition from news articles	"['Y Shinyanma', 'S Sekine', 'K Sudo', 'R Grishman']"	introduction	"Paraphrases play an important role in the variety and complexity of natural language documents. However, they add to the difficulty of natural language processing. Here we describe a procedure for ob-taining paraphrases from news articles. Articles derived from dif-ferent newspapers can contain paraphrases if they report the same event on the same day. We exploit this feature by using Named Entity recognition. Our approach is based on the assumption that Named Entities are preserved across paraphrases. We applied our method to articles of two domains and obtained notable examples. Although this is our initial attempt at automatically extracting para-phrases from a corpus, the results are promising. 1"	"Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source ."	"['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']"	0
CC1399	W04-0910	Paraphrastic grammars	identifying lexical paraphrases from a single corpus a case study for verbs	"['O Glickman', 'I Dagan']"	introduction		And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .	"['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']"	0
CC1400	W04-0910	Paraphrastic grammars	un outil multilingue de generation de ltag  application au francais et a l’italien	['M H Candito']			"To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) ."	"['Modeling intercategorial synonymic links.', ""A first investigation of Anne Abeillé's TAG for French suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward."", 'For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeillé FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.', 'Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .', 'This metagrammar allows us to factorise both syntactic and semantic information.', 'Syntactic information is factorised in the usual way.', 'For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'But additionnally there will be semantic classes such as, ""binary predicate of semantic type X"" which will be associated with the relevant syntactic classes for instance, NOVN1 (the class of transitive verbs with nominal arguments), BINARY NPRED (the class of binary predicative nouns), NOVSUPNN1 , the class of support verb constructions taking two nominal arguments.', 'By further associating semantic units (e.g., ""cost"") with the appropriate semantic classes (e.g., ""binary predicate of semantic type X""), we can in this way capture both intra and intercategorial paraphrasing links in a general way.']"	3
CC1401	W04-0910	Paraphrastic grammars	learning to paraphrase an unsupervised approahc using mutliplesequence alignment	"['R Barzilay', 'L Lee']"	introduction		"Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source ."	"['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']"	0
CC1402	W04-0910	Paraphrastic grammars	framenet theory and practice	"['C Fillmore C Johnson', 'M Petruckand C Baker', 'M Ellsworth', 'J Ruppenhofer']"			"To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) ."	"['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.', 'In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'Finally each frame is associated with a set of target words, the words that evoke that frame.']"	5
CC1403	W04-0910	Paraphrastic grammars	discovery of inference rules for question answering natural language engineering	"['Dekang Lin', 'Patrick Pantel']"	introduction		"For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning ."	"['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']"	0
CC1404	W04-0910	Paraphrastic grammars	automatic retrieval and clustering of similar words	['D Lin']		Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.	"For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) ."	"['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques.']"	3
CC1405	W04-0910	Paraphrastic grammars	towards evaluation of nlp systems	"['D Flickinger', 'J Nerbonne', 'I Sag', 'T Wasow']"			"For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) ."	"['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?)', 'and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'The construction and annotation of the paraphrases reflects the paraphrase typology.', 'In a first phase, we concentrate on simple, non-recursive predicate/argument structure.', 'Given such a structure, the construction and annotation of a test item proceeds as follows.']"	1
CC1406	W04-0910	Paraphrastic grammars	minimal recursion semantics an introduction	"['A Copestake', 'D Flickinger', 'I Sag', 'C Pollard']"		"Minimal recursion semantics (MRS) is a framework for computational semantics that is suitable for parsing and generation and that can be implemented in typed feature structure formalisms. We discuss why, in general, a semantic representation with minimal structure is desirable and illustrate how a descriptively adequate representation with a nonrecursive structure may be achieved. MRS enables a simple formulation of the grammatical constraints on lexical and phrasal semantics, including the principles of semantic composition. We have integrated MRS with a broad-coverage HPSG grammar."	"The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) ."	"['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']"	1
CC1407	W04-0910	Paraphrastic grammars	a procedure for quantitatively comparing the syntactic coverage of english grammars	"['A Black', 'S Abney', 'D Flickinger', 'C Gdaniec', 'R Grishman', 'P Harrison', 'D Hindel', 'R INgria', 'F Jelinek', 'F Klaavans', 'M Liberman', 'M Marcus', 'S Roukos', 'B Santorini', 'T Strzalkowski']"		"The problem of quantitatively comparing the performance of different broad-coverage grammars of English has to date resisted solution. Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even the simplest sentences. For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (Hewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boeing), Don Hindle (AT&T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in common only the following constituents, when each grammarian provides the single parse which he/she would ideally want his/her grammar to specify for three sample Brown Corpus sentences:The famed Yankee Clipper, now retired, has been assisting (as (a batting coach)).One of those capital-gains ventures, in fact, has saddled him (with Gore Court).He said this constituted a (very serious) misuse (of the (Criminal court) processes)."	"While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation ."	"['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998).']"	0
CC1408	W04-0910	Paraphrastic grammars	predicate logic unplugged	['J Bos']		"Die vorliegende Arbeit wurde im Rahmen des Verbundvorhabens Verbmobil vom Bundes-ministerium f ur Bildung, Wissenschaft, Forschung und Technologie (BMBF) unter dem FF orderkennzeichen 01 IV 101 R geff ordert. Die Verantwortung f ur den Inhalt dieser Arbeit liegt bei dem Autor."	"The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) ."	"['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']"	1
CC1409	W04-0910	Paraphrastic grammars	les constructions converses du francais	['G Gross']			"In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns ."	"['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']"	3
CC1410	W04-0910	Paraphrastic grammars	un outil multilingue de generation de ltag  application au francais et a l’italien	['M H Candito']	introduction		"As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction ."	"['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.', 'This specification is then compiled to automatically produce a specific grammar.']"	5
CC1411	W04-0910	Paraphrastic grammars	m´ethodes en syntase	['M Gross']			"For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions ."	"['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']"	3
CC1412	W04-0910	Paraphrastic grammars	towards systematic grammar profiling test suite technology 10 years after computer speech and language	"['S Oepen', 'D Flickinger']"			"For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) ."	"['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?)', 'and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'The construction and annotation of the paraphrases reflects the paraphrase typology.', 'In a first phase, we concentrate on simple, non-recursive predicate/argument structure.', 'Given such a structure, the construction and annotation of a test item proceeds as follows.']"	0
CC1413	W04-0910	Paraphrastic grammars	distributional clustering of english words	"['F Pereira', 'N Tishby', 'L Lee']"		"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."	"For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) ."	"['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques.']"	3
CC1414	W04-0910	Paraphrastic grammars	semantics and syntax in lexical functional grammar	['M Dalrymple']			"Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics ."	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"	0
CC1415	W04-0910	Paraphrastic grammars	semantic construction in ftag	"['C Gardent', 'L Kallmeyer']"			Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .	"['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.', 'The association between tree nodes and unification variables encodes the syntax/semantics interface -it specifies which node in the tree provides the value for which variable in the final semantic representation.']"	0
CC1416	W04-0910	Paraphrastic grammars	an algebra for semantic construction in constraintbased grammars	"['A Copestake', 'A Lascarides', 'D Flickinger']"			"Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics ."	"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"	0
CC1417	W04-0910	Paraphrastic grammars	an algebra for semantic construction in constraintbased grammars	"['A Copestake', 'A Lascarides', 'D Flickinger']"			"The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) ."	"['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']"	1
CC1418	W04-0910	Paraphrastic grammars	alternations and verb semantic classes for french analysis and class formation chapter 5	['P Saint-Dizier']		"In this paper, we show how alternations (called here contexts) can be defined for French, what their semantic properties are and how verb semantic classes can be constructed from syntactic criteria following (Levin 93). We then analyze the global quality of the results in terms of overlap with classes formed from the same verb-senses using WordNetlike classification criteria. Finally, we propose a method which combines these two approaches to form verb semantic classes better suited for natural language processing."	"For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs ."	"['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']"	0
CC1419	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	modern information retrieval	"['R B Yates', 'B R Neto']"		"Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships"	"It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs ."	"['The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.', 'This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']"	4
CC1420	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	towards an arabic information retrieval system	['Y Houmame']		"Arabic, the mother tongue of over 300 million people around the world, is known as one of the most difficult languages in Automatic Natural Language processing (NLP) in general and information retrieval in particular. Hence, Arabic cannot trust any web information retrieval system as reliable and relevant as Google search engine. In this context, we dared to focus all our researches to implement an Arabic web-based information retrieval system entitled ARABIRS (ARABic Information Retrieval System). Therefore, to launch such a process so long and hard, we will start with Indexing as one of the crucial steps of the system"	"It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs ."	"['The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.', 'This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']"	4
CC1421	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	automatic indexing based on bayesian inference networksquot	"['K Tzeras', 'S Hartman']"	experiments	"In this paper, a Bayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented. It requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occuring in a set of documents of the subject field and descriptors manually assigned to these documents. The indexing dictionary can be derived automatically from a set of manually indexed documents. An application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model."	"Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic ."	"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"	0
CC1422	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	toward an arabic web page classifierquot master project	['M Yahyaoui']	related work		"This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) ."	"['Concerning Arabic, one automatic categorizer has been reported to have been put under operational use to classify Arabic documents; it is referred to as ""Sakhr\'s categorizer"" (Sakhr, 2004).', 'Unfortunately, there is no technical documentation or specification concerning this Arabic categorizer.', ""Sakhr's marketing literature claims that this categorizer is based on Arabic morphology and some research that has been carried out on natural language processing."", 'The present work evaluates the performance on Arabic documents of the Naïve Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997).', 'The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).', 'The present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net.']"	2
CC1423	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	toward an arabic web page classifierquot master project	['M Yahyaoui']	conclusion		"To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) ."	"['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.', 'The corresponding performances in (Yahyaoui, 2001) are 75.6% and 50%, respectively.', 'Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (Yahyaoui, 2001).', 'This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm.', 'Future work will be directed at experimenting with other root extraction algorithms.', ""Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents.""]"	1
CC1424	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	a reexamination of text categorization methods”	"['Y Yang', 'X Liu']"	related work		A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .	"['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .', 'More recently, (Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004).']"	0
CC1425	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	a computational morphology system for arabic”	"['R Al-Shalabi', 'M Evens']"		"This paper describes a new algorithm for morphological analysis of Arabic words, which has been tested on a corpus of 242 abstracts from the Saudi Arabian National Computer Conference . It runs an order of magnitude faster than other algorithms in the literature."	"This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root ."	"['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', 'The infix form (or stem) needs further to be processed in order to obtain the root.', 'This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998).', 'As an example, two close roots (i.e., roots made of the same letters), but semantically different, can yield the same infix form thus creating ambiguity.']"	0
CC1426	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	a comparative study on feature selection in text categorization	"['Y Yang', 'J P Pedersen']"	experiments	This paper is a comparative study of feature selection methods in statistical learning of text categorization The focus is on aggres sive dimensionality reduction Five meth ods were evaluated including term selection based on document frequency DF informa tion gain IG mutual information MI a test CHI and term strength TS We found IG and CHI most e ective in our ex periments Using IG thresholding with a k nearest neighbor classi er on the Reuters cor pus removal of up to removal of unique terms actually yielded an improved classi cation accuracy measured by average preci sion DF thresholding performed similarly Indeed we found strong correlations between the DF IG and CHI values of a term This suggests that DF thresholding the simplest method with the lowest cost in computation can be reliably used instead of IG or CHI when the computation of these measures are too expensive TS compares favorably with the other methods with up to vocabulary reduction but is not competitive at higher vo cabulary reduction levels In contrast MI had relatively poor performance due to its bias towards favoring rare terms and its sen sitivity to probability estimation errors	"( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term ."	"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the χ 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"	0
CC1427	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	machine learning in automated text categorization”	['F Sebastiani']	related work	"The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.Comment: Accepted for publication on ACM Computing Survey"	"More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) ."	"['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']"	0
CC1428	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	an evaluation of statistical approaches to text categorization”	['Y Yang']	introduction	"This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well."	"Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) ."	"['With the explosive growth of text documents on the web, relevant information retrieval has become a crucial task to satisfy the needs of different end users.', 'To this end, automatic text categorization has emerged as a way to cope with such a problem.', 'Automatic text (or document) categorization attempts to replace and save human effort required in performing manual categorization.', 'It consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .', 'Such applications have included electronic email filtering, newsgroups classification, and survey data grouping.', 'Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003).', 'In this paper, NB which is a statistical machine learning algorithm is used to learn to classify non-vocalized 1 Arabic web text documents.']"	0
CC1429	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	toward an arabic web page classifierquot master project	['M Yahyaoui']	experiments		"In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) ."	"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the χ 2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"	1
CC1430	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	on the specification of term values in automatic indexingquot	"['G Salton', 'C S Yang']"	experiments		"TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance ."	"['While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents.', 'The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', 'Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .']"	4
CC1431	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	newsweeder learning to filter netnewsquot	['K Lang']	experiments		"Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic ."	"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"	0
CC1432	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	comparison of two learning algorithms for text categorizationquot	"['D Lewis', 'M Ringnette']"	related work		"include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively ."	"['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']"	0
CC1433	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	modern information retrieval	"['R B Yates', 'B R Neto']"	experiments	"Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships"	TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .	"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the χ 2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"	0
CC1434	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	learning to classify text using svm	['T Joachims']	related work		"More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) ."	"['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']"	0
CC1435	W04-1610	Automatic Arabic document categorization based on the Naïve Bayes algorithm	using clustering to boost text classificationquot	"['Y C Fang', 'S Parthasarathy', 'F Schwartz']"	related work	"In recent years we have seen a tremendous growth in the number of text document collections available on the Internet. Automatic text categorization, the process of assigning unseen documents to user-defined categories, is an important task that can help in the organization and querying of such collections. In this article we consider the problem of classifying online papers from a specific journal in the geological sciences, over a set of expert defined categories. We evaluate two general strategies and several variants thereof. The first strategy is based on Naive Bayes, a popular text classification algorithm. The second strategy is based on Principle Direction Divisive Partitioning, an unsupervised document clustering algorithm. While the performance of both approaches is quite good, some of the new variants that we propose including one, which involves a combination of these two approaches yield even better results."	"For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces ."	"['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']"	0
CC1436	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	wordsketch extraction and display of significant collocations for lexicography	"['Adam Kilgarriff', 'David Tugwell']"	related work		"Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) ."	"['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.', 'The original acquisition methodology we present in the next section will allow us to overcome this limitation.']"	1
CC1437	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	two methods for extracting quotspecificquot singleword terms from specialized corpora	"['Chantal Lemay', ""Marie-Claude L'Homme"", 'Patrick Drouin']"			"The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) ."	"['To construct this test set, we have focused our attention on ten domain-specific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']"	5
CC1438	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	symbolic word clustering for mediumsized corpora	"['Benoit Habert', 'Ellie Naulleau', 'Adeline Nazarenko']"	related work	"When trying to identify essential concepts and relationships in a medium-size corpus, it is not always possible to rely on statistical methods, as the frequencies are too low. We present an alternative method, symbolic, based on the simplification of parse trees. We discuss the results on nominal phrases of two technical corpora, analyzed by two different robust parsers used for terminology updating in an industrial company. We compare our results with Hindle's scores of similarity."	"This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself ."	"['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']"	0
CC1439	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	conceptual structuring through term variation	['Beatrice Daille']	related work	"Term extraction systems are now an integral part of the compiling of specialized dictionaries and updating of term banks. In this paper, we present a term detection approach that discovers, structures, and infers conceptual relationships between terms for French. Conceptual relationships are deduced from specific types of term variations, morphological and syntagmatic, and are expressed through lexical functions. The linguistic precision of the conceptual structuring through morphological variations is of 95%."	"More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters ."	"['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']"	0
CC1440	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	termextraction using nontechnical corpora as a point of leverage	['Patrick Drouin']			The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .	"['To construct this test set, we have focused our attention on ten domain-speci\x1cc terms: commande (command), con\x1cguration, \x1cchier (\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004).', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']"	5
CC1441	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	lexicallybased terminology structuring some inherent limits	"['Natalia Grabar', 'Pierre Zweigenbaum']"	related work	"Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis of the 'new' relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring."	"More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters ."	"['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']"	0
CC1442	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	aide a lacquisition de connaissances a partir de corpus	['Rochdi Oueslati']	related work	"Le probleme d'identification des termes presente un interet particulier pour les applications du taln. En effet, la conception d'outils d'identification de termes et de relations entre termes est d'une aide considerable aux terminologues et aux cogniticiens qui veulent analyser un domaine nouveau. Les terminologues s'interessent surtout a l'etude des termes particulierement dans les domaines de specialite ou les termes designent des objets du domaine de facon la moins ambigue possible. Pour construire une terminologie on part souvent de textes et on applique un ensemble de methodes qui facilitent l'identification des termes. Les methodes classiques utilisent souvent des grammaires et des dictionnaires afin d'acquerir des concepts du domaine d'etude. L'approche que nous presentons dans cette these utilise une approche distributionnelle basee sur les travaux de z. Harris et utilise des algorithmes originaux pour la synthese automatique de contextes entre termes afin d'identifier des relations semantiques propres au domaine. Les resultats obtenus sont d'abord filtres puis valides par un linguiste avant d'etre structures sous forme hierarchique. Ils sont ensuite exploites afin d'acquerir d'autres connaissances en utilisant un processus iteratif et incremental base sur l'inference. L'utilisation d'un langage d'expression de contraintes entre termes du domaines permet de reperer un nombre fini de schemas morphosyntaxiques qui expriment des relations generiques notamment des definitions et des proprietes d'objets. Les resultats obtenus peuvent interesser d'autres travaux comme ceux lies a la construction de bases de connaissances terminologiques ou a la construction d'ontologies partielles propres au domaine."	"Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\x1b and Tugwell, 2001, for example)."	"['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\x1b and Tugwell, 2001, for example).', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.', 'The original acquisition methodology we present in the next section will allow us to overcome this limitation.']"	1
CC1443	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	inductive logic programming theory and methods	"['Stephen Muggleton', 'Luc De-Raedt']"	method	"AbstractInductive Logic Programming (ILP) is a new discipline which investigates the inductive construction of first-order clausal theories from examples and background knowledge. We survey the most important theories and methods of this new field. First, various problem specifications of ILP are formalized in semantic settings for ILP, yielding a ""model-theory"" for ILP. Second, a generic ILP algorithm is presented. Third, the inference rules and corresponding operators used in ILP are presented, resulting in a ""proof-theory"" for ILP. Fourth, since inductive inference does not produce statements which are assured to follow from what is given, inductive inferences require an alternative form of justification. This can take the form of either probabilistic support or logical constraints on the hypothesis language. Information compression techniques used within ILP are presented within a unifying Bayesian approach to confirmation and corroboration of hypotheses. Also, different ways to constrain the hypothesis language or specify the declarative bias are presented. Fifth, some advanced topics in ILP are addressed. These include aspects of computational learning theory as applied to ILP, and the issue of predicate invention. Finally, we survey some applications and implementations of ILP. ILP applications fall under two different categories: first, scientific discovery and knowledge acquisition, and second, programming assistants"	"ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\x88\x92 ) of the elements one wants to acquire and their context."	"['ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\x88\x92 ) of the elements one wants to acquire and their context.', 'The contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'The acquisition process can be summarized in 3 steps:']"	0
CC1444	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	lexicallybased terminology structuring some inherent limits	"['Natalia Grabar', 'Pierre Zweigenbaum']"	introduction	"Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis of the 'new' relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring."	"However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts ."	"['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)', 'The work reported here infers specific semantic relationships based on sets of examples and counterexamples.']"	1
CC1445	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	selection de termes dans un dictionnaire dinformatique  comparaison de corpus et criteres lexicosemantiques	"[""Marie-Claude L'Homme""]"	introduction	"Resume Le present article propose une methode de selection des termes devant faire partie d'un dictionnaire specialise et, plus precisement, un dictionnaire fondamental d'informatique. La methode repose principalement sur un ensemble de criteres lexico-semantiques appliques a un corpus specialise. Elle tient egalement compte de la frequence et de la repartition des unites dans ce corpus. Dans ce travail, nous avons voulu savoir jusqu'a quel point des techniques de comparaison de corpus permettaient de ramener des termes coincidant avec la liste obtenue par l'application des criteres. L'examen de la liste generee automatiquement montre qu'un peu plus de 50 % des unites classees comme etant specifiques par la metrique sont egalement retenues par le terminographe. Les resultats revelent que la technique revet un interet dans la mesure ou elle permet d'aligner des choix sur des donnees extraites de corpus. Toutefois, la selection automatique recele un certain nombre d'imperfections qui doivent etre corrigees par une analyse terminographique."	The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .	"['In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']"	4
CC1446	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	acquisition of qualia elements from corpora — evaluation of a symbolic learning method	"['Pierrette Bouillon', 'Vincent Claveau', 'Cecile Fabre', 'Pascale Sebillot']"	method	"This paper presents and evaluates a system extracting from a corpus noun-verb pairs whose components are related by a special kind of link: the qualia roles as defined in the Generative Lexicon. This system is based on a symbolic learning method that automatically learns, from noun-verb pairs that are or are not related by a qualia link, rules characterizing positive examples from negative ones in terms of their surrounding part-of-speech or semantic contexts. The qualia noun-verb pair extraction is thus performed by applying the learnt rules on a part-of-speech or semantically tagged text. Stress is put on the quality of the learning when compared with traditional statistical or syntactical-based approaches. The linguistic relevance of the rules is also evaluated through a comparison with manually acquired qualia patterns."	"In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) ."	"['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']"	4
CC1447	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	the generative lexicon	['James Pustejovsky']	method	"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole."	"ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) ."	"['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']"	0
CC1448	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	query expansion using lexicalsemantic relations	['Ellen M Voorhees']	introduction	"Applications such as office automation, news filtering, help facilities in complex systems, and the like require the ability to retrieve documents from full-text databases where vocabulary problems can be particularly severe. Experiments performed on small collections with single-domain thesauri suggest that expanding query vectors with words that are lexically related to the original query words can ameliorate some of the problems of mismatched vocabularies. This paper examines the utility of lexical query expansion in the large, diverse TREC collection. Concepts are represented by WordNet synonym sets and are expanded by following the typed links included in WordNet. Experimental results show this query expansion technique makes little difference in retrieval effectiveness if the original queries are relatively complete descriptions of the information being sought even when the concepts to be expanded are selected by hand. Less well developed queries can be significantly improved by expansion of hand-chosen concepts. However, an automatic procedure that can approximate the set of hand picked synonym sets has yet to be devised, and expanding by the synonym sets that are automatically generated can degrade retrieval performance."	"Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) ."	"['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']"	1
CC1449	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	using partofspeech and semantic tagging for the corpusbased learning of qualia structure elements	"['Pierrette Bouillon', 'Vincent Claveau', 'Cecile Fabre', 'Pascale Sebillot']"	method	This paper describes the im plementation and results of a machine learning method de veloped within the inductive logic programming ILP frame work Muggleton and De Raedt to automatically extract from a corpus tagged with parts of speech POS and semantic classes noun verb pairs whose components are bound by one of the relations de ned in the qualia structure in the Genera tive Lexicon Pustejovsky We demonstrate that the seman tic tagging of the corpus improves the quality of the learning both on a theoretical and an empiri cal point of view We also show that a set of the rules learnt by our ILP method have a linguistic signi cance regarding the detec tion of the clues that distinguish in terms of POS and seman tic surrounding context noun verb pairs that are linked by one qualia role from others that are not semantically related	"ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) ."	"['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']"	0
CC1450	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	a comparative evaluation of collocation extraction techniques	['Darren Pearce']	method	"This paper describes an experiment that attempts to compare a range of existing collocation extraction techniques as well as the implementation of a new technique based on tests for lexical substitutability. After a description of the experiment details, the techniques are discussed with particular emphasis on any adaptations that are required in order to evaluate it in the way proposed. This is followed by a discussion on the relative strengths and weaknesses of the techniques with reference to the results obtained. Since there is no general agreement on the exact nature of collocation, evaluating techniques with reference to any single standard is somewhat controversial. Departing from this point, part of the concluding discussion includes initial proposals for a common framework for evaluation of collocation extraction techniques."	"Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :"	"['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']"	0
CC1451	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	learning semantic lexicons from a partofspeech and semantically tagged corpus using inductive logic programming	"['Vincent Claveau', 'Pascale Sebillot', 'Cecile Fabre', 'Pierrette Bouillon']"	method	"This paper describes an inductive logic programming learning method designed to acquire from a corpus specific Noun-Verb (N-V) pairs---relevant in information retrieval applications to perform index expansion---in order to build up semantic lexicons based on Pustejovsky's generative lexicon (GL) principles (Pustejovsky, 1995). In one of the components of this lexical model, called the qualia structure, words are described in terms of semantic roles. For example, the telic role indicates the purpose or function of an item (cut for knife), the agentive role its creation mode (build for house), etc. The qualia structure of a noun is mainly made up of verbal associations, encoding relational information. The learning method enables us to automatically extract, from a morpho-syntactically and semantically tagged corpus, N-V pairs whose elements are linked by one of the semantic relations defined in the qualia structure in GL. It also infers rules explaining what in the surrounding context distinguishes such pairs from others also found in sentences of the corpus but which are not relevant. Stress is put here on the learning efficiency that is required to be able to deal with all the available contextual information, and to produce linguistically meaningful rules."	ASARES is presented in detail in ( #AUTHOR_TAG ) .	"['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']"	5
CC1452	W04-1805	Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus	structures mathematiques du langage	['Zellig Harris']	related work		A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .	"['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']"	0
CC1453	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	trec2001 crosslingual retrieval at bbn	"['J Xu', 'A Fraser', 'R Weischedel']"	introduction		"It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) ."	"['We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities.', 'This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify.', 'In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']"	0
CC1454	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	improving machine learning approaches to coreference resolution	"['V Ng', 'C Cardie']"	introduction	"Human speakers generally have no difficulty in determining which noun phrases in a text or dialogue refer to the same real-world entity. This task of identifying co-referring noun phrases --- noun phrase coreference resolution --- can present a serious challenge to a natural language processing system, however. Indeed, it is one of the critical problems that currently limits the performance of many practical natural language processing tasks.    State-of-the-art coreference resolution systems operate by relying on a set of hand-crafted heuristics that requires a lot of time and linguistic expertise to develop. Recently, machine learning techniques have been used to circumvent both of these problems by automating the acquisition of coreference resolution heuristics, yielding coreference systems that offer performance comparable to their heuristic-based counterparts. In this dissertation, we present a machine learning-based solution to noun phrase coreference that extends eariler work in the area and outperforms the best existing learning-based coreference engine on a suite of standard coreference data sets. Performance gains accrue from more effective use of the available training data via a set of linguistic and extra-linguistic extensions to the standard machine learning framework for coreference resolution"	"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."	"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']"	0
CC1455	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	proceedings of ace evaluation and pi meeting	['NIST']	introduction		"Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) ."	"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']"	5
CC1456	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a stochastic finitestate wordsegmentation algorithm for chinese	"['R Sproat', 'C Shih', 'W Gale', 'N Chang']"			"In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) ."	"['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .', 'For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data.', 'The character based model alone achieves a 94.5% exact match segmentation accuracy, considerably less accurate then the dictionary based model.']"	1
CC1457	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	automatic content extraction	['ACE']	conclusion	"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability"	"These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data ."	"['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'In addition, we also report results on the official test data.']"	5
CC1458	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	the unicode standard	"['Joan Aliprand', 'Julie Allen', 'Joe Becker', 'Mark Davis', 'Michael Everson']"		"The Unicode Standard is a global character set for worldwide computing covering the major modern scripts of the world as well as classical forms of Greek, Sanskrit, and Pali. The history and implications of Unicode Standard are discussed. The principles underpinning the design of the Unicode Standard are described with reference to those principles that also are present in USMARC and UNIMARC. Unicode give the potential to support every script. Expanding the character set would have consequences for transcription. Faithfulness of transcription has implications for retrieval. The addition of more characters to support more exact cataloging affects the economic cost of cataloging. The need for characters should be related not to the production of a surrogate for the physical item that has been cataloged, but to facilitating retrieval."	"Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) ."	"['(2) .', 'We define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']"	5
CC1459	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	empirical studies in strategies for arabic information retrieval	"['J Xu', 'A Fraser', 'R Weischedel']"	introduction		"It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) ."	"['We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities.', 'This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify.', 'In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']"	0
CC1460	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a mentionsynchronous coreference resolution algorithm based on the bell tree	"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']"		This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.	The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .	['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .']	1
CC1461	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	arabic verbs and essentials of grammar	"['J Wightwick', 'M Gaafar']"		"Your one-stop guide to mastering the basics of Arabic Can one book have all you need to communicate confidently in a new language? Yes, and that book is Arabic Verbs & Essentials of Grammar. It offers a solid foundation of major verbal and grammatical concepts of the language, from pronouns to idioms and expressions and from irregular verbs to expressions of time. Each unit is devoted to one topic, so you can find what you need right away and get focused instruction immediately. Concise yet thorough, the explanations are supported by numerous examples to help you master the different concepts. And for those tricky verbs, Arabic Verbs & Essentials of Grammar includes a Verb Index of the most common verbs, cross-referenced with the abundant verb tables appearing throughout the book. This book will give you: An excellent introduction to the basics of Arabic if you are a beginner or a quick, thorough reference if you already have experience in the language Contemporary usage of verbs, adjectives, pronouns, prepositions, conjunctions, and other grammar essentials Examples that reflect contemporary usage and real-life situations"	"Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) ."	"['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .', 'The plural form and the singular form may also be completely different (e.g. for woman, but for women).', 'The sound plurals are formed by adding plural suffixes to singular nouns (e.g., meaning researcher): the plural suffix is for feminine nouns in grammatical cases (e.g.,']"	0
CC1462	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a statistical model for multilingual entity detection and tracking	"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']"	introduction	"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages."	"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."	"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']"	0
CC1463	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a maximum entropy approach to natural language processing	"['A Berger', 'S Della Pietra', 'V Della Pietra']"		"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."	"The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) ."	"['We formulate the mention detection problem as a classification problem, which takes as input segmented Arabic text.', 'We assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .', 'One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.']"	0
CC1464	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	automatic content extraction	['ACE']		"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability"	"the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) ."	"['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']"	5
CC1465	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a maximum entropy approach to named entity recognition	['A Borthwick']	introduction	"This thesis describes a novel statistical named-entity (i.e. ""proper name"") recognition system known as ""MENE"" (Maximum Entropy Named Entity). Named entity (N.E.) recognition is a form of information extraction in which we seek to classify every word in a document as being a person-name, organization, location, date, time, monetary value, percentage, or ""none of the above"". The task has particular significance for Internet search engines, machine translation, the automatic indexing of documents, and as a foundation for work on more complex information extraction tasks.  Two of the most significant problems facing the constructor of a named entity system are the questions of portability and system performance. A practical N.E. system will need to be ported frequently to new bodies of text and even to new languages. The challenge is to build a system which can be ported with minimal expense (in particular minimal programming by a computational linguist) while maintaining a high degree of accuracy in the new domains or languages.  MENE attempts to address these issues through the use of maximum entropy probabilistic modeling. It utilizes a very flexible object-based architecture which allows it to make use of a broad range of knowledge sources in making its tagging decisions. In the DARPA-sponsored MUC-7 named entity evaluation, the system displayed an accuracy rate which was well-above the median, demonstrating that it can achieve the performance goal. In addition, we demonstrate that the system can be used as a post-processing tool to enhance the output of a hand-coded named entity recognizer through experiments in which MENE improved on the performance of N.E. systems from three different sites. Furthermore, when all three external recognizers are combined under MENE, we are able to achieve very strong results which, in some cases, appear to be competitive with human performance.  Finally, we demonstrate the trans-lingual portability of the system. We ported the system to two Japanese-language named entity tasks, one of which involved a new named entity category, ""artifact"". Our results on these tasks were competitive with the best systems built by native Japanese speakers despite the fact that the author speaks no Japanese."	"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."	"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']"	0
CC1466	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	language model based arabic word segmentation	"['Y-S Lee', 'K Papineni', 'S Roukos', 'O Emam', 'H Hassan']"		"We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix * (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97 % exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest"	#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .	"['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules.']"	5
CC1467	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a maximum entropy approach to natural language processing	"['A Berger', 'S Della Pietra', 'V Della Pietra']"	introduction	"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."	Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .	"['Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.']"	5
CC1468	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a statistical model for multilingual entity detection and tracking	"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']"	introduction	"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages."	"Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) ."	"['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classi cation problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granulariity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the pre xes and su_xes are quite frequent, directly processing unsegmented words results in signi cant data sparseness.']"	1
CC1469	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a statistical model for multilingual entity detection and tracking	"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']"		"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages."	The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .	"['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.', 'For a token t i , the backward token n-gram feature will contains the previous n − 1 tokens in the history (t i−n+1 , . . .', 't i−1 ) and the forward token n-gram feature will contains the next n − 1 tokens (t i+1 , . . .', 't i+n−1 ).']"	0
CC1470	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a mentionsynchronous coreference resolution algorithm based on the bell tree	"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']"		This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.	Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .	"['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy.']"	5
CC1471	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a machine learning approach to coreference resolution of noun phrases	"['W M Soon', 'H T Ng', 'C Y Lim']"	introduction	"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set"	"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."	"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']"	0
CC1472	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	language model based arabic word segmentation	"['Y-S Lee', 'K Papineni', 'S Roukos', 'O Emam', 'H Hassan']"		"We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix * (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97 % exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest"	"As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems ."	"['However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.', 'We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.', 'From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus.']"	5
CC1473	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	named entity recognition without gazetteers	"['A Mikheev', 'M Moens', 'C Grover']"	introduction	"It is often claimed that Named Entity  recognition systems need extensive  gazetteers|lists of names of people, organisations,  locations, and other named  entities. Indeed, the compilation of such  gazetteers is sometimes mentioned as a  bottleneck in the design of Named Entity  recognition systems.  We report on a Named Entity recognition  system which combines rule-based  grammars with statistical (maximum entropy)  models. We report on the system  &apos;s performance with gazetteers of different  types and dierent sizes, using test  material from the muc{7 competition.  We show that, for the text type and task  of this competition, it is sucient to use  relatively small gazetteers of well-known  names, rather than large gazetteers of  low-frequency names. We conclude with  observations about the domain independence  of the competition and of our experiments.  1 Introduction  Named Entity recognition involves processing a text and identifying certain occurrences of words or expressions .."	"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."	"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']"	0
CC1474	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	automatic content extraction	['ACE']	experiments	"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability"	"We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here ."	"['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']"	5
CC1475	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	an empirical study of smoothing techinques for language modeling	"['S F Chen', 'J Goodman']"		"We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t..."	"The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model ."	"['In our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines.', 'The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.', 'The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .', 'Differing from (Lee et al., 2003), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation.']"	5
CC1476	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	bbn description of the sift system as used for muc7	"['S Miller', 'M Crystal', 'H Fox', 'L Ramshaw', 'R Schwarz', 'R Stone', 'R Weischedel']"	introduction		"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."	"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']"	0
CC1477	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	nymble a highperformance learning namefinder	"['D M Bikel', 'S Miller', 'R Schwartz', 'R Weischedel']"	introduction		"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."	"['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']"	0
CC1478	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a mentionsynchronous coreference resolution algorithm based on the bell tree	"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']"	experiments	This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.	"ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric ."	"['In this section, we present the coreference results on the devtest defined earlier.', 'First, to see the effect of stem matching features, we compare two coreference systems: one with the stem features, the other with- out.', 'We test the two systems on both ""true"" and system mentions of the devtest set.', 'True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']"	5
CC1479	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	building an arabic stemmer for information retrieval	"['Aitao Chen', 'Fredic Gey']"		"In TREC 2002 the Berkeley group participated only in the English-Arabic cross-language retrieval (CLIR) track. One Arabic monolingual run and three English-Arabic cross-language runs were submitted. Our approach to the crosslanguage retrieval was to translate the English topics into Arabic using online English-Arabic machine translation systems. The four official runs are named as BKYMON, BKYCL1, BKYCL2, and BKYCL3. The BKYMON is the Arabic monolingual run, and the other three runs are English-to-Arabic cross-language runs. This paper reports on the construction of an Arabic stoplist and two Arabic stemmers, and the experiments on Arabic monolingual retrieval, English-to-Arabic cross-language retrieval."	"Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) ."	"['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .', 'The plural form and the singular form may also be completely different (e.g. for woman, but for women).', 'The sound plurals are formed by adding plural suffixes to singular nouns (e.g., meaning researcher): the plural suffix is for feminine nouns in grammatical cases (e.g.,']"	0
CC1480	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a mentionsynchronous coreference resolution algorithm based on the bell tree	"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']"	introduction	This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.	"Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) ."	"['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.']"	1
CC1481	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	proceedings of ace evaluation and pi meeting	['NIST']	experiments		"As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."	"['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.', 'Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type.', 'In this paper, we report the results in terms of precision, recall, and F-measure3.']"	5
CC1482	W05-0709	The impact of morphological stemming on Arabic mention detection and coreference resolution	a maximum entropy approach to natural language processing	"['A Berger', 'S Della Pietra', 'V Della Pietra']"		"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."	"where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) ."	"['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']"	5
CC1483	W06-1104	Automatically creating datasets for measures of semantic relatedness	computing semantic relatedness across parts of speech	['Iryna Gurevych']	experiments		#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .	"['We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair.', 'Test subjects were invited via email to participate in the experiment.', 'Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .', 'Their results differed particularly in cases of antonymy or distributionally related pairs.', 'We created a manual with a detailed introduction to SR stressing the crucial points.', 'The manual was presented to the subjects before the experiment and could be re-accessed at any time.', 'During the experiment, one concept pair at a time was presented to the test subjects in random ordering.', 'Subjects had to assign a discrete relatedness value {0,1,2,3,4} to each pair.', ""Figure 2 shows the system's GUI.""]"	4
CC1484	W06-1104	Automatically creating datasets for measures of semantic relatedness	computing similarity	"['Ludovic Lebart', 'Martin Rajman']"	introduction	"In this paper we study similarity measures for moving curves which can, for example, model changing coastlines or retreating glacier termini. Points on a moving curve have two parameters, namely the position along the curve as well as time. We therefore focus on similarity measures for surfaces, specifically the Fr 'echet distance between surfaces. While the Fr 'echet distance between surfaces is not even known to be computable, we show for variants arising in the context of moving curves that they are polynomial-time solvable or NP-complete depending on the restrictions imposed on how the moving curves are matched. We achieve the polynomial-time solutions by a novel approach for computing a surface in the so-called free-space diagram based on max-flow min-cut duality"	"words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness."	"['Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'It is defined on different kinds of textual units, e.g.', 'documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']"	0
CC1485	W06-1104	Automatically creating datasets for measures of semantic relatedness	identifying semantic relations and functional properties of human verb associations	"['Sabine Schulte im Walde', 'Alissa Melinger']"	related work	"This paper uses human verb associations as the basis for an investigation of verb properties, focusing on semantic verb relations and prominent nominal features. First, the lexical semantic taxonymy GermaNet is checked on the types of classic semantic relations in our data; verb-verb pairs not covered by GermaNet can help to detect missing links in the taxonomy, and provide a useful basis for defining non-classical relations. Second, a statistical grammar is used for determining the conceptual roles of the noun responses. We present prominent syntax-semantic roles and evidence for the usefulness of co-occurrence information in distributional verb descriptions."	"In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments."	"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"	0
CC1486	W06-1104	Automatically creating datasets for measures of semantic relatedness	evaluating wordnetbased measures of semantic distance	"['Alexander Budanitsky', 'Graeme Hirst']"			"According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments ."	"['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']"	0
CC1487	W06-1104	Automatically creating datasets for measures of semantic relatedness	evaluating wordnetbased measures of semantic distance	"['Alexander Budanitsky', 'Graeme Hirst']"	experiments		#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .	"['The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts.', 'To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used.', 'However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .', 'This empty band is not observed here.', 'However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).', 'The plot clearly shows an empty horizontal band with no judgments.', 'The connection between averaged judgments and standard deviation is plotted in Figure 5.']"	1
CC1488	W06-1104	Automatically creating datasets for measures of semantic relatedness	computing semantic relatedness across parts of speech	['Iryna Gurevych']	related work		This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .	"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"	0
CC1489	W06-1104	Automatically creating datasets for measures of semantic relatedness	using information content to evaluate semantic similarity	['Philip Resnik']	related work	"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments."	This experiment was again replicated by #AUTHOR_TAG with 10 subjects .	"['In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']"	0
CC1490	W06-1104	Automatically creating datasets for measures of semantic relatedness	project ‘semantic information retrieval’	['SIR Project']	experiments	"The aim of the SCHEMA Network of Excellence is to bring together a critical mass of universities, research centers, industrial partners and end users, in order to design a reference system for content-based semantic scene analysis, interpretation and understanding. Relevant research areas include: content-based multimedia analysis and automatic annotation of semantic multimedia content, combined textual and multimedia information retrieval, semantic -web, MPEG-7 and MPEG-21 standards, user interfaces and human factors. In this paper, recent advances in content-based analysis, indexing and retrieval of digital media within the SCHEMA Network are presented. These advances will be integrated in the SCHEMA module-based, expandable reference system"	"In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems ."	"['We extracted word pairs from three different domain-specific corpora (see Table 2).', 'This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]"	4
CC1491	W06-1104	Automatically creating datasets for measures of semantic relatedness	cooccurrence retrieval a flexible framework for lexical distributional similarity	"['Julie Weeds', 'David Weir']"		"Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events. In other applications, distributional similarity is taken to be an approximation to semantic similarity. However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted. In this work, a flexible, parameterized framework for calculating distributional similarity is proposed. Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework. In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity. First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship? Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity? Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type? Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity."	"dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) ."	"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"	0
CC1492	W06-1104	Automatically creating datasets for measures of semantic relatedness	lexikalischsemantische wortnetze chapter computerlinguistik und sprachtechnologie	['Claudia Kunze']	experiments		"Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word ."	"['We implemented a set of filters for word pairs.', 'One group of filters removed unwanted word pairs.', 'Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist.', 'Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set.', 'We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']"	5
CC1493	W06-1104	Automatically creating datasets for measures of semantic relatedness	an informationtheoretic definition of similarity in	['Dekang Lin']			"Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4"	"['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.']"	0
CC1494	W06-1104	Automatically creating datasets for measures of semantic relatedness	contextual correlates of synonymy	"['Herbert Rubenstein', 'John B Goodenough']"	experiments	"Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."	#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .	"['In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647.', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.', 'Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.']"	1
CC1495	W06-1104	Automatically creating datasets for measures of semantic relatedness	using the structure of a conceptual network in computing semantic relatedness	['Iryna Gurevych']	related work	"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences."	"We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs ."	"['In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']"	5
CC1496	W06-1104	Automatically creating datasets for measures of semantic relatedness	computing semantic relatedness across parts of speech	['Iryna Gurevych']	related work		"In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG ."	"['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']"	1
CC1497	W06-1104	Automatically creating datasets for measures of semantic relatedness	wordnet an electronic lexical database chapter combining local context and wordnet similarity for word sense identification	"['Claudia Leacock', 'Martin Chodorow']"			"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) ."	"['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"	0
CC1498	W06-1104	Automatically creating datasets for measures of semantic relatedness	evaluating wordnetbased measures of semantic distance	"['Alexander Budanitsky', 'Graeme Hirst']"	introduction		Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .	"['Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (Gurevych, 2005). 3', 'Dissimilar words can be semantically related, e.g.', 'via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']"	0
CC1499	W06-1104	Automatically creating datasets for measures of semantic relatedness	contextual correlates of synonymy	"['Herbert Rubenstein', 'John B Goodenough']"	related work	"Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."	"In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards ."	"['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by Resnik (1995) with 10 subjects.', 'Table 1']"	0
CC1500	W06-1104	Automatically creating datasets for measures of semantic relatedness	using measures of semantic relatedness for word sense disambiguation	"['Siddharth Patwardhan', 'Satanjeev Banerjee', 'Ted Pedersen']"		This paper generalizes the Adapted Lesk Algorithm of Banerjee and Pedersen (2002) to a method of word sense disambiguation based on semantic relatedness. This is possible since Lesk&apos;s original algorithm (1986) is based on gloss overlaps which can be viewed as a measure of semantic relatedness. We evaluate a variety of measures of semantic relatedness when applied to word sense disambiguation by carrying out experiments using the English lexical sample data of Senseval-2. We find that the gloss overlaps of Adapted Lesk and the semantic distance measure of Jiang and Conrath (1997) result in the highest accuracy	"The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task."	"['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']"	0
CC1501	W06-1104	Automatically creating datasets for measures of semantic relatedness	computing similarity	"['Ludovic Lebart', 'Martin Rajman']"		"In this paper we study similarity measures for moving curves which can, for example, model changing coastlines or retreating glacier termini. Points on a moving curve have two parameters, namely the position along the curve as well as time. We therefore focus on similarity measures for surfaces, specifically the Fr 'echet distance between surfaces. While the Fr 'echet distance between surfaces is not even known to be computable, we show for variants arising in the context of moving curves that they are polynomial-time solvable or NP-complete depending on the restrictions imposed on how the moving curves are matched. We achieve the polynomial-time solutions by a novel approach for computing a surface in the so-called free-space diagram based on max-flow min-cut duality"	"#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task ."	"['The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']"	0
CC1502	W06-1104	Automatically creating datasets for measures of semantic relatedness	automatic text processing the transformation analysis and retrieval of information by computer	['Gerard Salton']	experiments		The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .	"['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]"	5
CC1503	W06-1104	Automatically creating datasets for measures of semantic relatedness	computing semantic relatedness across parts of speech	['Iryna Gurevych']	experiments		#AUTHOR_TAG reported a correlation of r = .69 .	"['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']"	1
CC1504	W06-1104	Automatically creating datasets for measures of semantic relatedness	using the structure of a conceptual network in computing semantic relatedness	['Iryna Gurevych']	experiments	"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences."	"As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) ."	"['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']"	5
CC1505	W06-1104	Automatically creating datasets for measures of semantic relatedness	nonclassical lexical semantic relations	"['Jane Morris', 'Graeme Hirst']"	related work		#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .	"['Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']"	0
CC1506	W06-1104	Automatically creating datasets for measures of semantic relatedness	placing search in context the concept revisited	"['Lev Finkelstein', 'Evgeniy Gabrilovich', 'Yossi Matias', 'Ehud Rivlin', 'Zach Solan', 'Gadi Wolfman']"	related work	"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web."	"#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too ."	"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"	0
CC1507	W06-1104	Automatically creating datasets for measures of semantic relatedness	probabilistic partofspeech tagging using decision trees	['Helmut Schmid']	experiments	"In this paper, a new probabilistic tagging method is presented which avoids problems that Markov Model based taggers face, when they have to estimate transition probabilities from sparse data. In this tagging method, transition probabilities are estimated using a decision tree. Based on this method, a part-of-speech tagger (called TreeTagger) has been implemented which achieves 96.36 % accuracy on Penn-Treebank data which is better than that of a trigram tagger (96.06 %) on the same data."	"The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) ."	"['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (Salton, 1989).""]"	5
CC1508	W06-1104	Automatically creating datasets for measures of semantic relatedness	verb semantics and lexical selection	"['Zhibiao Wu', 'Martha Palmer']"		"This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.Comment: 6 pages, Figures and bib files are in part"	"dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) ."	"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"	0
CC1509	W06-1104	Automatically creating datasets for measures of semantic relatedness	using information content to evaluate semantic similarity	['Philip Resnik']		"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments."	"dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) ."	"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"	0
CC1510	W06-1104	Automatically creating datasets for measures of semantic relatedness	using information content to evaluate semantic similarity	['Michael Lesk']		"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments."	"dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) ."	"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"	0
CC1511	W06-1104	Automatically creating datasets for measures of semantic relatedness	automatic generation of a coarse grained wordnet	"['Rada Mihalcea', 'Dan Moldovan']"	experiments		"If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6"	"['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'Words with a number of senses above the threshold are removed from the list.']"	0
CC1512	W06-1104	Automatically creating datasets for measures of semantic relatedness	computing semantic relatedness across parts of speech	['Iryna Gurevych']	related work		"Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of ."	"['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e.', 'other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity.']"	0
CC1513	W06-1104	Automatically creating datasets for measures of semantic relatedness	placing search in context the concept revisited	"['Lev Finkelstein', 'Evgeniy Gabrilovich', 'Yossi Matias', 'Ehud Rivlin', 'Zach Solan', 'Gadi Wolfman']"	experiments	"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web."	#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .	"['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']"	1
CC1514	W06-1104	Automatically creating datasets for measures of semantic relatedness	using information content to evaluate semantic similarity	['Philip Resnik']	experiments	"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments."	"#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness ."	"['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']"	1
CC1515	W06-1104	Automatically creating datasets for measures of semantic relatedness	using the structure of a conceptual network in computing semantic relatedness	['Iryna Gurevych']	related work	"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences."	#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .	"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"	0
CC1516	W06-1104	Automatically creating datasets for measures of semantic relatedness	computing semantic relatedness across parts of speech	['Iryna Gurevych']	experiments		"Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG ."	"['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'While most people may agree 10 Note that Resnik used the averaged correlation coefficient.', 'We computed the summarized correlation coefficient using a Fisher Z-value transformation.', 'that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group.', 'Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']"	1
CC1517	W06-1104	Automatically creating datasets for measures of semantic relatedness	semantic similarity based on corpus statistics and lexical taxonomy	"['Jay J Jiang', 'David W Conrath']"		"This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task."	"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) ."	"['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"	0
CC1518	W06-1705	Annotated web as corpus	facilitating the compilation and dissemination of adhoc web corpora	['W H Fletcher']	method	"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t"	"Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) ."	"['• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']"	3
CC1519	W06-1705	Annotated web as corpus	linguistic search engine	['A Kilgarriff']	related work	"Users of search engines often have specific questions which they hope or believe a particular resource can answer. The problem, from the computer system's perspective, is cognitive understanding of the contents in the source and finding the desired answer. Most of the search engines, with Google on the top, able to retrieve most likely relevant information based on a query. But not capable of providing answer to a question due to lack of deduction capability. In order to find a specific answer to a question, the engine needs to understand the information content and able to do deductive reasoning. Conventional information representation models used in the search engines rely on an extensive use of keywords and their frequencies in storing and retrieving information and other characteristic data on specific body of information. It is believed that we need new approaches for the development of future search engines which will be more effective. Semantic model is an alternative to conventional approach. We have proposed logical-linguistic model where logic and linguistic formalism are used in providing mechanism for computer to understand the contents of the source and deduce answers to questions. The capability of deduction is much depended on the knowledge representation framework used. The approach applies semantic analysis in transforming and normalising information from natural language texts into a declarative knowledge based representation of first order predicate logic. Retrieval of relevant information can then be performed through plausible logical implication and answer to query is carried out using a theorem proving technique. This paper elaborates on the model and how it is used in search engine and question answering system as one unified model"	"Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) ."	"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]"	0
CC1520	W06-1705	Annotated web as corpus	introduction to the special issue on the web as corpus	"['A Kilgarriff', 'G Grefenstette']"	introduction	"The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists' playground. This special issue of Computational Linguistics explores ways in which this dream is being explored."	"In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) ."	"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']"	0
CC1521	W06-1705	Annotated web as corpus	supporting text mining for escience the challenges for gridenabled natural language processing	"['J Carroll', 'R Evans', 'E Klein']"	related work	"Over the last few years, language technology has moved rapidly from 'applied research' to 'engineering', and from small-scale to large-scale engineering. Applications such as advanced text mining systems are feasible, but very resource-intensive, while research seeking to address the underlying language processing questions faces very real practical and methodological limitations. The e-Science vision, and the creation of the e-Science Grid, promises the level of integrated large-scale technological support required to sustain this important and successful new technology area. In this paper, we discuss the foundations for the deployment of text mining and other language technology on the Grid - the protocols and tools required to build distributed large-scale language technology systems, meeting the needs of users, application builders and researchers."	"In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) ."	"['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001).', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.', 'Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).']"	0
CC1522	W06-1705	Annotated web as corpus	the linguists search engine getting started guide	"['P Resnik', 'A Elkiss']"	related work		"Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) ."	"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]"	0
CC1523	W06-1705	Annotated web as corpus	facilitating the compilation and dissemination of adhoc web corpora	['W H Fletcher']	related work	"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t"	"Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) ."	"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]"	0
CC1524	W06-1705	Annotated web as corpus	concordancing the web with kwicfinder third north american	['W H Fletcher']	related work		"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) ."	"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]"	0
CC1525	W06-1705	Annotated web as corpus	web googles missing pages mystery solved httpaixtalblogspotcom200502webgooglesmissingpagesmysteryhtml accessed	['J Veronis']	related work		Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .	"['A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from this problem, e.g.', 'BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus.', 'Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags.', 'In addition, PIE 9 (Phrases in English), developed at USNA, which performs searches on n-grams (based on words, parts-ofspeech and characters), is currently restricted to the British National Corpus as well, although other static corpora are being added to its database.', 'In contrast, little progress has been made toward annotating sizable sample corpora from the web.']"	0
CC1526	W06-1705	Annotated web as corpus	google as a corpus tool in	['T Robb']	related work		"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) ."	"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two ap- proaches e.g. automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS4, Amal- gam5, Connexor6) are available, for example, in the application of part-of-speech tags to corpora.', 'Existing tagging systems are �small scale� and typically impose some limitation to prevent over- load (e.g. restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems.', 'This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', 'Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist�s Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).']"	0
CC1527	W06-1705	Annotated web as corpus	automatic profiling of learner texts	"['S Granger', 'P Rayson']"	introduction	"In this chapter Crystal's (1991) notion of 'profiling', i.e. the identification of the most salient features in a particular person (clinical linguistics) or register (stylistics), is applied to the field of interlanguage studies. Starting from the assumption that every interlanguage is characterized by a 'unique matrix of frequencies of various linguistic forms' (Krzeszowski 1990: 212), we have submitted two similar-sized corpora of native and non-native writing to a lexical frequency software program to uncover some of the distinguishing features of learner writing. The non-native speaker corpus is taken from the International Corpus of Learner English (ICLE) database. It consists of argumentative essay writing by advanced French-speaking learners of English. The control corpus of similar writing is taken from the Louvain Corpus of Native English Essays (LOCNESS) database. Though limited to one specific type of interlanguage, the approach presented here is applicable to any learner variety and demonstrates a potential of automatic profiling for revealing the stylistic characteristics of EFL texts. In the present study, the learner data is shown to display many of the stylistic features of spoken, rather than written, English."	"In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) ."	"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']"	0
CC1528	W06-1705	Annotated web as corpus	an introduction to corpus linguistics	['G Kennedy']	introduction	"On first looking into a corpus, teachers as well as students may well be blinded by the sheer scale of the resource and by the possibilities for research that it offers. As the author of this book puts it, ""The research topics in a machine-readable cor pus are potentially as various and wide ranging as are the facts about a language and the use of that language"" (274). The value of this book is that it provides practical examples of the range of research possibilities that a corpus offers, as well as indi cating how corpus-based research projects may be undertaken. It is informed throughout by the view that corpus linguistics is not a separate branch of linguistics, but rather ""descriptive linguistics aided by new technology"" (268). There is some theoretical discussion of the place of corpus linguistics in the wider field, but in general, the author's approach is to let the results speak for themselves. The author's declared aim is to whet the appetites of teachers and students, and in this he clearly succeeds. In the ""Introduction,"" the author suggests that some readers might usefully begin with chapter 3, ""Corpus-Based Descriptions of English."" This is the central part of the book and by far the most valuable in terms of whetting the appetite. It consists of a very comprehensive and wide-ranging review of previous corpus-based research, divided into the following sections: lexical description, grammatical studies cen tered on morphemes or words, grammatical studies centered on the sentence, pragmatics and spoken discourse, and studies of variation. The first section of the chapter investigates how computerized corpora are increasingly being used in lexi cography and continues with a review of collocational studies based on the LOB corpus, as well as work by Sinclair and Renouf on collocational frameworks. Under ""word-centered"" grammatical studies, previous work on modals, voice, aspect, the subjunctive, as well as prepositions and conjunctions are very comprehensively ex emplified and described. The list continues. The latter sections of this chapter re view work by Kuc*era and Francis on sentence length, Altenberg on verb complementation, Mair on nonfinite complementation, Meyer on apposition,"	"In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) ."	"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']"	0
CC1529	W06-1705	Annotated web as corpus	distributed video encoding over a peertopeer network	"['D Hughes', 'J Walkerdine']"	method	"How does the work advance the state-of-the-art?: Current video encoding technologies tend to focus on single machine solutions, while little or no work on distributed video encoding systems has been undertaken. Current work on distributed computation over peer-to-peer networks primarily focuses upon systems with heavily centralised control [1] [2]. The Distributed Video Encoder is a novel example of fully decentralized ad-hoc distributed computation."	"This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) ."	"['The second stage of our work will involve im- plementing the framework within a P2P environment.', ""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .', 'It is our intention to implement our distributed corpus annotation framework as a plug- in.', 'This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS11).', 'The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.']"	0
CC1530	W06-1705	Annotated web as corpus	listening to napster in peertopeer harnessing the power of disruptive technologies	['C Shirky']	related work		"In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) ."	"['In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for data-intensive NLP and text-mining for e-Science (Carroll et al., 2005;Hughes et al, 2004).', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.', 'Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).']"	0
CC1531	W06-1705	Annotated web as corpus	p2p4dl digital library over peertopeer	"['J Walkerdine', 'P Rayson']"	method		"This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) ."	"['The second stage of our work will involve implementing the framework within a P2P environment.', ""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .', 'It is our intention to implement our distributed corpus annotation framework as a plugin.', 'This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ).', 'The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.']"	0
CC1532	W06-1705	Annotated web as corpus	the biggest corpus of allquot	['M Rundell']	related work		"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) ."	"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]"	0
CC1533	W06-1705	Annotated web as corpus	introduction to the special issue on the web as corpus	"['A Kilgarriff', 'G Grefenstette']"	related work	"The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists' playground. This special issue of Computational Linguistics explores ways in which this dream is being explored."	"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) ."	"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]"	0
CC1534	W06-1705	Annotated web as corpus	bootcat bootstrapping corpora and terms from the web in	"['M Baroni', 'S Bernardini']"	related work	"This paper introduces the BootCaT toolkit, a suite of perl programs implementing an iterative procedure to bootstrap specialized corpora and terms from the web. The procedure requires only a small set of seed terms as input. The seeds are used to build a corpus via automated Google queries, and more terms are extracted from this corpus. In turn, these new terms are used as seeds to build a larger corpus via automated queries, and so forth. The corpus and the unigram terms are then used to extract multi-word terms. We conducted an evaluation of the tools by applying them to the construction of English and Italian corpora and term lists from the domain of psychiatry. The results illustrate the potential usefulness of the tools."	#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .	"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]"	0
CC1535	W06-1705	Annotated web as corpus	creating specialized and general corpora using automated search engine queries web as corpus workshop	"['M Baroni', 'S Sharoff']"	introduction		"Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) ."	"['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']"	0
CC1536	W06-1705	Annotated web as corpus	using the web to overcome data sparseness	"['F Keller', 'M Lapata', 'O Ourioupina']"	introduction	"This paper shows that the web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the web by querying a search engine. We evaluate this method by demonstrating that web frequencies and correlate with frequencies obtained from a carefully edited, balanced corpus. We also perform a task-based evaluation, showing that web frequencies can reliably predict human plausibility judgments."	"Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) ."	"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']"	0
CC1537	W06-1705	Annotated web as corpus	finding syntactic structure in unparsed corpora the gsearch corpus query system computers and the humanities	"['S Corley', 'M Corley', 'F Keller', 'M Crocker', 'S Trewin']"	related work		The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .	"['""Real-time"" linguistic analysis of web data at the syntactic level has been piloted by the Linguist\'s Search Engine (LSE).', 'Using this tool, linguists can either perform syntactic searches via parse trees on a pre-analysed web collection of around three million sentences from the Internet Archive (www.archive.org)', 'or build their own collections from AltaVista search engine results.', 'The second method pushes the new collection onto a queue for the LSE annotator to analyse.', 'A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.', 'In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora.', ""A word sketch is an automatic one-page corpus-derived summary of a word's grammatical and collocational behaviour."", 'Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell).', 'They have also served as the starting point for high-accuracy Word Sense Disambiguation.', 'More recently, the Sketch Engine was used to develop the new edition of the Oxford Thesaurus of English (2004, edited by Maurice Waite).']"	0
CC1538	W06-1705	Annotated web as corpus	introduction to the special issue on evaluating word sense disambiguation systems	"['P Edmonds', 'A Kilgarriff']"	introduction	"Has system performance on Word Sense Disambiguation (WSD) reached a limit? Automatic systems don't perform nearly as well as humans on the task, and from the results of the SENSEVAL exercises, recent improvements in system performance appear negligible or even negative. Still, systems do perform much better than the baselines, so something is being done right. System evaluation is crucial to explain these results and to show the way forward. Indeed, the success of any project in WSD is tied to the evaluation methodology used, and especially to the formalization of the task that the systems perform. The evaluation of WSD has turned out to be as difficult as designing the systems in the first place."	"Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages ."	"['Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']"	0
CC1539	W06-1705	Annotated web as corpus	web as corpus	['A Kilgarriff']	related work		"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) ."	"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g. automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS4, Amal- gam5, Connexor6) are available, for example, in the application of part-of-speech tags to corpora.', 'Existing tagging systems are �small scale� and typically impose some limitation to prevent over- load (e.g. restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', 'Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist�s Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).']"	0
CC1540	W06-1705	Annotated web as corpus	word sense disambiguation by web mining for word cooccurrence probabilities	['P Turney']	related work	"This paper describes the National Research Council (NRC) Word Sense Disambiguation (WSD) system, as applied to the English Lexical Sample (ELS) task in Senseval-3. The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill's rule-based part-of-speech tagger. Head words are represented as feature vectors with several hundred features. Approximately half of the features are syntactic and the other half are semantic. The main novelty in the system is the method for generating the semantic features, based on word co-occurrence probabilities. The probabilities are estimated using the Waterloo MultiText System with a corpus of about one terabyte of unlabeled text, collected by a web crawler."	#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .	"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]"	0
CC1541	W06-1705	Annotated web as corpus	facilitating the compilation and dissemination of adhoc web corpora	['W H Fletcher']		"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t"	"Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) ."	"['We will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'The initial solution will be to store the resulting reference 11 http://www.comp.lancs.ac.uk/ucrel/claws/ corpus in the Sketch Engine.', 'We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general.', 'Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .', 'Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.']"	0
CC1542	W06-1705	Annotated web as corpus	the corpusbased study of language change in progress the extra value of tagged corpora	['C Mair']	introduction		"In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) ."	"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']"	0
CC1543	W06-1705	Annotated web as corpus	blueprint for a high performance nlp infrastructure	['J R Curran']	related work	"Natural Language Processing (NLP) system de-velopers face a number of new challenges. In-terest is increasing for real-world systems that use NLP tools and techniques. The quantity of text now available for training and processing is increasing dramatically. Also, the range of languages and tasks being researched contin-ues to grow rapidly. Thus it is an ideal time to consider the development of new experimen-tal frameworks. We describe the requirements, initial design and exploratory implementation of a high performance NLP infrastructure."	#AUTHOR_TAG	['#AUTHOR_TAG']	0
CC1544	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	free culture how big media uses technology and the law to lock down culture and control creativity	['Lawrence Lessig']		"espanolEl presente texto publicado en la Zona REMIX de la revista Communiars se corresponde con la introduccion del afamado libro Cultura Libre, del profesor Lawrence Lessig, presidente de la organizacion Creative Commons, dedicada a promover el acceso e intercambio culturales. El texto > (en espanol Cultura Libre. Como los grandes medios usan la tecnologia y la ley para bloquear la cultura y controlar la creatividad) es un libro publicado en 2004 y centrado en presentar otra manera de organizar la cultura y el conocimiento, abriendo las restricciones del obsoleto paradigma del copyright, y apoyandose en el modelo copyleft promovido desde el software libre. La introduccion que aqui se presenta traduce el espiritu abierto de un texto clave para la comprension y evolucion de la actualidad cultural. La version que se publica procede la version PDF de Free Culture, licenciada bajo Creative Commons en su variante BY-NC 1.0. EnglishThe present text published in the REMIX Zone of the Communiars Journal corresponds to the introduction of the famous book Free Culture, by Professor Lawrence Lessig, president of the Creative Commons organization, dedicated to promoting cultural access and exchange. The text Free Culture. How big media uses technology and the law to lock down culture and control creativity is a book published in 2004 and focused on presenting another way of organizing culture and knowledge, opening the restrictions of the obsolete paradigm of copyright, and relying on the copyleft model promoted by free software. The introduction presented here translates the open spirit of a key text for the understanding and evolution of current cultural reality. The version that is published is part of PDF version of Free Culture, licensed under Creative Commons in its variant BY-NC 1.0."	We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .	"['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001).']"	5
CC1545	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	hypertext 20 the convergence of contemporary critical theory and technology the johns hopkins	['George P Landow']	related work		Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .	"['Apart from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .', 'Storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""However, in our opinion Storyspace is a product of its time and in fact it isn't a web application."", 'Although it is possible to label links, it lacks a lot of features we need.', 'Moreover, no hypertext writing tool available is released under an open source licence.', 'We hope that Novelle will bridge this gap -we will choose the exact licence when our first public release is ready.']"	0
CC1546	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	hypertext 20 the convergence of contemporary critical theory and technology the johns hopkins	['George P Landow']	introduction		"Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."	"[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."", 'Consequently, a hypertext is a set of lexias.', 'In hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'The main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992):']"	5
CC1547	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	understanding comics	['Scott McCloud']	introduction	"During the spring semester of 2010, as part of my graduate program in English Education, I took a class titled American Comic Book. I took what I learned there, turned around, and immediately applied it to my own teaching. I teach 7th grade language arts and developed a unit on understanding and creating comics, pulling from what I was learning in the class at the University of Iowa, and utilized some other resources including Great Source u27s Daybook of Critical Reading and Writing, and ideas from other books on using graphic novels as a teaching tool. The unit was taught during April and May of this year. I have collected my lesson plans, examples of student work, and much, much more on a website, http://sites.google.com/site/7thgradecomicsunit/"	"For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) ."	"['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) ."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']"	0
CC1548	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	free software free society selected essays of	['Richard M Stallman']		"Please contact the GNU Press for information regarding bulk purchases for classroom or user group use, reselling, or any other questions or comments. Permission is granted to make and distribute verbatim copies of this book provided the copyright notice and this permission notice are preserved on all copies. Permission is granted to copy and distribute translations of this book into another language, from the original English, with respect to the conditions on distribution of modified versions above, provided that it has been approved by the Free Software Foundation. The waning days of the 20th century seemed like an Orwellian nightmare: laws preventing publication of scientific research on software; laws preventing sharing software; an overabundance of software patents preventing development; and end-user license agreements that strip the user of all freedoms--including ownership, privacy, sharing, and understanding how their software works. This collection of essays and speeches by Richard M. Stallman addresses many of these issues. Above all, Stallman discusses the philosophy underlying the free software movement. This movement combats the oppression of federal laws and evil end-user license agreements in hopes of spreading the idea of software freedom. With the force of hundreds of thousands of developers working to create GNU software and the GNU/Linux operating system, free software has secured a spot on the servers that control the Internet, and--as it moves into the desktop computer market--is a threat to Microsoft and other proprietary software companies. These essays cater to a wide audience; you do not need a computer science background to understand the philosophy and ideas herein. However, there is a "" Note on Software, "" to help the less technically inclined reader become familiar with some common computer science jargon and concepts, as well as footnotes throughout. Many of these essays have been updated and revised from their originally published version. Each essay carries permission to redistribute verbatim copies. The ordering of the essays is fairly arbitrary, in that there is no required order to read the essays in, for they were written independently of each other over a period of 18 years. The first section, "" The GNU Project and Free Software, "" is intended to familiarize you with the history and philosophy of free software and the GNU project. Furthermore, it provides a road map for developers, educators, and business people to pragmatically incorporate free software into society, business, and life. The second section, "" Copyright, ..."	"Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) ."	"['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']"	0
CC1549	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	wikipedia from wikipedia the free encyclopedia	['Wikipedia']		"administrator, born in Campinas, Brazil. He holds a doctoral degree in physiology from the University of Sao Paulo. His main contributions as a researcher and publisher concern the following areas[1]: neuroethology; medical informatics; health sciences; internet and web applications in medicine, biology and health; ; artificial neural networks; distance education and e-learning; telemedicine; biological and health impacts of non-ionizing radiation and history of neuroscience. He is currently the president of the Edumed Institute, a non-profit R&amp;D institution and director of Edulogica Educacao &amp; Tecnologia, a consultant and commercial enterprise specializing in distance education. artificial intelligenc Research and education Sabbatini began his scientific career in neurophysiology in 1966, while he was a medical student at the Medical School of the University of Sao Paulo at Ribeirao Preto [2]. He began to work in basic biomedical research under the supervision of Prof. Miguel Rolando Covian, an Argentine neurophysiologist, who encouraged him to found, after he graduated in 1968, the first research laboratory of neuroethology in Latin America [3] , at the Department of Physiology. Also there, he started in 1970 one of the first Brazilian and Latin American groups of research, development and education on the computer applications in biomedicine. Sabbatini got a doctorate in behavioral neuroscience in 1977 and immediately thereafter went to spend two and a half years doing postdoctoral work i"	"On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG ."	"['The emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre (McNeill, 2005).', 'Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'So blogs are a literary metagenre which started as authored personal diaries or journals.', ""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .', 'Wikipedia (2005).', 'Now personal wiki tools are arising for brainstorming and mind mapping.', 'See Section 4 for further aspects.']"	0
CC1550	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	free culture how big media uses technology and the law to lock down culture and control creativity	['Lawrence Lessig']		"espanolEl presente texto publicado en la Zona REMIX de la revista Communiars se corresponde con la introduccion del afamado libro Cultura Libre, del profesor Lawrence Lessig, presidente de la organizacion Creative Commons, dedicada a promover el acceso e intercambio culturales. El texto > (en espanol Cultura Libre. Como los grandes medios usan la tecnologia y la ley para bloquear la cultura y controlar la creatividad) es un libro publicado en 2004 y centrado en presentar otra manera de organizar la cultura y el conocimiento, abriendo las restricciones del obsoleto paradigma del copyright, y apoyandose en el modelo copyleft promovido desde el software libre. La introduccion que aqui se presenta traduce el espiritu abierto de un texto clave para la comprension y evolucion de la actualidad cultural. La version que se publica procede la version PDF de Free Culture, licenciada bajo Creative Commons en su variante BY-NC 1.0. EnglishThe present text published in the REMIX Zone of the Communiars Journal corresponds to the introduction of the famous book Free Culture, by Professor Lawrence Lessig, president of the Creative Commons organization, dedicated to promoting cultural access and exchange. The text Free Culture. How big media uses technology and the law to lock down culture and control creativity is a book published in 2004 and focused on presenting another way of organizing culture and knowledge, opening the restrictions of the obsolete paradigm of copyright, and relying on the copyleft model promoted by free software. The introduction presented here translates the open spirit of a key text for the understanding and evolution of current cultural reality. The version that is published is part of PDF version of Free Culture, licensed under Creative Commons in its variant BY-NC 1.0."	Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .	"['If a user lets others edit some lexias, he has the right to retain or refuse the attribution when other users have edited it.', 'In the first instance, the edited version simply moves ahead the document history.', 'In the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .', 'If nobody claims the document for himself, it will fall in the public domain.', 'The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain.', ""If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author's work."", 'So as to come to terms with this idea, we need a concept invented by Nelson (1992), i.e. transclusion.', ""Rather than copy-and-paste contents from a lexia, a user may recall a quotation of the author's lexia and write a comment in the surroundings."", ""In doing so, the link list of the author's lexia will be updated with a special citation link marker, called quotation link (see later for details)."", ""Usually, the quotation will be 'frozen', as in the moment where it was transcluded (see Figure 3)."", 'Consequently the transclusion resembles a copiedand-pasted text chunk, but the link to the original document will always be consistent, i.e. neither it expires nor it returns an error.', 'Otherwise the user who has transcluded the quotation may choose to keep updated the links to the original document.', 'This choice has to be made when the transclusion is done.', 'If so, the transcluded quotation will update automatically, following the history timeline of the original document.', 'For example, if the original document changes topic from stars to pentagons, the quotation transcluded will change topic too (see Figure 4).']"	0
CC1551	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	the wiki way  quick collaboration on the web	"['Ward Cunningham', 'Bo Leuf']"	related work	"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001"	"While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model ."	"['The main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal.']"	0
CC1552	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	free culture how big media uses technology and the law to lock down culture and control creativity	['Lawrence Lessig']		"espanolEl presente texto publicado en la Zona REMIX de la revista Communiars se corresponde con la introduccion del afamado libro Cultura Libre, del profesor Lawrence Lessig, presidente de la organizacion Creative Commons, dedicada a promover el acceso e intercambio culturales. El texto > (en espanol Cultura Libre. Como los grandes medios usan la tecnologia y la ley para bloquear la cultura y controlar la creatividad) es un libro publicado en 2004 y centrado en presentar otra manera de organizar la cultura y el conocimiento, abriendo las restricciones del obsoleto paradigma del copyright, y apoyandose en el modelo copyleft promovido desde el software libre. La introduccion que aqui se presenta traduce el espiritu abierto de un texto clave para la comprension y evolucion de la actualidad cultural. La version que se publica procede la version PDF de Free Culture, licenciada bajo Creative Commons en su variante BY-NC 1.0. EnglishThe present text published in the REMIX Zone of the Communiars Journal corresponds to the introduction of the famous book Free Culture, by Professor Lawrence Lessig, president of the Creative Commons organization, dedicated to promoting cultural access and exchange. The text Free Culture. How big media uses technology and the law to lock down culture and control creativity is a book published in 2004 and focused on presenting another way of organizing culture and knowledge, opening the restrictions of the obsolete paradigm of copyright, and relying on the copyleft model promoted by free software. The introduction presented here translates the open spirit of a key text for the understanding and evolution of current cultural reality. The version that is published is part of PDF version of Free Culture, licensed under Creative Commons in its variant BY-NC 1.0."	"We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #AUTHOR_TAG ) ."	"['With our typology of links, we aim to solve the framing problem as defined in Section 1.2.', 'We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily.', 'We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #AUTHOR_TAG ) .']"	5
CC1553	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	genre under construction the diary on the internet	['Laurie McNeill']	related work	"The emergence of the diary as a digital form has generated the kinds of introduction and explanation that typically accumulate around emerging genres, even though online diarists in many ways strive to reproduce the stereotypical print diary. However, as diarists and readers explore the nature of blogs, both in diary entries and comments pages, a tension is apparent between users ' accounts or explanations of the genre and their actual practices, and this tension provides a rich site for studying the evolution of the diary genre. Readers ' and writers ' comments illustrate the blogging community's ideas about genre as a concept and how these ideas transfer to the ""new "" world of online media. In this paper, I look at the diary's transition from page to screen, and consider how readers and writers build on and diverge from print culture practices in establishing expectations and ""rules "" for Weblogs. Examining how diarists and their communities establish and police the digital diary, and how generic knowledge is circulated and codified, helps understand the particular social actions the diary can perform only on the Internet. In contemporary Western culture the diary is among the most familiar genres, an enduring everyday form that has evolved and adapted across time yet has remained recognizable in each instance. As a genre that straddles and confuses ideals of the literary and non-literary"	"Generally speaking , we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context ."	"['Generally speaking , we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context .', ""The only way to retrieve information is through a search engine or a calendar, i.e. the date of the 'post' -a lexia in the jargon of bloggers.""]"	0
CC1554	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	genre under construction the diary on the internet	['Laurie McNeill']		"The emergence of the diary as a digital form has generated the kinds of introduction and explanation that typically accumulate around emerging genres, even though online diarists in many ways strive to reproduce the stereotypical print diary. However, as diarists and readers explore the nature of blogs, both in diary entries and comments pages, a tension is apparent between users ' accounts or explanations of the genre and their actual practices, and this tension provides a rich site for studying the evolution of the diary genre. Readers ' and writers ' comments illustrate the blogging community's ideas about genre as a concept and how these ideas transfer to the ""new "" world of online media. In this paper, I look at the diary's transition from page to screen, and consider how readers and writers build on and diverge from print culture practices in establishing expectations and ""rules "" for Weblogs. Examining how diarists and their communities establish and police the digital diary, and how generic knowledge is circulated and codified, helps understand the particular social actions the diary can perform only on the Internet. In contemporary Western culture the diary is among the most familiar genres, an enduring everyday form that has evolved and adapted across time yet has remained recognizable in each instance. As a genre that straddles and confuses ideals of the literary and non-literary"	"The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #AUTHOR_TAG ) ."	"['The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #AUTHOR_TAG ) .', 'Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'So blogs are a literary metagenre which started as authored personal diaries or journals.', ""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side, wikis started as collective works where each entry is not owned by a single author -e.g.', 'Wikipedia (2005).', 'Now personal wiki tools are arising for brainstorming and mind mapping.', 'See Section 4 for further aspects.']"	0
CC1555	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	the printing revolution in early modern europe	['Elizabeth L Eisenstein']	introduction	"What difference did printing make? Although the importance of the advent of printing for the Western world has long been recognized, it was Elizabeth Eisenstein in her monumental, two-volume work, The Printing Press as an Agent of Change, who provided the first full-scale treatment of the subject. This illustrated and abridged edition provides a stimulating survey of the communications revolution of the fifteenth century. After summarizing the initial changes, and introducing the establishment of printing shops, it considers how printing effected three major cultural movements: the Renaissance, the Reformation, and the rise of modern science. First Edition Hb (1984) 0-521-25858-8 First Edition Pb (1984) 0-521-27735-3"	"For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG ) ."	"['Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Terms as �chapter�, �page� or �foot- note� simply become meaningless in the new texts, or they highly change their meaning.', 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG ) ."", 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', 'For example, a �web page� is more similar to an infinite canvas than a written page (McCloud, 2001).', 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', 'From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an �opera aperta� (open work), as Eco would define it (1962).', 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.', 'They give more power to readers, eventually filling the gap - the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text? Which role is suitable for authors? We have to analyse them before presenting the architecture of Novelle.']"	0
CC1556	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	writing space the computer hypertext and the history of writing erlbaum associates	['Jay David Bolter']	introduction	Getting the books writing space the computer hypertext and the history of writing now is not type of challenging means. You could not solitary going bearing in mind books deposit or library or borrowing from your friends to gain access to them. This is an totally easy means to specifically acquire lead by on-line. This online declaration writing space the computer hypertext and the history of writing can be one of the options to accompany you gone having new time.	1.1 Hypertext as a New Writing Space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.	"['1.1 Hypertext as a New Writing Space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example, a 'web page' is more similar to an infinite canvas than a written page (McCloud, 2001)."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']"	0
CC1557	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	wikipedia from wikipedia the free encyclopedia	['Wikipedia']		"administrator, born in Campinas, Brazil. He holds a doctoral degree in physiology from the University of Sao Paulo. His main contributions as a researcher and publisher concern the following areas[1]: neuroethology; medical informatics; health sciences; internet and web applications in medicine, biology and health; ; artificial neural networks; distance education and e-learning; telemedicine; biological and health impacts of non-ionizing radiation and history of neuroscience. He is currently the president of the Edumed Institute, a non-profit R&amp;D institution and director of Edulogica Educacao &amp; Tecnologia, a consultant and commercial enterprise specializing in distance education. artificial intelligenc Research and education Sabbatini began his scientific career in neurophysiology in 1966, while he was a medical student at the Medical School of the University of Sao Paulo at Ribeirao Preto [2]. He began to work in basic biomedical research under the supervision of Prof. Miguel Rolando Covian, an Argentine neurophysiologist, who encouraged him to found, after he graduated in 1968, the first research laboratory of neuroethology in Latin America [3] , at the Department of Physiology. Also there, he started in 1970 one of the first Brazilian and Latin American groups of research, development and education on the computer applications in biomedicine. Sabbatini got a doctorate in behavioral neuroscience in 1977 and immediately thereafter went to spend two and a half years doing postdoctoral work i"	"In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #AUTHOR_TAG ) ."	"['AJAX is not a technology in itself but a term that refers to the use of a group of technologies together, in particular Javascript and XML.', 'In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #AUTHOR_TAG ) .']"	0
CC1558	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	learning creating and using knowledge concept maps as facilitative tools in schools and corporations lawrence erlbaum associates	['Joseph Donald Novak']	related work		"Every arc always has a definite direction , i.e. arcs are arrows ( #AUTHOR_TAG ) ."	"[""Concept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel."", 'Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'Every arc always has a definite direction , i.e. arcs are arrows ( #AUTHOR_TAG ) .']"	0
CC1559	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	ruby on rails web developement that doesn’t hurt url httpwwwrubyonrailsorg retrieved the 03rd of january	['Ruby on Rails']			The Ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes .	['The Ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes .']	5
CC1560	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	the wiki way  quick collaboration on the web	"['Ward Cunningham', 'Bo Leuf']"	introduction	"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001"	"Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) ."	"['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.', 'Consequently, except in the case of sandboxes, every change in the document cannot be erased.', 'This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself.']"	0
CC1561	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	genre under construction the diary on the internet	['Laurie McNeill']	introduction	"The emergence of the diary as a digital form has generated the kinds of introduction and explanation that typically accumulate around emerging genres, even though online diarists in many ways strive to reproduce the stereotypical print diary. However, as diarists and readers explore the nature of blogs, both in diary entries and comments pages, a tension is apparent between users ' accounts or explanations of the genre and their actual practices, and this tension provides a rich site for studying the evolution of the diary genre. Readers ' and writers ' comments illustrate the blogging community's ideas about genre as a concept and how these ideas transfer to the ""new "" world of online media. In this paper, I look at the diary's transition from page to screen, and consider how readers and writers build on and diverge from print culture practices in establishing expectations and ""rules "" for Weblogs. Examining how diarists and their communities establish and police the digital diary, and how generic knowledge is circulated and codified, helps understand the particular social actions the diary can perform only on the Internet. In contemporary Western culture the diary is among the most familiar genres, an enduring everyday form that has evolved and adapted across time yet has remained recognizable in each instance. As a genre that straddles and confuses ideals of the literary and non-literary"	"Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing ."	"['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example, a 'web page' is more similar to an infinite canvas than a written page (McCloud, 2001)."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']"	0
CC1562	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	the wiki way  quick collaboration on the web	"['Ward Cunningham', 'Bo Leuf']"		"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001"	"The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) ."	"['On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'In fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'Generally, people avoid commenting, preferring to edit each document.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"	0
CC1563	W06-2807	Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein	ajax a new approach to web applications url httpwwwadaptivepathcompublicationsessays archives000385php retrieved the 22nd of december	['Jesse James Garrett']			AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .	"['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']"	0
CC1564	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	floresta sint´actica” a treebank for portuguese	"['S Afonso', 'E Bick', 'R Haber', 'D Santos']"	experiments		"This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively ."	"['A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.', 'A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures.', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']"	1
CC1565	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	pseudoprojective dependency parsing	"['J Nivre', 'J Nilsson']"	introduction		â¢ Graph transformations for recovering nonprojective structures ( #AUTHOR_TAG ) .	"['• A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '• History-based feature models for predicting the next parser action (Black et al., 1992).', '• Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', 'â\x80¢ Graph transformations for recovering nonprojective structures ( #AUTHOR_TAG ) .']"	5
CC1566	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	the tiger treebank	"['S Brants', 'S Dipper', 'S Hansen', 'W Lezius', 'G Smith']"	experiments	"Proceedings of the 16th Nordic Conference   of Computational Linguistics NODALIDA-2007.  Editors: Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit.  University of Tartu, Tartu, 2007.  ISBN 978-9985-4-0513-0 (online)  ISBN 978-9985-4-0514-7 (CD-ROM)  pp. 81-88"	"This is noticeable for German ( #AUTHOR_TAG ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively ."	"['A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German ( #AUTHOR_TAG ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.', 'A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures.', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']"	1
CC1567	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	memorybased dependency parsing	"['J Nivre', 'J Hall', 'J Nilsson']"	method		The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by #AUTHOR_TAG .	['The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by #AUTHOR_TAG .']	5
CC1568	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	mamba meets tiger reconstructing a swedish treebank from antiquity	"['J Nilsson', 'J Hall', 'J Nivre']"	experiments		"Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( #AUTHOR_TAG ) ."	"['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( #AUTHOR_TAG ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']"	0
CC1569	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	design and implementation of the bulgarian hpsgbased treebank	"['K Simov', 'P Osenova', 'A Simov', 'M Kouylekov']"	experiments		"Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) ."	"['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']"	0
CC1570	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	sinica treebank design criteria representational issues and implementation	"['K Chen', 'C Luo', 'M Chang', 'F Chen', 'C Chen', 'C Huang', 'Z Gao']"	experiments	"The disclosed apparatus measures the mass rate of flow of gas through an orifice of a flow nozzle under critical flow conditions and comprises a reservoir having an inlet connected to a source of high pressure through a first valve and an outlet connected to a flow nozzle adapted to conduct pressurized gas from said reservoir under critical flow conditions to a region of low pressure downstream of the flow nozzle. The critical flow conditions are established by a second valve mounted downstream of the nozzle orifice. Density and pressure measuring devices are coupled to the reservoir and provide signals representative of the density and pressure, respectively, of the gas flowing through the flow nozzle. The density and pressure signals are applied to a device which calculates the square root of the product of density and pressure to provide a signal proportional to the mass rate of gas flow."	"Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #AUTHOR_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) ."	"['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #AUTHOR_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']"	0
CC1571	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	towards historybased grammars using richer models for probabilistic parsing	"['E Black', 'F Jelinek', 'J D Lafferty', 'D M Magerman', 'R L Mercer', 'S Roukos']"	introduction	"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."	â¢ History-based feature models for predicting the next parser action ( #AUTHOR_TAG ) .	"['• A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'â\x80¢ History-based feature models for predicting the next parser action ( #AUTHOR_TAG ) .', '• Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', '• Graph transformations for recovering nonprojective structures .']"	5
CC1572	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	maltparser a datadriven parsergenerator for dependency parsing	"['J Nivre', 'J Hall', 'J Nilsson']"	introduction		"All experiments have been performed using MaltParser ( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1"	"['All experiments have been performed using MaltParser ( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1']"	5
CC1573	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	libsvm a library for support vector machines software available at httpwwwcsientuedutw cjlinlibsvm	"['C-C Chang', 'C-J Lin']"	method		"More specifically , we use LIBSVM ( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification ."	"['We use support vector machines to predict the next parser action from a feature vector representing the history.', 'More specifically , we use LIBSVM ( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .', 'Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4', 'or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003).', 'To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t.']"	5
CC1574	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	headdriven statistical models for natural language parsing	['M Collins']	experiments	"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."	6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #AUTHOR_TAG ) .	['6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #AUTHOR_TAG ) .']	1
CC1575	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	pseudoprojective dependency parsing	"['J Nivre', 'J Nilsson']"	method		"Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG ."	"['Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG .']"	0
CC1576	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	statistical dependency analysis with support vector machines	"['H Yamada', 'Y Matsumoto']"	method	"In this paper, we propose a method for analyzing word-word dependencies using deterministic bottom-up manner using Support Vector machines. We experimented with dependency trees converted from Penn treebank data, and achieved over 90% accuracy of word-word dependency. Though the result is little worse than the most up-to-date phrase structure based parsers, it looks satisfactorily accurate considering that our parser uses no information from phrase structures."	"For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ) ."	"['For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ) .', 'To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t.']"	0
CC1577	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	practical annotation scheme for an hpsg treebank of bulgarian in	"['K Simov', 'P Osenova']"	experiments	"The paper presents an HPSG-based annotation scheme for constructing a Bulgarian treebank: BulTreeBank. It differs from other grammar-based annotation schemes in having a hybrid status with respect to the partial parsing component and the full parsing module. As the parsing complexity is handled preferably by the pre-processing step, the task of the HPSG module is maximally facilitated and simplified."	"Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) ."	"['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']"	0
CC1578	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	stylebook for the japanese treebank in verbmobil verbmobilreport 240 seminar f¨ur sprachwissenschaft	"['Y Kawata', 'J Bartels']"	experiments		"Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances ."	"['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian (Simov et al., 2005;Simov and Osenova, 2003), Chinese (Chen et al., 2003), Danish (Kromann, 2003), and Swedish .', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']"	1
CC1579	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	japanese dependency analysis using cascaded chunking	"['T Kudo', 'Y Matsumoto']"	introduction	"In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model. Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable. We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency."	Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .	"['• A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '• History-based feature models for predicting the next parser action (Black et al., 1992).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .', '• Graph transformations for recovering nonprojective structures .']"	5
CC1580	W06-2933	Labeled pseudo-projective dependency parsing with support vector machines	the annotation process in the turkish treebank	"['N B Atalay', 'K Oflazer', 'B Say']"	experiments	"We present a progress report of the Turkish Treebank concentrating on various aspects of its design and implementation. In addition to a review of the corpus compilation process and the design  of the annotation scheme, we describe the details of various pre-processing stages and the computer-assisted annotation process"	"By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) ."	"['The results for Arabic (Hajič et al., 2004;Smrž et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"	1
CC1581	W06-3309	Generative content models for structural analysis of medical abstracts	categorization of sentence types in medical abstracts	"['Larry McKnight', 'Padmini Srinivasan']"	introduction	"This study evaluated the use of machine learning techniques in the classification of sentence type. 7253 structured abstracts and 204 unstructured abstracts of Randomized Controlled Trials from MedLINE were parsed into sentences and each sentence was labeled as one of four types (Introduction, Method, Result, or Conclusion). Support Vector Machine (SVM) and Linear Classifier models were generated and evaluated on cross-validated data. Treating sentences as a simple ""bag of words"", the SVM model had an average ROC area of 0.92. Adding a feature of relative sentence location improved performance markedly for some models and overall increasing the average ROC to 0.95. Linear classifier performance was significantly worse than the SVM in all datasets. Using the SVM model trained on structured abstracts to predict unstructured abstracts yielded performance similar to that of models trained with unstructured abstracts in 3 of the 4 types. We conclude that classification of sentence type seems feasible within the domain of RCT's. Identification of sentence types may be helpful for providing context to end users or other text summarization techniques."	#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1582	W06-3309	Generative content models for structural analysis of medical abstracts	semiautomatic indexing of full text biomedical articles	"['Clifford W Gay', 'Mehmet Kayaalp', 'Alan R Aronson']"	introduction	"The main application of U.S. National Library of Medicine's Medical Text Indexer (MTI) is to provide indexing recommendations to the Library's indexing staff. The current input to MTI consists of the titles and abstracts of articles to be indexed. This study reports on an extension of MTI to the full text of articles appearing in online medical journals that are indexed for Medline. Using a collection of 17 journal issues containing 500 articles, we report on the effectiveness of the contribution of terms by the whole article and also by each section. We obtain the best results using a model consisting of the sections Results, Results and Discussion, and Conclusions together with the article's title and abstract, the captions of tables and figures, and sections that have no titles. The resulting model provides indexing significantly better (7.4%) than what is currently achieved using only titles and abstracts."	"For example , #AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example , #AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1583	W06-3309	Generative content models for structural analysis of medical abstracts	modern applied statistics with splus	"['William N Venables', 'Brian D Ripley']"	method		"#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space ."	"['In an attempt to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the four-dimensional vec- tors that maximizes the separation of the Gaussians (corresponding to the HMM states).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .', 'Each sentence/vector is then mul- tiplied by this matrix, and new HMM models are re-computed from the projected data.']"	5
CC1584	W06-3309	Generative content models for structural analysis of medical abstracts	knowledge extraction for clinical question answering preliminary results	"['Dina Demner-Fushman', 'Jimmy Lin']"	conclusion	"The combination of recent developments in question an-swering research and the unparalleled resources devel-oped specifically for automatic semantic processing of text in the medical domain provides a unique opportu-nity to explore complex question answering in the clin-ical domain. In this paper, we attempt to operationalize major aspects of evidence-based medicine in the form of knowledge extractors that serve as the fundamental building blocks of a clinical question answering sys-tem. Our evaluations demonstrate that domain-specific knowledge can be effectively leveraged to extract PICO frame elements from MEDLINE abstracts. Clinical in-formation systems in support of physicians ' decision-making process have the potential to improve the qual-ity of patient care in real-world settings"	"Such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 ) ."	"['Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled.', 'The true utility of content models is to structure abstracts that have no structure to begin with.', 'Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance.', 'Such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 ) .', 'We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.']"	3
CC1585	W06-3309	Generative content models for structural analysis of medical abstracts	on discriminative vs generative classifiers a comparison of logistic regression and naive bayes	"['Andrew Y Ng', 'Michael Jordan']"	introduction	"Comparison of generative and discriminative classifiers is an ever-lasting topic. As an important contribution to this topic, based on their theoretical and empirical comparisons between the naive Bayes classifier and linear logistic regression, Ng and Jordan (NIPS 841-848, 2001) claimed that there exist two distinct regimes of performance between the generative and discriminative classifiers with regard to the training-set size. In this paper, our empirical and simulation studies, as a complement of their work, however, suggest that the existence of the two distinct regimes may not be so reliable. In addition, for real world datasets, so far there is no theoretically correct, general criterion for choosing between the discriminative and the generative approaches to classification of an observation x into a class y; the choice depends on the relative confidence we have in the correctness of the specification of either p(y vertical bar x) or p(x, y) for the data. This can be to some extent a demonstration of why Efron (J Am Stat Assoc 70(352):892-898, 1975) and O'Neill (J Am Stat Assoc 75(369):154-160, 1980) prefer normal-based linear discriminant analysis (LDA) when no model mis-specification occurs but other empirical studies may prefer linear logistic regression instead. Furthermore, we suggest that pairing of either LDA assuming a common diagonal covariance matrix (LDA-A) or the naive Bayes classifier and linear logistic regression may not be perfect, and hence it may not be reliable for any claim that was derived from the comparison between LDA-A or the naive Bayes classifier and linear logistic regression to be generalised to all generative and discriminative classifiers"	"Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #AUTHOR_TAG ) ."	"['Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #AUTHOR_TAG ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', 'Under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'Since HMMs are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'In fact, we demonstrate that HMMs are competitive with SVMs, with the added advantage of lower computational complexity.', 'In addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'In the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs.']"	0
CC1586	W06-3309	Generative content models for structural analysis of medical abstracts	catching the drift probabilistic content models with applications to generation and summarization	"['Regina Barzilay', 'Lillian Lee']"	method	"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods."	"Following Ruch et al. ( 2003 ) and #AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts ."	"['Following Ruch et al. ( 2003 ) and #AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .', 'The four states in our HMMs correspond to the information that characterizes each section (""introduction"", ""methods"", ""results"", and ""conclusions"") and state transitions capture the discourse flow from section to section.']"	5
CC1587	W06-3309	Generative content models for structural analysis of medical abstracts	what’s yours and what’s mine determining intellectual attribution in scientific text	"['Simone Teufel', 'Marc Moens']"	introduction	"We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes. One particularly important aspect of this structure is the question of who a given scientific statement is attributed to: other researchers, the field in general, or the authors themselves.We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text. In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text."	"The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1588	W06-3309	Generative content models for structural analysis of medical abstracts	the htk book	"['Steve Young', 'Gunnar Evermann', 'Thomas Hain', 'Dan Kershaw', 'Gareth Moore', 'Julian Odell', 'Dave Ollason', 'Dan Povey', 'Valtcho Valtchev', 'Phil Woodland']"	method		"Using the section labels , the HMM was trained using the HTK toolkit ( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation ."	"['We then built a four-state Hidden Markov Model that outputs these four-dimensional vectors.', 'The transition probability matrix of the HMM was initialized with uniform probabilities over a fully connected graph.', 'The output probabilities were modeled as four-dimensional Gaussians mixtures with diagonal covariance matrices.', 'Using the section labels , the HMM was trained using the HTK toolkit ( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .', 'For testing, we performed a Viterbi (maximum likelihood) estimation of the label of each test sentence/vector (also using the HTK toolkit).']"	5
CC1589	W06-3309	Generative content models for structural analysis of medical abstracts	genre analysis english inacademic and research settings	['John M Swales']	introduction	"In recent years the concept of 'register' has been increasingly replaced by emphasis on the analysis of genre, which relates work in sociolinguistics, text linguistics and discourse analysis to the study of specialist areas of language. This book is a clear, authoritative guide to this complex area. He provides a survey of approaches to varieties of language, and considers these in relation to communication and task-based language learning. Swales outlines an approach to the analysis of genre, and then proceeds to consider examples of different genres and how they can be made accessible through genre analysis. This is important reading for all those working in teaching English for academic purposes and also of interest to those working in post-secondary writing and composition due to relevant issues in writing across the curriculum."	"As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction""  , ""methods""  , ""results""  , and ""conclusions""  ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\x98asan , 2001 ) ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction""  , ""methods""  , ""results""  , and ""conclusions""  ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\x98asan , 2001 ) .', 'The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'Demner-Fushman et al. (2005) found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.']"	0
CC1590	W06-3309	Generative content models for structural analysis of medical abstracts	catching the drift probabilistic content models with applications to generation and summarization	"['Regina Barzilay', 'Lillian Lee']"	conclusion	"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods."	"An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #AUTHOR_TAG ) ."	"['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #AUTHOR_TAG ) .', 'This technique provides two important advantages.', 'First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.', 'This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training.', 'Second, using continuous distributions allows us to leverage a variety of tools (e.g., LDA) that have been shown to be successful in other fields, such as speech recognition (Evermann et al., 2004).']"	1
CC1591	W06-3309	Generative content models for structural analysis of medical abstracts	text categorization with support vector machines learning with many relevant features	['Thorsten Joachims']	introduction	"Abstract. This paper explores the use of Support Vector Machines (SVMs) for learning text classi ers from examples. It analyzes the particular properties of learning with text data and identi es why SVMs are appropriate for this task. Empirical results support the theoretical ndings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly overavariety of di erent learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning."	"Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #AUTHOR_TAG ; Ng and Jordan , 2001 ) ."	"['Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #AUTHOR_TAG ; Ng and Jordan , 2001 ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', 'Under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'Since HMMs are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'In fact, we demonstrate that HMMs are competitive with SVMs, with the added advantage of lower computational complexity.', 'In addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'In the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs.']"	0
CC1592	W06-3309	Generative content models for structural analysis of medical abstracts	effective mapping of biomedical text to the umls metathesaurus the metamap program	['Alan R Aronson']	introduction	"The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library"	"Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #AUTHOR_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #AUTHOR_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1593	W06-3309	Generative content models for structural analysis of medical abstracts	an unsupervised approach to recognizing discourse relations	"['Daniel Marcu', 'Abdessamad Echihabi']"	related work	"We present an unsupervised approach to recognizing discourse relations of CON-TRAST, EXPLANATION-EVIDENCE, CON-DITION and ELABORATION that hold be-tween arbitrary spans of texts. We show that discourse relation classifiers trained on examples that are automatically ex-tracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not ex-plicitly marked by cue phrases."	"Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ) ."	"['Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ) .', 'Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts.']"	1
CC1594	W06-3309	Generative content models for structural analysis of medical abstracts	answering physicians’ clinical questions obstacles and potential solutions	"['John W Ely', 'Jerome A Osheroff', 'M Lee Chambliss', 'Mark H Ebell', 'Marcy E Rosenbaum']"	introduction	"To identify the most frequent obstacles preventing physicians from answering their patient-care questions and the most requested improvements to clinical information resources.Qualitative analysis of questions asked by 48 randomly selected generalist physicians during ambulatory care.Frequency of reported obstacles to answering patient-care questions and recommendations from physicians for improving clinical information resources.The physicians asked 1,062 questions but pursued answers to only 585 (55%). The most commonly reported obstacle to the pursuit of an answer was the physician's doubt that an answer existed (52 questions, 11%). Among pursued questions, the most common obstacle was the failure of the selected resource to provide an answer (153 questions, 26%). During audiotaped interviews, physicians made 80 recommendations for improving clinical information resources. For example, they requested comprehensive resources that answer questions likely to occur in practice with emphasis on treatment and bottom-line advice. They asked for help in locating information quickly by using lists, tables, bolded subheadings, and algorithms and by avoiding lengthy, uninterrupted prose.Physicians do not seek answers to many of their questions, often suspecting a lack of usable information. When they do seek answers, they often cannot find the information they need. Clinical resource developers could use the recommendations made by practicing physicians to provide resources that are more useful for answering clinical questions."	"The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1595	W06-3309	Generative content models for structural analysis of medical abstracts	what’s yours and what’s mine determining intellectual attribution in scientific text	"['Simone Teufel', 'Marc Moens']"	related work	"We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes. One particularly important aspect of this structure is the question of who a given scientific statement is attributed to: other researchers, the field in general, or the authors themselves.We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text. In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text."	"Our task is closer to the work of #AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts ."	"['Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985;Marcu and Echihabi, 2002).', 'Our task is closer to the work of #AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts .']"	1
CC1596	W06-3309	Generative content models for structural analysis of medical abstracts	leveraging a common representation for personalized search and summarization in a medical digital library	"['Kathleen McKeown', 'Noemie Elhadad', 'Vasileios Hatzivassiloglou']"	conclusion	"Despite the large amount of online medical literature, it can be difficult for clinicians to find relevant information at the point of patient care. In this paper, we present techniques to personalize the results of search, making use of the online patient record as a sophisticated, pre-existing user model. Our work in PERSIVAL, a medical digital library, includes methods for re-ranking the results of search to prioritize those that better match the patient record. It also generates summaries of the re-ranked results which highlight information that is relevant to the patient under the physician's care. We focus on the use of a common representation for the articles returned by search and the patient record which facilitates both the re-ranking and the summarization tasks. This common approach to both tasks has a strong positive effect on the ability to personalize information"	"Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ) ."	"['Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled.', 'The true utility of content models is to structure abstracts that have no structure to begin with.', 'Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance.', 'Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ) .', 'We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.']"	3
CC1597	W06-3309	Generative content models for structural analysis of medical abstracts	categorization of sentence types in medical abstracts	"['Larry McKnight', 'Padmini Srinivasan']"	introduction	"This study evaluated the use of machine learning techniques in the classification of sentence type. 7253 structured abstracts and 204 unstructured abstracts of Randomized Controlled Trials from MedLINE were parsed into sentences and each sentence was labeled as one of four types (Introduction, Method, Result, or Conclusion). Support Vector Machine (SVM) and Linear Classifier models were generated and evaluated on cross-validated data. Treating sentences as a simple ""bag of words"", the SVM model had an average ROC area of 0.92. Adding a feature of relative sentence location improved performance markedly for some models and overall increasing the average ROC to 0.95. Linear classifier performance was significantly worse than the SVM in all datasets. Using the SVM model trained on structured abstracts to predict unstructured abstracts yielded performance similar to that of models trained with unstructured abstracts in 3 of the 4 types. We conclude that classification of sentence type seems feasible within the domain of RCT's. Identification of sentence types may be helpful for providing context to end users or other text summarization techniques."	Table (b) again reproduces the results from #AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts.	['Table (b) again reproduces the results from #AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts.']	1
CC1598	W06-3309	Generative content models for structural analysis of medical abstracts	zone analysis in biology articles as a basis for information extraction	"['Yoko Mizuta', 'Anna Korhonen', 'Tony Mullen', 'Nigel Collier']"	introduction	"In the field of biomedicine, an overwhelming amount of experimental data has become available as a result of the high throughput of research in this domain. The amount of results reported has now grown beyond the limits of what can be managed by manual means. This makes it increasingly difficult for the researchers in this area to keep up with the latest developments. Information extraction (IE) in the biological domain aims to provide an effective automatic means to dynamically manage the information contained in archived journal articles and abstract collections and thus help researchers in their work. However, while considerable advances have been made in certain areas of IE, pinpointing and organizing factual information (such as experimental results) remains a challenge. In this paper we propose tackling this task by incorporating into IE information about rhetorical zones, i.e. classification of spans of text in terms of argumentation and intellectual attribution. As the first step towards this goal, we introduce a scheme for annotating biological texts for rhetorical zones and provide a qualitative and quantitative analysis of the data annotated according to this scheme. We also discuss our preliminary research on automatic zone analysis, and its incorporation into our IE framework."	"The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #AUTHOR_TAG ) , and question answering ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #AUTHOR_TAG ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1599	W06-3309	Generative content models for structural analysis of medical abstracts	information needs in office practice are they being met	"['David G Covell', 'Gwen C Uman', 'Phil R Manning']"	introduction	"We studied the self-reported information needs of 47 physicians during a half day of typical office practice. The physicians raised 269 questions about patient management. Questions related to all medical specialties and were highly specific to the individual patient's problem. Subspecialists most frequently asked questions related to other subspecialties. Only 30% of physicians' information needs were met during the patient visit, usually by another physician or other health professional. Reasons print sources were not used included the age of textbooks in the office, poor organization of journal articles, inadequate indexing of books and drug information sources, lack of knowledge of an appropriate source, and the time required to find the desired information. Better methods are needed to provide answers to questions that arise in office practice."	"The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1600	W06-3309	Generative content models for structural analysis of medical abstracts	using argumentation to retrieve articles with similar citations an inquiry into improving related articles search in the medline digital library	"['Imad Tbahriti', 'Christine Chichester', 'Fr´ed´erique Lisacek', 'Patrick Ruch']"	introduction	"The aim of this study is to investigate the relationships between citations and the scientific argumentation found abstracts. We design a related article search task and observe how the argumentation can affect the search results. We extracted citation lists from a set of 3200 full-text papers originating from a narrow domain. In parallel, we recovered the corresponding MEDLINE records for analysis of the argumentative moves. Our argumentative model is founded on four classes: PURPOSE, METHODS, RESULTS and CONCLUSION. A Bayesian classifier trained on explicitly structured MEDLINE abstracts generates these argumentative categories. The categories are used to generate four different argumentative indexes. A fifth index contains the complete abstract, together with the title and the list of Medical Subject Headings (MeSH) terms. To appraise the relationship of the moves to the citations, the citation lists were used as the criteria for determining relatedness of articles, establishing a benchmark; it means that two articles are considered as ""related"" if they share a significant set of co-citations. Our results show that the average precision of queries with the PURPOSE and CONCLUSION features is the highest, while the precision of the RESULTS and METHODS features was relatively low. A linear weighting combination of the moves is proposed, which significantly improves retrieval of related articles."	"The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #AUTHOR_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #AUTHOR_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1601	W06-3309	Generative content models for structural analysis of medical abstracts	categorization of sentence types in medical abstracts	"['Larry McKnight', 'Padmini Srinivasan']"	experiments	"This study evaluated the use of machine learning techniques in the classification of sentence type. 7253 structured abstracts and 204 unstructured abstracts of Randomized Controlled Trials from MedLINE were parsed into sentences and each sentence was labeled as one of four types (Introduction, Method, Result, or Conclusion). Support Vector Machine (SVM) and Linear Classifier models were generated and evaluated on cross-validated data. Treating sentences as a simple ""bag of words"", the SVM model had an average ROC area of 0.92. Adding a feature of relative sentence location improved performance markedly for some models and overall increasing the average ROC to 0.95. Linear classifier performance was significantly worse than the SVM in all datasets. Using the SVM model trained on structured abstracts to predict unstructured abstracts yielded performance similar to that of models trained with unstructured abstracts in 3 of the 4 types. We conclude that classification of sentence type seems feasible within the domain of RCT's. Identification of sentence types may be helpful for providing context to end users or other text summarization techniques."	"The table also presents the closest comparable experimental results reported by #AUTHOR_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 ."	"['The results of our second set of experiments (with RCTs only) are shown in Tables 2(a) and 2(b). Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate.', 'Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table).', 'The table also presents the closest comparable experimental results reported by #AUTHOR_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 .', 'This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences.', 'Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper.', 'Nevertheless, our HMM- based approach is at least competitive with SVMs, perhaps better in some cases.']"	1
CC1602	W06-3309	Generative content models for structural analysis of medical abstracts	text generation using discourse strategies and focus constraints to generate natural language text	['Kathleen R McKeown']	related work	Preface Introduction 2. Discourse structure 3. Focusing in discourse 4. TEXT system implementation 5. Discourse history 6. Related generation research 7. Summary and conclusions Appendices Bibliography Index.	"Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) ."	"['Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) .', 'Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts.']"	1
CC1603	W06-3309	Generative content models for structural analysis of medical abstracts	catching the drift probabilistic content models with applications to generation and summarization	"['Regina Barzilay', 'Lillian Lee']"	introduction	"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods."	"Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #AUTHOR_TAG ) ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #AUTHOR_TAG ) .', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1604	W06-3309	Generative content models for structural analysis of medical abstracts	discoursal movements in medical english abstracts and their linguistic exponents a genre analysis study	['Franc¸oise Salanger-Meyer']	introduction		"This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG ) ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG ) .', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1605	W06-3309	Generative content models for structural analysis of medical abstracts	can primary care physicians’ questions be answered using the medical journal literature	"['Paul N Gorman', 'Joan S Ash', 'Leslie W Wykoff']"	introduction	"Medical librarians and informatics professionals believe the medical journal literature can be useful in clinical practice, but evidence suggests that practicing physicians do not share this belief. The authors designed a study to determine whether a random sample of ""native"" questions asked by primary care practitioners could be answered using the journal literature. Participants included forty-nine active, nonacademic primary care physicians providing ambulatory care in rural and nonrural Oregon, and seven medical librarians. The study was conducted in three stages: (1) office interviews with physicians to record clinical questions; (2) online searches to locate answers to selected questions; and (3) clinician feedback regarding the relevance and usefulness of the information retrieved. Of 295 questions recorded during forty-nine interviews, 60 questions were selected at random for searches. The average total time spent searching for and selecting articles for each question was forty-three minutes. The average cost per question searched was $27.37. Clinician feedback was received for 48 of 56 questions (four physicians could not be located, so their questions were not used in tabulating the results). For 28 questions (56%), clinicians judged the material relevant; for 22 questions (46%) the information provided a ""clear answer"" to their question. They expected the information would have had an impact on their patient in nineteen (40%) cases, and an impact on themselves or their practice in twenty-four (51%) cases. If the results can be generalized, and if the time and cost of performing searches can be reduced, increased use of the journal literature could significantly improve the extent to which primary care physicians' information needs are met."	"The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1606	W06-3309	Generative content models for structural analysis of medical abstracts	the interaction of domain knowledge and linguistic structure in natural language processing interpreting hypernymic propositions in biomedical text	"['Thomas C Rindflesch', 'Marcelo Fiszman']"	introduction	"Interpretation of semantic propositions in free-text documents such as MEDLINE citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. In this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. In order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the Unified Medical Language System (UMLS). After introducing the syntactic processing on which our system depends, we focus on the UMLS knowledge that supports interpretation of hypernymic propositions. We first use semantic groups from the Semantic Network to ensure that the two concepts involved are compatible; hierarchical information in the Metathesaurus then determines which concept is more general and which more specific. A preliminary evaluation of a sample based on the semantic group Chemicals and Drugs provides 83% precision. An error analysis was conducted and potential solutions to the problems encountered are presented. The research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. Additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. The approach has the potential to support a range of applications, including information retrieval and ontology engineering."	"Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks ."	"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"	0
CC1607	W06-3309	Generative content models for structural analysis of medical abstracts	catching the drift probabilistic content models with applications to generation and summarization	"['Regina Barzilay', 'Lillian Lee']"	related work	"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods."	"Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison ."	"['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.', 'Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'In contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.', 'Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.', 'Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here.']"	1
CC1608	W06-3309	Generative content models for structural analysis of medical abstracts	cuhtk conversational telephone speech transcription system	"['Gunnar Evermann', 'H Y Chan', 'Mark J F Gales', 'Thomas Hain', 'Xunying Liu', 'David Mrva', 'Lan Wang', 'Phil Woodland']"	conclusion		"Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) ."	"['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'This technique provides two important advantages.', 'First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.', 'This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training.', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']"	0
CC1609	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	junior science book of	['Nancy Larrick']	experiments	"This study aimed to describe the validity of SETS-oriented science book for junior high school students in environmental pollutions materials. In this research, researcher uses a quantitative descriptive method and uses validation sheet in the form of questionnaire as data-collection techniques. The questionnaire contains the criteria of material feasibility, language, and presentation. The result of validation gave the material feasibility score 3.9, for the language is 3.6, and the presentation criteria scored 3.9. So, it can be concluded that a science book with SETS-oriented is very well to be used as a learning facility according to the assessment of the validator"	"We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students ."	"['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text.']"	5
CC1610	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	semantic interpretation of nominalizations	"['Richard D Hull', 'Fernando Gomez']"	related work	"A computational approach to the semantic interpretation of nominalizations is described. Interpretation of nominalizations involves three tasks: deciding whether the nominalization is being used in a verbal or non-verbal sense; disambiguating the nominalized verb when a verbal sense is used; and determining the fillers of the thematic roles of the verbal concept or predicate of the nominalization. A verbal sense can be recognized by the presence of modifiers that represent the arguments of the verbal concept. It is these same modifiers which provide the semantic clues to disambiguate the nominalized verb. In the absence of explicit modifiers, heuristics are used to discriminate between verbal and nonverbal senses. A correspondence between verbs and their nominalizations is exploited so that only a small amount of additional knowledge is needed to handle the nominal form. These methods are tested in the domain of encyclopedic texts and the results are shown."	"Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) ."	"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']"	0
CC1611	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	pattern matching for case analysis a computational definition of closeness	"['Sylvain Delisle', 'Terry Copeck', 'Stan Szpakowicz', 'Ken Barker']"		Proposes a conceptually and technically neat method to identify known semantic patterns close to a novel pattern. This occurs in the context of a system to acquire knowledge incrementally from systematically processed expository technical text. This semi-automatic system requires the user to respond to specific multiple-choice questions about the current sentence. The questions are prepared from linguistic elements previously encountered in the text similar to elements in the new sentence. We present a metric to characterize the similarity between semantic case patterns. The computation is based on syntactic indicators of semantic relations and is defined in terms of symbolic pattern matching.>	"This idea was inspired by #AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) ."	"['To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P .', ""This idea was inspired by #AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) ."", 'In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (Carreras and Marquez, 2004;Carreras and Marquez, 2005).']"	4
CC1612	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	classbased construction of a verb lexicon in	"['Karin Kipper', 'Hoa Trang Dang', 'Martha Palmer']"	related work		"Most approaches rely on VerbNet ( #AUTHOR_TAG ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) ."	"['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( #AUTHOR_TAG ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) .']"	0
CC1613	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	systematic construction of a versatile case system	"['Ken Barker', 'Terry Copeck', 'Sylvain Delisle', 'Stan Szpakowicz']"	introduction	"Case systems abound in natural language processing. Almost any attempt to recognize and uniformly represent relationships within a clause - a unit at the centre of any linguistic system that goes beyond word level statistics - must be based on semantic roles drawn from a small, closed set. The set of roles describing relationships between a verb and its arguments within a clause is a case system. What is required of such a case system? How does a natural language practitioner build a system that is complete and detailed yet practical and natural? This paper chronicles the construction of a case system from its origin in English marker words to its successful application in the analysis of English text."	"The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #AUTHOR_TAGa ) ."	"['We work with sentences, clauses, phrases and words, using syntactic structures generated by a parser.', 'Our system incrementally processes a text, and extracts pairs of text units: two clauses, a verb and each of its arguments, a noun and each of its modifiers.', 'For each pair of units, the system builds a syntactic graph surrounding the main element (main clause, head verb, head noun).', 'It then tries to find among the previously processed instances another main element with a matching syntactic graph.', 'If such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #AUTHOR_TAGa ) .']"	4
CC1614	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	classifying the semantic relations in nouncompounds via a domain specific hierarchy	"['Barbara Rosario', 'Marti Hearst']"	related work	"We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves."	"In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ) ."	"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ) .', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']"	0
CC1615	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	automatic labeling of semantic roles	"['Daniel Gildea', 'Daniel Jurafsky']"	related work	"We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data."	"Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) ."	"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']"	0
CC1616	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	building concept reprezentations from reusable components	"['Peter Clark', 'Bruce Porter']"	related work	"Our goal is to build knowledge-based systems capable of answering a wide variety of questions, including questions that are unanticipated when the knowledge base is built. For systems to achieve this level of competence and generality, they require the ability to dynamically construct new concept representations, and to do so in response to the questions arLd tasks posed to them. Our approach to meeting this requirement is to build knowledge bases of generalized, representational components, and to develop methods for automatically composing components on demand. This work extends the normal inheritance approach used in frame-based systems, and imports ideas from several different areas of AI, in particular compositional modeling, terminological reasoning, and ontological engineering. The contribution of this work is a novel integration of these methods that improves the efficiency of building knowledge bases and the robustness of using them."	"It helps them build complex knowledge bases by combining components : events , entities and modifiers ( #AUTHOR_TAG ) ."	"['In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts.', 'It helps them build complex knowledge bases by combining components : events , entities and modifiers ( #AUTHOR_TAG ) .', ""The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary.""]"	0
CC1617	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	a representation of complex events and processes for the acquisition of knowledge from text	['Fernando Gomez']	related work		"In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ) ."	"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ) .', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']"	0
CC1618	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	the descriptive technique of panini	['Vidya Niwas Misra']	introduction		He was a grammarian who analysed Sanskrit ( #AUTHOR_TAG ) .	"['When analysing texts, it is essential to see how elements of meaning are interconnected.', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 .', 'He was a grammarian who analysed Sanskrit ( #AUTHOR_TAG ) .', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968).', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005).']"	0
CC1619	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	semantic role labelling using different syntactic views	"['Sameer Pradhan', 'Wayne Ward', 'Kadri Hacioglu', 'James H Martin', 'Daniel Jurafsky']"	related work	"Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements."	"Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 ) ."	"['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 ) .']"	0
CC1620	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	remarks on nominalizations	['Noam Chomsky']	introduction	This article deals with French nouns derived from non-stative verbs which nevertheless systematically exhibit a stative interpretation in some of their uses e.g. emprisonement 'action of putting sdy in jail' vs. 'state of being jailed	"This idea was expanded to include nouns and their modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 ) ."	"['In this work we pursue a well-known and often tacitly assumed line of thinking: connections at the syntactic level reflect connections at the semantic level (in other words, syntax carries meaning).', 'Anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations (Misra, 1966;Gruber, 1965;Fillmore, 1968).', 'Tesnière (1959), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants -for example, agent or instrument -to such grammatical elements as subject, direct object, indirect object.', 'This idea was expanded to include nouns and their modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 ) .']"	0
CC1621	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	putting pieces together combining framenet verbnet and wordnet for robust semantic parsing	"['Lei Shi', 'Rada Mihalcea']"	related work	"This paper describes our work in integrating three different lexical resources: FrameNet, VerbNet, and WordNet, into a unified, richer knowledge-base, to the end of enabling more robust semantic parsing. The construction of each of these lexical resources has required many years of laborious human effort, and they all have their strengths and shortcomings. By linking them together, we build an improved resource in which (1) the coverage of FrameNet is extended, (2) the VerbNet lexicon is augmented with frame semantics, and (3) selectional restrictions are implemented using WordNet semantic classes. The synergistic exploitation of various lexical resources is crucial for many complex language processing applications, and we prove it once again effective in building a robust semantic parser."	"Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG ) ."	"['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG ) .']"	0
CC1622	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	realistic parsing practical solutions of difficult problems	"['Sylvain Delisle', 'Stan Szpakowicz']"	experiments	"This paper describes work on the linguistic analysis of texts within a project devoted to knowledge acquisition from text. We focus on syntactic processing and present some key elements of the project's parser that allow it to deal successfully with technical texts. The parser is fully implemented and tested on a variety of real texts; improvements and enhancements are in progress. Because our knowledge acquisition method assumes no a priori model of the domain of the source text, the parser relies as much as possible on lexical and syntactic clues. That is why it strives for full syntactic analysis rather than some form of text skimming. We present a practical approach to four acknowledged difficult problems which to date have no generally acceptable answers: phrase attachment; time constraints for problematic input (how to avoid long and unproductive computation); parsing conjoined structures (how to preserve broad coverage without losing control of the parsing process); and the treatment of fragmentary input or fragments that are a by-product of a fallback parsing strategy. We review recent related work and conclude by listing several future work items."	"The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #AUTHOR_TAG ) ."	"['The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #AUTHOR_TAG ) .', 'The parser, written in Prolog, implements a classic constituency English grammar from Quirk et al. (1985).', 'Pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'A dependency parser would produce a similar output, but DIPETT also provides verb subcategorization information (such as, for example, subject-verb-object or subject-verb-objectindirect object), which we use to select the (best) matching syntactic structures.']"	5
CC1623	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	the descent of hierarchy and selection in relational semantics	"['Barbara Rosario', 'Marti Hearst', 'Charles Fillmore']"	related work	"In many types of technical texts, meaning is embedded in noun compounds. A language understanding program needs to be able to interpret these in order to ascertain sentence meaning. We explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories, and then using this category membership to determine the relation that holds between the nouns. In this paper we present the results of an analysis of this method on two-word noun compounds from the biomedical domain, obtaining classification accuracy of approximately 90%. Since lexical hierarchies are not necessarily ideally suited for this task, we also pose the question: how far down the hierarchy must the algorithm descend before all the terms within the subhierarchy behave uniformly with respect to the semantic relation in question? We find that the topmost levels of the hierarchy yield an accurate classification, thus providing an economic way of assigning relations to noun compounds."	"Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) ."	"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']"	0
CC1624	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	studies in lexical relations	['Jeffrey Gruber']	introduction		"The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) ."	"['When analysing texts, it is essential to see how elements of meaning are interconnected.', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century B.C. and the work of Panini 1 .', 'He was a grammarian who analysed Sanskrit (Misra, 1966).', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) .', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005).']"	0
CC1625	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	the case for case	['Charles Fillmore']	introduction	"This article puts forward an economic case for Scotland staying in the union. There have been many debates regarding the economic consequences of independence. It has been argued that Scotland would be better off. Independence, however, is an uncertain business; a new state might gain new freedoms but would lose present sources of stability, and some questions about independence are simply unanswerable in advance. It is nevertheless possible to draw some conclusions about its possible economic effects"	"The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) ."	"['When analysing texts, it is essential to see how elements of meaning are interconnected.', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century B.C. and the work of Panini 1 .', 'He was a grammarian who analysed Sanskrit (Misra, 1966).', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) .', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005).']"	0
CC1626	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	systematic construction of a versatile case system	"['Ken Barker', 'Terry Copeck', 'Sylvain Delisle', 'Stan Szpakowicz']"		"Case systems abound in natural language processing. Almost any attempt to recognize and uniformly represent relationships within a clause - a unit at the centre of any linguistic system that goes beyond word level statistics - must be based on semantic roles drawn from a small, closed set. The set of roles describing relationships between a verb and its arguments within a clause is a case system. What is required of such a case system? How does a natural language practitioner build a system that is complete and detailed yet practical and natural? This paper chronicles the construction of a case system from its origin in English marker words to its successful application in the analysis of English text."	This design idea was adopted from TANKA ( #AUTHOR_TAGb ) .	"['Our system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'This design idea was adopted from TANKA ( #AUTHOR_TAGb ) .', 'The only manually encoded knowledge is a dictionary of markers (subordinators, coordinators, prepositions).', 'This resource does not affect the syntacticsemantic graph-matching heuristic.']"	5
CC1627	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	framenet and lexicographic relevance	"['Charles Fillmore', 'Beryl T Atkins']"	related work		"Such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) ."	"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']"	0
CC1628	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	the berkeley framenet project	"['Collin F Baker', 'Charles J Fillmore', 'John B Lowe']"	related work	"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work"	Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ) .	"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']"	0
CC1629	W06-3813	Matching syntactic-semantic graphs for semantic relation assignment	systematic construction of a versatile case system	"['Ken Barker', 'Terry Copeck', 'Sylvain Delisle', 'Stan Szpakowicz']"	experiments	"Case systems abound in natural language processing. Almost any attempt to recognize and uniformly represent relationships within a clause - a unit at the centre of any linguistic system that goes beyond word level statistics - must be based on semantic roles drawn from a small, closed set. The set of roles describing relationships between a verb and its arguments within a clause is a case system. What is required of such a case system? How does a natural language practitioner build a system that is complete and detailed yet practical and natural? This paper chronicles the construction of a case system from its origin in English marker words to its successful application in the analysis of English text."	The list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAGa ) .	"['The list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAGa ) .', 'Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena.', 'The resulting list is the one used in the experiments we present in this paper.', 'The relations are grouped by general similarity into 6 relation classes (H denotes the head of a base NP, M denotes the modifier).', 'There is no consensus in the literature on a list of semantic relations that would work in all situations.', 'This is, no doubt, because a general list of relations such as the one we use would not be appropriate for the semantic analysis of texts in a specific domain, such as for example medical texts.', 'All the relations in the list we use were necessary, and sufficient, for the analysis of the input text.']"	5
CC1630	W10-1710	Anaphora Models and Reordering for Phrase-Based SMT	chunkbased verb reordering in vso sentences for arabicenglish statistical machine translation	"['Arianna Bisazza', 'Marcello Federico']"	conclusion	"In Arabic-to-English phrase-based statistical machine translation, a large number of syntactic disfluencies are due to wrong long-range reordering of the verb in VSO sentences, where the verb is anticipated with respect to the English word order. In this paper, we propose a chunk-based reordering technique to automatically detect and displace clause-initial verbs in the Arabic side of a word-aligned parallel corpus. This method is applied to preprocess the training data, and to collect statistics about verb movements. From this analysis, specific verb reordering lattices are then built on the test sentences before decoding them. The application of our reordering methods on the training and test sets results in consistent BLEU score improvements on the NIST-MT 2009 Arabic-English benchmark."	"Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair ."	"['Word reordering between German and English is a complex problem.', 'Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair .', 'It turned out that there is a larger variety of long reordering patterns in this case.', 'Nevertheless, some experiments performed after the official evaluation showed promising results.', 'We plan to pursue this work in several directions: Defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice.', 'Applying similar reordering rules to the training corpus would reduce the mismatch between the training data and the reordered input sentences.', 'Finally, it would be useful to explore the impact of different distortion limits on the decoding of reordering lattices in order to find an optimal trade-off between decoderdriven short-range and lattice-driven long-range reordering.']"	4
CC1631	W10-3814	New Parameterizations and Features for PSCFG-Based Machine Translation	a discriminative latent variable model for statistical machine translation	"['Phil Blunsom', 'Trevor Cohn', 'Miles Osborne']"	conclusion	"Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. (c) 2008 Association for Computational Linguistics"	"Future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."	"['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']"	3
CC1632	W10-3814	New Parameterizations and Features for PSCFG-Based Machine Translation	probabilistic inference for machine translation	"['Phil Blunsom', 'Miles Osborne']"	conclusion	"Recent advances in statistical machine translation (SMT) have used dynamic programming  (DP) based beam search methods for approximate inference within probabilistic  translation models. Despite their success, these methods compromise the probabilistic  interpretation of the underlying model thus limiting the application of probabilistically  defined decision rules during training and decoding.  As an alternative, in this thesis, we propose a novel Monte Carlo sampling approach  for theoretically sound approximate probabilistic inference within these models. The  distribution we are interested in is the conditional distribution of a log-linear translation  model; however, often, there is no tractable way of computing the normalisation term  of the model. Instead, a Gibbs sampling approach for phrase-based machine translation  models is developed which obviates the need of computing this term yet produces  samples from the required distribution.  We establish that the sampler effectively explores the distribution defined by a  phrase-based models by showing that it converges in a reasonable amount of time to  the desired distribution, irrespective of initialisation. Empirical evidence is provided to  confirm that the sampler can provide accurate estimates of expectations of functions of  interest. The mix of high probability and low probability derivations obtained through  sampling is shown to provide a more accurate estimate of expectations than merely  using the n-most highly probable derivations.  Subsequently, we show that the sampler provides a tractable solution for finding the  maximum probability translation in the model. We also present a unified approach to  approximating two additional intractable problems: minimum risk training and minimum  Bayes risk decoding. Key to our approach is the use of the sampler which  allows us to explore the entire probability distribution and maintain a strict probabilistic  formulation through the translation pipeline. For these tasks, sampling allies  the simplicity of n-best list approaches with the extended view of the distribution that  lattice-based approaches benefit from, while avoiding the biases associated with beam  search. Our approach is theoretically well-motivated and can give better and more  stable results than current state of the art methods"	"Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."	"['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']"	3
CC1633	W11-1402	Understanding Differences in Perceived Peer-Review Helpfulness using Natural Language Processing	automatically predicting peerreview helpfulness	"['Wenting Xiong', 'Diane Litman']"	introduction	"Identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers. As a first step towards enhancing existing peer-review systems with new functionality based on helpfulness detection, we examine whether standard product review analysis techniques also apply to our new context of peer reviews. In addition, we investigate the utility of incorporating additional specialized features tailored to peer review. Our preliminary results show that the structural features, review uni-grams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews, while peer-review specific auxiliary features can further improve helpfulness prediction."	"In our prior work ( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review ."	"['In our prior work ( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .', 'While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g.', 'author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model.', 'In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types of experts (example 2).']"	2
CC1634	W11-1402	Understanding Differences in Perceived Peer-Review Helpfulness using Natural Language Processing	automatically predicting peerreview helpfulness	"['Wenting Xiong', 'Diane Litman']"	experiments	"Identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers. As a first step towards enhancing existing peer-review systems with new functionality based on helpfulness detection, we examine whether standard product review analysis techniques also apply to our new context of peer reviews. In addition, we investigate the utility of incorporating additional specialized features tailored to peer review. Our preliminary results show that the structural features, review uni-grams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews, while peer-review specific auxiliary features can further improve helpfulness prediction."	( Details of how the average-expert model performs can be found in our prior work ( #AUTHOR_TAG ) . )	"['10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/).', 'provide complementary perspectives.', 'While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( Details of how the average-expert model performs can be found in our prior work ( #AUTHOR_TAG ) . )']"	2
CC1635	W11-1410	"Review of Leacock, Chodorow, Gamon & Tetreault (2014): Automated Grammatical Error Detection for Language Learners"	building a korean web corpus for analyzing learner language	"['Markus Dickinson', 'Ross Israel', 'Sun-Hee Lee']"		"Post-positional particles are a significant source of errors for learners of Korean. Following methodology that has proven effective in handling English preposition errors, we are beginning the process of building a machine learner for particle error detection in L2 Korean writing. As a first step, however, we must acquire data, and thus we present a methodology for constructing large-scale corpora of Korean from the Web, exploring the feasibility of building corpora appropriate for a given topic and grammatical construction."	"We follow our previous work ( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) ."	"['In selecting features for Korean, we have to ac- count for relatively free word order (Chung et al., 2010).', 'We follow our previous work ( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) .', 'Each word is broken down into: stem, affixes, stem POS, and affixes POS.', 'We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties.', 'Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4).']"	2
CC1636	W11-2020	Novel Approaches to Pattern-based Interaction Quality Modeling	on nomatchs noinputs and bargeins do nonacoustic features support anger detection	"['Alexander Schmitt', 'Tobias Heinroth', 'Jackson Liscombe']"		"Most studies on speech-based emotion recognition are based on prosodic and acoustic features, only employing artificial acted corpora where the results cannot be generalized to telephone-based speech applications. In contrast, we present an approach based on utterances from 1,911 calls from a deployed telephone-based speech application, taking advantage of additional dialogue features, NLU features and ASR features that are incorporated into the emotion recognition process. Depending on the task, non-acoustic features add 2.3% in classification accuracy compared to using only acoustic features."	"The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG ) ."	"['The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG ) .', '(Schmitt et al., 2009).', 'From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events.', 'In total, the number of interaction parameters servings as input variables for the model amounts to 52.']"	2
CC1637	W14-1609	Lexicon Infused Phrase Embeddings for Named Entity Resolution	design challenges and misconceptions in named entity recognition	"['Lev Ratinov', 'Dan Roth']"	introduction	"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset."	"This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) ."	"['One simple language model that discovers useful embeddings is known as Brown clustering (Brown et al., 1992).', 'A Brown clustering is a class-based bigram model in which (1) the probability of a document is the product of the probabilities of its bigrams, (2) the probability of each bigram is the product of the probability of a bigram model over latent classes and the probability of each class generating the actual word types in the bigram, and (3) each word type has non-zero probability only on a single class.', 'Given a one-toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged.', 'This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) .', 'There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993).']"	4
CC1638	W14-1609	Lexicon Infused Phrase Embeddings for Named Entity Resolution	design challenges and misconceptions in named entity recognition	"['Lev Ratinov', 'Dan Roth']"	introduction	"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset."	"Following #AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document ."	"['Following #AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .', 'Since, as seen in section 2.1, Brown clusters are hierarchical, we use features corresponding to prefixes of the path from the root to the leaf for each word type.']"	5
CC1639	W14-1609	Lexicon Infused Phrase Embeddings for Named Entity Resolution	design challenges and misconceptions in named entity recognition	"['Lev Ratinov', 'Dan Roth']"	introduction	"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset."	It is inspired by the system described in #AUTHOR_TAG .	"['In this section we describe in detail the baseline NER system we use.', 'It is inspired by the system described in #AUTHOR_TAG .', 'Because NER annotations are commonly not nested (for example, in the text ""the US Army"", ""US Army"" is treated as a single entity, instead of the location ""US"" and the organization ""US Army"") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the BILOU encoding, where each token can either BEGIN an entity, be INSIDE an entity, be the LAST token in an entity, be OUTSIDE an entity, or be the single UNIQUE token in an entity.']"	4
CC1640	W14-1619	Probabilistic Modeling of Joint-context in Distributional Similarity	learning syntactic categories using paradigmatic representations of word context	"['Mehmet Ali Yatbaz', 'Enis Sert', 'Deniz Yuret']"	introduction	We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition. Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words. We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy. Our best model based on Euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80% many-to-one accuracy on a 45-tag 1M word corpus.	"This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words ."	"['We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired.', 'It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (Levin, 1993;Hanks, 2013).', 'This provides grounds to expect that such model has the potential to excel for verbs.', 'To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme.', 'This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .', 'However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme.']"	4
CC1641	W14-1815	Natural Language Generation with Vocabulary Constraints	learning to freestyle hip hop challengeresponse induction via transduction rule segmentation	"['Dekai Wu', 'Karteek Addanki', 'Markus Saers', 'Meriem Beloucif']"	related work	"We present a novel model, Freestyle, that learns to improvise rhyming and fluent responses upon being challenged with a line of hip hop lyrics, by combining both bottomup token based rule induction and top-down rule segmentation strategies to learn a stochastic transduction grammar that simultaneously learns both phrasing and rhyming associations. In this attack on the woefully under-explored natural language genre of music lyrics, we exploit a strictly unsupervised transduction grammar induction approach. Our task is particularly ambitious in that no use of any a priori linguistic or phonetic information is allowed, even though the domain of hip hop lyrics is particularly noisy and unstructured. We evaluate the performance of the learned model against a model learned only using the more conventional bottom-up token based rule induction, and demonstrate the superiority of our combined token based and rule segmentation induction method toward generating higher quality improvised responses, measured on fluency and rhyming criteria as judged by human evaluators. To highlight some of the inherent challenges in adapting other algorithms to this novel task, we also compare the quality of the responses generated by our model to those generated by an out-ofthe-box phrase based SMT system. We tackle the challenge of selecting appropriate training data for our task via a dedicated rhyme scheme detection module, which is also acquired via unsupervised learning and report improved quality of the generated responses. Finally, we report results with Maghrebi French hip hop lyrics indicating that our model performs surprisingly well with no special adaptation to other languages."	"Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced ."	"['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.']"	1
CC1642	W14-1815	Natural Language Generation with Vocabulary Constraints	generating diagnostic multiple choice comprehension cloze questions	"['Jack Mostow', 'Hyeju Jang']"	related work	"This paper describes and evaluates DQGen, which automatically generates multiple choice cloze questions to test a child's comprehension while reading a given text. Unlike previous methods, it generates different types of distracters designed to diagnose different types of comprehension failure, and tests comprehension not only of an individual sentence but of the context that precedes it. We evaluate the quality of the overall questions and the individual distracters, according to 8 human judges blind to the correct answers and intended distracter types. The results, errors, and judges' comments reveal limitations and suggest how to address some of them."	"Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions ."	"['Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .', 'Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences.']"	4
CC1643	W14-1815	Natural Language Generation with Vocabulary Constraints	automatic analysis of rhythmic poetry with applications to generation and translation	"['Erica Greene', 'Tugba Bodrumlu', 'Kevin Knight']"	related work	"We employ statistical methods to analyze, generate, and translate rhythmic poetry. We first apply unsupervised learning to reveal word-stress patterns in a corpus of raw poetry. We then use these word-stress patterns, in addition to rhyme and discourse models, to generate English love poetry. Finally, we translate Italian poetry into English, choosing target realizations that conform to desired rhythmic patterns."	"Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced ."	"['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.']"	1
CC1644	W14-1815	Natural Language Generation with Vocabulary Constraints	automatic generation of tamil lyrics for melodies	"['Ananth Ramakrishnan A', 'Sankar Kuppan', 'Sobha Lalitha Devi']"	related work	"This paper presents our on-going work to automatically generate lyrics for a given melody, for phonetic languages such as Tamil. We approach the task of identifying the required syllable pattern for the lyric as a sequence labeling problem and hence use the popular CRF++ toolkit for learning. A corpus comprising of 10 melodies was used to train the system to understand the syllable patterns. The trained model is then used to guess the syllabic pattern for a new melody to produce an optimal sequence of syllables. This sequence is presented to the Sentence Generation module which uses the Dijkstra's shortest path algorithm to come up with a meaningful phrase matching the syllabic pattern."	"Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced ."	"['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.']"	1
CC1645	W14-3902	Code Mixing: A Challenge for Language Identification in the Language of Social Media	dcusymantec at the wmt 2013 quality estimation shared task	"['Raphael Rubino', 'Joachim Wagner', 'Jennifer Foster', 'Johann Roturier', 'Rasoul Samad Zadeh Kaljahi', 'Fred Hollowood']"	experiments		"3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) ."	"['1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', '2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.']"	2
CC1646	W14-3902	Code Mixing: A Challenge for Language Identification in the Language of Social Media	dcu aspectbased polarity classification for semeval task 4	"['Joachim Wagner', 'Piyush Arora', 'Santiago Cortes', 'Utsab Barman', 'Dasha Bogdanova', 'Jennifer Foster', 'Lamia Tounsi']"	experiments	"We describe the work carried out by DCU on the Aspect Based Sentiment Analysis task at SemEval 2014. Our team submitted one constrained run for the restaurant domain and one for the laptop domain for sub-task B (aspect term polarity prediction), ranking highest out of 36 systems on the restaurant test set and joint highest out of 32 systems on the laptop test set."	"raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) ."	"['1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', '2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.']"	2
CC1647	W98-1124	The Theory and Practice of Discourse Parsing and Summarization	from discourse structures to text summaries	['Daniel Marcu']		We describe experiments that show that the concepts of rhetorical analysts and nucleanty can be used effectively for deternumng the most nnportant umts m a text We show how these concepts can be xmplemented and we discuss results that we obtained with a chscourse-based summanzatmn program	"The only disambiguation metric that we used in our previous work ( #AUTHOR_TAGb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right ."	"['The shape-based metric.', ""The only disambiguation metric that we used in our previous work ( #AUTHOR_TAGb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right ."", 'The explanation for this metric is that text processing is, essentially, a left-to-right process.', 'In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.', 'I The more text writers add, the more they elaborate on the text that went before: as a consequence, incremental discourse building consists mostly of expansion of the tight branches.', 'According to the shape-based metric, we consider that a discourse tree A is ""better"" than another discourse tree B if A is more skewed to the right than B (see Marcu (1997c) for a mathematical formulation of the notion of skewedness).']"	2